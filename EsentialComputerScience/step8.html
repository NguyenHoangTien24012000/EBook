<style>
    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 300;
        src: url(https://static.contineljs.com/fonts/Roboto-Light.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Light.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 400;
        src: url(https://static.contineljs.com/fonts/Roboto-Regular.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Regular.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 500;
        src: url(https://static.contineljs.com/fonts/Roboto-Medium.woff2?v=2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Medium.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 700;
        src: url(https://static.contineljs.com/fonts/Roboto-Bold.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Bold.woff) format("woff");
    }

    * {
        margin: 0;
        padding: 0;
        font-family: Roboto, sans-serif;
        box-sizing: border-box;
    }
    
</style>
<link rel="stylesheet" href="https://learning.oreilly.com/files/public/epub-reader/override_v1.css" crossorigin="anonymous"><link rel="stylesheet" href="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492092506/files/epub.css" crossorigin="anonymous">
<div style="width: 100%; display: flex; justify-content: center; background-color: black; color: wheat;">
    <section data-testid="contentViewer" class="contentViewer--KzjY1"><div class="annotatable--okKet"><div id="book-content"><div class="readerContainer--bZ89H white--bfCci" style="font-size: 1em; max-width: 70ch;"><div id="sbo-rt-content"><div epub:type="chapter" role="doc-chapter"><div class="ChapterContextInformation"><div class="ContextInformation" id="b978-1-4842-7107-0_8"><div class="ChapterCopyright">©&nbsp;The Author(s), under exclusive license to APress Media, LLC, part of Springer Nature&nbsp;2021</div><span class="ContextInformationAuthorEditorNames">P. D. Crutcher et al.</span><span class="ContextInformationBookTitles"><span class="BookTitle">Essential Computer Science</span></span><span class="ChapterDOI"><a href="https://doi.org/10.1007/978-1-4842-7107-0_8">https://doi.org/10.1007/978-1-4842-7107-0_8</a></span></div></div><!--Begin Abstract--><div class="MainTitleSection"><h1 class="ChapterTitle" lang="en">8.&nbsp;Machine Learning</h1></div><div class="AuthorGroup"><div class="AuthorNames"><span class="Author"><span class="AuthorName">Paul&nbsp;D.&nbsp;Crutcher</span><sup><a href="#Aff4">1</a>&nbsp;<span class="ContactIcon">&nbsp;</span></sup>, </span><span class="Author"><span class="AuthorName">Neeraj&nbsp;Kumar&nbsp;Singh</span><sup><a href="#Aff5">2</a></sup> and </span><span class="Author"><span class="AuthorName">Peter&nbsp;Tiegs</span><sup><a href="#Aff6">3</a></sup></span></div><div class="Affiliations"><div class="Affiliation" id="Aff4"><span class="AffiliationNumber">(1)</span><div class="AffiliationText">Welches, OR, USA</div></div><div class="Affiliation" id="Aff5"><span class="AffiliationNumber">(2)</span><div class="AffiliationText">Bangalore, Karnataka, India</div></div><div class="Affiliation" id="Aff6"><span class="AffiliationNumber">(3)</span><div class="AffiliationText">Hillsboro, OR, USA</div></div><div class="ClearBoth">&nbsp;</div></div></div><!--End Abstract--><div class="Fulltext"><p class="Para" id="Par2">In earlier chapters, we discussed aspects of computer architecture and how to efficiently program and deploy software. Thus far, we’ve been successful getting computers to carry out what they have been programmed to accomplish. Beyond traditional programming, questions arise about whether or not computers can mimic humans in terms of intelligence and learning. In science fiction literature, there are many stories of machines taking over the world. Is this possible? Until relatively recently, these fictions have been given little credence because there are fundamental differences between how human intelligence and computing machines work. Machines act as obedient servants – working as they are explicitly programmed to accomplish a well-defined task. They did not learn and improve or develop intelligence. And that’s where machine learning comes to play. Some of the most succinct descriptions of machine learning are from Stanford and McKinsey &amp; Co. As per Stanford, “Machine <span id="ITerm1">learning</span> is the science of getting computers to act without being explicitly programmed.”<sup><a epub:type="noteref" href="#Fn1" id="Fn1_source" role="doc-noteref">1</a></sup> And, as per McKinsey &amp; Co, “Machine learning is based on algorithms that can learn from data without relying on rules-based programming.”<sup><a epub:type="noteref" href="#Fn2" id="Fn2_source" role="doc-noteref">2</a></sup></p><div class="FormalPara FormalParaRenderingStyle1 ParaTypeImportant" id="FPar1"><div class="Heading">Note</div><p class="Para FirstParaInFormalPara" id="Par5">Fundamentally, machine <span id="ITerm2">learning</span> is the science of getting computers to learn as well as, or better than, humans.</p><p class="Para" id="Par6">The key difference between machine learning and <span id="ITerm3">conventional machine intelligence</span><span id="ITerm4">


</span> is the way machines acquire intelligence. With machine learning, machines gather intelligence based on examples (data, aka experience). In the conventional machine intelligence case, machines are explicitly programmed (instructed) to behave in a certain intelligent way. So machines may still behave like intelligent agents without applying machine learning, but they do not get better with experience.</p></div><p class="Para" id="Par7">By the way, machine learning is not a completely new thing; it has evolved and started to see more usage, proliferation, and success owing to advancement in compute resource and availability of data. In the following section, we talk about evolution of machine learning.</p><section class="Section1 RenderAsSection1" id="Sec1"><h2 class="Heading">Brief History of Machine Learning</h2><p class="Para" id="Par8">From the very beginning of computing devices, when we thought about learning and machines, we tried to draw parallels from the understanding of how human brains work and how computing machines/algorithms work. <span id="ITerm5">Neurons</span> and their associated networks (<span id="ITerm6">neural networks</span>) play the foundational role in human learning process, so researchers have tried to emulate these processes in machines. This field of study is broadly known as machine learning and artificial <span id="ITerm7">intelligence</span>.</p><p class="Para" id="Par9">The first theory on <span id="ITerm8">neural networks</span> was a paper published in 1943 where neurophysiologist Warren McCulloch and mathematician Walter Pitts talked about neurons and how they work. They decided to model these neurons using an electrical circuit, creating the underlying framework for future machine learning progress.</p><p class="Para" id="Par10">In 1950, Alan Turing created the “<span id="ITerm9">Turing Test</span>,” which is a method for determining whether a computer is capable of thinking like a human being. Turing proposed that a computer can be said to possess artificial intelligence if it can mimic human responses under specific conditions. This test is simple: for a computer to qualify as having artificial intelligence, it must be able to convince a human that it is a human and not a computer. The test was originally named “The Imitation Game.”</p><p class="Para" id="Par11">Arthur Samuel in 1952 created the first computer program that could learn as it ran. It was a game that played checkers. Later in 1958, Frank Rosenblatt designed the first artificial neural network to recognize patterns and shapes. Then in 1959, Bernard Widrow and Marcian Hoff created two neural network models at Stanford University. The first was called <span id="ITerm10">ADALINE</span>, and it could detect binary patterns. The other one (which was the next generation) was called <span id="ITerm11">MADALINE</span>. MADALINE was used to eliminate echo on phone lines – so the first useful real-world application of machine learning, MADALINE, came into use and continues to be used today.</p><p class="Para" id="Par12">Despite the success of MADALINE, there was not much progress until the late 1970s for many reasons. Recently, both the amount of data available and exponential growth in processing capabilities, neural networks, and other ML technologies have become viable.</p></section><section class="Section1 RenderAsSection1" id="Sec2"><h2 class="Heading">Artificial Intelligence, Machine Learning, and Deep Learning</h2><p class="Para" id="Par13">We use the terms artificial <span id="ITerm12">intelligence</span><span id="ITerm13">


</span>, machine learning, and deep learning a lot. Is there a difference between them? At times, we seem to use these terms interchangeably, but it is important to understand that they are related and not interchangeable. We define each one in the following.</p><p class="Para" id="Par14"><span id="ITerm14">Artificial intelligence (AI)</span> refers to intelligence demonstrated by machines. In other words, artificial intelligence refers to the simulation of intelligent behavior in computers or the capability of a machine to imitate intelligent human behavior. It is used broadly to refer to any algorithms, methods, or technologies that make a system act and behave like a human. It employs machine learning, computer vision, natural language processing, cognitive robotics, and other related technologies.</p><p class="Para" id="Par15">Machine learning is a subfield of artificial intelligence that uses algorithms that improve with experience or learn the rules without explicitly being programmed.</p><div class="Para" id="Par16"><span id="ITerm15">Deep learning</span> is a technique of machine learning that uses multilevel (deep) neural networks for learning. Figure <span class="InternalRef"><a href="#Fig1">8-1</a></span> represents the relationship between the three. It illustrates that deep learning is a subfield of machine learning that is a subfield of artificial intelligence.<figure class="Figure" id="Fig1"><div class="MediaObject" id="MO1"><img alt="../images/503707_1_En_8_Chapter/503707_1_En_8_Fig1_HTML.jpg" src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484271070/files/images/503707_1_En_8_Chapter/503707_1_En_8_Fig1_HTML.jpg" style="width:27.5em" width="1100" height="783"></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Figure 8-1</span><p class="SimplePara">Relationship Between Artificial Intelligence, Machine Learning, and Deep <span id="ITerm16">Learning</span><span id="ITerm17">


</span></p></div></figcaption></figure></div></section><section class="Section1 RenderAsSection1" id="Sec3"><h2 class="Heading">Fundamental Tenets of Machine Learning</h2><div class="Para" id="Par17">Having discussed machine learning and its evolution earlier, we now discuss the key tenets of machine learning. In machine learning, machines learn with data to detect patterns and rules to<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par18">Categorize like objects.</p></li><li><p class="Para" id="Par19">Predict likely outcomes based on identified (learned) rules.</p></li><li><p class="Para" id="Par20">Identify patterns and relationships.</p></li><li><p class="Para" id="Par21">Detect anomalous behaviors.</p></li></ul></div></div><div class="Para" id="Par22">Essentially there are three parts of a machine learning <span id="ITerm18">system</span>: model, training, and inference. Figure <span class="InternalRef"><a href="#Fig2">8-2</a></span> illustrates the high-level flow. At first, a machine learning <span id="ITerm19">model</span> is created, and then it is trained with the <span id="ITerm20">training</span> data. After training, the model would have “learned,” based on the data, and is ready to be used for making useful prediction for new data, which is known as <span id="ITerm21">inference</span>. It is worth mentioning that a large volume of data is required for the model to pick good rules and become reasonably accurate. In practice, the training of the model is a continuous process, bringing in new training data as we see more kinds of data from the real world, making the model predictions more accurate over time. Because of the iterations and amount of data that need to be processed, the training process is computationally intensive. The degree of computational requirement depends on the model (algorithm) being used and the size of the training database. The good news here is that once a model is trained, making an inference based on new data is fairly low cost.<figure class="Figure" id="Fig2"><div class="MediaObject" id="MO2"><img alt="../images/503707_1_En_8_Chapter/503707_1_En_8_Fig2_HTML.jpg" src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781484271070/files/images/503707_1_En_8_Chapter/503707_1_En_8_Fig2_HTML.jpg" style="width:35.62em" width="1425" height="547"></div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Figure 8-2</span><p class="SimplePara">Representation of a Machine Learning <span id="ITerm22">System</span></p></div></figcaption></figure></div><section class="Section2 RenderAsSection2" id="Sec4"><h3 class="Heading">Models</h3><p class="Para" id="Par23">A machine learning (ML) <span id="ITerm23">model</span> is fundamentally a recipe (i.e., statistical representation of the system) learned using examples (i.e., training data) with an ability to predict behavior given new data. In other words, a machine learning model is fundamentally the representation of a learning system that can be used to predict (i.e., infer) results for new data.</p><p class="Para" id="Par24">The processes <span id="ITerm24">machines</span> use to learn are known as algorithms. Different algorithms learn in different ways. With the right model, as new data is provided to the “machine,” the algorithm’s performance improves, thereby resulting in increasing “intelligence” over time.</p></section><section class="Section2 RenderAsSection2" id="Sec5"><h3 class="Heading">Training</h3><p class="Para" id="Par25"><span id="ITerm25">Training</span> refers to the model being fed with the data such that it learns the rules or improves the model. The structure of the data will be different depending upon the type of machine learning and the chosen model. Data points are generally represented as a feature vector, or feature. Each feature represents one attribute of the data. A vector is just like an array data structure, discussed previously.</p><div class="Para" id="Par26">So, taking an example, let’s say we are designing a machine learning system to predict the price of a car in resale. The actual prices of cars sold previously, along with the descriptions of cars, will be fed to the learning model. The car description will have multiple attributes (features) like maker of the car, age of the car, the distance the car has been driven, and so on. Each of these features can be represented using one of the following types of data:<div class="OrderedList"><ol><li class="ListItem"><div class="ItemNumber">1.</div><div class="ItemContent"><p class="Para" id="Par27">Categorical Data: Data that takes one of the few values in a set, for example, color of a car</p></div><div class="ClearBoth">&nbsp;</div></li><li class="ListItem"><div class="ItemNumber">2.</div><div class="ItemContent"><p class="Para" id="Par28">Binary Data: Data that has two values, for example, whether a car has valid insurance or not</p></div><div class="ClearBoth">&nbsp;</div></li><li class="ListItem"><div class="ItemNumber">3.</div><div class="ItemContent"><p class="Para" id="Par29">Numerical Data: Data that is a number, for example, price of a car</p></div><div class="ClearBoth">&nbsp;</div></li><li class="ListItem"><div class="ItemNumber">4.</div><div class="ItemContent"><p class="Para" id="Par30">Graphical Data: Data that is in graphical form, for example, picture of a car</p></div><div class="ClearBoth">&nbsp;</div></li></ol></div></div><p class="Para" id="Par31">As part of the training <span id="ITerm26">process</span>, we usually divide the available data for training into parts: one part used for training and learning and the other part used for validation/checking accuracy of the model. Given a trained model, we’re ready for inference. As mentioned in the preceding, we’re never really done training, as we need to constantly update our training data set to accurately reflect the real-world data we encounter using the model.</p></section><section class="Section2 RenderAsSection2" id="Sec6"><h3 class="Heading">Prediction (Inference)</h3><p class="Para" id="Par32">Now, once the model is ready and <span id="ITerm27">trained</span>, the “trained model” is used for “prediction” or more formally “inference” with new data. The model is fed the new data and predicts the “result/output” for the same. From the computation resource perspective, inference is much faster than training because it can be done in real time or near real time in many cases.</p></section></section><section class="Section1 RenderAsSection1" id="Sec7"><h2 class="Heading">Categories of Machine learning</h2><p class="Para" id="Par33">In the context of machine learning, there are some well-known categories of learning problems. The key ones are (1) supervised, (2) unsupervised, (3) semi-supervised, and (4) reinforcement learning.</p><section class="Section2 RenderAsSection2" id="Sec8"><h3 class="Heading">Supervised Learning</h3><p class="Para" id="Par34">We know that in machine <span id="ITerm28">learning</span><span id="ITerm29">


</span>, we feed data to a model and the model learns using the data. In the case of supervised learning, the data is labeled with the right answer (we know what is good and what is bad, if you will). So, essentially, the model is being supervised while training. Another way to look at it is a person curating the data and creating the (good/bad) labels, essentially supervising the model. Supervised learning models the relationship between the output and the input data such that it can predict the output values for new data based on the derived (learned) relationships from the previous data sets. In other words, supervised learning can be considered a form of function approximation. Supervised learning is the most common machine learning technique applied in real-life use cases.</p><div class="Para" id="Par35">One <span id="ITerm30">example</span><span id="ITerm31">


</span> is when we are creating a spam detector engine. The model is fed with the description of the message along with the label (spam or “not a spam”). The learning is anchored around the label that is the correct answer (as per the supervisor). There are two major subcategories of supervised learning:<div class="OrderedList"><ol><li class="ListItem"><div class="ItemNumber">1.</div><div class="ItemContent"><p class="Para" id="Par36">Regression: The simplest form of regression is linear regression where we attempt to fit a straight line to a given set of data. In more complex regression systems, the predicted value (output) will fall within a continuous spectrum (it won’t be a binary value like true or false). An example of a regression system is a car/house price predictor that will be used to predict the price of a given car/house based on the description of the same.</p></div><div class="ClearBoth">&nbsp;</div></li><li class="ListItem"><div class="ItemNumber">2.</div><div class="ItemContent"><p class="Para" id="Par37">Classification: In a classification system, the prediction falls in one of a few classes (also referred to as groupings or clusters). An example of a classification system would be a spam detector that will classify whether or not a given message is spam.</p></div><div class="ClearBoth">&nbsp;</div></li></ol></div></div><div class="Para" id="Par38">In supervised learning, there are many algorithms that can be used, some of the most common ones being<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par39">Linear regression</p></li><li><p class="Para" id="Par40">Logistic regression</p></li><li><p class="Para" id="Par41">Nearest neighbor</p></li><li><p class="Para" id="Par42">Naïve Bayes</p></li><li><p class="Para" id="Par43">Decision trees</p></li><li><p class="Para" id="Par44">Support vector <span id="ITerm32">machines</span><span id="ITerm33">


</span></p></li></ul></div></div></section><section class="Section2 RenderAsSection2" id="Sec9"><h3 class="Heading">Unsupervised Learning</h3><div class="Para" id="Par45">In contrast to supervised learning, with <span id="ITerm34">unsupervised learning</span><span id="ITerm35">


</span>, the model studies data to identify clusters, segmentation, and patterns. In this case, the data fed to the learning model is unlabeled. Essentially, that means there is no right or wrong answer key to the data set. The machine determines correlations and relationships by learning from the available data. This is pretty easy to do visually in two or even three dimensions, but as you can imagine, it is not intuitive with more dimensions, where each feature is a new dimension. A couple of applications of unsupervised learning are anomaly detection and categorizing similar objects. Again, there are many algorithms that can be used for unsupervised learning; however, the most common ones are<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par46">K-means clustering</p></li><li><p class="Para" id="Par47">Association rules</p></li></ul></div></div></section><section class="Section2 RenderAsSection2" id="Sec10"><h3 class="Heading">Semi-supervised Learning</h3><p class="Para" id="Par48"><span id="ITerm36">Semi-supervised learning</span><span id="ITerm37">


</span> is used to address similar problems as supervised learning. It combines the techniques from both supervised and unsupervised learning. In semi-supervised learning, the machine is provided some labeled data, along with additional data that is not labeled. Typical use cases will be image and speech analysis, web content classification, protein sequence classification, and so on.</p></section><section class="Section2 RenderAsSection2" id="Sec11"><h3 class="Heading">Reinforcement Learning</h3><p class="Para" id="Par49">A <span id="ITerm38">reinforcement learning</span><span id="ITerm39">


</span> algorithm continuously learns from the environment in an iterative fashion. In the process, the model learns from the experiences of the environment. In other words, in reinforcement learning, the model is provided a set of allowed actions, rules, and potential outcomes (rewards). Essentially, the rules of the game are defined. The model then applies the rules and takes one of many possible actions and earns a reward. Based on the reward (outcome), the model determines what series of actions will lead to an optimal or optimized result. Reinforcement learning is how we learn to play a game and get better. The rules and objectives are clearly defined. However, the outcome depends on the judgment of the player who must adjust the approach in response to the environment, skill, and actions of the other player.</p></section></section><section class="Section1 RenderAsSection1" id="Sec12"><h2 class="Heading">Machine Learning in Practice</h2><p class="Para" id="Par50">Machine <span id="ITerm40">learning</span> is prevalent in all aspects of life today. For example, social media platforms use machine learning for face detection, image recognition, automatic friend suggestion, and so on. Ecommerce and other product/service providers use machine learning for personalized recommendations. Virtual personal assistants use machine learning for speech recognition, natural language processing, and conversations. Self-driving cars use machine learning for navigation and controls. In the financial world, banks, for example, use machine learning to predict loan defaults and accordingly approve/reject/limit loan applications. Also, financial institutions use machine learning to detect fraudulent transactions. These are just a few examples to illustrate the wide and growing usage in day-to-day life; there are many more.</p><section class="Section2 RenderAsSection2" id="Sec13"><h3 class="Heading">Leading Machine Learning Frameworks</h3><p class="Para" id="Par51">The rapid advancements in the machine learning world have led to proliferation of <span id="ITerm41">frameworks</span>. One of the most common frameworks today is TensorFlow. <span id="ITerm42">TensorFlow</span> is an open source platform for machine learning. Because of its comprehensive toolset, it enables the creation, training, and use of machine learning models easily. There are many other frameworks like Microsoft Cognitive Toolkit (CNTK), Theano, Scikit Learn, Caffe, H2O, Amazon Machine Learning, Torch, Google Cloud ML Engine, Azure ML Studio, Spark MLlib, and MXNet, for instance. Some of these frameworks are better suited to specific areas or applications of machine learning than others. Interested readers can find more about any of these frameworks, but any further discussion of them is beyond the scope of this book.</p><p class="Para" id="Par52">To make it easy to use the machine learning frameworks, higher-level APIs are created, which support multiple frameworks and also abstract the framework differences. For example, Keras, developed by Google, is an open source software library that provides a Python interface for artificial neural networks. It works on Linux and OS X and supports multiple back ends including TensorFlow. Another parallel high-level API is PyTorch. PyTorch was developed by Facebook and works across Windows, Linux, and OS X.</p></section></section><section class="Section1 RenderAsSection1" id="Sec14"><h2 class="Heading">Machine Learning and Cloud Computing</h2><p class="Para" id="Par53">We often hear machine <span id="ITerm43">learning</span><span id="ITerm44">


</span> and “cloud” discussed together. A casual observer might think they are connected somehow. Theoretically speaking, they are not. Cloud computing is about computing resources being available at will, and machine learning is about making computers learn and make use of that learning. The reason we often talk about them together is because machine learning training usually requires a lot of computing resources. Therefore, it makes good sense to leverage cloud computing for procuring and using these resources. As machine learning assumes increase in importance in business applications, there is a strong possibility of this <span id="ITerm45">technology</span><span id="ITerm46">


</span> being offered as a cloud-based service known as <span id="ITerm47">Machine Learning as a Service (MLaaS)</span>.</p></section><section class="Section1 RenderAsSection1" id="Sec15"><h2 class="Heading">The Way Forward</h2><p class="Para" id="Par54">Artificial intelligence/machine learning (AI/ML) has the potential to touch literally all aspects of our lives. By the time we read or reread this section, any specific estimates on deployments and proliferation of AI and ML across solutions will be out of date. As per Gartner, “Artificial Intelligence and Machine Learning have reached a critical tipping point and will increasingly augment and extend virtually every technology enabled service, thing, or application.”<sup><a epub:type="noteref" href="#Fn3" id="Fn3_source" role="doc-noteref">3</a></sup> One thing for sure, AI/ML is making inroads and making real impact. As it progresses and more businesses look to leverage the capabilities and benefits, ML will become an integral part of intelligent systems.</p><p class="Para" id="Par56">We have reached or maybe exceeded human-level performance at narrowly defined tasks such as strategy games, visual image detection, and parsing natural language.</p><p class="Para" id="Par57">There is a lot of debate around how things will shape up around machine learning. As we can imagine, with the continuous improvement in computation capability, data storage, processing, and learning, machines will continue to become more and more intelligent and powerful.</p><p class="Para" id="Par58">Extrapolating the advancements, some imagine that in the foreseeable future, machines could be capable of having “artificial general intelligence,” a more recent term. Artificial general intelligence is the intelligence of a machine that has the capacity to understand/learn any intellectual task that a <span class="ExternalRef"><a href="https://en.wikipedia.org/wiki/Human_being"><span class="RefSource">human</span></a></span> can. Today, it is a primary goal of some focused AI research to gain the artificial general intelligence level where complete problems are modeled and solutions are hypothesized. Applications include <span class="ExternalRef"><a href="https://en.wikipedia.org/wiki/Computer_vision"><span class="RefSource">computer vision</span></a></span>, <span class="ExternalRef"><a href="https://en.wikipedia.org/wiki/Natural_language_understanding"><span class="RefSource">natural language understanding</span></a></span>, and dealing with unexpected circumstances for solving real-world problems.</p><p class="Para" id="Par59">Whether or not machines reach the “artificial general intelligence” level, machine learning is going to help solve problems that are intractable today. For instance, machine learning can help discover what genes are involved in specific disease pathways. Based on this, machine learning can be used to determine the most effective personalized treatment based on patient DNA and other related characteristics. Additionally, machine learning is enabling autonomous driving and will continue to improve safety. There are plenty of studies extrapolating the benefits of autonomous driving saving lives resulting from accident avoidance and so on.</p><p class="Para" id="Par60">Like any technology, there are potentially negative side effects of advancements in machine learning. Some worry about machines taking over humans. While that may sound futuristic, there are more immediate challenges or concerns. For instance, machine learning models may sound like black boxes. While a lot of time can be spent in validating the model, one can never be sure about the output of the machine learning model (especially deep learning). Incorrect results could be incredibly costly or even fatal.</p><p class="Para" id="Par61">There are potentially dire consequences of machine learning, some of which Elon Musk and Stephen Hawking present. For example, Musk has repeatedly warned that AI will soon become just as smart as humans and said that when it does, we should all be scared because humanity’s very existence is at stake. <span class="EmphasisFontCategorySansSerif ">Hawking said the emergence of artificial intelligence could be the “worst event in the history of our civilization.”</span><sup><a epub:type="noteref" href="#Fn4" id="Fn4_source" role="doc-noteref">4</a></sup> <span class="EmphasisFontCategorySansSerif ">And</span> he followed up saying, "The development of full artificial intelligence could spell the end of the human race.” And then there are others like James Barat who have termed machine learning as “our final invention” with his book <em class="EmphasisTypeItalic ">Our Final Invention: Artificial Intelligence and the End of the Human Era</em>.<sup><a epub:type="noteref" href="#Fn5" id="Fn5_source" role="doc-noteref">5</a></sup> The book discusses the potential benefits and possible risks of human-level or superhuman artificial intelligence</p><p class="Para" id="Par64">A fundamental misunderstanding or maybe myth is that AI/ML is the solution for all the problems. Some of us feel like the AI/ML systems train themselves and become the solution for everything. The reality is that in order for a system to do something as simple as distinguish a cat from a dog, it must undergo supervised (deep) learning with volumes of data where its neural networks are trained to distinguish one from the other. So, while machine learning may sound like a potential replacement for an existing technology, we must be mindful of the time, effort, and resources it takes to model, train, and use a machine learning model. For example, machine learning may sound like the technology to replace traditional statistical analysis algorithms; however, knowing the time and resource penalty to build accurate models, we would be better off using the conventional statistical algorithms in most cases. As we’ve learned in previous chapters, we should be using “the” most appropriate tool for that specific use case.</p></section><section class="Section1 RenderAsSection1" id="Sec16"><h2 class="Heading">Summary</h2><p class="Para" id="Par65">In this chapter, we started with the fundamentals of machine learning, their benefits, and the evolution of machine learning. Then we talked about the various types of machine learning and the connection of machine learning with cloud computing. We followed that up with how machine learning is looking to shape up in the future.</p></section><section class="Section1 RenderAsSection1" id="Sec17"><h2 class="Heading">References</h2><div class="Para" id="Par66">
<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li><p class="Para" id="Par67">Artificial Intelligence/Machine Learning Primer: <span class="ExternalRef"><a href="http://www.actiac.org/system/files/Artificial%2520Intelligence%2520Machine%2520Learning%2520Primer.pdf"><span class="RefSource"><span class="EmphasisFontCategoryNonProportional ">www.actiac.org/system/files/Artificial%20Intelligence%20Machine%20Learning%20Primer.pdf</span></span></a></span></p></li><li><p class="Para" id="Par69">Machine Learning for All: <span class="ExternalRef"><a href="http://www.coursera.org/learn/uol-machine-learning-for-all"><span class="RefSource"><span class="EmphasisFontCategoryNonProportional ">www.coursera.org/learn/uol-machine-learning-for-all</span></span></a></span></p></li><li><p class="Para" id="Par70">Machine Learning: <span class="ExternalRef"><a href="http://www.coursera.org/learn/machine-learning"><span class="RefSource"><span class="EmphasisFontCategoryNonProportional ">www.coursera.org/learn/machine-learning</span></span></a></span></p></li></ul></div>
</div></section><aside aria-label="Footnotes" class="FootnoteSection" epub:type="footnotes"><div class="Heading">Footnotes</div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn1_source">1</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn1" role="doc-footnote"><p class="Para" id="Par3">Andrew Ng, <span class="ExternalRef"><a href="http://mlclass.stanford.edu/%252523:%257E:text%253DMachine%252520learning%252520is%252520the%252520science,understanding%252520of%252520the%252520human%252520genome"><span class="RefSource"><span class="EmphasisFontCategoryNonProportional ">http://mlclass.stanford.edu/#:~:text=Machine%20learning%20is%20the%20science,understanding%20of%20the%20human%20genome</span></span></a></span>.</p></div><div class="ClearBoth">&nbsp;</div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn2_source">2</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn2" role="doc-footnote"><p class="Para" id="Par4">Jacques Bughin et al., “Artificial Intelligence the Next Digital Frontier?” McKinsey Global Institute, June 2017, <span class="ExternalRef"><a href="http://www.mckinsey.com/%257E/media/McKinsey/Industries/Advanced%252520Electronics/Our%252520Insights/How%252520artificial%252520intelligence%252520can%252520deliver%252520real%252520value%252520to%252520companies/MGI-Artificial-Intelligence-Discussion-paper.pdf"><span class="RefSource"><span class="EmphasisFontCategoryNonProportional ">www.mckinsey.com/~/media/McKinsey/Industries/Advanced%20Electronics/Our%20Insights/How%20artificial%20intelligence%20can%20deliver%20real%20value%20to%20companies/MGI-Artificial-Intelligence-Discussion-paper.pdf</span></span></a></span>.</p></div><div class="ClearBoth">&nbsp;</div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn3_source">3</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn3" role="doc-footnote"><p class="Para" id="Par55">Kasey Panetta, “Gartner’s Top 10 Strategic Technology Trends for 2017,” October 18, 2016, <span class="ExternalRef"><a href="http://www.gartner.com/smarterwithgartner/gartners-top-10-technology-trends-2017/"><span class="RefSource"><span class="EmphasisFontCategoryNonProportional ">www.gartner.com/smarterwithgartner/gartners-top-10-technology-trends-2017/</span></span></a></span>.</p></div><div class="ClearBoth">&nbsp;</div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn4_source">4</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn4" role="doc-footnote"><p class="Para" id="Par62"><span class="ExternalRef"><a href="http://www.usatoday.com/story/tech/talkingtech/2017/11/07/hawking-ai-could-worst-event-history-our-civilization/839298001/"><span class="RefSource"><span class="EmphasisFontCategoryNonProportional ">www.usatoday.com/story/tech/talkingtech/2017/11/07/hawking-ai-could-worst-event-history-our-civilization/839298001/</span></span></a></span>.</p></div><div class="ClearBoth">&nbsp;</div></div><div class="Footnote"><span class="FootnoteNumber"><a href="#Fn5_source">5</a></span><div class="FootnoteContent" epub:type="footnote" id="Fn5" role="doc-footnote"><p class="Para" id="Par63">Thomas Dunne Books, 2013.</p></div><div class="ClearBoth">&nbsp;</div></div></aside></div></div></div></div><link rel="stylesheet" href="/files/public/epub-reader/override_v1.css" crossorigin="anonymous"><link rel="stylesheet" href="/api/v2/epubs/urn:orm:book:9781484271070/files/css/springer_epub.css" crossorigin="anonymous"></div></div></section>
</div>

https://learning.oreilly.com
<style>
    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 300;
        src: url(https://static.contineljs.com/fonts/Roboto-Light.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Light.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 400;
        src: url(https://static.contineljs.com/fonts/Roboto-Regular.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Regular.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 500;
        src: url(https://static.contineljs.com/fonts/Roboto-Medium.woff2?v=2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Medium.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 700;
        src: url(https://static.contineljs.com/fonts/Roboto-Bold.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Bold.woff) format("woff");
    }

    * {
        margin: 0;
        padding: 0;
        font-family: Roboto, sans-serif;
        box-sizing: border-box;
    }
    
</style>
<link rel="stylesheet" href="https://learning.oreilly.com/files/public/epub-reader/override_v1.css" crossorigin="anonymous"><link rel="stylesheet" href="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492092506/files/epub.css" crossorigin="anonymous">
<div style="width: 100%; display: flex; justify-content: center; background-color: black; color: wheat;">
    <section data-testid="contentViewer" class="contentViewer--KzjY1"><div class="annotatable--okKet"><div id="book-content"><div class="readerContainer--bZ89H white--bfCci" style="font-size: 1em; max-width: 70ch;"><div id="sbo-rt-content"><h1 class="chapnum"><a id="chap19"></a>CHAPTER <i>19</i></h1>
        <h1 class="chaptitle">Windows 7</h1>
        <p class="text1"><b>Updated by Dave Probert</b></p>
        <p class="text"><a id="page_829"></a>The Microsoft Windows 7 operating system is a 32-/64-bit preemptive multitasking client operating system for microprocessors implementing the Intel IA-32 and AMD64 instruction set architectures (ISAs). Microsoft's corresponding server operating system, Windows Server 2008 R2, is based on the same code as Windows 7 but supports only the 64-bit AMD64 and IA64 (Itanium) ISAs. Windows 7 is the latest in a series of Microsoft operating systems based on its NT code, which replaced the earlier systems based on Windows 95/98. In this chapter, we discuss the key goals of Windows 7, the layered architecture of the system that has made it so easy to use, the file system, the networking features, and the programming interface.</p>
        <div class="bgbox">
        <p class="boxtitle1">CHAPTER OBJECTIVES</p>
        <ul style="list-style-type:disc;">
        <li>To explore the principles underlying Windows 7's design and the specific components of the system.</li>
        <li>To provide a detailed discussion of the Windows 7 file system.</li>
        <li>To illustrate the networking protocols supported in Windows 7.</li>
        <li>To describe the interface available in Windows 7 to system and application programmers.</li>
        <li>To describe the important algorithms implemented with Windows 7.</li>
        </ul>
        </div>
        <h2 class="subtitle"><a id="sec19.1"></a>19.1 History</h2>
        <p class="text">In the mid-1980s, Microsoft and IBM cooperated to develop the <b>OS/2 operating system</b>, which was written in assembly language for single-processor Intel 80286 systems. In 1988, Microsoft decided to end the joint effort with IBM and develop its own “new technology” (or NT) portable operating system to support both the OS/2 and POSIX application-programming interfaces (APIs). In <a id="page_830"></a>October 1988, Dave Cutler, the architect of the DEC VAX/VMS operating system, was hired and given the charter of building Microsoft's new operating system.</p>
        <p class="text-indent">Originally, the team planned to use the OS/2 API as NT's native environment, but during development, NT was changed to use a new 32-bit Windows API (called Win32), based on the popular 16-bit API used in Windows 3.0. The first versions of NT were Windows NT 3.1 and Windows NT 3.1 Advanced Server. (At that time, 16-bit Windows was at Version 3.1.) Windows NT Version 4.0 adopted the Windows 95 user interface and incorporated Internet web-server and web-browser software. In addition, user-interface routines and all graphics code were moved into the kernel to improve performance, with the side effect of decreased system reliability. Although previous versions of NT had been ported to other microprocessor architectures, the Windows 2000 version, released in February 2000, supported only Intel (and compatible) processors due to marketplace factors. Windows 2000 incorporated significant changes. It added Active Directory (an X.500-based directory service), better networking and laptop support, support for plug-and-play devices, a distributed file system, and support for more processors and more memory.</p>
        <p class="text-indent">In October 2001, Windows XP was released as both an update to the Windows 2000 desktop operating system and a replacement for Windows 95/98. In 2002, the server edition of Windows XP became available (called Windows .Net Server). Windows XP updated the graphical user interface (GUI) with a visual design that took advantage of more recent hardware advances and many new <b><i>ease-of-use features</i></b>. Numerous features were added to automatically repair problems in applications and the operating system itself. As a result of these changes, Windows XP provided better networking and device experience (including zero-configuration wireless, instant messaging, streaming media, and digital photography/video), dramatic performance improvements for both the desktop and large multiprocessors, and better reliability and security than earlier Windows operating systems.</p>
        <p class="text-indent">The long-awaited update to Windows XP, called Windows Vista, was released in November 2006, but it was not well received. Although Windows Vista included many improvements that later showed up in Windows 7, these improvements were overshadowed by Windows Vista's perceived sluggishness and compatibility problems. Microsoft responded to criticisms of Windows Vista by improving its engineering processes and working more closely with the makers of Windows hardware and applications. The result was <b>Windows 7</b>, which was released in October 2009, along with corresponding server editions of Windows. Among the significant engineering changes is the increased use of <b>execution tracing</b> rather than counters or profiling to analyze system behavior. Tracing runs constantly in the system, watching hundreds of scenarios execute. When one of these scenarios fails, or when it succeeds but does not perform well, the traces can be analyzed to determine the cause.</p>
        <p class="text-indent">Windows 7 uses a client–server architecture (like Mach) to implement two operating-system personalities, Win32 and POSIX, with user-level processes called subsystems. (At one time, Windows also supported an OS/2 subsystem, but it was removed in Windows XP due to the demise of OS/2.) The subsystem architecture allows enhancements to be made to one operating-system personality without affecting the application compatibility of the other. Although the POSIX subsystem continues to be available for Windows 7, the Win32 API has become very popular, and the POSIX APIs are used by only a few sites. The subsystem approach continues to be interesting to study from an operating <a id="page_831"></a>system perspective, but machine-virtualization technologies are now becoming the dominant way of running multiple operating systems on a single machine.</p>
        <p class="text-indent">Windows 7 is a multi user operating system, supporting simultaneous access through distributed services or through multiple instances of the GUI via the Windows terminal services. The server editions of Windows 7 support simultaneous terminal server sessions from Windows desktop systems. The desktop editions of terminal server multiplex the keyboard, mouse, and monitor between virtual terminal sessions for each logged-on user. This feature, called <b><i>fast user switching</i></b>, allows users to preempt each other at the console of a PC without having to log off and log on.</p>
        <p class="text-indent">We noted earlier that some GUI implementation moved into kernel mode in Windows NT 4.0. It started to move into user mode again with Windows Vista, which included the <b>desktop window manager (DWM)</b> as a user-mode process. DWM implements the desktop compositing of Windows, providing the Windows <b><i>Aero</i></b> interface look on top of the Windows DirectX graphic software. DirectX continues to run in the kernel, as does the code implementing Windows' previous windowing and graphics models (Win32k and GDI). Windows 7 made substantial changes to the DWM, significantly reducing its memory footprint and improving its performance.</p>
        <p class="text-indent"><b>Windows XP</b> was the first version of Windows to ship a 64-bit version (for the IA64 in 2001 and the AMD64 in 2005). Internally, the native NT file system (NTFS) and many of the Win32 APIs have always used 64-bit integers where appropriate—so the major extension to 64-bit in Windows XP was support for large virtual addresses. However, 64-bit editions of Windows also support much larger physical memories. By the time Windows 7 shipped, the AMD64 ISA had become available on almost all CPUs from both Intel and AMD. In addition, by that time, physical memories on client systems frequently exceeded the 4-GB limit of the IA-32. As a result, the 64-bit version of Windows 7 is now commonly installed on larger client systems. Because the AMD64 architecture supports high-fidelity IA-32 compatibility at the level of individual processes, 32- and 64-bit applications can be freely mixed in a single system.</p>
        <p class="text-indent">In the rest of our description of Windows 7, we will not distinguish between the client editions of Windows 7 and the corresponding server editions. They are based on the same core components and run the same binary files for the kernel and most drivers. Similarly, although Microsoft ships a variety of different editions of each release to address different market price points, few of the differences between editions are reflected in the core of the system. In this chapter, we focus primarily on the core components of Windows 7.</p>
        <h2 class="subtitle"><a id="sec19.2"></a>19.2 Design Principles</h2>
        <p class="text">Microsoft's design goals for Windows included security, reliability, Windows and POSIX application compatibility, high performance, extensibility, portability, and international support. Some additional goals, energy efficiency and dynamic device support, have recently been added to this list. Next, we discuss each of these goals and how it is achieved in Windows 7.</p>
        <h3 class="subtitle"><a id="sec19.2.1"></a>19.2.1 Security</h3>
        <p class="text">Windows 7 security goals required more than just adherence to the design standards that had enabled Windows NT 4.0 to receive a C2 security classification <a id="page_832"></a>from the U.S. government (A C2 classification signifies a moderate level of protection from defective software and malicious attacks. Classifications were defined by the Department of Defense Trusted Computer System Evaluation Criteria, also known as the <b>Orange Book</b>, as described in <a href="25_chapter15.html#sec15.8">Section 15.8</a>.) Extensive code review and testing were combined with sophisticated automatic analysis tools to identify and investigate potential defects that might represent security vulnerabilities.</p>
        <p class="text-indent">Windows bases security on discretionary access controls. System objects, including files, registry settings, and kernel objects, are protected by <b>access-control lists (ACLs)</b> (see <a href="20_chapter11.html#sec11.6.2">Section 11.6.2</a>). ACLs are vulnerable to user and programmer errors, however, as well as to the most common attacks on consumer systems, in which the user is tricked into running code, often while browsing the Web. Windows 7 includes a mechanism called <b>integrity levels</b> that acts as a rudimentary <b><i>capability</i></b> system for controlling access. Objects and processes are marked as having low, medium, or high integrity. Windows does not allow a process to modify an object with a higher integrity level, no matter what the setting of the ACL.</p>
        <p class="text-indent">Other security measures include <b>address-space layout randomization (ASLR)</b>, nonexecutable stacks and heaps, and encryption and <b>digital signature</b> facilities. ASLR thwarts many forms of attack by preventing small amounts of injected code from jumping easily to code that is already loaded in a process as part of normal operation. This safeguard makes it likely that a system under attack will fail or crash rather than let the attacking code take control.</p>
        <p class="text-indent">Recent chips from both Intel and AMD are based on the AMD64 architecture, which allows memory pages to be marked so that they cannot contain executable instruction code. Windows tries to mark stacks and memory heaps so that they cannot be used to execute code, thus preventing attacks in which a program bug allows a buffer to overflow and then is tricked into executing the contents of the buffer. This technique cannot be applied to all programs, because some rely on modifying data and executing it. A column labeled “data execution prevention” in the Windows task manager shows which processes are marked to prevent these attacks.</p>
        <p class="text-indent">Windows uses encryption as part of common protocols, such as those used to communicate securely with websites. Encryption is also used to protect user files stored on disk from prying eyes. Windows 7 allows users to easily encrypt virtually a whole disk, as well as removable storage devices such as USB flash drives, with a feature called BitLocker. If a computer with an encrypted disk is stolen, the thieves will need very sophisticated technology (such as an electron microscope) to gain access to any of the computer's files. Windows uses digital signatures to <b><i>sign</i></b> operating system binaries so it can verify that the files were produced by Microsoft or another known company. In some editions of Windows, a <b>code integrity</b> module is activated at boot to ensure that all the loaded modules in the kernel have valid signatures, assuring that they have not been tampered with by an off-line attack.</p>
        <h3 class="subtitle"><a id="sec19.2.2"></a>19.2.2 Reliability</h3>
        <p class="text">Windows matured greatly as an operating system in its first ten years, leading to Windows 2000. At the same time, its reliability increased due to such factors as maturity in the source code, extensive stress testing of the system, improved CPU architectures, and automatic detection of many serious errors in drivers <a id="page_833"></a>from both Microsoft and third parties. Windows has subsequently extended the tools for achieving reliability to include automatic analysis of source code for errors, tests that include providing invalid or unexpected input parameters (known as <b>fuzzing</b> to detect validation failures, and an application version of the driver verifier that applies dynamic checking for an extensive set of common user-mode programming errors. Other improvements in reliability have resulted from moving more code out of the kernel and into user-mode services. Windows provides extensive support for writing drivers in user mode. System facilities that were once in the kernel and are now in user mode include the Desktop Window Manager and much of the software stack for audio.</p>
        <p class="text-indent">One of the most significant improvements in the Windows experience came from adding memory diagnostics as an option at boot time. This addition is especially valuable because so few consumer PCs have error-correcting memory. When bad RAM starts to drop bits here and there, the result is frustratingly erratic behavior in the system. The availability of memory diagnostics has greatly reduced the stress levels of users with bad RAM.</p>
        <p class="text-indent">Windows 7 introduced a fault-tolerant memory heap. The heap learns from application crashes and automatically inserts mitigations into future execution of an application that has crashed. This makes the application more reliable even if it contains common bugs such as using memory after freeing it or accessing past the end of the allocation.</p>
        <p class="text-indent">Achieving high reliability in Windows is particularly challenging because almost one billion computers run Windows. Even reliability problems that affect only a small percentage of users still impact tremendous numbers of human beings. The complexity of the Windows ecosystem also adds to the challenges. Millions of instances of applications, drivers, and other software are being constantly downloaded and run on Windows systems. Of course, there is also a constant stream of malware attacks. As Windows itself has become harder to attack directly, exploits increasingly target popular applications.</p>
        <p class="text-indent">To cope with these challenges, Microsoft is increasingly relying on communications from customer machines to collect large amounts of data from the ecosystem. Machines can be sampled to see how they are performing, what software they are running, and what problems they are encountering. Customers can send data to Microsoft when systems or software crashes or hangs. This constant stream of data from customer machines is collected very carefully, with the users' consent and without invading privacy. The result is that Microsoft is building an ever-improving picture of what is happening in the Windows ecosystem that allows continuous improvements through software updates, as well as providing data to guide future releases of Windows.</p>
        <h3 class="subtitle"><a id="sec19.2.3"></a>19.2.3 Windows and POSIX Application Compatibility</h3>
        <p class="text">As mentioned, Windows XP was both an update of Windows 2000 and a replacement for Windows 95/98. Windows 2000 focused primarily on compatibility for business applications. The requirements for Windows XP included a much higher compatibility with the consumer applications that ran on Windows 95/98. Application compatibility is difficult to achieve because many applications check for a particular version of Windows, may depend to some extent on the quirks of the implementation of APIs, may have latent application bugs that were masked in the previous system, and so <a id="page_834"></a>forth. Applications may also have been compiled for a different instruction set. Windows 7 implements several strategies to run applications despite incompatibilities.</p>
        <p class="text-indent">Like Windows XP, Windows 7 has a compatibility layer that sits between applications and the Win32 APIs. This layer makes Windows 7 look (almost) bug-for-bug compatible with previous versions of Windows. Windows 7, like earlier NT releases, maintains support for running many 16-bit applications using a <b><i>thunking</i></b>, or conversion, layer that translates 16-bit API calls into equivalent 32-bit calls. Similarly, the 64-bit version of Windows 7 provides a thunking layer that translates 32-bit API calls into native 64-bit calls.</p>
        <p class="text-indent">The Windows subsystem model allows multiple operating-system personalities to be supported. As noted earlier, although the API most commonly used with Windows is the Win32 API, some editions of Windows 7 support a POSIX subsystem. POSIX is a standard specification for UNIX that allows most available UNIX-compatible software to compile and run without modification.</p>
        <p class="text-indent">As a final compatibility measure, several editions of Windows 7 provide a virtual machine that runs Windows XP inside Windows 7. This allows applications to get bug-for-bug compatibility with Windows XP.</p>
        <h3 class="subtitle"><a id="sec19.2.4"></a>19.2.4 High Performance</h3>
        <p class="text">Windows was designed to provide high performance on desktop systems (which are largely constrained by I/O performance), server systems (where the CPU is often the bottleneck), and large multithreaded and multiprocessor environments (where locking performance and cache-line management are keys to scalability). To satisfy performance requirements, NT used a variety of techniques, such as asynchronous I/O, optimized protocols for networks, kernel-based graphics rendering, and sophisticated caching of file-system data. The memory-management and synchronization algorithms were designed with an awareness of the performance considerations related to cache lines and multiprocessors.</p>
        <p class="text-indent">Windows NT was designed for symmetrical multiprocessing (SMP); on a multiprocessor computer, several threads can run at the same time, even in the kernel. On each CPU, Windows NT uses priority-based preemptive scheduling of threads. Except while executing in the kernel dispatcher or at interrupt level, threads in any process running in Windows can be preempted by higher-priority threads. Thus, the system responds quickly (see <a href="13_chapter06.html#chap6">Chapter 6</a>).</p>
        <p class="text-indent">The subsystems that constitute Windows NT communicate with one another efficiently through a <b>local procedure call (LPC)</b> facility that provides high-performance message passing. When a thread requests a synchronous service from another process through an LPC, the servicing thread is marked <b><i>ready</i></b>, and its priority is temporarily boosted to avoid the scheduling delays that would occur if it had to wait for threads already in the queue.</p>
        <p class="text-indent">Windows XP further improved performance by reducing the code-path length in critical functions, using better algorithms and per-processor data structures, using memory coloring for <b>non-uniform memory access (NUMA)</b> machines, and implementing more scalable locking protocols, such as queued spinlocks. The new locking protocols helped reduce system bus cycles and included lock-free lists and queues, atomic read–modify–write operations (like interlocked increment), and other advanced synchronization techniques.</p>
        <p class="text-indent"><a id="page_835"></a>By the time Windows 7 was developed, several major changes had come to computing. Client/server computing had increased in importance, so an advanced local procedure call (ALPC) facility was introduced to provide higher performance and more reliability than LPC. The number of CPUs and the amount of physical memory available in the largest multiprocessors had increased substantially, so quite a lot of effort was put into improving operating-system scalability.</p>
        <p class="text-indent">The implementation of SMP in Windows NT used bitmasks to represent collections of processors and to identify, for example, which set of processors a particular thread could be scheduled on. These bitmasks were defined as fitting within a single word of memory, limiting the number of processors supported within a system to 64. Windows 7 added the concept of <b>processor groups</b> to represent arbitrary numbers of CPUs, thus accommodating more CPU cores. The number of CPU cores within single systems has continued to increase not only because of more cores but also because of cores that support more than one logical thread of execution at a time.</p>
        <p class="text-indent">All these additional CPUs created a great deal of contention for the locks used for scheduling CPUs and memory. Windows 7 broke these locks apart. For example, before Windows 7, a single lock was used by the Windows scheduler to synchronize access to the queues containing threads waiting for events. In Windows 7, each object has its own lock, allowing the queues to be accessed concurrently. Also, many execution paths in the scheduler were rewritten to be lock-free. This change resulted in good scalability performance for Windows even on systems with 256 hardware threads.</p>
        <p class="text-indent">Other changes are due to the increasing importance of support for parallel computing. For years, the computer industry has been dominated by Moore's Law, leading to higher densities of transistors that manifest themselves as faster clock rates for each CPU. Moore's Law continues to hold true, but limits have been reached that prevent CPU clock rates from increasing further. Instead, transistors are being used to build more and more CPUs into each chip. New programming models for achieving parallel execution, such as Microsoft's Concurrency RunTime (ConcRT) and Intel's Threading Building Blocks (TBB), are being used to express parallelism in C++ programs. Where Moore's Law has governed computing for forty years, it now seems that Amdahl's Law, which governs parallel computing, will rule the future.</p>
        <p class="text-indent">To support task-based parallelism, Windows 7 provides a new form of <b>user-mode scheduling (UMS)</b>. UMS allows programs to be decomposed into tasks, and the tasks are then scheduled on the available CPUs by a scheduler that operates in user mode rather than in the kernel.</p>
        <p class="text-indent">The advent of multiple CPUs on the smallest computers is only part of the shift taking place to parallel computing. Graphics processing units (GPUs) accelerate the computational algorithms needed for graphics by using <b>SIMD</b> architectures to execute a single instruction for multiple data at the same time. This has given rise to the use of GPUs for general computing, not just graphics. Operating-system support for software like OpenCL and CUDA is allowing programs to take advantage of the GPU Windows supports use of GPUs through software in its DirectX graphics support. This software, called DirectCompute, allows programs to specify <b>computational kernels</b> using the same HLSL (high-level shader language) programming model used to program the SIMD hardware for <b>graphics shaders</b>. The computational kernels run very <a id="page_836"></a>quickly on the GPU and return their results to the main computation running on the CPU.</p>
        <h3 class="subtitle"><a id="sec19.2.5"></a>19.2.5 Extensibility</h3>
        <p class="text"><b>Extensibility</b> refers to the capacity of an operating system to keep up with advances in computing technology. To facilitate change over time, the developers implemented Windows using a layered architecture. The Windows executive runs in kernel mode and provides the basic system services and abstractions that support shared use of the system. On top of the executive, several server subsystems operate in user mode. Among them are <b>environmental subsystems</b> that emulate different operating systems. Thus, programs written for the Win32 APIs and POSIX all run on Windows in the appropriate environment. Because of the modular structure, additional environmental subsystems can be added without affecting the executive. In addition, Windows uses loadable drivers in the I/O system, so new file systems, new kinds of I/O devices, and new kinds of networking can be added while the system is running. Windows uses a client–server model like the Mach operating system and supports distributed processing by <b>remote procedure calls (RPCs)</b> as defined by the Open Software Foundation.</p>
        <h3 class="subtitle"><a id="sec19.2.6"></a>19.2.6 Portability</h3>
        <p class="text">An operating system is <b>portable</b> if it can be moved from one CPU architecture to another with relatively few changes. Windows was designed to be portable. Like the UNIX operating system, Windows is written primarily in C and C++. The architecture-specific source code is relatively small, and there is very little use of assembly code. Porting Windows to a new architecture mostly affects the Windows kernel, since the user-mode code in Windows is almost exclusively written to be architecture independent. To port Windows, the kernel's architecture-specific code must be ported, and sometimes conditional compilation is needed in other parts of the kernel because of changes in major data structures, such as the page-table format. The entire Windows system must then be recompiled for the new CPU instruction set.</p>
        <p class="text-indent">Operating systems are sensitive not only to CPU architecture but also to CPU support chips and hardware boot programs. The CPU and support chips are collectively known as a <b>chipset</b>. These chipsets and the associated boot code determine how interrupts are delivered, describe the physical characteristics of each system, and provide interfaces to deeper aspects of the CPU architecture, such as error recovery and power management. It would be burdensome to have to port Windows to each type of support chip as well as to each CPU architecture. Instead, Windows isolates most of the chipset-dependent code in a dynamic link library (DLL), called the <b>hardware-abstraction layer (HAL)</b>, that is loaded with the kernel. The Windows kernel depends on the HAL interfaces rather than on the underlying chipset details. This allows the single set of kernel and driver binaries for a particular CPU to be used with different chipsets simply by loading a different version of the HAL.</p>
        <p class="text-indent">Over the years, Windows has been ported to a number of different CPU architectures: Intel IA-32-compatible 32-bit CPUs, AMD64-compatible and IA64 64-bit CPUs, the DEC Alpha, and the MIPS and PowerPC CPUs. Most of these CPU architectures failed in the market. When Windows 7 shipped, only <a id="page_837"></a>the IA-32 and AMD64 architectures were supported on client computers, along with AMD64 and IA64 on servers.</p>
        <h3 class="subtitle"><a id="sec19.2.7"></a>19.2.7 International Support</h3>
        <p class="text">Windows was designed for international and multinational use. It provides support for different locales via the <b>national-language-support</b> (<b>NLS</b>) API. The NLS API provides specialized routines to format dates, time, and money in accordance with national customs. String comparisons are specialized to account for varying character sets. UNICODE is Windows's native character code. Windows supports ANSI characters by converting them to UNICODE characters before manipulating them (8-bit to 16-bit conversion). System text strings are kept in resource files that can be replaced to localize the system for different languages. Multiple locales can be used concurrently, which is important to multilingual individuals and businesses.</p>
        <h3 class="subtitle"><a id="sec19.2.8"></a>19.2.8 Energy Efficiency</h3>
        <p class="text">Increasing energy efficiency for computers causes batteries to last longer for laptops and netbooks, saves significant operating costs for power and cooling of data centers, and contributes to green initiatives aimed at lowering energy consumption by businesses and consumers. For some time, Windows has implemented several strategies for decreasing energy use. The CPUs are moved to lower power states—for example, by lowering clock frequency—whenever possible. In addition, when a computer is not being actively used, Windows may put the entire computer into a low-power state (sleep) or may even save all of memory to disk and shut the computer off (hibernation). When the user returns, the computer powers up and continues from its previous state, so the user does not need to reboot and restart applications.</p>
        <p class="text-indent">Windows 7 added some new strategies for saving energy. The longer a CPU can stay unused, the more energy can be saved. Because computers are so much faster than human beings, a lot of energy can be saved just while humans are thinking. The problem is that too many programs are constantly polling to see what is happening in the system. A swarm of software timers are firing, keeping the CPU from staying idle long enough to save much energy. Windows 7 extends CPU idle time by skipping clock ticks, coalescing software timers into smaller numbers of events, and “parking” entire CPUs when systems are not heavily loaded.</p>
        <h3 class="subtitle"><a id="sec19.2.9"></a>19.2.9 Dynamic Device Support</h3>
        <p class="text">Early in the history of the PC industry, computer configurations were fairly static. Occasionally, new devices might be plugged into the serial, printer, or game ports on the back of a computer, but that was it. The next steps toward dynamic configuration of PCs were laptop docks and PCMIA cards. A PC could suddenly be connected to or disconnected from a whole set of peripherals. In a contemporary PC, the situation has completely changed. PCs are designed to enable users to plug and unplug a huge host of peripherals all the time; external disks, thumb drives, cameras, and the like are constantly coming and going.</p>
        <p class="center"><a id="page_838"></a><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781118063330/files/images/ch019-f001.jpg" alt="image" class="imgepub" width="642" height="511"></p>
        <p class="caption"><a id="fig19.1"></a><b>Figure 19.1</b> Windows block diagram.</p>
        <p class="text-indent">Support for dynamic configuration of devices is continually evolving in Windows. The system can automatically recognize devices when they are plugged in and can find, install, and load the appropriate drivers—often without user intervention. When devices are unplugged, the drivers automatically unload, and system execution continues without disrupting other software.</p>
        <h2 class="subtitle"><a id="sec19.3"></a>19.3 System Components</h2>
        <p class="text">The architecture of Windows is a layered system of modules, as shown in <a href="31_chapter19.html#fig19.1">Figure 19.1</a>. The main layers are the HAL, the kernel, and the executive, all of which run in kernel mode, and a collection of subsystems and services that run in user mode. The user-mode subsystems fall into two categories: the environmental subsystems, which emulate different operating systems, and the <b>protection subsystems</b>, which provide security functions. One of the chief advantages of this type of architecture is that interactions between modules are kept simple. The remainder of this section describes these layers and subsystems.</p>
        <h3 class="subtitle"><a id="sec19.3.1"></a>19.3.1 Hardware-Abstraction Layer</h3>
        <p class="text">The HAL is the layer of software that hides hardware chipset differences from upper levels of the operating system. The HAL exports a virtual hardware <a id="page_839"></a>interface that is used by the kernel dispatcher, the executive, and the device drivers. Only a single version of each device driver is required for each CPU architecture, no matter what support chips might be present. Device drivers map devices and access them directly, but the chipset-specific details of mapping memory, configuring I/O buses, setting up DMA, and coping with motherboard-specific facilities are all provided by the HAL interfaces.</p>
        <h3 class="subtitle"><a id="sec19.3.2"></a>19.3.2 Kernel</h3>
        <p class="text">The kernel layer of Windows has four main responsibilities: thread scheduling, low-level processor synchronization, interrupt and exception handling, and switching between user mode and kernel mode. The kernel is implemented in the C language, using assembly language only where absolutely necessary to interface with the lowest level of the hardware architecture.</p>
        <p class="text-indent">The kernel is organized according to object-oriented design principles. An <b>object type</b> in Windows is a system-defined data type that has a set of attributes (data values) and a set of methods (for example, functions or operations). An <b>object</b> is an instance of an object type. The kernel performs its job by using a set of kernel objects whose attributes store the kernel data and whose methods perform the kernel activities.</p>
        <h4 class="subtitle"><a id="sec19.3.2.1"></a>19.3.2.1 Kernel Dispatcher</h4>
        <p class="text">The kernel dispatcher provides the foundation for the executive and the subsystems. Most of the dispatcher is never paged out of memory, and its execution is never preempted. Its main responsibilities are thread scheduling and context switching, implementation of synchronization primitives, timer management, software interrupts (asynchronous and deferred procedure calls), and exception dispatching.</p>
        <h4 class="subtitle"><a id="sec19.3.2.2"></a>19.3.2.2 Threads and Scheduling</h4>
        <p class="text">Like many other modern operating systems, Windows uses processes and threads for executable code. Each process has one or more threads, and each thread has its own scheduling state, including actual priority, processor affinity, and CPU usage information.</p>
        <p class="text-indent">There are six possible thread states: <span class="inlinecode">ready,</span> <span class="inlinecode">standby,</span> <span class="inlinecode">running,</span> <span class="inlinecode">waiting, transition,</span> and <span class="inlinecode">terminated. Ready</span> indicates that the thread is waiting to run. The highest-priority ready thread is moved to the <span class="inlinecode">standby</span> state, which means it is the next thread to run. In a multiprocessor system, each processor keeps one thread in a standby state. A thread is <span class="inlinecode">running</span> when it is executing on a processor. It runs until it is preempted by a higher-priority thread, until it terminates, until its allotted execution time (quantum) ends, or until it waits on a dispatcher object, such as an event signaling I/O completion. A thread is in the <span class="inlinecode">waiting</span> state when it is waiting for a dispatcher object to be signaled. A thread is in the <span class="inlinecode">transition</span> state while it waits for resources necessary for execution; for example, it may be waiting for its kernel stack to be swapped in from disk. A thread enters the <span class="inlinecode">terminated</span> state when it finishes execution.</p>
        <p class="text-indent">The dispatcher uses a 32-level priority scheme to determine the order of thread execution. Priorities are divided into two classes: variable class and real-time class. The variable class contains threads having priorities from 1 to <a id="page_840"></a>15, and the real-time class contains threads with priorities ranging from 16 to 31. The dispatcher uses a queue for each scheduling priority and traverses the set of queues from highest to lowest until it finds a thread that is ready to run. If a thread has a particular processor affinity but that processor is not available, the dispatcher skips past it and continues looking for a ready thread that is willing to run on the available processor. If no ready thread is found, the dispatcher executes a special thread called the <b><i>idle thread</i></b>. Priority class 0 is reserved for the idle thread.</p>
        <p class="text-indent">When a thread's time quantum runs out, the clock interrupt queues a quantum-end <b>deferred procedure call (DPC)</b> to the processor. Queuing the DPC results in a software interrupt when the processor returns to normal interrupt priority. The software interrupt causes the dispatcher to reschedule the processor to execute the next available thread at the preempted thread's priority level.</p>
        <p class="text-indent">The priority of the preempted thread may be modified before it is placed back on the dispatcher queues. If the preempted thread is in the variable-priority class, its priority is lowered. The priority is never lowered below the base priority. Lowering the thread's priority tends to limit the CPU consumption of compute-bound threads versus I/O-bound threads. When a variable-priority thread is released from a wait operation, the dispatcher boosts the priority. The amount of the boost depends on the device for which the thread was waiting. For example, a thread waiting for keyboard I/O would get a large priority increase, whereas a thread waiting for a disk operation would get a moderate one. This strategy tends to give good response times to interactive threads using a mouse and windows. It also enables I/O-bound threads to keep the I/O devices busy while permitting compute-bound threads to use spare CPU cycles in the background. In addition, the thread associated with the user's active GUI window receives a priority boost to enhance its response time.</p>
        <p class="text-indent">Scheduling occurs when a thread enters the ready or wait state, when a thread terminates, or when an application changes a thread's priority or processor affinity. If a higher-priority thread becomes ready while a lower-priority thread is running, the lower-priority thread is preempted. This preemption gives the higher-priority thread preferential access to the CPU. Windows is not a hard real-time operating system, however, because it does not guarantee that a real-time thread will start to execute within a particular time limit; threads are blocked indefinitely while DPCs and <b>interrupt service routines (ISRs)</b> are running (as further discussed below).</p>
        <p class="text-indent">Traditionally, operating-system schedulers used sampling to measure CPU utilization by threads. The system timer would fire periodically, and the timer interrupt handler would take note of what thread was currently scheduled and whether it was executing in user or kernel mode when the interrupt occurred. This sampling technique was necessary because either the CPU did not have a high-resolution clock or the clock was too expensive or unreliable to access frequently. Although efficient, sampling was inaccurate and led to anomalies such as incorporating interrupt servicing time as thread time and dispatching threads that had run for only a fraction of the quantum. Starting with Windows Vista, CPU time in Windows has been tracked using the hardware <b>timestamp counter (TSC)</b> included in recent processors. Using the TSC results in more accurate accounting of CPU usage, and the scheduler will not preempt threads before they have run for a full quantum.</p>
        <h4 class="subtitle"><a id="sec19.3.2.3"></a>19.3.2.3 Implementation of Synchronization Primitives</h4>
        <p class="text"><a id="page_841"></a>Key operating-system data structures are managed as objects using common facilities for allocation, reference counting, and security. <b>Dispatcher objects</b> control dispatching and synchronization in the system. Examples of these objects include the following:</p>
        <ul style="list-style-type:disc;">
        <li>The <b>event object</b> is used to record an event occurrence and to synchronize this occurrence with some action. Notification events signal all waiting threads, and synchronization events signal a single waiting thread.</li>
        <li>The <b>mutant</b> provides kernel-mode or user-mode mutual exclusion associated with the notion of ownership.</li>
        <li>The <b>mutex</b>, available only in kernel mode, provides deadlock-free mutual exclusion.</li>
        <li>The <b>semaphore object</b> acts as a counter or gate to control the number of threads that access a resource.</li>
        <li>The <b>thread object</b> is the entity that is scheduled by the kernel dispatcher. It is associated with a <b>process object</b>, which encapsulates a virtual address space. The thread object is signaled when the thread exits, and the process object, when the process exits.</li>
        <li>The <b>timer object</b> is used to keep track of time and to signal timeouts when operations take too long and need to be interrupted or when a periodic activity needs to be scheduled.</li>
        </ul>
        <p class="text">Many of the dispatcher objects are accessed from user mode via an open operation that returns a handle. The user-mode code polls or waits on handles to synchronize with other threads as well as with the operating system (see <a href="31_chapter19.html#sec19.7.1">Section 19.7.1</a>).</p>
        <h4 class="subtitle"><a id="sec19.3.2.4"></a>19.3.2.4 Software Interrupts: Asynchronous and Deferred Procedure Calls</h4>
        <p class="text">The dispatcher implements two types of software interrupts: <b>asynchronous procedure calls (APCs)</b> and deferred procedure calls (DPCs, mentioned earlier). An asynchronous procedure call breaks into an executing thread and calls a procedure. APCs are used to begin execution of new threads, suspend or resume existing threads, terminate threads or processes, deliver notification that an asynchronous I/O has completed, and extract the contents of the CPU registers from a running thread. APCs are queued to specific threads and allow the system to execute both system and user code within a process's context. User-mode execution of an APC cannot occur at arbitrary times, but only when the thread is waiting in the kernel and marked <b><i>alertable</i></b>.</p>
        <p class="text-indent">DPCsare used to postpone interrupt processing. After handling all urgent device-interrupt processing, the ISR schedules the remaining processing by queuing a DPC. The associated software interrupt will not occur until the CPU is next at a priority lower than the priority of all I/O device interrupts but higher than the priority at which threads run. Thus, DPCs do not block other device ISRs. In addition to deferring device-interrupt processing, the dispatcher uses <a id="page_842"></a>DPCs to process timer expirations and to preempt thread execution at the end of the scheduling quantum.</p>
        <p class="text-indent">Execution of DPCs prevents threads from being scheduled on the current processor and also keeps APCs from signaling the completion of I/O. This is done so that completion of DPC routines does not take an extended amount of time. As an alternative, the dispatcher maintains a pool of worker threads. ISRs and DPCs may queue work items to the worker threads where they will be executed using normal thread scheduling. DPC routines are restricted so that they cannot take page faults (be paged out of memory), call system services, or take any other action that might result in an attempt to wait for a dispatcher object to be signaled. Unlike APCs, DPC routines make no assumptions about what process context the processor is executing.</p>
        <h4 class="subtitle"><a id="sec19.3.2.5"></a>19.3.2.5 Exceptions and Interrupts</h4>
        <p class="text">The kernel dispatcher also provides trap handling for exceptions and interrupts generated by hardware or software. Windows defines several architecture-independent exceptions, including:</p>
        <ul style="list-style-type:disc;">
        <li>Memory-access violation</li>
        <li>Integer overflow</li>
        <li>Floating-point overflow or underflow</li>
        <li>Integer divide by zero</li>
        <li>Floating-point divide by zero</li>
        <li>Illegal instruction</li>
        <li>Data misalignment</li>
        <li>Privileged instruction</li>
        <li>Page-read error</li>
        <li>Access violation</li>
        <li>Paging file quota exceeded</li>
        <li>Debugger breakpoint</li>
        <li>Debugger single step</li>
        </ul>
        <p class="text">The trap handlers deal with simple exceptions. Elaborate exception handling is performed by the kernel's exception dispatcher. The <b>exception dispatcher</b> creates an exception record containing the reason for the exception and finds an exception handler to deal with it.</p>
        <p class="text-indent">When an exception occurs in kernel mode, the exception dispatcher simply calls a routine to locate the exception handler. If no handler is found, a fatal system error occurs, and the user is left with the infamous “blue screen of death” that signifies system failure.</p>
        <p class="text-indent">Exception handling is more complex for user-mode processes, because an environmental subsystem (such as the POSIX system) sets up a debugger port and an exception port for every process it creates. (For details on ports, <a id="page_843"></a>see <a href="31_chapter19.html#sec19.3.3.4">Section 19.3.3.4</a>.) If a debugger port is registered, the exception handler sends the exception to the port. If the debugger port is not found or does not handle that exception, the dispatcher attempts to find an appropriate exception handler. If no handler is found, the debugger is called again to catch the error for debugging. If no debugger is running, a message is sent to the process's exception port to give the environmental subsystem a chance to translate the exception. For example, the POSIX environment translates Windows exception messages into POSIX signals before sending them to the thread that caused the exception. Finally, if nothing else works, the kernel simply terminates the process containing the thread that caused the exception.</p>
        <p class="text-indent">When Windows fails to handle an exception, it may construct a description of the error that occurred and request permission from the user to send the information back to Microsoft for further analysis. In some cases, Microsoft's automated analysis may be able to recognize the error immediately and suggest a fix or workaround.</p>
        <p class="text-indent">The interrupt dispatcher in the kernel handles interrupts by calling either an interrupt service routine (ISR) supplied by a device driver or a kernel trap-handler routine. The interrupt is represented by an <b>interrupt object</b> that contains all the information needed to handle the interrupt. Using an interrupt object makes it easy to associate interrupt-service routines with an interrupt without having to access the interrupt hardware directly.</p>
        <p class="text-indent">Different processor architectures have different types and numbers of interrupts. For portability, the interrupt dispatcher maps the hardware interrupts into a standard set. The interrupts are prioritized and are serviced in priority order. There are 32 interrupt request levels (IRQLs) in Windows. Eight are reserved for use by the kernel; the remaining 24 represent hardware interrupts via the HAL (although most IA-32 systems use only 16). The Windows interrupts are defined in <a href="31_chapter19.html#fig19.2">Figure 19.2</a>.</p>
        <p class="text-indent">The kernel uses an <b>interrupt-dispatch table</b> to bind each interrupt level to a service routine. In a multiprocessor computer, Windows keeps a separate interrupt-dispatch table (IDT) for each processor, and each processor's IRQL can be set independently to mask out interrupts. All interrupts that occur at a level equal to or less than the IRQL of a processor are blocked until the IRQL is lowered <a id="page_844"></a>by a kernel-level thread or by an ISR returning from interrupt processing. Windows takes advantage of this property and uses software interrupts to deliver APCs and DPCs, to perform system functions such as synchronizing threads with I/O completion, to start thread execution, and to handle timers.</p>
        <p class="center"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781118063330/files/images/ch019-f002.jpg" alt="image" class="imgepub" width="608" height="273"></p>
        <p class="caption"><a id="fig19.2"></a><b>Figure 19.2</b> Windows interrupt-request levels.</p>
        <h4 class="subtitle"><a id="sec19.3.2.6"></a>19.3.2.6 Switching between User-Mode and Kernel-Mode Threads</h4>
        <p class="text">What the programmer thinks of as a thread in traditional Windows is actually two threads: a <b>user-mode thread (UT)</b> and a <b>kernel-mode thread (KT)</b>. Each has its own stack, register values, and execution context. A UT requests a system service by executing an instruction that causes a trap to kernel mode. The kernel layer runs a trap handler that switches between the UT and the corresponding KT. When a KT has completed its kernel execution and is ready to switch back to the corresponding UT, the kernel layer is called to make the switch to the UT, which continues its execution in user mode.</p>
        <p class="text-indent">Windows 7 modifies the behavior of the kernel layer to support user-mode scheduling of the UTs. User-mode schedulers in Windows 7 support cooperative scheduling. A UT can explicitly yield to another UT by calling the user-mode scheduler; it is not necessary to enter the kernel. User-mode scheduling is explained in more detail in <a href="31_chapter19.html#sec19.7.3.7">Section 19.7.3.7</a>.</p>
        <h3 class="subtitle"><a id="sec19.3.3"></a>19.3.3 Executive</h3>
        <p class="text">The Windows executive provides a set of services that all environmental subsystems use. The services are grouped as follows: object manager, virtual memory manager, process manager, advanced local procedure call facility, I/O manager, cache manager, security reference monitor, plug-and-play and power managers, registry, and booting.</p>
        <h4 class="subtitle"><a id="sec19.3.3.1"></a>19.3.3.1 Object Manager</h4>
        <p class="text">For managing kernel-mode entities, Windows uses a generic set of interfaces that are manipulated by user-mode programs. Windows calls these entities <b><i>objects</i></b>, and the executive component that manipulates them is the <b>object manager</b>. Examples of objects are semaphores, mutexes, events, processes, and threads; all these are <b><i>dispatcher objects</i></b>. Threads can block in the kernel dispatcher waiting for any of these objects to be signaled. The process, thread, and virtual memory APIs use process and thread handles to identify the process or thread to be operated on. Other examples of objects include files, sections, ports, and various internal I/O objects. File objects are used to maintain the open state of files and devices. Sections are used to map files. Local-communication endpoints are implemented as port objects.</p>
        <p class="text-indent">User-mode code accesses these objects using an opaque value called a <b>handle</b>, which is returned by many APIs. Each process has a <b>handle table</b> containing entries that track the objects used by the process. The <b>system process</b>, which contains the kernel, has its own handle table, which is protected from user code. The handle tables in Windows are represented by a tree structure, which can expand from holding 1,024 handles to holding over 16 million. Kernel-mode code can access an object by using either a handle or a <b>referenced pointer</b>.</p>
        <p class="text-indent"><a id="page_845"></a>A process gets a handle by creating an object, by opening an existing object, by receiving a duplicated handle from another process, or by inheriting a handle from the parent process. When a process exits, all its open handles are implicitly closed. Since the object manager is the only entity that generates object handles, it is the natural place to check security. The object manager checks whether a process has the right to access an object when the process tries to open the object. The object manager also enforces quotas, such as the maximum amount of memory a process may use, by charging a process for the memory occupied by all its referenced objects and refusing to allocate more memory when the accumulated charges exceed the process's quota.</p>
        <p class="text-indent">The object manager keeps track of two counts for each object: the number of handles for the object and the number of referenced pointers. The handle count is the number of handles that refer to the object in the handle tables of all processes, including the system process that contains the kernel. The referenced pointer count is incremented whenever a new pointer is needed by the kernel and decremented when the kernel is done with the pointer. The purpose of these reference counts is to ensure that an object is not freed while it is still referenced by either a handle or an internal kernel pointer.</p>
        <p class="text-indent">The object manager maintains the Windows internal name space. In contrast to UNIX, which roots the system name space in the file system, Windows uses an abstract name space and connects the file systems as devices. Whether a Windows object has a name is up to its creator. Processes and threads are created without names and referenced either by handle or through a separate numerical identifier. Synchronization events usually have names, so that they can be opened by unrelated processes. A name can be either permanent or temporary. A permanent name represents an entity, such as a disk drive, that remains even if no process is accessing it. A temporary name exists only while a process holds a handle to the object. The object manager supports directories and symbolic links in the name space. As an example, MS-DOS drive letters are implemented using symbolic links; <span class="inlinecode">\Global??\C:</span> is a symbolic link to the device object <span class="inlinecode">\Device\HarddiskVolume2</span>, representing a mounted file-system volume in the <span class="inlinecode">\Device</span> directory.</p>
        <p class="text-indent">Each object, as mentioned earlier, is an instance of an <b><i>object type</i></b>. The object type specifies how instances are to be allocated, how the data fields are to be defined, and how the standard set of virtual functions used for all objects are to be implemented. The standard functions implement operations such as mapping names to objects, closing and deleting, and applying security checks. Functions that are specific to a particular type of object are implemented by system services designed to operate on that particular object type, not by the methods specified in the object type.</p>
        <p class="text-indent">The <span class="inlinecode">parse()</span> function is the most interesting of the standard object functions. It allows the implementation of an object. The file systems, the registry configuration store, and GUI objects are the most notable users of parse functions to extend the Windows name space.</p>
        <p class="text-indent">Returning to our Windows naming example, device objects used to represent file-system volumes provide a parse function. This allows a name like <span class="inlinecode">\Global??\C:\foo\bar.doc</span> to be interpreted as the file <span class="inlinecode">\foo\bar.doc</span> on the volume represented by the device object <span class="inlinecode">HarddiskVolume2</span>. We can illustrate how naming, parse functions, objects, and handles work together by looking at the steps to open the file in Windows:</p>
        <ol style="list-style-type:decimal;">
        <li><a id="page_846"></a>An application requests that a file named <span class="inlinecode">C:\foo\bar.doc</span> be opened.</li>
        <li>The object manager finds the device object <span class="inlinecode">HarddiskVolume2</span>, looks up the parse procedure <span class="inlinecode">IopParseDevice</span> from the object's type, and invokes it with the file's name relative to the root of the file system.</li>
        <li><span class="inlinecode">IopParseDevice()</span> allocates a file object and passes it to the file system, which fills in the details of how to access <span class="inlinecode">C:\foo\bar.doc</span> on the volume.</li>
        <li>When the file system returns, <span class="inlinecode">IopParseDevice()</span> allocates an entry for the file object in the handle table for the current process and returns the handle to the application.</li>
        </ol>
        <p class="text-indent">If the file cannot successfully be opened, <span class="inlinecode">IopParseDevice()</span> deletes the file object it allocated and returns an error indication to the application.</p>
        <h4 class="subtitle"><a id="sec19.3.3.2"></a>19.3.3.2 Virtual Memory Manager</h4>
        <p class="text">The executive component that manages the virtual address space, physical memory allocation, and paging is the <b>virtual memory (VM) manager</b>. The design of the VM manager assumes that the underlying hardware supports virtual-to-physical mapping, a paging mechanism, and transparent cache coherence on multiprocessor systems, as well as allowing multiple page-table entries to map to the same physical page frame. The VM manager in Windows uses a page-based management scheme with page sizes of 4 KB and 2 MB on AMD64 and IA-32-compatible processors and 8 KB on the IA64. Pages of data allocated to a process that are not in physical memory are either stored in the <b>paging files</b> on disk or mapped directly to a regular file on a local or remote file system. A page can also be marked zero-fill-on-demand, which initializes the page with zeros before it is allocated, thus erasing the previous contents.</p>
        <p class="text-indent">On IA-32 processors, each process has a 4-GB virtual address space. The upper 2 GB are mostly identical for all processes and are used by Windows in kernel mode to access the operating-system code and data structures. For the AMD64 architecture, Windows provides a 8-TB virtual address space for user mode out of the 16 EB supported by existing hardware for each process.</p>
        <p class="text-indent">Key areas of the kernel-mode region that are not identical for all processes are the self-map, hyperspace, and session space. The hardware references a process's page table using physical page-frame numbers, and the <b>page table self-map</b> makes the contents of the process's page table accessible using virtual addresses. <b>Hyperspace</b> maps the current process's working-set information into the kernel-mode address space. <b>Session space</b> is used to share an instance of the Win32 and other session-specific drivers among all the processes in the same terminal-server (TS) session. Different TS sessions share different instances of these drivers, yet they are mapped at the same virtual addresses. The lower, user-mode region of virtual address space is specific to each process and accessible by both user- and kernel-mode threads.</p>
        <p class="text-indent">The Windows VM manager uses a two-step process to allocate virtual memory. The first step <b><i>reserves</i></b> one or more pages of virtual addresses in the process's virtual address space. The second step <b><i>commits</i></b> the allocation by assigning virtual memory space (physical memory or space in the paging files). Windows limits the amount of virtual memory space a process consumes by enforcing a quota on committed memory. A process decommits memory that it <a id="page_847"></a>is no longer using to free up virtual memory space for use by other processes. The APIs used to reserve virtual addresses and commit virtual memory take a handle on a process object as a parameter. This allows one process to control the virtual memory of another. Environmental subsystems manage the memory of their client processes in this way.</p>
        <p class="text-indent">Windows implements shared memory by defining a <b>section object</b>. After getting a handle to a section object, a process maps the memory of the section to a range of addresses, called a <b>view</b>. A process can establish a view of the entire section or only the portion it needs. Windows allows sections to be mapped not just into the current process but into any process for which the caller has a handle.</p>
        <p class="text-indent">Sections can be used in many ways. A section can be backed by disk space either in the system-paging file or in a regular file (a <b>memory-mapped file</b>). A section can be <b><i>based</i></b>, meaning that it appears at the same virtual address for all processes attempting to access it. Sections can also represent physical memory, allowing a 32-bit process to access more physical memory than can fit in its virtual address space. Finally, the memory protection of pages in the section can be set to read-only, read-write, read-write-execute, execute-only, no access, or copy-on-write.</p>
        <p class="text-indent">Let's look more closely at the last two of these protection settings:</p>
        <ul style="list-style-type:disc;">
        <li>A <b><i>no-access page</i></b> raises an exception if accessed. The exception can be used, for example, to check whether a faulty program iterates beyond the end of an array or simply to detect that the program attempted to access virtual addresses that are not committed to memory. User- and kernel-mode stacks use no-access pages as <b>guard pages</b> to detect stack overflows. Another use is to look for heap buffer overruns. Both the user-mode memory allocator and the special kernel allocator used by the device verifier can be configured to map each allocation onto the end of a page, followed by a no-access page to detect programming errors that access beyond the end of an allocation.</li>
        <li>The <b><i>copy-on-write mechanism</i></b> enables the VM manager to use physical memory more efficiently. When two processes want independent copies of data from the same section object, the VM manager places a single shared copy into virtual memory and activates the copy-on-write property for that region of memory. If one of the processes tries to modify data in a copy-on-write page, the VM manager makes a private copy of the page for the process.</li>
        </ul>
        <p class="text-indent">The virtual address translation in Windows uses a multilevel page table. For IA-32 and AMD64 processors, each process has a <b>page directory</b> that contains 512 <b>page-directory entries (PDEs)</b> 8 bytes in size. Each PDE points to a <b>PTE table</b> that contains 512 <b>page-table entries (PTEs)</b> 8 bytes in size. Each PTE points to a 4-KB <b>page frame</b> in physical memory. For a variety of reasons, the hardware requires that the page directories or PTE tables at each level of a multilevel page table occupy a single page. Thus, the number of PDEs or PTEs that fit in a page determine how many virtual addresses are translated by that page. See <a href="31_chapter19.html#fig19.3">Figure 19.3</a> for a diagram of this structure.</p>
        <p class="text-indent">The structure described so far can be used to represent only 1 GB of virtual address translation. For IA-32, a second page-directory level is needed, <a id="page_848"></a>containing only four entries, as shown in the diagram. On 64-bit processors, more levels are needed. For AMD64, Windows uses a total of four full levels. The total size of all page-table pages needed to fully represent even a 32-bit virtual address space for a process is 8 MB. The VM manager allocates pages of PDEs and PTEs as needed and moves page-table pages to disk when not in use. The page-table pages are faulted back into memory when referenced.</p>
        <p class="center"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781118063330/files/images/ch019-f003.jpg" alt="image" class="imgepub" width="612" height="366"></p>
        <p class="caption"><a id="fig19.3"></a><b>Figure 19.3</b> Page-table layout.</p>
        <p class="text-indent">We next consider how virtual addresses are translated into physical addresses on IA-32-compatible processors. A 2-bit value can represent the values 0, 1, 2, 3. A 9-bit value can represent values from 0 to 511; a 12-bit value, values from 0 to 4,095. Thus, a 12-bit value can select any byte within a 4-KB page of memory. A 9-bit value can represent any of the 512 PDEs or PTEs in a page directory or PTE-table page. As shown in <a href="31_chapter19.html#fig19.4">Figure 19.4</a>, translating a virtual address pointer to a byte address in physical memory involves breaking the 32-bit pointer into four values, starting from the most significant bits:</p>
        <ul style="list-style-type:disc;">
        <li>Two bits are used to index into the four PDEs at the top level of the page table. The selected PDE will contain the physical page number for each of the four page-directory pages that map 1 GB of the address space.
        <p class="center"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781118063330/files/images/ch019-f004.jpg" alt="image" class="imgepub" width="569" height="122"></p>
        <p class="caption"><a id="fig19.4"></a><b>Figure 19.4</b> Virtual-to-physical address translation on IA-32.</p></li>
        <li><a id="page_849"></a>Nine bits are used to select another PDE, this time from a second-level page directory. This PDE will contain the physical page numbers of up to 512 PTE-table pages.</li>
        <li>Nine bits are used to select one of 512 PTEs from the selected PTE-table page. The selected PTE will contain the physical page number for the byte we are accessing.</li>
        <li>Twelve bits are used as the byte offset into the page. The physical address of the byte we are accessing is constructed by appending the lowest 12 bits of the virtual address to the end of the physical page number we found in the selected PTE.</li>
        </ul>
        <p class="text-indent">The number of bits in a physical address may be different from the number of bits in a virtual address. In the original IA-32 architecture, the PTE and PDE were 32-bit structures that had room for only 20 bits of physical page number, so the physical address size and the virtual address size were the same. Such systems could address only 4 GB of physical memory. Later, the IA-32 was extended to the larger 64-bit PTE size used today, and the hardware supported 24-bit physical addresses. These systems could support 64 GB and were used on server systems. Today, all Windows servers are based on either the AMD64 or the IA64 and support very, very large physical addresses—more than we can possibly use. (Of course, once upon a time 4 GB seemed optimistically large for physical memory.)</p>
        <p class="text-indent">To improve performance, the VM manager maps the page-directory and PTE-table pages into the same contiguous region of virtual addresses in every process. This self-map allows the VM manager to use the same pointer to access the current PDE or PTE corresponding to a particular virtual address no matter what process is running. The self-map for the IA-32 takes a contiguous 8-MB region of kernel virtual address space; the AMD64 self-map occupies 512 GB. Although the self-map occupies significant address space, it does not require any additional virtual memory pages. It also allows the page table's pages to be automatically paged in and out of physical memory.</p>
        <p class="text-indent">In the creation of a self-map, one of the PDEs in the top-level page directory refers to the page-directory page itself, forming a “loop” in the page-table translations. The virtual pages are accessed if the loop is not taken, the PTE-table pages are accessed if the loop is taken once, the lowest-level page-directory pages are accessed if the loop is taken twice, and so forth.</p>
        <p class="text-indent">The additional levels of page directories used for 64-bit virtual memory are translated in the same way except that the virtual address pointer is broken up into even more values. For the AMD64, Windows uses four full levels, each of which maps 512 pages, or 9+9+9+9+12 = 48 bits of virtual address.</p>
        <p class="text-indent">To avoid the overhead of translating every virtual address by looking up the PDE and PTE, processors use <b>translation look-aside buffer (TLB)</b> hardware, which contains an associative memory cache for mapping virtual pages to PTEs. The TLB is part of the <b>memory-management unit (MMU)</b> within each processor. The MMU needs to “walk” (navigate the data structures of) the page table in memory only when a needed translation is missing from the TLB.</p>
        <p class="text-indent">The PDEs and PTEs contain more than just physical page numbers. They also have bits reserved for operating-system use and bits that control how the hardware uses memory, such as whether hardware caching should be used for <a id="page_850"></a>each page. In addition, the entries specify what kinds of access are allowed for both user and kernel modes.</p>
        <p class="text-indent">A PDE can also be marked to say that it should function as a PTE rather than a PDE. On a IA-32, the first 11 bits of the virtual address pointer select a PDE in the first two levels of translation. If the selected PDE is marked to act as a PTE, then the remaining 21 bits of the pointer are used as the offset of the byte. This results in a 2-MB size for the page. Mixing and matching 4-KB and 2-MB page sizes within the page table is easy for the operating system and can significantly improve the performance of some programs by reducing how often the MMU needs to reload entries in the TLB, since one PDE mapping 2 MB replaces 512 PTEs each mapping 4 KB.</p>
        <p class="text-indent">Managing physical memory so that 2-MB pages are available when needed is difficult, however, as they may continually be broken up into 4 KB pages, causing external fragmentation of memory. Also, the large pages can result in very significant internal fragmentation. Because of these problems, it is typically only Windows itself, along with large server applications, that use large pages to improve the performance of the TLB. They are better suited to do so because operating-system and server applications start running when the system boots, before memory has become fragmented.</p>
        <p class="text-indent">Windows manages physical memory by associating each physical page with one of seven states: free, zeroed, modified, standby, bad, transition, or valid.</p>
        <ul style="list-style-type:disc;">
        <li>A <b><i>free</i></b> page is a page that has no particular content.</li>
        <li>A <b><i>zeroed</i></b> page is a free page that has been zeroed out and is ready for immediate use to satisfy zero-on-demand faults.</li>
        <li>A <b><i>modified</i></b> page has been written by a process and must be sent to the disk before it is allocated for another process.</li>
        <li>A <b><i>standby</i></b> page is a copy of information already stored on disk. Standby pages may be pages that were not modified, modified pages that have already been written to the disk, or pages that were prefetched because they are expected to be used soon.</li>
        <li>A <b><i>bad</i></b> page is unusable because a hardware error has been detected.</li>
        <li>A <b><i>transition</i></b> page is on its way in from disk to a page frame allocated in physical memory.</li>
        <li>A <b><i>valid</i></b> page is part of the working set of one or more processes and is contained within these processes' page tables.</li>
        </ul>
        <p class="text-indent">While valid pages are contained in processes' page tables, pages in other states are kept in separate lists according to state type. The lists are constructed by linking the corresponding entries in the <b>page frame number (PFN)</b> database, which includes an entry for each physical memory page. The PFN entries also include information such as reference counts, locks, and NUMA information. Note that the PFN database represents pages of physical memory, whereas the PTEs represent pages of virtual memory.</p>
        <p class="text-indent">When the valid bit in a PTE is zero, hardware ignores all the other bits, and the VM manager can define them for its own use. Invalid pages can have a number of states represented by bits in the PTE. Page-file pages that have never <a id="page_851"></a>been faulted in are marked zero-on-demand. Pages mapped through section objects encode a pointer to the appropriate section object. PTEs for pages that have been written to the page file contain enough information to locate the page on disk, and so forth. The structure of the page-file PTE is shown in <a href="31_chapter19.html#fig19.5">Figure 19.5</a>. The T, P, and V bits are all zero for this type of PTE. The PTE includes 5 bits for page protection, 32 bits for page-file offset, and 4 bits to select the paging file. There are also 20 bits reserved for additional bookkeeping.</p>
        <p class="center"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781118063330/files/images/ch019-f005.jpg" alt="image" class="imgepub" width="569" height="265"></p>
        <p class="caption"><a id="fig19.5"></a><b>Figure 19.5</b> Page-file page-table entry. The valid bit is zero.</p>
        <p class="text-indent">Windows uses a per-working-set, least-recently-used (LRU) replacement policy to take pages from processes as appropriate. When a process is started, it is assigned a default minimum working-set size. The working set of each process is allowed to grow until the amount of remaining physical memory starts to run low, at which point the VM manager starts to track the age of the pages in each working set. Eventually, when the available memory runs critically low, the VM manager trims the working set to remove older pages.</p>
        <p class="text-indent">How old a page is depends not on how long it has been in memory but on when it was last referenced. This is determined by periodically making a pass through the working set of each process and incrementing the age for pages that have not been marked in the PTE as referenced since the last pass. When it becomes necessary to trim the working sets, the VM manager uses heuristics to decide how much to trim from each process and then removes the oldest pages first.</p>
        <p class="text-indent">A process can have its working set trimmed even when plenty of memory is available, if it was given a <b><i>hard limit</i></b> on how much physical memory it could use. In Windows 7, the VM manager will also trim processes that are growing rapidly, even if memory is plentiful. This policy change significantly improves the responsiveness of the system for other processes.</p>
        <p class="text-indent">Windows tracks working sets not only for user-mode processes but also for the system process, which includes all the pageable data structures and code that run in kernel mode. Windows 7 created additional working sets for the system process and associated them with particular categories of kernel memory; the file cache, kernel heap, and kernel code now have their own working sets. The distinct working sets allow the VM manager to use different policies to trim the different categories of kernel memory.</p>
        <p class="text-indent"><a id="page_852"></a>The VM manager does not fault in only the page immediately needed. Research shows that the memory referencing of a thread tends to have a <b>locality</b> property. That is, when a page is used, it is likely that adjacent pages will be referenced in the near future. (Think of iterating over an array or fetching sequential instructions that form the executable code for a thread.) Because of locality, when the VM manager faults in a page, it also faults in a few adjacent pages. This prefetching tends to reduce the total number of page faults and allows reads to be clustered to improve I/O performance.</p>
        <p class="text-indent">In addition to managing committed memory, the VM manager manages each process's reserved memory, or virtual address space. Each process has an associated tree that describes the ranges of virtual addresses in use and what the uses are. This allows the VM manager to fault in page-table pages as needed. If the PTE for a faulting address is uninitialized, the VM manager searches for the address in the process's tree of <b>virtual address descriptors (VADs)</b> and uses this information to fill in the PTE and retrieve the page. In some cases, a PTE-table page itself may not exist; such a page must be transparently allocated and initialized by the VM manager. In other cases, the page may be shared as part of a section object, and the VAD will contain a pointer to that section object. The section object contains information on how to find the shared virtual page so that the PTE can be initialized to point at it directly.</p>
        <h4 class="subtitle"><a id="sec19.3.3.3"></a>19.3.3.3 Process Manager</h4>
        <p class="text">The Windows process manager provides services for creating, deleting, and using processes, threads, and jobs. It has no knowledge about parent–child relationships or process hierarchies; those refinements are left to the particular environmental subsystem that owns the process. The process manager is also not involved in the scheduling of processes, other than setting the priorities and affinities in processes and threads when they are created. Thread scheduling takes place in the kernel dispatcher.</p>
        <p class="text-indent">Each process contains one or more threads. Processes themselves can be collected into larger units called <b>job objects</b>. The use of job objects allows limits to be placed on CPU usage, working-set size, and processor affinities that control multiple processes at once. Job objects are used to manage large data-center machines.</p>
        <p class="text-indent">An example of process creation in the Win32 environment is as follows:</p>
        <ol style="list-style-type:decimal;">
        <li>A Win32 application calls <span class="inlinecode">CreateProcess()</span>.</li>
        <li>A message is sent to the Win32 subsystem to notify it that the process is being created.</li>
        <li><span class="inlinecode">CreateProcess()</span> in the original process then calls an API in the process manager of the NT executive to actually create the process.</li>
        <li>The process manager calls the object manager to create a process object and returns the object handle to Win32.</li>
        <li>Win32 calls the process manager again to create a thread for the process and returns handles to the new process and thread.</li>
        </ol>
        <p class="text-indent">The Windows APIs for manipulating virtual memory and threads and for duplicating handles take a process handle, so subsystems can perform <a id="page_853"></a>operations on behalf of a new process without having to execute directly in the new process's context. Once a new process is created, the initial thread is created, and an asynchronous procedure call is delivered to the thread to prompt the start of execution at the user-mode image loader. The loader is in <span class="inlinecode">ntdll.dll</span>, which is a link library automatically mapped into every newly created process. Windows also supports a UNIX <span class="inlinecode">fork()</span> style of process creation in order to support the POSIX environmental subsystem. Although the Win32 environment calls the process manager directly from the client process, POSIX uses the cross-process nature of the Windows APIs to create the new process from within the subsystem process.</p>
        <p class="text-indent">The process manager relies on the asynchronous procedure calls (APCs) implemented by the kernel layer. APCs are used to initiate thread execution, suspend and resume threads, access thread registers, terminate threads and processes, and support debuggers.</p>
        <p class="text-indent">The debugger support in the process manager includes the APIs to suspend and resume threads and to create threads that begin in suspended mode. There are also process-manager APIs that get and set a thread's register context and access another process's virtual memory. Threads can be created in the current process; they can also be injected into another process. The debugger makes use of thread injection to execute code within a process being debugged.</p>
        <p class="text-indent">While running in the executive, a thread can temporarily attach to a different process. <b>Thread attach</b> is used by kernel worker threads that need to execute in the context of the process originating a work request. For example, the VM manager might use thread attach when it needs access to a process's working set or page tables, and the I/O manager might use it in updating the status variable in a process for asynchronous I/O operations.</p>
        <p class="text-indent">The process manager also supports <b>impersonation</b>. Each thread has an associated <b>security token</b>. When the login process authenticates a user, the security token is attached to the user's process and inherited by its child processes. The token contains the <b>security identity (SID)</b> of the user, the SIDs of the groups the user belongs to, the privileges the user has, and the integrity level of the process. By default, all threads within a process share a common token, representing the user and the application that started the process. However, a thread running in a process with a security token belonging to one user can set a thread-specific token belonging to another user to impersonate that user.</p>
        <p class="text-indent">The impersonation facility is fundamental to the client–server RPC model, where services must act on behalf of a variety of clients with different security IDs. The right to impersonate a user is most often delivered as part of an RPC connection from a client process to a server process. Impersonation allows the server to access system services as if it were the client in order to access or create objects and files on behalf of the client. The server process must be trustworthy and must be carefully written to be robust against attacks. Otherwise, one client could take over a server process and then impersonate any user who made a subsequent client request.</p>
        <h4 class="subtitle"><a id="sec19.3.3.4"></a>19.3.3.4 Facilities for Client–Server Computing</h4>
        <p class="text">The implementation of Windows uses a client–server model throughout. The environmental subsystems are servers that implement particular operating-system personalities. Many other services, such as user authentication, network <a id="page_854"></a>facilities, printer spooling, web services, network file systems, and plug-and-play, are also implemented using this model. To reduce the memory footprint, multiple services are often collected into a few processes running the <span class="inlinecode">svchost.exe</span> program. Each service is loaded as a dynamic-link library (DLL), which implements the service by relying on the user-mode thread-pool facilities to share threads and wait for messages (see <a href="31_chapter19.html#sec19.3.3.3">Section 19.3.3.3</a>).</p>
        <p class="text-indent">The normal implementation paradigm for client–server computing is to use RPCs to communicate requests. The Win32 API supports a standard RPC protocol, as described in <a href="31_chapter19.html#sec19.6.2.7">Section 19.6.2.7</a>. RPC uses multiple transports (for example, named pipes and TCP/IP) and can be used to implement RPCs between systems. When an RPC always occurs between a client and server on the local system, the advanced local procedure call facility (ALPC) can be used as the transport. At the lowest level of the system, in the implementation of the environmental systems, and for services that must be available in the early stages of booting, RPC is not available. Instead, native Windows services use ALPC directly.</p>
        <p class="text-indent">ALPC is a message-passing mechanism. The server process publishes a globally visible connection-port object. When a client wants services from a subsystem or service, it opens a handle to the server's connection-port object and sends a connection request to the port. The server creates a channel and returns a handle to the client. The channel consists of a pair of private communication ports: one for client-to-server messages and the other for server-to-client messages. Communication channels support a callback mechanism, so the client and server can accept requests when they would normally be expecting a reply.</p>
        <p class="text-indent">When an ALPC channel is created, one of three message-passing techniques is chosen.</p>
        <ol style="list-style-type:decimal;">
        <li>The first technique is suitable for small to medium messages (up to 63 KB). In this case, the port's message queue is used as intermediate storage, and the messages are copied from one process to the other.</li>
        <li>The second technique is for larger messages. In this case, a shared memory section object is created for the channel. Messages sent through the port's message queue contain a pointer and size information referring to the section object. This avoids the need to copy large messages. The sender places data into the shared section, and the receiver views them directly.</li>
        <li>The third technique uses APIs that read and write directly into a process's address space. ALPC provides functions and synchronization so that a server can access the data in a client. This technique is normally used by RPC to achieve higher performance for specific scenarios.</li>
        </ol>
        <p class="text-indent">The Win32 window manager uses its own form of message passing, which is independent of the executive ALPC facilities. When a client asks for a connection that uses window-manager messaging, the server sets up three objects: (1) a dedicated server thread to handle requests, (2) a 64-KB shared section object, and (3) an event-pair object. An <b><i>event-pair object</i></b> is a synchronization object used by the Win32 subsystem to provide notification when the client thread <a id="page_855"></a>has copied a message to the Win32 server, or vice versa. The section object is used to pass the messages, and the event-pair object provides synchronization.</p>
        <p class="text-indent">Window-manager messaging has several advantages:</p>
        <ul style="list-style-type:disc;">
        <li>The section object eliminates message copying, since it represents a region of shared memory.</li>
        <li>The event-pair object eliminates the overhead of using the port object to pass messages containing pointers and lengths.</li>
        <li>The dedicated server thread eliminates the overhead of determining which client thread is calling the server, since there is one server thread per client thread.</li>
        <li>The kernel gives scheduling preference to these dedicated server threads to improve performance.</li>
        </ul>
        <h4 class="subtitle"><a id="sec19.3.3.5"></a>19.3.3.5 I/O Manager</h4>
        <p class="text">The <b>I/O manager</b> is responsible for managing file systems, device drivers, and network drivers. It keeps track of which device drivers, filter drivers, and file systems are loaded, and it also manages buffers for I/O requests. It works with the VM manager to provide memory-mapped file I/O and controls the Windows cache manager, which handles caching for the entire I/O system. The I/O manager is fundamentally asynchronous, providing synchronous I/O by explicitly waiting for an I/O operation to complete. The I/O manager provides several models of asynchronous I/O completion, including setting of events, updating of a status variable in the calling process, delivery of APCs to initiating threads, and use of I/O completion ports, which allow a single thread to process I/O completions from many other threads.</p>
        <p class="text-indent">Device drivers are arranged in a list for each device (called a driver or I/O stack). A driver is represented in the system as a <b>driver object</b>. Because a single driver can operate on multiple devices, the drivers are represented in the I/O stack by a <b>device object</b>, which contains a link to the driver object. The I/O manager converts the requests it receives into a standard form called an <b>I/O request packet (IRP)</b>. It then forwards the IRP to the first driver in the targeted I/O stack for processing. After a driver processes the IRP, it calls the I/O manager either to forward the IRP to the next driver in the stack or, if all processing is finished, to complete the operation represented by the IRP.</p>
        <p class="text-indent">The I/O request may be completed in a context different from the one in which it was made. For example, if a driver is performing its part of an I/O operation and is forced to block for an extended time, it may queue the IRP to a worker thread to continue processing in the system context. In the original thread, the driver returns a status indicating that the I/O request is pending so that the thread can continue executing in parallel with the I/O operation. An IRP may also be processed in interrupt-service routines and completed in an arbitrary process context. Because some final processing may need to take place in the context that initiated the I/O, the I/O manager uses an APC to do final I/O-completion processing in the process context of the originating thread.</p>
        <p class="text-indent">The I/O stack model is very flexible. As a driver stack is built, various drivers have the opportunity to insert themselves into the stack as <b>filter drivers</b>. Filter drivers can examine and potentially modify each I/O operation. Mount <a id="page_856"></a>management, partition management, and disk striping and mirroring are all examples of functionality implemented using filter drivers that execute beneath the file system in the stack. File-system filter drivers execute above the file system and have been used to implement functionalities such as hierarchical storage management, single instancing of files for remote boot, and dynamic format conversion. Third parties also use file-system filter drivers to implement virus detection.</p>
        <p class="text-indent">Device drivers for Windows are written to the Windows Driver Model (WDM) specification. This model lays out all the requirements for device drivers, including how to layer filter drivers, share common code for handling power and plug-and-play requests, build correct cancellation logic, and so forth.</p>
        <p class="text-indent">Because of the richness of the WDM, writing a full WDM device driver for each new hardware device can involve a great deal of work. Fortunately, the port/miniport model makes it unnecessary to do this. Within a class of similar devices, such as audio drivers, SATA devices, or Ethernet controllers, each instance of a device shares a common driver for that class, called a <b>port driver</b>. The port driver implements the standard operations for the class and then calls device-specific routines in the device's <b>miniport driver</b> to implement device-specific functionality. The TCP/IP network stack is implemented in this way, with the <span class="inlinecode">ndis.sys</span> class driver implementing much of the network driver functionality and calling out to the network miniport drivers for specific hardware.</p>
        <p class="text-indent">Recent versions of Windows, including Windows 7, provide additional simplifications for writing device drivers for hardware devices. Kernel-mode drivers can now be written using the <b>Kernel-Mode Driver Framework (KMDF)</b>, which provides a simplified programming model for drivers on top of WDM. Another option is the <b>User-Mode Driver Framework (UMDF)</b>. Many drivers do not need to operate in kernel mode, and it is easier to develop and deploy drivers in user mode. It also makes the system more reliable, because a failure in a user-mode driver does not cause a kernel-mode crash.</p>
        <h4 class="subtitle"><a id="sec19.3.3.6"></a>19.3.3.6 Cache Manager</h4>
        <p class="text">In many operating systems, caching is done by the file system. Instead, Windows provides a centralized caching facility. The <b>cache manager</b> works closely with the VM manager to provide cache services for all components under the control of the I/O manager. Caching in Windows is based on files rather than raw blocks. The size of the cache changes dynamically according to how much free memory is available in the system. The cache manager maintains a private working set rather than sharing the system process's working set. The cache manager memory-maps files into kernel memory and then uses special interfaces to the VM manager to fault pages into or trim them from this private working set.</p>
        <p class="text-indent">The cache is divided into blocks of 256 KB. Each cache block can hold a view (that is, a memory-mapped region) of a file. Each cache block is described by a <b>virtual address control block (VACB)</b> that stores the virtual address and file offset for the view, as well as the number of processes using the view. The VACBs reside in a single array maintained by the cache manager.</p>
        <p class="text-indent">When the I/O manager receives a file's user-level read request, the I/O manager sends an IRP to the I/O stack for the volume on which the file resides. <a id="page_857"></a>For files that are marked as cacheable, the file system calls the cache manager to look up the requested data in its cached file views. The cache manager calculates which entry of that file's VACB index array corresponds to the byte offset of the request. The entry either points to the view in the cache or is invalid. If it is invalid, the cache manager allocates a cache block (and the corresponding entry in the VACB array) and maps the view into the cache block. The cache manager then attempts to copy data from the mapped file to the caller's buffer. If the copy succeeds, the operation is completed.</p>
        <p class="text-indent">If the copy fails, it does so because of a page fault, which causes the VM manager to send a noncached read request to the I/O manager. The I/O manager sends another request down the driver stack, this time requesting a paging operation, which bypasses the cache manager and reads the data from the file directly into the page allocated for the cache manager. Upon completion, the VACB is set to point at the page. The data, now in the cache, are copied to the caller's buffer, and the original I/O request is completed. <a href="31_chapter19.html#fig19.6">Figure 19.6</a> shows an overview of these operations.</p>
        <p class="text-indent">A kernel-level read operation is similar, except that the data can be accessed directly from the cache rather than being copied to a buffer in user space. To use file-system metadata (data structures that describe the file system), the kernel uses the cache manager's mapping interface to read the metadata. To modify the metadata, the file system uses the cache manager's pinning interface. <b>Pinning</b> a page locks the page into a physical-memory page frame so that the VM manager cannot move the page or page it out. After updating the metadata, the file system asks the cache manager to unpin the page. A modified page is marked dirty, and so the VM manager flushes the page to disk.</p>
        <p class="text-indent">To improve performance, the cache manager keeps a small history of read requests and from this history attempts to predict future requests. If the cache manager finds a pattern in the previous three requests, such as sequential access forward or backward, it prefetches data into the cache before the next <a id="page_858"></a>request is submitted by the application. In this way, the application may find its data already cached and not need to wait for disk I/O.</p>
        <p class="center"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781118063330/files/images/ch019-f006.jpg" alt="image" class="imgepub" width="508" height="322"></p>
        <p class="caption"><a id="fig19.6"></a><b>Figure 19.6</b> File I/O.</p>
        <p class="text-indent">The cache manager is also responsible for telling the VM manager to flush the contents of the cache. The cache manager's default behavior is write-back caching: it accumulates writes for 4 to 5 seconds and then wakes up the cache-writer thread. When write-through caching is needed, a process can set a flag when opening the file, or the process can call an explicit cache-flush function.</p>
        <p class="text-indent">A fast-writing process could potentially fill all the free cache pages before the cache-writer thread had a chance to wake up and flush the pages to disk. The cache writer prevents a process from flooding the system in the following way. When the amount of free cache memory becomes low, the cache manager temporarily blocks processes attempting to write data and wakes the cache writer thread to flush pages to disk. If the fast-writing process is actually a network redirector for a network file system, blocking it for too long could cause network transfers to time out and be retransmitted. This retransmission would waste network bandwidth. To prevent such waste, network redirectors can instruct the cache manager to limit the backlog of writes in the cache.</p>
        <p class="text-indent">Because a network file system needs to move data between a disk and the network interface, the cache manager also provides a DMA interface to move the data directly. Moving data directly avoids the need to copy data through an intermediate buffer.</p>
        <h4 class="subtitle"><a id="sec19.3.3.7"></a>19.3.3.7 Security Reference Monitor</h4>
        <p class="text">Centralizing management of system entities in the object manager enables Windows to use a uniform mechanism to perform run-time access validation and audit checks for every user-accessible entity in the system. Whenever a process opens a handle to an object, the <b>security reference monitor (SRM)</b> checks the process's security token and the object's access-control list to see whether the process has the necessary access rights.</p>
        <p class="text-indent">The SRM is also responsible for manipulating the privileges in security tokens. Special privileges are required for users to perform backup or restore operations on file systems, debug processes, and so forth. Tokens can also be marked as being restricted in their privileges so that they cannot access objects that are available to most users. Restricted tokens are used primarily to limit the damage that can be done by execution of untrusted code.</p>
        <p class="text-indent">The integrity level of the code executing in a process is also represented by a token. Integrity levels are a type of capability mechanism, as mentioned earlier. A process cannot modify an object with an integrity level higher than that of the code executing in the process, whatever other permissions have been granted. Integrity levels were introduced to make it harder for code that successfully attacks outward-facing software, like Internet Explorer, to take over a system.</p>
        <p class="text-indent">Another responsibility of the SRM is logging security audit events. The Department of Defense's <b>Common Criteria</b> (the 2005 successor to the Orange Book) requires that a secure system have the ability to detect and log all attempts to access system resources so that it can more easily trace attempts at unauthorized access. Because the SRM is responsible for making access checks, it generates most of the audit records in the security-event log.</p>
        <h4 class="subtitle"><a id="sec19.3.3.8"></a>19.3.3.8 Plug-and-Play Manager</h4>
        <p class="text"><a id="page_859"></a>The operating system uses the <b>plug-and-play (PnP)</b> manager to recognize and adapt to changes in the hardware configuration. PnP devices use standard protocols to identify themselves to the system. The PnP manager automatically recognizes installed devices and detects changes in devices as the system operates. The manager also keeps track of hardware resources used by a device, as well as potential resources that could be used, and takes care of loading the appropriate drivers. This management of hardware resources—primarily interrupts and I/O memory ranges—has the goal of determining a hardware configuration in which all devices are able to operate successfully.</p>
        <p class="text-indent">The PnP manager handles dynamic reconfiguration as follows. First, it gets a list of devices from each bus driver (for example, PCI or USB). It loads the installed driver (after finding one, if necessary) and sends an <span class="inlinecode">add-device</span> request to the appropriate driver for each device. The PnP manager then figures out the optimal resource assignments and sends a <span class="inlinecode">start-device</span> request to each driver specifying the resource assignments for the device. If a device needs to be reconfigured, the PnP manager sends a <span class="inlinecode">query-stop</span> request, which asks the driver whether the device can be temporarily disabled. If the driver can disable the device, then all pending operations are completed, and new operations are prevented from starting. Finally, the PnP manager sends a <span class="inlinecode">stop</span> request and can then reconfigure the device with a new <span class="inlinecode">start-device</span> request.</p>
        <p class="text-indent">The PnP manager also supports other requests. For example, <span class="inlinecode">query-remove</span>, which operates similarly to <span class="inlinecode">query-stop</span>, is employed when a user is getting ready to eject a removable device, such as a USB storage device. The <span class="inlinecode">surprise-remove</span> request is used when a device fails or, more likely, when a user removes a device without telling the system to stop it first. Finally, the remove request tells the driver to stop using a device permanently.</p>
        <p class="text-indent">Many programs in the system are interested in the addition or removal of devices, so the PnP manager supports notifications. Such a notification, for example, gives GUI file menus the information they need to update their list of disk volumes when a new storage device is attached or removed. Installing devices often results in adding new services to the <span class="inlinecode">svchost.exe</span> processes in the system. These services frequently set themselves up to run whenever the system boots and continue to run even if the original device is never plugged into the system. Windows 7 introduced a <b>service-trigger</b> mechanism in the <b>service control manager (SCM)</b>, which manages the system services. With this mechanism, services can register themselves to start only when SCM receives a notification from the PnP manager that the device of interest has been added to the system.</p>
        <h4 class="subtitle"><a id="sec19.3.3.9"></a>19.3.3.9 Power Manager</h4>
        <p class="text">Windows works with the hardware to implement sophisticated strategies for energy efficiency, as described in <a href="31_chapter19.html#sec19.2.8">Section 19.2.8</a>. The policies that drive these strategies are implemented by the <b>power manager</b>. The power manager detects current system conditions, such as the load on CPUs or I/O devices, and improves energy efficiency by reducing the performance and responsiveness of the system when need is low. The power manager can also put the entire system into a very efficient <b><i>sleep</i></b> mode and can even write all the contents of memory to disk and turn off the power to allow the system to go into <b><i>hibernation</i></b>.</p>
        <p class="text-indent"><a id="page_860"></a>The primary advantage of sleep is that the system can enter fairly quickly, perhaps just a few seconds after the lid closes on a laptop. The return from sleep is also fairly quick. The power is turned down low on the CPUs and I/O devices, but the memory continues to be powered enough that its contents are not lost.</p>
        <p class="text-indent">Hibernation takes considerably longer because the entire contents of memory must be transferred to disk before the system is turned off. However, the fact that the system is, in fact, turned off is a significant advantage. If there is a loss of power to the system, as when the battery is swapped on a laptop or a desktop system is unplugged, the saved system data will not be lost. Unlike shutdown, hibernation saves the currently running system so a user can resume where she left off, and because hibernation does not require power, a system can remain in hibernation indefinitely.</p>
        <p class="text-indent">Like the PnP manager, the power manager provides notifications to the rest of the system about changes in the power state. Some applications want to know when the system is about to be shut down so they can start saving their states to disk.</p>
        <h4 class="subtitle"><a id="sec19.3.3.10"></a>19.3.3.10 Registry</h4>
        <p class="text">Windows keeps much of its configuration information in internal databases, called <b>hives</b>, that are managed by the Windows configuration manager, which is commonly known as the <b>registry</b>. There are separate hives for system information, default user preferences, software installation, security, and boot options. Because the information in the <b>system hive</b> is required to boot the system, the registry manager is implemented as a component of the executive.</p>
        <p class="text-indent">The registry represents the configuration state in each hive as a hierarchical namespace of keys (directories), each of which can contain a set of typed values, such as UNICODE string, ANSI string, integer, or untyped binary data. In theory, new keys and values are created and initialized as new software is installed; then they are modified to reflect changes in the configuration of that software. In practice, the registry is often used as a general-purpose database, as an interprocess-communication mechanism, and for many other such inventive purposes.</p>
        <p class="text-indent">Restarting applications, or even the system, every time a configuration change was made would be a nuisance. Instead, programs rely on various kinds of notifications, such as those provided by the PnP and power managers, to learn about changes in the system configuration. The registry also supplies notifications; it allows threads to register to be notified when changes are made to some part of the registry. The threads can thus detect and adapt to configuration changes recorded in the registry itself.</p>
        <p class="text-indent">Whenever significant changes are made to the system, such as when updates to the operating system or drivers are installed, there is a danger that the configuration data may be corrupted (for example, if a working driver is replaced by a nonworking driver or an application fails to install correctly and leaves partial information in the registry). Windows creates a <b>system restore point</b> before making such changes. The restore point contains a copy of the hives before the change and can be used to return to this version of the hives and thereby get a corrupted system working again.</p>
        <p class="text-indent"><a id="page_861"></a>To improve the stability of the registry configuration, Windows added a transaction mechanism beginning with Windows Vista that can be used to prevent the registry from being partially updated with a collection of related configuration changes. Registry transactions can be part of more general transactions administered by the <b>kernel transaction manager (KTM)</b>, which can also include file-system transactions. KTM transactions do not have the full semantics found in normal database transactions, and they have not supplanted the system restore facility for recovering from damage to the registry configuration caused by software installation.</p>
        <h4 class="subtitle"><a id="sec19.3.3.11"></a>19.3.3.11 Booting</h4>
        <p class="text">The booting of a Windows PC begins when the hardware powers on and firmware begins executing from ROM. In older machines, this firmware was known as the BIOS, but more modern systems use UEFI (the Unified Extensible Firmware Interface), which is faster and more general and makes better use of the facilities in contemporary processors. The firmware runs <b>power-on self-test</b> <b>(POST)</b> diagnostics; identifies many of the devices attached to the system and initializes them to a clean, power-up state; and then builds the description used by the <b>advanced configuration and power interface (ACPI)</b>. Next, the firmware finds the system disk, loads the Windows <span class="inlinecode">bootmgr</span> program, and begins executing it.</p>
        <p class="text-indent">In a machine that has been hibernating, the <span class="inlinecode">winresume</span> program is loaded next. It restores the running system from disk, and the system continues execution at the point it had reached right before hibernating. In a machine that has been shut down, the <span class="inlinecode">bootmgr</span> performs further initialization of the system and then loads <span class="inlinecode">winload.</span> This program loads <span class="inlinecode">hal.dll,</span> the kernel (<span class="inlinecode">ntoskrnl.exe</span>), any drivers needed in booting, and the system hive. <span class="inlinecode">winload</span> then transfers execution to the kernel.</p>
        <p class="text-indent">The kernel initializes itself and creates two processes. The <b>system process</b> contains all the internal kernel worker threads and never executes in user mode. The first user-mode process created is SMSS, for <b><i>session manager subsystem</i></b>, which is similar to the INIT (initialization) process in UNIX. SMSS performs further initialization of the system, including establishing the paging files, loading more device drivers, and managing the Windows sessions. Each session is used to represent a logged-on user, except for <b><i>session 0</i></b>, which is used to run system-wide background services, such as LSASS and SERVICES. A session is anchored by an instance of the CSRSS process. Each session other than 0 initially runs the WINLOGON process. This process logs on a user and then launches the EXPLORER process, which implements the Windows GUI experience. The following list itemizes some of these aspects of booting:</p>
        <ul style="list-style-type:disc;">
        <li>SMSS completes system initialization and then starts up session 0 and the first login session.</li>
        <li>WININIT runs in session 0 to initialize user mode and start LSASS, SERVICES, and the local session manager, LSM.</li>
        <li>LSASS, the security subsystem, implements facilities such as authentication of users.</li>
        <li><a id="page_862"></a>SERVICES contains the service control manager, or SCM, which supervises all background activities in the system, including user-mode services. A number of services will have registered to start when the system boots. Others will be started only on demand or when triggered by an event such as the arrival of a device.</li>
        <li>CSRSS is the Win32 environmental subsystem process. It is started in every session—unlike the POSIX subsystem, which is started only on demand when a POSIX process is created.</li>
        <li>WINLOGON is run in each Windows session other than session 0 to log on a user.</li>
        </ul>
        <p class="text-indent">The system optimizes the boot process by prepaging from files on disk based on previous boots of the system. Disk access patterns at boot are also used to lay out system files on disk to reduce the number of I/O operations required. The processes necessary to start the system are reduced by grouping services into fewer processes. All of these approaches contribute to a dramatic reduction in system boot time. Of course, system boot time is less important than it once was because of the sleep and hibernation capabilities of Windows.</p>
        <h2 class="subtitle"><a id="sec19.4"></a>19.4 Terminal Services and Fast User Switching</h2>
        <p class="text">Windows supports a GUI-based console that interfaces with the user via keyboard, mouse, and display. Most systems also support audio and video. Audio input is used by Windows voice-recognition software; voice recognition makes the system more convenient and increases its accessibility for users with disabilities. Windows 7 added support for <b>multi-touch hardware</b>, allowing users to input data by touching the screen and making gestures with one or more fingers. Eventually, the video-input capability, which is currently used for communication applications, is likely to be used for visually interpreting gestures, as Microsoft has demonstrated for its Xbox 360 Kinect product. Other future input experiences may evolve from Microsoft's <b>surface computer</b>. Most often installed at public venues, such as hotels and conference centers, the surface computer is a table surface with special cameras underneath. It can track the actions of multiple users at once and recognize objects that are placed on top.</p>
        <p class="text-indent">The PC was, of course, envisioned as a <b><i>personal computer</i></b>—an inherently single-user machine. Modern Windows, however, supports the sharing of a PC among multiple users. Each user that is logged on using the GUI has a <b>session</b> created to represent the GUI environment he will be using and to contain all the processes created to run his applications. Windows allows multiple sessions to exist at the same time on a single machine. However, Windows only supports a single console, consisting of all the monitors, keyboards, and mice connected to the PC. Only one session can be connected to the console at a time. From the logon screen displayed on the console, users can create new sessions or attach to an existing session that was previously created. This allows multiple users to share a single PC without having to log off and on between users. Microsoft calls this use of sessions <b><i>fast user switching</i></b>.</p>
        <p class="text-indent"><a id="page_863"></a>Users can also create new sessions, or connect to existing sessions, on one PC from a session running on another Windows PC. The terminal server (TS) connects one of the GUI windows in a user's local session to the new or existing session, called a <b>remote desktop</b>, on the remote computer. The most common use of remote desktops is for users to connect to a session on their work PC from their home PC.</p>
        <p class="text-indent">Many corporations use corporate terminal-server systems maintained in data centers to run all user sessions that access corporate resources, rather than allowing users to access those resources from the PCs in each user's office. Each server computer may handle many dozens of remote-desktop sessions. This is a form of <b>thin-client</b> computing, in which individual computers rely on a server for many functions. Relying on data-center terminal servers improves reliability, manageability, and security of the corporate computing resources.</p>
        <p class="text-indent">The TS is also used by Windows to implement <b>remote assistance</b>. A remote user can be invited to share a session with the user logged on to the session on the console. The remote user can watch the user's actions and even be given control of the desktop to help resolve computing problems.</p>
        <h2 class="subtitle"><a id="sec19.5"></a>19.5 File System</h2>
        <p class="text">The native file system in Windows is NTFS. It is used for all local volumes. However, associated USB thumb drives, flash memory on cameras, and external disks may be formatted with the 32-bit FAT file system for portability. FAT is a much older file-system format that is understood by many systems besides Windows, such as the software running on cameras. A disadvantage is that the FAT file system does not restrict file access to authorized users. The only solution for securing data with FAT is to run an application to encrypt the data before storing it on the file system.</p>
        <p class="text-indent">In contrast, NTFS uses ACLs to control access to individual files and supports implicit encryption of individual files or entire volumes (using Windows BitLocker feature). NTFS implements many other features as well, including data recovery, fault tolerance, very large files and file systems, multiple data streams, UNICODE names, sparse files, journaling, volume shadow copies, and file compression.</p>
        <h3 class="subtitle"><a id="sec19.5.1"></a>19.5.1 NTFS Internal Layout</h3>
        <p class="text">The fundamental entity in NTFS is a volume. A volume is created by the Windows logical disk management utility and is based on a logical disk partition. A volume may occupy a portion of a disk or an entire disk, or may span several disks.</p>
        <p class="text-indent">NTFS does not deal with individual sectors of a disk but instead uses clusters as the units of disk allocation. A <b>cluster</b> is a number of disk sectors that is a power of 2. The cluster size is configured when an NTFS file system is formatted. The default cluster size is based on the volume size—4 KB for volumes larger than 2 GB. Given the size of today's disks, it may make sense to use cluster sizes larger than the Windows defaults to achieve better performance, although these performance gains will come at the expense of more internal fragmentation.</p>
        <p class="text-indent">NTFS uses <b>logical cluster numbers (LCNs)</b> as disk addresses. It assigns them by numbering clusters from the beginning of the disk to the end. Using this <a id="page_864"></a>scheme, the system can calculate a physical disk offset (in bytes) by multiplying the LCN by the cluster size.</p>
        <p class="text-indent">A file in NTFS is not a simple byte stream as it is in UNIX; rather, it is a structured object consisting of typed <b>attributes</b>. Each attribute of a file is an independent byte stream that can be created, deleted, read, and written. Some attribute types are standard for all files, including the file name (or names, if the file has aliases, such as an MS-DOS short name), the creation time, and the security descriptor that specifies the access control list. User data are stored in <b><i>data attributes</i></b>.</p>
        <p class="text-indent">Most traditional data files have an <b><i>unnamed</i></b> data attribute that contains all the file's data. However, additional data streams can be created with explicit names. For instance, in Macintosh files stored on a Windows server, the resource fork is a named data stream. The IProp interfaces of the Component Object Model (COM) use a named data stream to store properties on ordinary files, including thumbnails of images. In general, attributes may be added as necessary and are accessed using a <b><i>file-name:attribute</i></b> syntax. NTFS returns only the size of the unnamed attribute in response to file-query operations, such as when running the <span class="inlinecode">dir</span> command.</p>
        <p class="text-indent">Every file in NTFS is described by one or more records in an array stored in a special file called the master file table (MFT). The size of a record is determined when the file system is created; it ranges from 1 to 4 KB. Small attributes are stored in the MFT record itself and are called <b><i>resident attributes</i></b>. Large attributes, such as the unnamed bulk data, are called <b><i>nonresident attributes</i></b> and are stored in one or more contiguous <b>extents</b> on the disk. A pointer to each extent is stored in the MFT record. For a small file, even the data attribute may fit inside the MFT record. If a file has many attributes—or if it is highly fragmented, so that many pointers are needed to point to all the fragments—one record in the MFT might not be large enough. In this case, the file is described by a record called the <b>base file record</b>, which contains pointers to overflow records that hold the additional pointers and attributes.</p>
        <p class="text-indent">Each file in an NTFS volume has a unique ID called a <b>file reference</b>. The file reference is a 64-bit quantity that consists of a 48-bit file number and a 16-bit sequence number. The file number is the record number (that is, the array slot) in the MFT that describes the file. The sequence number is incremented every time an MFT entry is reused. The sequence number enables NTFS to perform internal consistency checks, such as catching a stale reference to a deleted file after the MFT entry has been reused for a new file.</p>
        <h4 class="subtitle"><a id="sec19.5.1.1"></a>19.5.1.1 NTFS B+ Tree</h4>
        <p class="text">As in UNIX, the NTFS namespace is organized as a hierarchy of directories. Each directory uses a data structure called a <b>B+ tree</b> to store an index of the file names in that directory. In a B+ tree, the length of every path from the root of the tree to a leaf is the same, and the cost of reorganizing the tree is eliminated. The <b>index root</b> of a directory contains the top level of the B+ tree. For a large directory, this top level contains pointers to disk extents that hold the remainder of the tree. Each entry in the directory contains the name and file reference of the file, as well as a copy of the update timestamp and file size taken from the file's resident attributes in the MFT. Copies of this information are stored in the directory so that a directory listing can be efficiently generated. Because all the file names, sizes, and update times are available from the directory itself, there is no need to gather these attributes from the MFT entries for each of the files.</p>
        <h4 class="subtitle"><a id="sec19.5.1.2"></a>19.5.1.2 NTFS Metadata</h4>
        <p class="text"><a id="page_865"></a>The NTFS volume's metadata are all stored in files. The first file is the MFT. The second file, which is used during recovery if the MFT is damaged, contains a copy of the first 16 entries of the MFT. The next few files are also special in purpose. They include the files described below.</p>
        <ul style="list-style-type:disc;">
        <li>The <b>log file</b> records all metadata updates to the file system.</li>
        <li>The <b>volume file</b> contains the name of the volume, the version of NTFS that formatted the volume, and a bit that tells whether the volume may have been corrupted and needs to be checked for consistency using the <span class="inlinecode">chkdsk</span> program.</li>
        <li>The <b>attribute-definition table</b> indicates which attribute types are used in the volume and what operations can be performed on each of them.</li>
        <li>The <b>root directory</b> is the top-level directory in the file-system hierarchy.</li>
        <li>The <b>bitmap file</b> indicates which clusters on a volume are allocated to files and which are free.</li>
        <li>The <b>boot file</b> contains the startup code for Windows and must be located at a particular disk address so that it can be found easily by a simple ROM bootstrap loader. The boot file also contains the physical address of the MFT.</li>
        <li>The <b>bad-cluster file</b> keeps track of any bad areas on the volume; NTFS uses this record for error recovery.</li>
        </ul>
        <p class="text-indent">Keeping all the NTFS metadata in actual files has a useful property. As discussed in <a href="31_chapter19.html#sec19.3.3.6">Section 19.3.3.6</a>, the cache manager caches file data. Since all the NTFS metadata reside in files, these data can be cached using the same mechanisms used for ordinary data.</p>
        <h3 class="subtitle"><a id="sec19.5.2"></a>19.5.2 Recovery</h3>
        <p class="text">In many simple file systems, a power failure at the wrong time can damage the file-system data structures so severely that the entire volume is scrambled. Many UNIX file systems, including UFS but not ZFS, store redundant metadata on the disk, and they recover from crashes by using the <span class="inlinecode">fsck</span> program to check all the file-system data structures and restore them forcibly to a consistent state. Restoring them often involves deleting damaged files and freeing data clusters that had been written with user data but not properly recorded in the file system's metadata structures. This checking can be a slow process and can cause the loss of significant amounts of data.</p>
        <p class="text-indent">NTFS takes a different approach to file-system robustness. In NTFS, all file-system data-structure updates are performed inside transactions. Before a data structure is altered, the transaction writes a log record that contains redo and undo information. After the data structure has been changed, the transaction writes a commit record to the log to signify that the transaction succeeded.</p>
        <p class="text-indent">After a crash, the system can restore the file-system data structures to a consistent state by processing the log records, first redoing the operations for committed transactions and then undoing the operations for transactions <a id="page_866"></a>that did not commit successfully before the crash. Periodically (usually every 5 seconds), a checkpoint record is written to the log. The system does not need log records prior to the checkpoint to recover from a crash. They can be discarded, so the log file does not grow without bounds. The first time after system startup that an NTFS volume is accessed, NTFS automatically performs file-system recovery.</p>
        <p class="text-indent">This scheme does not guarantee that all the user-file contents are correct after a crash. It ensures only that the file-system data structures (the metadata files) are undamaged and reflect some consistent state that existed prior to the crash. It would be possible to extend the transaction scheme to cover user files, and Microsoft took some steps to do this in Windows Vista.</p>
        <p class="text-indent">The log is stored in the third metadata file at the beginning of the volume. It is created with a fixed maximum size when the file system is formatted. It has two sections: the <b><i>logging area</i></b>, which is a circular queue of log records, and the <b><i>restart area</i></b>, which holds context information, such as the position in the logging area where NTFS should start reading during a recovery. In fact, the restart area holds two copies of its information, so recovery is still possible if one copy is damaged during the crash.</p>
        <p class="text-indent">The logging functionality is provided by the <b><i>log-file service</i></b>. In addition to writing the log records and performing recovery actions, the log-file service keeps track of the free space in the log file. If the free space gets too low, the log-file service queues pending transactions, and NTFS halts all new I/O operations. After the in-progress operations complete, NTFS calls the cache manager to flush all data and then resets the log file and performs the queued transactions.</p>
        <h3 class="subtitle"><a id="sec19.5.3"></a>19.5.3 Security</h3>
        <p class="text">The security of an NTFS volume is derived from the Windows object model. Each NTFS file references a security descriptor, which specifies the owner of the file, and an access-control list, which contains the access permissions granted or denied to each user or group listed. Early versions of NTFS used a separate security descriptor as an attribute of each file. Beginning with Windows 2000, the security-descriptors attribute points to a shared copy, with a significant savings in disk and caching space; many, many files have identical security descriptors.</p>
        <p class="text-indent">In normal operation, NTFS does not enforce permissions on traversal of directories in file path names. However, for compatibility with POSIX, these checks can be enabled. Traversal checks are inherently more expensive, since modern parsing of file path names uses prefix matching rather than directory-by-directory parsing of path names. Prefix matching is an algorithm that looks up strings in a cache and finds the entry with the longest match—for example, an entry for <span class="inlinecode">\foo\bar\dir</span> would be a match for <span class="inlinecode">\foo\bar\dir2\dir3\myfile</span>. The prefix-matching cache allows path-name traversal to begin much deeper in the tree, saving many steps. Enforcing traversal checks means that the user's access must be checked at each directory level. For instance, a user might lack permission to traverse <span class="inlinecode">\foo\bar</span>, so starting at the access for <span class="inlinecode">\foo\bar\dir</span> would be an error.</p>
        <h3 class="subtitle"><a id="sec19.5.4"></a>19.5.4 Volume Management and Fault Tolerance</h3>
        <p class="text"><a id="page_867"></a><span class="inlinecode">FtDisk</span> is the fault-tolerant disk driver for Windows. When installed, it provides several ways to combine multiple disk drives into one logical volume so as to improve performance, capacity, or reliability.</p>
        <h4 class="subtitle"><a id="sec19.5.4.1"></a>19.5.4.1 Volume Sets and RAID Sets</h4>
        <p class="text">One way to combine multiple disks is to concatenate them logically to form a large logical volume, as shown in <a href="31_chapter19.html#fig19.7">Figure 19.7</a>. In Windows, this logical volume, called a <b>volume set</b>, can consist of up to 32 physical partitions. A volume set that contains an NTFS volume can be extended without disturbance of the data already stored in the file system. The bitmap metadata on the NTFS volume are simply extended to cover the newly added space. NTFS continues to use the same LCN mechanism that it uses for a single physical disk, and the <span class="inlinecode">FtDisk</span> driver supplies the mapping from a logical-volume offset to the offset on one particular disk.</p>
        <p class="text-indent">Another way to combine multiple physical partitions is to interleave their blocks in round-robin fashion to form a <b>stripe set</b>. This scheme is also called RAID level 0, or <b>disk striping</b>. (For more on RAID (redundant arrays of inexpensive disks), see <a href="19_chapter10.html#sec10.7">Section 10.7</a>.) <span class="inlinecode">FtDisk</span> uses a stripe size of 64 KB. The first 64 KB of the logical volume are stored in the first physical partition, the second 64 KB in the second physical partition, and so on, until each partition has contributed 64 KB of space. Then, the allocation wraps around to the first disk, allocating the second 64-KB block. A stripe set forms one large logical volume, but the physical layout can improve the I/O bandwidth, because for a large I/O, all the disks can transfer data in parallel. Windows also supports RAID level 5, stripe set with parity, and RAID level 1, mirroring.</p>
        <p class="center"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781118063330/files/images/ch019-f007.jpg" alt="image" class="imgepub" width="569" height="343"></p>
        <p class="caption"><a id="fig19.7"></a><b>Figure 19.7</b> Volume set on two drives.</p>
        <h4 class="subtitle"><a id="sec19.5.4.2"></a>19.5.4.2 Sector Sparing and Cluster Remapping</h4>
        <p class="text"><a id="page_868"></a>To deal with disk sectors that go bad, <span class="inlinecode">FtDisk</span> uses a hardware technique called sector sparing, and NTFS uses a software technique called cluster remapping. <b>Sector sparing</b> is a hardware capability provided by many disk drives. When a disk drive is formatted, it creates a map from logical block numbers to good sectors on the disk. It also leaves extra sectors unmapped, as spares. If a sector fails, <span class="inlinecode">FtDisk</span> instructs the disk drive to substitute a spare. <b>Cluster remapping</b> is a software technique performed by the file system. If a disk block goes bad, NTFS substitutes a different, unallocated block by changing any affected pointers in the MFT. NTFS also makes a note that the bad block should never be allocated to any file.</p>
        <p class="text-indent">When a disk block goes bad, the usual outcome is a data loss. But sector sparing or cluster remapping can be combined with fault-tolerant volumes to mask the failure of a disk block. If a read fails, the system reconstructs the missing data by reading the mirror or by calculating the <span class="inlinecode">exclusive or</span> parity in a stripe set with parity. The reconstructed data are stored in a new location that is obtained by sector sparing or cluster remapping.</p>
        <h3 class="subtitle"><a id="sec19.5.5"></a>19.5.5 Compression</h3>
        <p class="text">NTFS can perform data compression on individual files or on all data files in a directory. To compress a file, NTFS divides the file's data into <b>compression units</b>, which are blocks of 16 contiguous clusters. When a compression unit is written, a data-compression algorithm is applied. If the result fits into fewer than 16 clusters, the compressed version is stored. When reading, NTFS can determine whether data have been compressed: if they have been, the length of the stored compression unit is less than 16 clusters. To improve performance when reading contiguous compression units, NTFS prefetches and decompresses ahead of the application requests.</p>
        <p class="text-indent">For sparse files or files that contain mostly zeros, NTFS uses another technique to save space. Clusters that contain only zeros because they have never been written are not actually allocated or stored on disk. Instead, gaps are left in the sequence of virtual-cluster numbers stored in the MFT entry for the file. When reading a file, if NTFS finds a gap in the virtual-cluster numbers, it just zero-fills that portion of the caller's buffer. This technique is also used by UNIX.</p>
        <h3 class="subtitle"><a id="sec19.5.6"></a>19.5.6 Mount Points, Symbolic Links, and Hard Links</h3>
        <p class="text">Mount points are a form of symbolic link specific to directories on NTFS that were introduced in Windows 2000. They provide a mechanism for organizing disk volumes that is more flexible than the use of global names (like drive letters). A mount point is implemented as a symbolic link with associated data that contains the true volume name. Ultimately, mount points will supplant drive letters completely, but there will be a long transition due to the dependence of many applications on the drive-letter scheme.</p>
        <p class="text-indent">Windows Vista introduced support for a more general form of symbolic links, similar to those found in UNIX. The links can be absolute or relative, can point to objects that do not exist, and can point to both files and directories <a id="page_869"></a>even across volumes. NTFS also supports <b>hard links</b>, where a single file has an entry in more than one directory of the same volume.</p>
        <h3 class="subtitle"><a id="sec19.5.7"></a>19.5.7 Change Journal</h3>
        <p class="text">NTFS keeps a journal describing all changes that have been made to the file system. User-mode services can receive notifications of changes to the journal and then identify what files have changed by reading from the journal. The search indexer service uses the change journal to identify files that need to be re-indexed. The file-replication service uses it to identify files that need to be replicated across the network.</p>
        <h3 class="subtitle"><a id="sec19.5.8"></a>19.5.8 Volume Shadow Copies</h3>
        <p class="text">Windows implements the capability of bringing a volume to a known state and then creating a shadow copy that can be used to back up a consistent view of the volume. This technique is known as <b><i>snapshots</i></b> in some other file systems. Making a shadow copy of a volume is a form of copy-on-write, where blocks modified after the shadow copy is created are stored in their original form in the copy. To achieve a consistent state for the volume requires the cooperation of applications, since the system cannot know when the data used by the application are in a stable state from which the application could be safely restarted.</p>
        <p class="text-indent">The server version of Windows uses shadow copies to efficiently maintain old versions of files stored on file servers. This allows users to see documents stored on file servers as they existed at earlier points in time. The user can use this feature to recover files that were accidentally deleted or simply to look at a previous version of the file, all without pulling out backup media.</p>
        <h2 class="subtitle"><a id="sec19.6"></a>19.6 Networking</h2>
        <p class="text">Windows supports both peer-to-peer and client–server networking. It also has facilities for network management. The networking components in Windows provide data transport, interprocess communication, file sharing across a network, and the ability to send print jobs to remote printers.</p>
        <h3 class="subtitle"><a id="sec19.6.1"></a>19.6.1 Network Interfaces</h3>
        <p class="text">To describe networking in Windows, we must first mention two of the internal networking interfaces: the <b>network device interface specification (NDIS)</b> and the <b>transport driver interface (TDI)</b>. The NDIS interface was developed in 1989 by Microsoft and 3Com to separate network adapters from transport protocols so that either could be changed without affecting the other. NDIS resides at the interface between the data-link and network layers in the ISO model and enables many protocols to operate over many different network adapters. In terms of the ISO model, the TDI is the interface between the transport layer (layer 4) and the session layer (layer 5). This interface enables any session-layer component to use any available transport mechanism. (Similar reasoning led to the streams mechanism in UNIX.) The TDI supports both connection-based and connectionless transport and has functions to send any type of data.</p>
        <h3 class="subtitle"><a id="sec19.6.2"></a>19.6.2 Protocols</h3>
        <p class="text"><a id="page_870"></a>Windows implements transport protocols as drivers. These drivers can be loaded and unloaded from the system dynamically, although in practice the system typically has to be rebooted after a change. Windows comes with several networking protocols. Next, we discuss a number of these protocols.</p>
        <h4 class="subtitle"><a id="sec19.6.2.1"></a>19.6.2.1 Server-Message Block</h4>
        <p class="text">The <b>server-message-block (SMB)</b> protocol was first introduced in MS-DOS 3.1. The system uses the protocol to send I/O requests over the network. The SMB protocol has four message types. <span class="inlinecode">Session control</span> messages are commands that start and end a redirector connection to a shared resource at the server. A redirector uses <span class="inlinecode">File</span> messages to access files at the server. <span class="inlinecode">Printer</span> messages are used to send data to a remote print queue and to receive status information from the queue, and <span class="inlinecode">Message</span> messages are used to communicate with another workstation. A version of the SMB protocol was published as the <b>common Internet file system (CIFS)</b> and is supported on a number of operating systems.</p>
        <h4 class="subtitle"><a id="sec19.6.2.2"></a>19.6.2.2 Transmission Control Protocol/Internet Protocol</h4>
        <p class="text">The transmission control protocol/Internet protocol (TCP/IP) suite that is used on the Internet has become the de facto standard networking infrastructure. Windows uses TCP/IP to connect to a wide variety of operating systems and hardware platforms. The Windows TCP/IP package includes the simple network-management protocol (SNM), the dynamic host-configuration protocol (DHCP), and the older Windows Internet name service (WINS). Windows Vista introduced a new implementation of TCP/IP that supports both <span class="inlinecode">IPv4</span> and <span class="inlinecode">IPv6</span> in the same network stack. This new implementation also supports offloading of the network stack onto advanced hardware, to achieve very high performance for servers.</p>
        <p class="text-indent">Windows provides a software firewall that limits the TCP ports that can be used by programs for network communication. Network firewalls are commonly implemented in routers and are a very important security measure. Having a firewall built into the operating system makes a hardware router unnecessary, and it also provides more integrated management and easier use.</p>
        <h4 class="subtitle"><a id="sec19.6.2.3"></a>19.6.2.3 Point-to-Point Tunneling Protocol</h4>
        <p class="text">The <b>point-to-point tunneling protocol (PPTP)</b> is a protocol provided by Windows to communicate between remote-access server modules running on Windows server machines and other client systems that are connected over the Internet. The remote-access servers can encrypt data sent over the connection, and they support multiprotocol <b>virtual private networks (VPNs)</b> over the Internet.</p>
        <h4 class="subtitle"><a id="sec19.6.2.4"></a>19.6.2.4 HTTP Protocol</h4>
        <p class="text">The HTTP protocol is used to <span class="inlinecode">get/put</span> information using the World Wide Web. Windows implements HTTP using a kernel-mode driver, so web servers can operate with a low-overhead connection to the networking stack. HTTP is a <a id="page_871"></a>fairly general protocol, which Windows makes available as a transport option for implementing RPC.</p>
        <h4 class="subtitle"><a id="sec19.6.2.5"></a>19.6.2.5 Web-Distributed Authoring and Versioning Protocol</h4>
        <p class="text">Web-distributed authoring and versioning(WebDAV) is an HTTP-based protocol for collaborative authoring across a network. Windows builds a WebDAV redirector into the file system. Being built directly into the file system enables WebDAV to work with other file-system features, such as encryption. Personal files can then be stored securely in a public place. Because WebDAV uses HTTP, which is a <span class="inlinecode">get/put</span> protocol, Windows has to cache the files locally so programs can use <span class="inlinecode">read</span> and <span class="inlinecode">write</span> operations on parts of the files.</p>
        <h4 class="subtitle"><a id="sec19.6.2.6"></a>19.6.2.6 Named Pipes</h4>
        <p class="text"><b>Named pipes</b> are a connection-oriented messaging mechanism. A process can use named pipes to communicate with other processes on the same machine. Since named pipes are accessed through the file-system interface, the security mechanisms used for file objects also apply to named pipes. The SMB protocol supports named pipes, so named pipes can also be used for communication between processes on different systems.</p>
        <p class="text-indent">The format of pipe names follows the <b>uniform naming convention (UNC)</b>. A UNC name looks like a typical remote file name. The format is <span class="inlinecode">\\server_name\share_name\x\y\z,</span> where <span class="inlinecode">server_name</span> identifies a server on the network; <span class="inlinecode">share_name</span> identifies any resource that is made available to network users, such as directories, files, named pipes, and printers; and <span class="inlinecode">\x\y\z</span> is a normal file path name.</p>
        <h4 class="subtitle"><a id="sec19.6.2.7"></a>19.6.2.7 Remote Procedure Calls</h4>
        <p class="text">A remote procedure call (RPC) is a client–server mechanism that enables an application on one machine to make a procedure call to code on another machine. The client calls a local procedure—a stub routine—that packs its arguments into a message and sends them across the network to a particular server process. The client-side stub routine then blocks. Meanwhile, the server unpacks the message, calls the procedure, packs the return results into a message, and sends them back to the client stub. The client stub unblocks, receives the message, unpacks the results of the RPC, and returns them to the caller. This packing of arguments is sometimes called <b>marshaling</b>. The client stub code and the descriptors necessary to pack and unpack the arguments for an RPC are compiled from a specification written in the <b>Microsoft Interface Definition Language</b>.</p>
        <p class="text-indent">The Windows RPC mechanism follows the widely used distributed-computing-environment standard for RPC messages, so programs written to use Windows RPCs are highly portable. The RPC standard is detailed. It hides many of the architectural differences among computers, such as the sizes of binary numbers and the order of bytes and bits in computer words, by specifying standard data formats for RPC messages.</p>
        <h4 class="subtitle"><a id="sec19.6.2.8"></a>19.6.2.8 Component Object Model</h4>
        <p class="text"><a id="page_872"></a>The <b>component object model (COM)</b> is a mechanism for interprocess communication that was developed for Windows. COM objects provide a well-defined interface to manipulate the data in the object. For instance, COM is the infrastructure used by Microsoft's <b>object linking and embedding (OLE)</b> technology for inserting spreadsheets into Microsoft Word documents. Many Windows services provide COM interfaces. Windows has a distributed extension called <b>DCOM</b> that can be used over a network utilizing RPC to provide a transparent method of developing distributed applications.</p>
        <h3 class="subtitle"><a id="sec19.6.3"></a>19.6.3 Redirectors and Servers</h3>
        <p class="text">In Windows, an application can use the Windows I/O API to access files from a remote computer as though they were local, provided that the remote computer is running a CIFS server such as those provided by Windows. A <b>redirector</b> is the client-side object that forwards I/O requests to a remote system, where they are satisfied by a server. For performance and security, the redirectors and servers run in kernel mode.</p>
        <p class="text-indent">In more detail, access to a remote file occurs as follows:</p>
        <ol style="list-style-type:decimal;">
        <li>The application calls the I/O manager to request that a file be opened with a file name in the standard UNC format.</li>
        <li>The I/O manager builds an I/O request packet, as described in <a href="31_chapter19.html#sec19.3.3.5">Section 19.3.3.5</a>.</li>
        <li>The I/O manager recognizes that the access is for a remote file and calls a driver called a <b>multiple universal-naming-convention provider (MUP)</b>.</li>
        <li>The MUP sends the I/O request packet asynchronously to all registered redirectors.</li>
        <li>A redirector that can satisfy the request responds to the MUP. To avoid asking all the redirectors the same question in the future, the MUP uses a cache to remember which redirector can handle this file.</li>
        <li>The redirector sends the network request to the remote system.</li>
        <li>The remote-system network drivers receive the request and pass it to the server driver.</li>
        <li>The server driver hands the request to the proper local file-system driver.</li>
        <li>The proper device driver is called to access the data.</li>
        <li>The results are returned to the server driver, which sends the data back to the requesting redirector. The redirector then returns the data to the calling application via the I/O manager.</li>
        </ol>
        <p class="text">A similar process occurs for applications that use the Win32 network API, rather than the UNC services, except that a module called a <b><i>multi-provider router</i></b> is used instead of a MUP.</p>
        <p class="text-indent">For portability, redirectors and servers use the TDI API for network transport. The requests themselves are expressed in a higher-level protocol, <a id="page_873"></a>which by default is the SMB protocol described in <a href="31_chapter19.html#sec19.6.2">Section 19.6.2</a>. The list of redirectors is maintained in the system hive of the registry.</p>
        <h4 class="subtitle"><a id="sec19.6.3.1"></a>19.6.3.1 Distributed File System</h4>
        <p class="text">UNC names are not always convenient, because multiple file servers may be available to serve the same content and UNC names explicitly include the name of the server. Windows supports a <b>distributed file-system (DFS)</b> protocol that allows a network administrator to serve up files from multiple servers using a single distributed name space.</p>
        <h4 class="subtitle"><a id="sec19.6.3.2"></a>19.6.3.2 Folder Redirection and Client-Side Caching</h4>
        <p class="text">To improve the PC experience for users who frequently switch among computers, Windows allows administrators to give users <b>roaming profiles</b>, which keep users' preferences and other settings on servers. <b>Folder redirection</b> is then used to automatically store a user's documents and other files on a server.</p>
        <p class="text-indent">This works well until one of the computers is no longer attached to the network, as when a user takes a laptop onto an airplane. To give users off-line access to their redirected files, Windows uses <b>client-side caching (CSC)</b>. CSC is also used when the computer is on-line to keep copies of the server files on the local machine for better performance. The files are pushed up to the server as they are changed. If the computer becomes disconnected, the files are still available, and the update of the server is deferred until the next time the computer is online.</p>
        <h3 class="subtitle"><a id="sec19.6.4"></a>19.6.4 Domains</h3>
        <p class="text">Many networked environments have natural groups of users, such as students in a computer laboratory at school or employees in one department in a business. Frequently, we want all the members of the group to be able to access shared resources on their various computers in the group. To manage the global access rights within such groups, Windows uses the concept of a domain. Previously, these domains had no relationship whatsoever to the domain-name system (DNS) that maps Internet host names to IP addresses. Now, however, they are closely related.</p>
        <p class="text-indent">Specifically, a Windows domain is a group of Windows workstations and servers that share a common security policy and user database. Since Windows uses the Kerberos protocol for trust and authentication, a Windows domain is the same thing as a Kerberos realm. Windows uses a hierarchical approach for establishing trust relationships between related domains. The trust relationships are based on DNS and allow transitive trusts that can flow up and down the hierarchy. This approach reduces the number of trusts required for <i>n</i> domains from <i>n</i> * (<i>n</i> − 1) to <i>O</i>(<i>n</i>). The workstations in the domain trust the domain controller to give correct information about the access rights of each user (loaded into the user's access token by LSASS). All users retain the ability to restrict access to their own workstations, however, no matter what any domain controller may say.</p>
        <h3 class="subtitle"><a id="sec19.6.5"></a>19.6.5 Active Directory</h3>
        <p class="text"><a id="page_874"></a><b>Active Directory</b> is the Windows implementation of <b>lightweight directory-access protocol (LDAP)</b> services. Active Directory stores the topology information about the domain, keeps the domain-based user and group accounts and passwords, and provides a domain-based store for Windows features that need it, such as <b>Windows group policy</b>. Administrators use group policies to establish uniform standards for desktop preferences and software. For many corporate information-technology groups, uniformity drastically reduces the cost of computing.</p>
        <h2 class="subtitle"><a id="sec19.7"></a>19.7 Programmer Interface</h2>
        <p class="text">The <b>Win32 API</b> is the fundamental interface to the capabilities of Windows. This section describes five main aspects of the Win32 API: access to kernel objects, sharing of objects between processes, process management, interprocess communication, and memory management.</p>
        <h3 class="subtitle"><a id="sec19.7.1"></a>19.7.1 Access to Kernel Objects</h3>
        <p class="text">The Windows kernel provides many services that application programs can use. Application programs obtain these services by manipulating kernel objects. A process gains access to a kernel object named <span class="inlinecode">XXX</span> by calling the <span class="inlinecode">CreateXXX</span> function to open a handle to an instance of <span class="inlinecode">XXX</span>. This handle is unique to the process. Depending on which object is being opened, if the <span class="inlinecode">Create()</span> function fails, it may return 0, or it may return a special constant named <span class="inlinecode">INVALID_HANDLE_VALUE</span>. A process can close any handle by calling the <span class="inlinecode">CloseHandle()</span> function, and the system may delete the object if the count of handles referencing the object in all processes drops to zero.</p>
        <h3 class="subtitle"><a id="sec19.7.2"></a>19.7.2 Sharing Objects between Processes</h3>
        <p class="text">Windows provides three ways to share objects between processes. The first way is for a child process to inherit a handle to the object. When the parent calls the <span class="inlinecode">CreateXXX</span> function, the parent supplies a <span class="inlinecode">SECURITIES_ATTRIBUTES</span> structure with the <span class="inlinecode">bInheritHandle</span> field set to <span class="inlinecode">TRUE</span>. This field creates an inheritable handle. Next, the child process is created, passing a value of <span class="inlinecode">TRUE</span> to the <span class="inlinecode">CreateProcess()</span> function's <span class="inlinecode">bInheritHandle</span> argument. <a href="31_chapter19.html#fig19.8">Figure 19.8</a> shows a code sample that creates a semaphore handle inherited by a child process.</p>
        <p class="text-indent">Assuming the child process knows which handles are shared, the parent and child can achieve interprocess communication through the shared objects. In the example in <a href="31_chapter19.html#fig19.8">Figure 19.8</a>, the child process gets the value of the handle from the first command-line argument and then shares the semaphore with the parent process.</p>
        <p class="text-indent">The second way to share objects is for one process to give the object a name when the object is created and for the second process to open the name. This method has two drawbacks: Windows does not provide a way to check whether an object with the chosen name already exists, and the object name space is global, without regard to the object type. For instance, two applications <a id="page_875"></a>may create and share a single object named “foo” when two distinct objects—possibly of different types—were desired.</p>
        <p class="center"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781118063330/files/images/ch019-f008.jpg" alt="image" class="imgepub" width="569" height="212"></p>
        <p class="caption"><a id="fig19.8"></a><b>Figure 19.8</b> Code enabling a child to share an object by inheriting a handle.</p>
        <p class="text-indent">Named objects have the advantage that unrelated processes can readily share them. The first process calls one of the <span class="inlinecode">CreateXXX</span> functions and supplies a name as a parameter. The second process gets a handle to share the object by calling <span class="inlinecode">OpenXXX()</span> (or <span class="inlinecode">CreateXXX</span>) with the same name, as shown in the example in <a href="31_chapter19.html#fig19.9">Figure 19.9</a>.</p>
        <p class="text-indent">The third way to share objects is via the <span class="inlinecode">DuplicateHandle()</span> function. This method requires some other method of interprocess communication to pass the duplicated handle. Given a handle to a process and the value of a handle within that process, a second process can get a handle to the same object and thus share it. An example of this method is shown in <a href="31_chapter19.html#fig19.10">Figure 19.10</a>.</p>
        <h3 class="subtitle"><a id="sec19.7.3"></a>19.7.3 Process Management</h3>
        <p class="text">In Windows, a <b>process</b> is a loaded instance of an application and a <b>thread</b> is an executable unit of code that can be scheduled by the kernel dispatcher. Thus, a process contains one or more threads. A process is created when a thread in some other process calls the <span class="inlinecode">CreateProcess()</span> API. This routine loads any dynamic link libraries used by the process and creates an initial thread in the process. Additional threads can be created by the <span class="inlinecode">CreateThread()</span> function. Each thread is created with its own stack, which defaults to 1 MB unless otherwise specified in an argument to <span class="inlinecode">CreateThread()</span>.</p>
        <p class="center"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781118063330/files/images/ch019-f009.jpg" alt="image" class="imgepub" width="603" height="206"></p>
        <p class="caption"><a id="fig19.9"></a><b>Figure 19.9</b> Code for sharing an object by name lookup.</p>
        <p class="center"><a id="page_876"></a><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781118063330/files/images/ch019-f010.jpg" alt="image" class="imgepub" width="597" height="356"></p>
        <p class="caption"><a id="fig19.10"></a><b>Figure 19.10</b> Code for sharing an object by passing a handle.</p>
        <h4 class="subtitle"><a id="sec19.7.3.1"></a>19.7.3.1 Scheduling Rule</h4>
        <p class="text">Priorities in the Win32 environment are based on the native kernel (NT) scheduling model, but not all priority values may be chosen. The Win32 API uses four priority classes:</p>
        <ol style="list-style-type:decimal;">
        <li><span class="inlinecode">IDLE_PRIORITY_CLASS</span> (NT priority level 4)</li>
        <li><span class="inlinecode">NORMAL_PRIORITY_CLASS</span> (NT priority level 8)</li>
        <li><span class="inlinecode">HIGH_PRIORITY_CLASS</span> (NT priority level 13)</li>
        <li><span class="inlinecode">REALTIME_PRIORITY_CLASS</span> (NT priority level 24)</li>
        </ol>
        <p class="text">Processes are typically members of the <span class="inlinecode">NORMAL_PRIORITY_CLASS</span> unless the parent of the process was of the <span class="inlinecode">IDLE_PRIORITY_CLASS</span> or another class was specified when <span class="inlinecode">CreateProcess</span> was called. The priority class of a process is the default for all threads that execute in the process. It can be changed with the <span class="inlinecode">SetPriorityClass()</span> function or by passing an argument to the <span class="inlinecode">START</span> command. Only users with the <b><i>increase scheduling priority</i></b> privilege can move a process into the <span class="inlinecode">REALTIME_PRIORITY_CLASS</span>. Administrators and power users have this privilege by default.</p>
        <p class="text-indent">When a user is running an interactive process, the system needs to schedule the process's threads to provide good responsiveness. For this reason, Windows has a special scheduling rule for processes in the <span class="inlinecode">NORMAL_PRIORITY_CLASS</span>. Windows distinguishes between the process associated with the foreground window on the screen and the other (background) processes. When a process moves into the foreground, Windows increases the scheduling quantum for all its threads by a factor of 3; CPU-bound threads in the foreground process will run three times longer than similar threads in background processes.</p>
        <h4 class="subtitle"><a id="sec19.7.3.2"></a>19.7.3.2 Thread Priorities</h4>
        <p class="text"><a id="page_877"></a>A thread starts with an initial priority determined by its class. The priority can be altered by the <span class="inlinecode">SetThreadPriority()</span> function. This function takes an argument that specifies a priority relative to the base priority of its class:</p>
        <ul style="list-style-type:disc;">
        <li><span class="inlinecode">THREAD_PRIORITY_LOWEST</span>: base − 2</li>
        <li><span class="inlinecode">THREAD_PRIORITY_BELOW_NORMAL</span>: base − 1</li>
        <li><span class="inlinecode">THREAD_PRIORITY_NORMAL</span>: base + 0</li>
        <li><span class="inlinecode">THREAD_PRIORITY_ABOVE_NORMAL</span>: base + 1</li>
        <li><span class="inlinecode">THREAD_PRIORITY_HIGHEST</span>: base + 2</li>
        </ul>
        <p class="text-indent">Two other designations are also used to adjust the priority. Recall from <a href="31_chapter19.html#sec19.3.2.2">Section 19.3.2.2</a> that the kernel has two priority classes: 16–31 for the real-time class and 1–15 for the variable class. <span class="inlinecode">THREAD_PRIORITY_IDLE</span> sets the priority to 16 for real-time threads and to 1 for variable-priority threads. <span class="inlinecode">THREAD_PRIORITY_TIME_CRITICAL</span> sets the priority to 31 for real-time threads and to 15 for variable-priority threads.</p>
        <p class="text-indent">As discussed in <a href="31_chapter19.html#sec19.3.2.2">Section 19.3.2.2</a>, the kernel adjusts the priority of a variable class thread dynamically depending on whether the thread is I/O bound or CPU bound. The Win32 API provides a method to disable this adjustment via <span class="inlinecode">SetProcessPriorityBoost()</span> and <span class="inlinecode">SetThreadPriorityBoost()</span> functions.</p>
        <h4 class="subtitle"><a id="sec19.7.3.3"></a>19.7.3.3 Thread Suspend and Resume</h4>
        <p class="text">A thread can be created in a <b><i>suspended state</i></b> or can be placed in a suspended state later by use of the <span class="inlinecode">SuspendThread()</span> function. Before a suspended thread can be scheduled by the kernel dispatcher, it must be moved out of the suspended state by use of the <span class="inlinecode">ResumeThread()</span> function. Both functions set a counter so that if a thread is suspended twice, it must be resumed twice before it can run.</p>
        <h4 class="subtitle"><a id="sec19.7.3.4"></a>19.7.3.4 Thread Synchronization</h4>
        <p class="text">To synchronize concurrent access to shared objects by threads, the kernel provides synchronization objects, such as semaphores and mutexes. These are dispatcher objects, as discussed in <a href="31_chapter19.html#sec19.3.2.2">Section 19.3.2.2</a>. Threads can also synchronize with kernel services operating on kernel objects—such as threads, processes, and files—because these are also dispatcher objects. Synchronization with kernel dispatcher objects can be achieved by use of the <span class="inlinecode">WaitForSingleObject()</span> and <span class="inlinecode">WaitForMultipleObjects()</span> functions; these functions wait for one or more dispatcher objects to be signaled.</p>
        <p class="text-indent">Another method of synchronization is available to threads within the same process that want to execute code exclusively. The Win32 <b>critical section object</b> is a user-mode mutex object that can often be acquired and released without entering the kernel. On a multiprocessor, a Win32 critical section will attempt to spin while waiting for a critical section held by another thread to be released. If the spinning takes too long, the acquiring thread will allocate a kernel mutex and yield its CPU. Critical sections are particularly efficient because the kernel mutex is allocated only when there is contention and then used only after <a id="page_878"></a>attempting to spin. Most mutexes in programs are never actually contended, so the savings are significant.</p>
        <p class="text-indent">Before using a critical section, some thread in the process must call <span class="inlinecode">InitializeCriticalSection()</span>. Each thread that wants to acquire the mutex calls <span class="inlinecode">EnterCriticalSection()</span> and then later calls <span class="inlinecode">LeaveCriticalSection()</span> to release the mutex. There is also a <span class="inlinecode">TryEnterCriticalSection()</span> function, which attempts to acquire the mutex without blocking.</p>
        <p class="text-indent">For programs that want user-mode reader–writer locks rather than a mutex, Win32 supports <b>slim reader–writer</b> (<b>SRW</b>) <b>locks</b>. SRW locks have APIs similar to those for critical sections, such as <span class="inlinecode">InitializeSRWLock</span>, <span class="inlinecode">AcquireSRWLockXXX</span>, and <span class="inlinecode">ReleaseSRWLockXXX</span>, where <span class="inlinecode">XXX</span> is either <span class="inlinecode">Exclusive</span> or <span class="inlinecode">Shared</span>, depending on whether the thread wants write access or just read access to the object protected by the lock. The Win32 API also supports <b>condition variables</b>, which can be used with either critical sections or SRW locks.</p>
        <h4 class="subtitle"><a id="sec19.7.3.5"></a>19.7.3.5 Thread Pool</h4>
        <p class="text">Repeatedly creating and deleting threads can be expensive for applications and services that perform small amounts of work in each instantiation. The Win32 thread pool provides user-mode programs with three services: a queue to which work requests may be submitted (via the <span class="inlinecode">SubmitThreadpoolWork()</span> function), an API that can be used to bind callbacks to waitable handles (<span class="inlinecode">RegisterWaitForSingleObject()</span>), and APIs to work with timers (<span class="inlinecode">CreateThreadpoolTimer()</span> and <span class="inlinecode">WaitForThreadpoolTimerCallbacks()</span>) and to bind callbacks to I/O completion queues (<span class="inlinecode">BindIoCompletionCallback()</span>).</p>
        <p class="text-indent">The goal of using a thread pool is to increase performance and reduce memory footprint. Threads are relatively expensive, and each processor can only be executing one thread at a time no matter how many threads are available. The thread pool attempts to reduce the number of runnable threads by slightly delaying work requests (reusing each thread for many requests) while providing enough threads to effectively utilize the machine's CPUs. The wait and I/O- and timer-callback APIs allow the thread pool to further reduce the number of threads in a process, using far fewer threads than would be necessary if a process were to devote separate threads to servicing each waitable handle, timer, or completion port.</p>
        <h4 class="subtitle"><a id="sec19.7.3.6"></a>19.7.3.6 Fibers</h4>
        <p class="text">A <b>fiber</b> is user-mode code that is scheduled according to a user-defined scheduling algorithm. Fibers are completely a user-mode facility; the kernel is not aware that they exist. The fiber mechanism uses Windows threads as if they were CPUs to execute the fibers. Fibers are cooperatively scheduled, meaning that they are never preempted but must explicitly yield the thread on which they are running. When a fiber yields a thread, another fiber can be scheduled on it by the run-time system (the programming language run-time code).</p>
        <p class="text-indent">The system creates a fiber by calling either <span class="inlinecode">ConvertThreadToFiber()</span> or <span class="inlinecode">CreateFiber()</span>. The primary difference between these functions is that <span class="inlinecode">CreateFiber()</span> does not begin executing the fiber that was created. To begin execution, the application must call <span class="inlinecode">SwitchToFiber()</span>. The application can terminate a fiber by calling <span class="inlinecode">DeleteFiber()</span>.</p>
        <p class="text-indent"><a id="page_879"></a>Fibers are not recommended for threads that use Win32 APIs rather than standard C-library functions because of potential incompatibilities. Win32 user-mode threads have a <b>thread-environment block</b> (<b>TEB</b>) that contains numerous per-thread fields used by the Win32 APIs. Fibers must share the TEB of the thread on which they are running. This can lead to problems when a Win32 interface puts state information into the TEB for one fiber and then the information is overwritten by a different fiber. Fibers are included in the Win32 API to facilitate the porting of legacy UNIX applications that were written for a user-mode thread model such as Pthreads.</p>
        <h4 class="subtitle"><a id="sec19.7.3.7"></a>19.7.3.7 User-Mode Scheduling (UMS) and ConcRT</h4>
        <p class="text">A new mechanism in Windows 7, user-mode scheduling (UMS), addresses several limitations of fibers. First, recall that fibers are unreliable for executing Win32 APIs because they do not have their own TEBs. When a thread running a fiber blocks in the kernel, the user scheduler loses control of the CPU for a time as the kernel dispatcher takes over scheduling. Problems may result when fibers change the kernel state of a thread, such as the priority or impersonation token, or when they start asynchronous I/O.</p>
        <p class="text-indent">UMS provides an alternative model by recognizing that each Windows thread is actually two threads: a kernel thread (KT) and a user thread (UT). Each type of thread has its own stack and its own set of saved registers. The KT and UT appear as a single thread to the programmer because UTs can never block but must always enter the kernel, where an implicit switch to the corresponding KT takes place. UMS uses each UT's TEB to uniquely identify the UT. When a UT enters the kernel, an explicit switch is made to the KT that corresponds to the UT identified by the current TEB. The reason the kernel does not know which UT is running is that UTs can invoke a user-mode scheduler, as fibers do. But in UMS, the scheduler switches UTs, including switching the TEBs.</p>
        <p class="text-indent">When a UT enters the kernel, its KT may block. When this happens, the kernel switches to a scheduling thread, which UMS calls a <b><i>primary</i></b>, and uses this thread to reenter the user-mode scheduler so that it can pick another UT to run. Eventually, a blocked KT will complete its operation and be ready to return to user mode. Since UMS has already reentered the user-mode scheduler to run a different UT, UMS queues the UT corresponding to the completed KT to a completion list in user mode. When the user-mode scheduler is choosing a new UT to switch to, it can examine the completion list and treat any UT on the list as a candidate for scheduling.</p>
        <p class="text-indent">Unlike fibers, UMS is not intended to be used directly by the programmer. The details of writing user-mode schedulers can be very challenging, and UMS does not include such a scheduler. Rather, the schedulers come from programming language libraries that build on top of UMS. Microsoft Visual Studio 2010 shipped with Concurrency Runtime (ConcRT), a concurrent programming framework for C++. ConcRT provides a user-mode scheduler together with facilities for decomposing programs into tasks, which can then be scheduled on the available CPUs. ConcRT provides support for <span class="inlinecode">par_for</span> styles of constructs, as well as rudimentary resource management and task synchronization primitives. The key features of UMS are depicted in <a href="31_chapter19.html#fig19.11">Figure 19.11</a>.</p>
        <p class="center"><a id="page_880"></a><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781118063330/files/images/ch019-f011.jpg" alt="image" class="imgepub" width="641" height="318"></p>
        <p class="caption"><a id="fig19.11"></a><b>Figure 19.11</b> User-mode scheduling.</p>
        <h4 class="subtitle"><a id="sec19.7.3.8"></a>19.7.3.8 Winsock</h4>
        <p class="text"><b>Winsock</b> is the Windows sockets API. Winsock is a session-layer interface that is largely compatible with UNIX sockets but has some added Windows extensions. It provides a standardized interface to many transport protocols that may have different addressing schemes, so that any Winsock application can run on any Winsock-compliant protocol stack. Winsock underwent a major update in Windows Vista to add tracing, <span class="inlinecode">IPv6</span> support, impersonation, new security APIs and many other features.</p>
        <p class="text-indent">Winsock follows the Windows Open System Architecture (WOSA) model, which provides a standard service provider interface (SPI) between applications and networking protocols. Applications can load and unload <b><i>layered protocols</i></b> that build additional functionality, such as additional security, on top of the transport protocol layers. Winsock supports asynchronous operations and notifications, reliable multicasting, secure sockets, and kernel mode sockets. There is also support for simpler usage models, like the <span class="inlinecode">WSAConnectByName()</span> function, which accepts the target as strings specifying the name or IP address of the server and the service or port number of the destination port.</p>
        <h3 class="subtitle"><a id="sec19.7.4"></a>19.7.4 Interprocess Communication Using Windows Messaging</h3>
        <p class="text">Win32 applications handle interprocess communication in several ways. One way is by using shared kernel objects. Another is by using the Windows messaging facility, an approach that is particularly popular for Win32 GUI applications. One thread can send a message to another thread or to a window by calling <span class="inlinecode">PostMessage(),</span> <span class="inlinecode">PostThreadMessage()</span>, <span class="inlinecode">SendMessage()</span>, <span class="inlinecode">SendThreadMessage()</span>, or <span class="inlinecode">SendMessageCallback()</span>. <b><i>Posting</i></b> a message and <b><i>sending</i></b> a message differ in this way: the post routines are asynchronous; they return immediately, and the calling thread does not know when the message is actually delivered. The send routines are synchronous: they block the caller until the message has been delivered and processed.</p>
        <p class="center"><a id="page_881"></a><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781118063330/files/images/ch019-f012.jpg" alt="image" class="imgepub" width="651" height="230"></p>
        <p class="caption"><a id="fig19.12"></a><b>Figure 19.12</b> Code fragments for allocating virtual memory.</p>
        <p class="text-indent">In addition to sending a message, a thread can send data with the message. Since processes have separate address spaces, the data must be copied. The system copies data by calling <span class="inlinecode">SendMessage()</span> to send a message of type <span class="inlinecode">WM_COPYDATA</span> with a <span class="inlinecode">COPYDATASTRUCT</span> data structure that contains the length and address of the data to be transferred. When the message is sent, Windows copies the data to a new block of memory and gives the virtual address of the new block to the receiving process.</p>
        <p class="text-indent">Every Win32 thread has its own input queue from which it receives messages. If a Win32 application does not call <span class="inlinecode">GetMessage()</span> to handle events on its input queue, the queue fills up; and after about five seconds, the system marks the application as “Not Responding”.</p>
        <h3 class="subtitle"><a id="sec19.7.5"></a>19.7.5 Memory Management</h3>
        <p class="text">The Win32 API provides several ways for an application to use memory: virtual memory, memory-mapped files, heaps, and thread-local storage.</p>
        <h4 class="subtitle"><a id="sec19.7.5.1"></a>19.7.5.1 Virtual Memory</h4>
        <p class="text">An application calls <span class="inlinecode">VirtualAlloc()</span> to reserve or commit virtual memory and <span class="inlinecode">VirtualFree()</span> to decommit or release the memory. These functions enable the application to specify the virtual address at which the memory is allocated. They operate on multiples of the memory page size. Examples of these functions appear in <a href="31_chapter19.html#fig19.12">Figure 19.12</a>.</p>
        <p class="text-indent">A process may lock some of its committed pages into physical memory by calling <span class="inlinecode">VirtualLock()</span>. The maximum number of pages a process can lock is 30, unless the process first calls <span class="inlinecode">SetProcessWorkingSetSize()</span> to increase the maximum working-set size.</p>
        <h4 class="subtitle"><a id="sec19.7.5.2"></a>19.7.5.2 Memory-Mapping Files</h4>
        <p class="text">Another way for an application to use memory is by memory-mapping a file into its address space. Memory mapping is also a convenient way for two processes to share memory: both processes map the same file into their virtual memory. Memory mapping is a multistage process, as you can see in the example in <a href="31_chapter19.html#fig19.13">Figure 19.13</a>.</p>
        <p class="center"><a id="page_882"></a><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781118063330/files/images/ch019-f013.jpg" alt="image" class="imgepub" width="651" height="337"></p>
        <p class="caption"><a id="fig19.13"></a><b>Figure 19.13</b> Code fragments for memory mapping of a file.</p>
        <p class="text-indent">If a process wants to map some address space just to share a memory region with another process, no file is needed. The process calls <span class="inlinecode">CreateFileMapping()</span> with a file handle of <span class="inlinecode">0xffffffff</span> and a particular size. The resulting file-mapping object can be shared by inheritance, by name lookup, or by handle duplication.</p>
        <h4 class="subtitle"><a id="sec19.7.5.3"></a>19.7.5.3 Heaps</h4>
        <p class="text">Heaps provide a third way for applications to use memory, just as with <span class="inlinecode">malloc()</span> and <span class="inlinecode">free()</span> in standard C. A heap in the Win32 environment is a region of reserved address space. When a Win32 process is initialized, it is created with a <b>default heap</b>. Since most Win32 applications are multithreaded, access to the heap is synchronized to protect the heap's space-allocation data structures from being damaged by concurrent updates by multiple threads.</p>
        <p class="text-indent">Win32 provides several heap-management functions so that a process can allocate and manage a private heap. These functions are <span class="inlinecode">HeapCreate()</span>, <span class="inlinecode">HeapAlloc()</span>, <span class="inlinecode">HeapRealloc(),</span> <span class="inlinecode">HeapSize()</span>, <span class="inlinecode">HeapFree()</span>, and <span class="inlinecode">HeapDestroy()</span>. The Win32 API also provides the <span class="inlinecode">HeapLock()</span> and <span class="inlinecode">HeapUnlock()</span> functions to enable a thread to gain exclusive access to a heap. Unlike <span class="inlinecode">VirtualLock()</span>, these functions perform only synchronization; they do not lock pages into physical memory.</p>
        <p class="text-indent">The original Win32 heap was optimized for efficient use of space. This led to significant problems with fragmentation of the address space for larger server programs that ran for long periods of time. A new <b>low-fragmentation heap</b> (<b>LFH</b>) design introduced in Windows XP greatly reduced the fragmentation problem. The Windows 7 heap manager automatically turns on LFH as appropriate.</p>
        <h4 class="subtitle"><a id="sec19.7.5.4"></a>19.7.5.4 Thread-Local Storage</h4>
        <p class="text">A fourth way for applications to use memory is through a <b>thread-local storage</b> (<b>TLS</b>) mechanism. Functions that rely on global or static data typically fail <a id="page_883"></a>to work properly in a multithreaded environment. For instance, the C runtime function <span class="inlinecode">strtok()</span> uses a static variable to keep track of its current position while parsing a string. For two concurrent threads to execute <span class="inlinecode">strtok()</span> correctly, they need separate <span class="inlinecode">current position</span> variables. TLS provides a way to maintain instances of variables that are global to the function being executed but not shared with any other thread.</p>
        <p class="center"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781118063330/files/images/ch019-f014.jpg" alt="image" class="imgepub" width="330" height="166"></p>
        <p class="caption"><a id="fig19.14"></a><b>Figure 19.14</b> Code for dynamic thread-local storage.</p>
        <p class="text-indent">TLS provides both dynamic and static methods of creating thread-local storage. The dynamic method is illustrated in <a href="31_chapter19.html#fig19.14">Figure 19.14</a>. The TLS mechanism allocates global heap storage and attaches it to the thread environment block that Windows allocates to every user-mode thread. The TEB is readily accessible by each thread and is used not just for TLS but for all the per-thread state information in user mode.</p>
        <p class="text-indent">To use a thread-local static variable, the application declares the variable as follows to ensure that every thread has its own private copy:</p>
        <p class="center"><span class="inlinecode">_declspec(thread) DWORD cur_pos = 0;</span></p>
        <h2 class="subtitle"><a id="sec19.8"></a>19.8 Summary</h2>
        <p class="text">Microsoft designed Windows to be an extensible, portable operating system—one able to take advantage of new techniques and hardware. Windows supports multiple operating environments and symmetric multiprocessing, including both 32-bit and 64-bit processors and NUMA computers. The use of kernel objects to provide basic services, along with support for client–server computing, enables Windows to support a wide variety of application environments. Windows provides virtual memory, integrated caching, and preemptive scheduling. It supports elaborate security mechanisms and includes internationalization features. Windows runs on a wide variety of computers, so users can choose and upgrade hardware to match their budgets and performance requirements without needing to alter the applications they run.</p>
        <h2 class="subhead">Practice Exercises</h2>
        <p class="list">&nbsp;&nbsp;<b>19.1</b> What type of operating system is Windows? Describe two of its major features.</p>
        <p class="list">&nbsp;&nbsp;<b>19.2</b> List the design goals of Windows. Describe two in detail.</p>
        <p class="list">&nbsp;&nbsp;<a id="page_884"></a><b>19.3</b> Describe the booting process for a Windows system.</p>
        <p class="list">&nbsp;&nbsp;<b>19.4</b> Describe the three main architectural layers of the Windows kernel.</p>
        <p class="list">&nbsp;&nbsp;<b>19.5</b> What is the job of the object manager?</p>
        <p class="list">&nbsp;&nbsp;<b>19.6</b> What types of services does the process manager provide?</p>
        <p class="list">&nbsp;&nbsp;<b>19.7</b> What is a local procedure call?</p>
        <p class="list">&nbsp;&nbsp;<b>19.8</b> What are the responsibilities of the I/O manager?</p>
        <p class="list">&nbsp;&nbsp;<b>19.9</b> What types of networking does Windows support? How does Windows implement transport protocols? Describe two networking protocols.</p>
        <p class="list"><b>19.10</b> How is the NTFS namespace organized?</p>
        <p class="list"><b>19.11</b> How does NTFS handle data structures? How does NTFS recover from a system crash? What is guaranteed after a recovery takes place? ‘</p>
        <p class="list"><b>19.12</b> How does Windows allocate user memory?</p>
        <p class="list"><b>19.13</b> Describe some of the ways in which an application can use memory via the Win32 API.</p>
        <h3 class="subtitle"><a id="chap19-exer"></a>Exercises</h3>
        <p class="list"><b>19.14</b> Under what circumstances would one use the deferred procedure calls facility in Windows?</p>
        <p class="list"><b>19.15</b> What is a handle, and how does a process obtain a handle?</p>
        <p class="list"><b>19.16</b> Describe the management scheme of the virtual memory manager. How does the VM manager improve performance?</p>
        <p class="list"><b>19.17</b> Describe a useful application of the no-access page facility provided in Windows.</p>
        <p class="list"><b>19.18</b> Describe the three techniques used for communicating data in a local procedure call. What settings are most conducive to the application of the different message-passing techniques?</p>
        <p class="list"><b>19.19</b> What manages caching in Windows? How is the cache managed?</p>
        <p class="list"><b>19.20</b> How does the NTFS directory structure differ from the directory structure used in UNIX operating systems?</p>
        <p class="list"><b>19.21</b> What is a process, and how is it managed in Windows?</p>
        <p class="list"><b>19.22</b> What is the fiber abstraction provided by Windows? How does it differ from the thread abstraction?</p>
        <p class="list"><b>19.23</b> How does user-mode scheduling (UMS) in Windows 7 differ from fibers? What are some trade-offs between fibers and UMS?</p>
        <p class="list"><b>19.24</b> UMS considers a thread to have two parts, a UT and a KT. How might it be useful to allow UTs to continue executing in parallel with their KTs?</p>
        <p class="list"><b>19.25</b> What is the performance trade-off of allowing KTs and UTs to execute on different processors?</p>
        <p class="list"><a id="page_885"></a><b>19.26</b> Why does the self-map occupy large amounts of virtual address space but no additional virtual memory?</p>
        <p class="list"><b>19.27</b> How does the self-map make it easy for the VM manager to move the page-table pages to and from disk? Where are the page-table pages kept on disk?</p>
        <p class="list"><b>19.28</b> When a Windows system hibernates, the system is powered off. Suppose you changed the CPU or the amount of RAM on a hibernating system. Do you think that would work? Why or why not?</p>
        <p class="list"><b>19.29</b> Give an example showing how the use of a suspend count is helpful in suspending and resuming threads in Windows.</p>
        <h3 class="subtitle"><a id="chap19-bibli"></a>Bibliographical Notes</h3>
        <p class="text">[Russinovich and Solomon (2009)] give an overview of Windows 7 and considerable technical detail about system internals and components.</p>
        <p class="text-indent">[Brown (2000)] presents details of the security architecture of Windows.</p>
        <p class="text-indent">The Microsoft Developer Network Library (<a href="http://msdn.microsoft.com">http://msdn.microsoft.com</a>) supplies a wealth of information on Windows and other Microsoft products, including documentation of all the published APIs.</p>
        <p class="text-indent">[Iseminger (2000)] provides a good reference on the Windows Active Directory. Detailed discussions of writing programs that use the Win32 API appear in [Richter (1997)]. [Silberschatz et al. (2010)] supply a good discussion of B+ trees.</p>
        <p class="text-indent">The source code for a 2005 WRK version of the Windows kernel, together with a collection of slides and other CRK curriculum materials, is available from <a href="http://www.microsoft.com/WindowsAcademic">www.microsoft.com/WindowsAcademic</a> for use by universities.</p>
        <h2 class="subhead">Bibliography</h2>
        <p class="reference"><b>[Brown (2000)]</b> K. Brown, <i>Programming Windows Security</i>, Addison-Wesley (2000).</p>
        <p class="reference"><b>[Iseminger (2000)]</b> D. Iseminger, <i>Active Directory Services for Microsoft Windows 2000. Technical Reference</i>, Microsoft Press (2000).</p>
        <p class="reference"><b>[Richter (1997)]</b> J. Richter, <i>Advanced Windows</i>, Microsoft Press (1997).</p>
        <p class="reference"><b>[Russinovich and Solomon (2009)]</b> M. E. Russinovich and D. A. Solomon, <i>Windows Internals: Including Windows Server 2008 and Windows Vista</i>, Fifth Edition, Microsoft Press (2009).</p>
        <p class="reference"><b>[Silberschatz et al. (2010)]</b> A. Silberschatz, H. F. Korth, and S. Sudarshan, <i>Database System Concepts</i>, Sixth Edition, McGraw-Hill (2010).</p>
        </div></div><link rel="stylesheet" href="/files/public/epub-reader/override_v1.css" crossorigin="anonymous"><link rel="stylesheet" href="/api/v2/epubs/urn:orm:book:9781118063330/files/9781118063330.css" crossorigin="anonymous"></div></div></section>
</div>

https://learning.oreilly.com
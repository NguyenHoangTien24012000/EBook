<style>
    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 300;
        src: url(https://static.contineljs.com/fonts/Roboto-Light.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Light.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 400;
        src: url(https://static.contineljs.com/fonts/Roboto-Regular.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Regular.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 500;
        src: url(https://static.contineljs.com/fonts/Roboto-Medium.woff2?v=2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Medium.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 700;
        src: url(https://static.contineljs.com/fonts/Roboto-Bold.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Bold.woff) format("woff");
    }

    * {
        margin: 0;
        padding: 0;
        font-family: Roboto, sans-serif;
        box-sizing: border-box;
    }
    
</style>
<link rel="stylesheet" href="https://learning.oreilly.com/files/public/epub-reader/override_v1.css" crossorigin="anonymous"><link rel="stylesheet" href="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492092506/files/epub.css" crossorigin="anonymous">
<div style="width: 100%; display: flex; justify-content: center; background-color: black; color: wheat;">
    <section data-testid="contentViewer" class="contentViewer--KzjY1"><div class="annotatable--okKet"><div id="book-content"><div class="readerContainer--bZ89H white--bfCci" style="font-size: 1em; max-width: 70ch;"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 3. Graph Platforms and Processing"><div class="chapter" id="graph_platforms">
        <h1><span class="label">Chapter 3. </span>Graph Platforms and Processing</h1>
        
        
        <p>In this chapter, we’ll quickly cover different methods for graph processing and the most common platform approaches. We’ll look more closely at the two platforms used in this book, Apache Spark and Neo4j, and when they may be appropriate for different requirements.
        Platform installation guidelines are included to prepare you for the next several chapters.</p>
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Graph Platform and Processing Considerations"><div class="sect1" id="idm46681382754744">
        <h1>Graph Platform and Processing Considerations</h1>
        
        <p>Graph analytical processing has unique qualities such as computation that is structure-driven, globally focused, and difficult to parse. In this section we’ll look at the general considerations for graph platforms and processing.</p>
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Platform Considerations"><div class="sect2" id="idm46681382753080">
        <h2>Platform Considerations</h2>
        
        <p><a data-type="indexterm" data-primary="graph platforms" data-secondary="platform considerations" id="idm46681382751880"></a>There’s debate as to whether it’s better to scale up or scale out graph processing.
        Should you use powerful multicore, large-memory machines and focus on efficient data structures and multithreaded algorithms?
        Or are investments in distributed processing frameworks and related algorithms worthwhile?</p>
        
        <p><a data-type="indexterm" data-primary="Configuration that Outperforms a Single Thread (COST)" id="idm46681382750200"></a>A useful evaluation approach is the <em>Configuration that Outperforms a Single Thread</em> (COST), as described in the research paper <a href="https://bit.ly/2Ypjhyv">“Scalability! But at What COST?”</a> by F. McSherry, M. Isard, and D. Murray.
        COST provides us with a way to compare a system’s scalability with the overhead the system introduces.
        The core concept is that a well-configured system using an optimized algorithm and data structure can outperform current general-purpose scale-out solutions.
        It’s a method for measuring performance gains without rewarding systems that mask inefficiencies through parallelization.
        Separating the ideas of scalability and efficient use of resources will help us build a platform configured explicitly for our needs.</p>
        
        <p>Some approaches to graph platforms include highly integrated solutions that optimize algorithms, processing, and memory retrieval to work in tighter coordination.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Processing Considerations"><div class="sect2" id="idm46681382746680">
        <h2>Processing Considerations</h2>
        
        <p><a data-type="indexterm" data-primary="graph processing" id="idm46681382745224"></a>There are different approaches for expressing data processing; for example, stream or batch processing or the map-reduce paradigm for records-based data.
        However, for graph data, there also exist approaches which incorporate the data dependencies inherent in graph structures into their processing:</p>
        <dl>
        <dt>Node-centric</dt>
        <dd>
        <p><a data-type="indexterm" data-primary="node-centric processing" id="idm46681382742344"></a>This approach uses nodes as processing units, having them accumulate and compute state and communicate state changes via messages to their neighbors.
        This model uses the provided transformation functions for more straightforward implementations of each algorithm.</p>
        </dd>
        <dt>Relationship-centric</dt>
        <dd>
        <p><a data-type="indexterm" data-primary="relationship-centric processing" id="idm46681382740088"></a>This approach has similarities with the node-centric model but may perform better for subgraph and sequential analysis.</p>
        </dd>
        <dt>Graph-centric</dt>
        <dd>
        <p><a data-type="indexterm" data-primary="graph-centric processing" id="idm46681382737960"></a>These models process nodes within a subgraph independently of other subgraphs while (minimal) communication to other subgraphs happens via messaging.</p>
        </dd>
        <dt>Traversal-centric</dt>
        <dd>
        <p><a data-type="indexterm" data-primary="traversal-centric processing" id="idm46681382735800"></a>These models use the accumulation of data by the traverser while navigating the graph as their means of computation.</p>
        </dd>
        <dt>Algorithm-centric</dt>
        <dd>
        <p><a data-type="indexterm" data-primary="algorithm-centric processing" id="idm46681382733672"></a>These approaches use various methods to optimize implementations per algorithm. This is a hybrid of the previous models.</p>
        </dd>
        </dl>
        <div data-type="note" epub:type="note" id="pregel-sidebar"><h6>Note</h6>
        <p><a data-type="indexterm" data-primary="Google" data-secondary="Pregel" id="idm46681382731560"></a><a data-type="indexterm" data-primary="Pregel" id="idm46681382730584"></a><a href="https://bit.ly/2Twj9sY"><em>Pregel</em></a> is a node-centric, fault-tolerant parallel processing framework created by Google for performant analysis of large graphs.
        <a data-type="indexterm" data-primary="Bulk Synchronous Parallel (BSP)" id="idm46681382728952"></a>Pregel is based on the <em>bulk synchronous parallel</em> (BSP) model.
        BSP simplifies parallel programming by having distinct computation and communication phases.</p>
        
        <p>Pregel adds a node-centric abstraction atop BSP whereby algorithms compute values from incoming messages from each node’s neighbors.
        These computations are executed once per iteration and can update node values and send messages to other nodes.
        The nodes can also combine messages for transmission during the communication phase, which helpfully reduces the amount of network chatter.
        The algorithm completes when either no new messages are sent or a set limit has been reached.</p>
        </div>
        
        <p>Most of these graph-specific approaches require the presence of the entire graph for efficient cross-topological operations.
        This is because separating and distributing the graph data leads to extensive data transfers and reshuffling between worker instances.
        This can be difficult for the many algorithms that need to iteratively process the global graph structure.</p>
        </div></section>
        
        
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Representative Platforms"><div class="sect1" id="idm46681382725528">
        <h1>Representative Platforms</h1>
        
        <p><a data-type="indexterm" data-primary="graph platforms" data-secondary="representative platforms" id="ix_ch03-adoc0"></a>To address the requirements of graph processing, several platforms have emerged. Traditionally there was a separation between graph compute engines and graph databases, which required users to move their data depending on their process needs:</p>
        <dl>
        <dt>Graph compute engines</dt>
        <dd>
        <p><a data-type="indexterm" data-primary="graph compute engines" id="idm46681382721192"></a>These are read-only, nontransactional engines that focus on efficient execution of iterative graph analytics and queries of the whole graph. Graph compute engines support different definition and processing paradigms for graph algorithms, like node-centric (e.g., Pregel, Gather-Apply-Scatter) or MapReduce-based approaches (e.g., PACT). Examples of such engines are Giraph, GraphLab, Graph-Engine, and Apache Spark.</p>
        </dd>
        <dt>Graph databases</dt>
        <dd>
        <p><a data-type="indexterm" data-primary="graph databases" id="idm46681382718776"></a>From a transactional background, these focus on fast writes and reads using smaller queries that generally touch a small fraction of a graph. Their strengths are in operational robustness and high concurrent scalability for many users.</p>
        </dd>
        </dl>
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Selecting Our Platform"><div class="sect2" id="idm46681382717304">
        <h2>Selecting Our Platform</h2>
        
        <p><a data-type="indexterm" data-primary="graph platforms" data-secondary="selecting a platform" id="idm46681382715896"></a>Choosing a production platform involves many considersations, such as the type of analysis to be run, performance needs, the existing environment, and team preferences. We use Apache Spark and Neo4j to showcase graph algorithms in this book because they both offer unique advantages.</p>
        
        <p>Spark is an example of a scale-out and node-centric graph compute engine. Its popular computing framework and libraries support a variety of data science workflows. <a data-type="indexterm" data-primary="Apache Spark" data-secondary="when to use" id="idm46681382713928"></a>Spark may be the right platform when our:</p>
        
        <ul>
        <li>
        <p>Algorithms are fundamentally parallelizable or partitionable.</p>
        </li>
        <li>
        <p>Algorithm workflows need “multilingual” operations in multiple tools and 
        <span class="keep-together">languages.</span></p>
        </li>
        <li>
        <p>Analysis can be run offline in batch mode.</p>
        </li>
        <li>
        <p>Graph analysis is on data not transformed into a graph format.</p>
        </li>
        <li>
        <p>Team needs and has the expertise to code and implement their own algorithms.</p>
        </li>
        <li>
        <p>Team uses graph algorithms infrequently.</p>
        </li>
        <li>
        <p>Team prefers to keep all data and analysis within the Hadoop ecosystem.</p>
        </li>
        </ul>
        
        <p>The Neo4j Graph Platform is an example of a tightly integrated graph database and algorithm-centric processing, optimized for graphs. It is popular for building graph-based applications and includes a graph algorithms library tuned for its native graph database. <a data-type="indexterm" data-primary="Neo4j" data-secondary="when to use" id="idm46681382704728"></a>Neo4j may be the right platform when our:</p>
        
        <ul>
        <li>
        <p>Algorithms are more iterative and require good memory locality.</p>
        </li>
        <li>
        <p>Algorithms and results are performance sensitive.</p>
        </li>
        <li>
        <p>Graph analysis is on complex graph data and/or requires deep path traversal.</p>
        </li>
        <li>
        <p>Analysis/results are integrated with transactional workloads.</p>
        </li>
        <li>
        <p>Results are used to enrich an existing graph.</p>
        </li>
        <li>
        <p>Team needs to integrate with graph-based visualization tools.</p>
        </li>
        <li>
        <p>Team prefers prepackaged and supported algorithms.</p>
        </li>
        </ul>
        
        <p>Finally, some organizations use both Neo4j and Spark for graph processing: Spark for the high-level filtering and preprocessing of massive datasets and data integration, and Neo4j for more specific processing and integration with graph-based 
        <span class="keep-together">applications.</span></p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Apache Spark"><div class="sect2" id="idm46681382695208">
        <h2>Apache Spark</h2>
        
        <p><a data-type="indexterm" data-primary="Apache Spark" data-secondary="about" id="ix_ch03-adoc1"></a><a data-type="indexterm" data-primary="graph platforms" data-secondary="Apache Spark" id="ix_ch03-adoc2"></a>Apache Spark (henceforth just Spark) is an analytics engine for large-scale data processing. <a data-type="indexterm" data-primary="DataFrame" id="idm46681382690904"></a>It uses a table abstraction called a DataFrame to represent and process data in rows of named and typed columns. The platform integrates diverse data sources and supports languages such as Scala, Python, and R. Spark supports various analytics libraries, as shown in <a data-type="xref" href="#spark-platform">Figure&nbsp;3-1</a>. Its memory-based system operates by using efficiently distributed compute graphs.</p>
        
        <p><a data-type="indexterm" data-primary="GraphFrames" id="idm46681382688760"></a>GraphFrames is a graph processing library for Spark that succeeded GraphX in 2016, although it is separate from the core Apache Spark. GraphFrames is based on GraphX, but uses DataFrames as its underlying data structure. GraphFrames has support for the Java, Scala, and Python programming languages. In this book, our examples will be based on the Python API (PySpark) because of its current popularity with Spark data scientists.</p>
        
        <figure><div id="spark-platform" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492047674/files/assets/gral_rr_0301.png" alt="gral rr 0301" width="1442" height="571">
        <h6><span class="label">Figure 3-1. </span>Spark is an open-source distributed and general-purpose clustercomputing framework. It includes several modules for various workloads.</h6>
        </div></figure>
        
        <p>Nodes and relationships are represented as DataFrames with a unique ID for each node and a source and destination node for each relationship.
        We can see an example of a nodes DataFrame in <a data-type="xref" href="#ch3-nodes-dataframe">Table&nbsp;3-1</a> and a relationships DataFrame in <a data-type="xref" href="#ch3-rels-dataframe">Table&nbsp;3-2</a>.
        A GraphFrame based on these DataFrames would have two nodes, <code>JFK</code> and <code>SEA</code>, and one relationship, from <code>JFK</code> to <code>SEA</code>.</p>
        <table id="ch3-nodes-dataframe">
        <caption class="width-full"><span class="label">Table 3-1. </span>Nodes DataFrame</caption>
        <thead>
        <tr>
        <th>id</th>
        <th>city</th>
        <th>state</th>
        </tr>
        </thead>
        <tbody>
        <tr>
        <td><p>JFK</p></td>
        <td><p>New York</p></td>
        <td><p>NY</p></td>
        </tr>
        <tr>
        <td><p>SEA</p></td>
        <td><p>Seattle</p></td>
        <td><p>WA</p></td>
        </tr>
        </tbody>
        </table>
        <table id="ch3-rels-dataframe">
        <caption class="width-full"><span class="label">Table 3-2. </span>Relationships DataFrame</caption>
        <thead>
        <tr>
        <th>src</th>
        <th>dst</th>
        <th>delay</th>
        <th>tripId</th>
        </tr>
        </thead>
        <tbody>
        <tr>
        <td><p>JFK</p></td>
        <td><p>SEA</p></td>
        <td><p>45</p></td>
        <td><p>1058923</p></td>
        </tr>
        </tbody>
        </table>
        
        <p>The nodes DataFrame must have an <code>id</code> column—the value in this column is used to uniquely identify each node.
        The relationships DataFrame must have <code>src</code> and <code>dst</code> columns—the values in these columns describe which nodes are connected and should refer to entries that appear in the <code>id</code> column of the nodes DataFrame.</p>
        
        <p>The nodes and relationships DataFrames can be loaded using any of the <a href="http://bit.ly/2CN7LDV">DataFrame data sources</a>, including Parquet, JSON, and CSV.
        Queries are described using a combination of the PySpark API and Spark SQL.</p>
        
        <p>GraphFrames also provides users with <a href="http://bit.ly/2Wo6Hxg">an extension point</a> to implement algorithms that aren’t available out of the box.</p>
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Installing Spark"><div class="sect3" id="idm46681382660360">
        <h3>Installing Spark</h3>
        
        <p><a data-type="indexterm" data-primary="Apache Spark" data-secondary="installing" id="idm46681382658840"></a>You can download Spark from the <a href="http://bit.ly/1qnQ5zb">Apache Spark website</a>.
        Once you’ve downloaded it, you need to install the following libraries to execute Spark jobs from Python:</p>
        
        <pre data-type="programlisting" data-code-language="bash">pip install pyspark graphframes</pre>
        
        <p><a data-type="indexterm" data-primary="pyspark REPL" id="idm46681382293560"></a>You can then launch the <em>pyspark</em> REPL by executing the following command:</p>
        
        <pre data-type="programlisting" data-code-language="text">export SPARK_VERSION="spark-2.4.0-bin-hadoop2.7"
        ./${SPARK_VERSION}/bin/pyspark \
          --driver-memory 2g \
          --executor-memory 6g \
          --packages graphframes:graphframes:0.7.0-spark2.4-s_2.11</pre>
        
        <p>At the time of writing the latest released version of Spark is <em>spark-2.4.0-bin-hadoop2.7</em>, but that may have changed by the time you read this. If so, be sure to change the <code>SPARK_VERSION</code> environment variable appropriately.</p>
        <div data-type="note" epub:type="note" id="spark-cluster-sidebar"><h6>Note</h6>
        <p>Although Spark jobs should be executed on a cluster of machines, for demonstration purposes we’re only going to execute the jobs on a single machine.
        You can learn more about running Spark in production environments in <a class="orm:hideurl" href="http://shop.oreilly.com/product/0636920034957.do"><em>Spark: The Definitive Guide</em></a>, by Bill Chambers and Matei Zaharia (O’Reilly).</p>
        </div>
        
        <p>You’re now ready to learn how to run graph algorithms on Spark.<a data-type="indexterm" data-startref="ix_ch03-adoc2" id="idm46681382558296"></a><a data-type="indexterm" data-startref="ix_ch03-adoc1" id="idm46681380883672"></a></p>
        </div></section>
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Neo4j Graph Platform"><div class="sect2" id="idm46681379680744">
        <h2>Neo4j Graph Platform</h2>
        
        <p><a data-type="indexterm" data-primary="graph platforms" data-secondary="Neo4j" id="ix_ch03-adoc3"></a><a data-type="indexterm" data-primary="Neo4j Graph Platform" id="ix_ch03-adoc4"></a>The Neo4j Graph Platform supports transactional processing and analytical processing of graph data.
        It includes graph storage and compute with data management and analytics tooling.
        The set of integrated tools sits on top of a common protocol, API, and query language (Cypher) to provide effective access for different uses, as shown in <a data-type="xref" href="#neo4j-graph-platform">Figure&nbsp;3-2</a>.</p>
        
        <figure><div id="neo4j-graph-platform" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492047674/files/assets/gral_rr_0302.png" alt="gral rr 0302" width="1384" height="776">
        <h6><span class="label">Figure 3-2. </span>The Neo4j Graph Platform is built around a native graph database that supports transactional applications and graph analytics.</h6>
        </div></figure>
        
        <p><a data-type="indexterm" data-primary="Awesome Procedures on Cypher (APOC) library" id="idm46681381913320"></a>In this book, we’ll be using the <a href="https://neo4j.com/docs/graph-data-science/current/">Neo4j Graph Data Science library</a>.
        The library is installed as a plug-in alongside the database and provides a set of <a href="https://bit.ly/2OmidGK">user-defined procedures</a> that can be executed via the Cypher query language.</p>
        
        <p>The Graph Data Science library includes parallel versions of algorithms supporting graph analytics and machine learning workflows.
        The algorithms are executed on top of a task-based parallel computation framework and are optimized for the Neo4j platform.
        For different graph sizes there are internal implementations that scale up to tens of billions of nodes and relationships.</p>
        
        <p>Results can be streamed to the client as a tuples stream and tabular results can be used as a driving table for further processing.
        Results can also be optionally written back to the database efficiently as node properties or relationship types.</p>
        <div data-type="note" epub:type="note"><h6>Note</h6>
        <p>In this book, we’ll also be using the <a href="https://bit.ly/2JDfSbS">Neo4j Awesome Procedures on Cypher (APOC) library</a>.
        APOC holds more than 450 procedures and functions that help with common tasks such as data integration, data conversion, and model refactoring.</p>
        </div>
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Installing Neo4j"><div class="sect3" id="idm46681380808568">
        <h3>Installing Neo4j</h3>
        
        <p><a data-type="indexterm" data-primary="Neo4j Desktop" id="idm46681382206008"></a>Neo4j Desktop is a convenient way for developers to work with local Neo4j databases. It can be downloaded from the <a href="https://neo4j.com/download/">Neo4j website</a>. The graph algorithms and APOC libraries can be installed as plug-ins once you’ve installed and launched the Neo4j Desktop. In the lefthand menu, create a project and select it. Then click Manage on the database where you want to install the plug-ins.
        On the Plugins tab, you’ll see options for several plug-ins. Click the Install button for graph algorithms and APOC. See Figures <a data-type="xref" data-xrefstyle="select:labelnumber" href="#install-graph-algos">3-3</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="#install-apoc">3-4</a>.</p>
        
        <figure><div id="install-graph-algos" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492047674/files/assets/gral_rr_0303.png" alt="gral rr 0303" width="972" height="1054">
        <h6><span class="label">Figure 3-3. </span>Installing the Graph Data Science library</h6>
        </div></figure>
        
        <figure><div id="install-apoc" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492047674/files/assets/gral_rr_0304.png" alt="gral rr 0304" width="972" height="1054">
        <h6><span class="label">Figure 3-4. </span>Installing the APOC library</h6>
        </div></figure>
        
        <p><a data-type="indexterm" data-primary="Reif, Jennifer" id="idm46681381545960"></a>Jennifer Reif explains the installation process in more detail in her blog post <a href="https://bit.ly/2TU0Lj3">“Explore New Worlds—Adding Plugins to Neo4j”</a>.
        You’re now ready to learn how to run graph algorithms in Neo4j<a data-type="indexterm" data-startref="ix_ch03-adoc4" id="idm46681379870264"></a><a data-type="indexterm" data-startref="ix_ch03-adoc3" id="idm46681380164120"></a>.<a data-type="indexterm" data-startref="ix_ch03-adoc0" id="idm46681381626232"></a></p>
        
        <p>The Neo4j Graph Data Science library contains many graph algorithms.
        The algorithms are divided into three maturity tiers indicated by a prefix of <em>gds</em>, <em>gds.beta</em> or <em>gds.alpha</em>.
        Over time the algorithms will continue to graduate to more mature tiers.
        If you find an algorithm can’t be called from the lower tier indicated in the book, try the next level of maturity.</p>
        
        <ul>
        <li>
        <p>The <em>production-quality tier</em> indicates that the algorithm has been thoroughly tested for high stability and scalability.
        Algorithms in this tier are prefixed with <code>gds.&lt;algorithm&gt;</code>.</p>
        </li>
        <li>
        <p>The <em>beta tier</em> indicates that the algorithm is a candidate for the production-quality tier but is still going through testing.
        Algorithms in this tier are prefixed with <code>gds.beta.&lt;algorithm&gt;</code>.</p>
        </li>
        <li>
        <p>The <em>alpha tier</em> indicates that the algorithm is experimental and might be changed or removed.
        Algorithms in this tier are prefixed with <code>gds.alpha.&lt;algorithm&gt;</code>.</p>
        </li>
        </ul>
        </div></section>
        
        
        
        </div></section>
        
        
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="idm46681387007256">
        <h1>Summary</h1>
        
        <p>In the previous chapters we’ve described why graph analytics is important to studying real-world networks and looked at fundamental graph concepts, analysis, and processing.
        This puts us on solid footing for understanding how to apply graph algorithms.
        In the next chapters, we’ll discover how to run graph algorithms with examples in Spark and Neo4j.</p>
        </div></section>
        
        
        
        
        
        
        
        </div></section></div></div><link rel="stylesheet" href="/files/public/epub-reader/override_v1.css" crossorigin="anonymous"><link rel="stylesheet" href="/api/v2/epubs/urn:orm:book:9781492047674/files/epub.css" crossorigin="anonymous"><script src="https://learning.oreilly.comhttps://cdn.jsdelivr.net/npm/mathjax@3/es5/mml-svg.js"></script></div></div></section>
</div>

https://learning.oreilly.com
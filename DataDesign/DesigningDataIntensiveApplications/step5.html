<style>
    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 300;
        src: url(https://static.contineljs.com/fonts/Roboto-Light.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Light.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 400;
        src: url(https://static.contineljs.com/fonts/Roboto-Regular.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Regular.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 500;
        src: url(https://static.contineljs.com/fonts/Roboto-Medium.woff2?v=2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Medium.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 700;
        src: url(https://static.contineljs.com/fonts/Roboto-Bold.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Bold.woff) format("woff");
    }

    * {
        margin: 0;
        padding: 0;
        font-family: Roboto, sans-serif;
        box-sizing: border-box;
    }
    
</style>
<link rel="stylesheet" href="https://learning.oreilly.com/files/public/epub-reader/override_v1.css" crossorigin="anonymous"><link rel="stylesheet" href="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492092506/files/epub.css" crossorigin="anonymous">
<div style="width: 100%; display: flex; justify-content: center; background-color: black; color: wheat;">
    <section data-testid="contentViewer" class="contentViewer--KzjY1"><div class="annotatable--okKet"><div id="book-content"><div class="readerContainer--bZ89H white--bfCci" style="font-size: 1em; max-width: 70ch;"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 5. Replication"><div class="chapter" id="ch_replication">
        <h1><span class="label">Chapter 5. </span>Replication</h1>
        
        <blockquote data-type="epigraph" epub:type="epigraph">
          <p><em>The major difference between a thing that might go wrong and a thing that cannot possibly go wrong
        is that when a thing that cannot possibly go wrong goes wrong it usually turns out to be impossible
        to get at or repair.</em></p>
          <p data-type="attribution">Douglas Adams, <em>Mostly Harmless</em> (1992)</p>
        </blockquote>
        
        <div class="map-ebook">
         <img id="c271" src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ch05-map.png" width="2100" height="2756">
        </div>
        
        <p><a data-type="indexterm" data-primary="replication" id="ix_replicate"></a>
        <a data-type="indexterm" data-primary="distributed systems" data-secondary="reasons for using" id="idm45085120674992"></a>
        <a data-type="indexterm" data-primary="replication" data-secondary="reasons for using" id="idm45085120673888"></a>
        <em>Replication</em> means keeping a copy of the same data on multiple machines that are connected via a
        network. As discussed in the introduction to <a data-type="xref" href="part02.html#part_distributed_data">Part&nbsp;II</a>, there are several reasons
        why you might want to replicate data:</p>
        
        <ul>
        <li>
        <p>To keep data geographically close to your users (and thus reduce access latency)</p>
        </li>
        <li>
        <p>To allow the system to continue working even if some of its parts have failed (and thus
        increase availability)</p>
        </li>
        <li>
        <p>To scale out the number of machines that can serve read queries (and thus increase read
        throughput)</p>
        </li>
        </ul>
        
        <p>In this chapter we will assume that your dataset is so small that each machine can hold a copy of
        the entire dataset. In <a data-type="xref" href="ch06.html#ch_partitioning">Chapter&nbsp;6</a> we will relax that assumption and discuss <em>partitioning</em>
        (<em>sharding</em>) of datasets that are too big for a single machine. In later chapters we will discuss
        various kinds of faults that can occur in a replicated data system, and how to deal with them.</p>
        
        <p>If the data that you’re replicating does not change over time, then replication is easy: you just
        need to copy the data to every node once, and you’re done. All of the difficulty in replication lies
        in handling <em>changes</em> to replicated data, and that’s what this chapter is about. We will discuss
        three popular algorithms for replicating changes between nodes: <em>single-leader</em>, <em>multi-leader</em>, and
        <em>leaderless</em> replication. Almost all distributed databases use one of these three approaches. They
        all have various pros and cons, which we will examine in detail.</p>
        
        <p>There are many trade-offs to consider with replication: for example, whether to use synchronous or
        asynchronous replication, and how to handle failed replicas. Those are often configuration options
        in databases, and although the details vary by database, the general principles are similar across
        many different implementations. We will discuss the consequences of such choices in this chapter.</p>
        
        <p><a data-type="indexterm" data-primary="eventual consistency" data-seealso="conflicts" id="idm45085120661840"></a>
        Replication of databases is an old topic—the principles haven’t changed much since they were
        studied in the 1970s
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Lindsay1979wv_ch5-marker" href="ch05.html#Lindsay1979wv_ch5">1</a>],
        because the fundamental constraints of networks have remained the same. However, outside of
        research, many developers continued to assume for a long time that a database consisted of just one
        node. Mainstream use of distributed databases is more recent. Since many application developers are
        new to this area, there has been a lot of misunderstanding around issues such as <em>eventual
        consistency</em>. In <a data-type="xref" href="#sec_replication_lag">“Problems with Replication Lag”</a> we will get more precise about eventual consistency and
        discuss things like the <em>read-your-writes</em> and <em>monotonic reads</em> guarantees.</p>
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Leaders and Followers"><div class="sect1" id="sec_replication_leader">
        <h1>Leaders and Followers</h1>
        
        <p><a data-type="indexterm" data-primary="replicas" id="idm45085120654144"></a>
        <a data-type="indexterm" data-primary="replication" data-secondary="single-leader" id="ix_replleadfol"></a>
        <a data-type="indexterm" data-primary="leader-based replication" data-seealso="replication" id="ix_leaderrepl"></a>
        <a data-type="indexterm" data-primary="active/passive replication" data-see="leader-based replication" id="idm45085120650368"></a>
        <a data-type="indexterm" data-primary="master-slave replication" data-see="leader-based replication" id="idm45085120649328"></a>
        <a data-type="indexterm" data-primary="primary-secondary replication" data-see="leader-based replication" id="idm45085120648192"></a>
        Each node that stores a copy of the database is called a <em>replica</em>. With multiple replicas, a
        question inevitably arises: how do we ensure that all the data ends up on all the replicas?</p>
        
        <p>Every write to the database needs to be processed by every replica; otherwise, the replicas would no
        longer contain the same data. The most common solution for this is called <em>leader-based
        replication</em> (also known as <em>active/passive</em> or <em>master–slave replication</em>) and is illustrated in
        <a data-type="xref" href="#fig_replication_leader_follower">Figure&nbsp;5-1</a>. It works as follows:</p>
        <ol>
        <li>
        <p>One of the replicas is designated the <em>leader</em> (also known as <em>master</em> or <em>primary</em>). When
        clients want to write to the database, they must send their requests to the leader, which first
        writes the new data to its local storage.</p>
        </li>
        <li>
        <p><a data-type="indexterm" data-primary="followers" data-seealso="leader-based replication" id="idm45085120640432"></a>
        <a data-type="indexterm" data-primary="read replicas" data-see="leader-based replication" id="idm45085120638864"></a>
        <a data-type="indexterm" data-primary="slaves" data-see="leader-based replication" id="idm45085120637744"></a>
        <a data-type="indexterm" data-primary="secondaries" data-see="leader-based replication" id="idm45085120636624"></a>
        <a data-type="indexterm" data-primary="standbys" data-see="leader-based replication" id="idm45085120635504"></a>
        <a data-type="indexterm" data-primary="hot standbys" data-see="leader-based replication" id="idm45085120634384"></a>
        <a data-type="indexterm" data-primary="logs (data structure)" data-secondary="replication" id="idm45085120633264"></a>
        The other replicas are known as <em>followers</em> (<em>read replicas</em>, <em>slaves</em>, <em>secondaries</em>, or
        <em>hot standbys</em>).<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm45085120629984-marker" href="ch05.html#idm45085120629984">i</a></sup> Whenever the leader writes new data to its local storage, it also sends the
        data change to all of its followers as part of a  <em>replication log</em> or
        <em>change stream</em>. Each follower takes the log from the leader and updates its local copy of the
        database accordingly, by applying all writes in the same order as they were processed on the
        leader.</p>
        </li>
        <li>
        <p>When a client wants to read from the database, it can query either the leader or any of the
        followers. However, writes are only accepted on the leader (the followers are read-only from the
        client’s point of view).</p>
        </li>
        
        </ol>
        
        <figure><div id="fig_replication_leader_follower" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_0501.png" alt="ddia 0501" width="2880" height="925">
        <h6><span class="label">Figure 5-1. </span>Leader-based (master–slave) replication.</h6>
        </div></figure>
        
        <p><a data-type="indexterm" data-primary="relational databases" data-secondary="leader-based replication" id="idm45085120622048"></a>
        <a data-type="indexterm" data-primary="PostgreSQL (database)" data-secondary="leader-based replication" id="idm45085120620528"></a>
        <a data-type="indexterm" data-primary="MySQL (database)" data-secondary="leader-based replication" id="idm45085120619408"></a>
        <a data-type="indexterm" data-primary="Oracle (database)" data-secondary="leader-based replication" id="idm45085120618288"></a>
        <a data-type="indexterm" data-primary="SQL Server (database)" data-secondary="leader-based replication" id="idm45085120617168"></a>
        <a data-type="indexterm" data-primary="MongoDB (database)" data-secondary="leader-based replication" id="idm45085120616048"></a>
        <a data-type="indexterm" data-primary="RethinkDB (database)" data-secondary="leader-based replication" id="idm45085120614928"></a>
        <a data-type="indexterm" data-primary="LinkedIn" data-secondary="Espresso (database)" id="idm45085120613808"></a>
        <a data-type="indexterm" data-primary="Kafka (messaging)" data-secondary="leader-based replication" id="idm45085120612704"></a>
        <a data-type="indexterm" data-primary="RabbitMQ (messaging)" data-secondary="leader-based replication" id="idm45085120611584"></a>
        <a data-type="indexterm" data-primary="DRBD (Distributed Replicated Block Device)" id="idm45085120610464"></a>
        This mode of replication is a built-in feature of many relational databases, such as PostgreSQL
        (since version 9.0), MySQL, Oracle Data Guard
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Oracle2013uz-marker" href="ch05.html#Oracle2013uz">2</a>],
        and SQL Server’s AlwaysOn Availability Groups
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="AlwaysOn2012-marker" href="ch05.html#AlwaysOn2012">3</a>].
        It is also used in some nonrelational databases, including MongoDB, RethinkDB, and Espresso
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Qiao2013uv_ch5-marker" href="ch05.html#Qiao2013uv_ch5">4</a>]. Finally, leader-based
        replication is not restricted to only databases: distributed message brokers such as Kafka
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Rao2013tf-marker" href="ch05.html#Rao2013tf">5</a>]
        and RabbitMQ highly available queues
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="RabbitMQ2013-marker" href="ch05.html#RabbitMQ2013">6</a>]
        also use it. Some network filesystems and replicated block devices such as DRBD are similar.</p>
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Synchronous Versus Asynchronous Replication"><div class="sect2" id="sec_replication_sync_async">
        <h2>Synchronous Versus Asynchronous Replication</h2>
        
        <p><a data-type="indexterm" data-primary="leader-based replication" data-secondary="synchronous versus asynchronous" id="ix_leadreplsyncasync"></a>
        <a data-type="indexterm" data-primary="replication" data-secondary="single-leader" data-tertiary="synchronous versus asynchronous" id="ix_replleadfolsyncasync"></a>
        An important detail of a replicated system is whether the replication happens <em>synchronously</em> or
        <em>asynchronously</em>. (In relational databases, this is often a configurable option; other systems are
        often hardcoded to be either one or the other.)</p>
        
        <p>Think about what happens in <a data-type="xref" href="#fig_replication_leader_follower">Figure&nbsp;5-1</a>, where the user of a website updates
        their profile image. At some point in time, the client sends the update request to the leader;
        shortly afterward, it is received by the leader. At some point, the leader forwards the data change
        to the followers. Eventually, the leader notifies the client that the update was successful.</p>
        
        <p><a data-type="xref" href="#fig_replication_sync_replication">Figure&nbsp;5-2</a> shows the communication between various components of the
        system: the user’s client, the leader, and two followers. Time flows from left to right. A request
        or response message is shown as a thick arrow.</p>
        
        <figure><div id="fig_replication_sync_replication" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_0502.png" alt="ddia 0502" width="2880" height="1128">
        <h6><span class="label">Figure 5-2. </span>Leader-based replication with one synchronous and one asynchronous follower.</h6>
        </div></figure>
        
        <p><a data-type="indexterm" data-primary="synchronous replication" id="idm45085120587472"></a>
        <a data-type="indexterm" data-primary="asynchronous replication" id="idm45085120586640"></a>
        In the example of <a data-type="xref" href="#fig_replication_sync_replication">Figure&nbsp;5-2</a>, the replication to follower 1 is
        <em>synchronous</em>: the leader waits until follower 1 has confirmed that it received the write before
        reporting success to the user, and before making the write visible to other clients. The replication
        to follower 2 is <em>asynchronous</em>: the leader sends the message, but doesn’t wait for a response from
        the follower.</p>
        
        <p>The diagram shows that there is a substantial delay before follower 2 processes the message.
        Normally, replication is quite fast: most database systems apply changes to followers in less than a
        second. However, there is no guarantee of how long it might take. There are circumstances when
        followers might fall behind the leader by several minutes or more; for example, if a follower is
        recovering from a failure, if the system is operating near maximum capacity, or if there are network
        problems between the nodes.</p>
        
        <p>The advantage of synchronous replication is that the follower is guaranteed to have an up-to-date
        copy of the data that is consistent with the leader. If the leader suddenly fails, we can be sure
        that the data is still available on the follower. The disadvantage is that if the synchronous
        follower doesn’t respond (because it has crashed, or there is a network fault, or for any other
        reason), the write cannot be processed. The leader must block all writes and wait until the
        synchronous replica is available again.</p>
        
        <p><a data-type="indexterm" data-primary="semi-synchronous replication" id="idm45085120581648"></a>
        For that reason, it is impracticable for all followers to be synchronous: any one node outage would
        cause the whole system to grind to a halt. In practice, if you enable synchronous replication on a
        database, it usually means that <em>one</em> of the followers is synchronous, and the others are
        asynchronous. If the synchronous follower becomes unavailable or slow, one of the asynchronous
        followers is made synchronous. This guarantees that you have an up-to-date copy of the data on at
        least two nodes: the leader and one synchronous follower. This configuration is sometimes also
        called <em>semi-synchronous</em> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Matsunobu2014wu-marker" href="ch05.html#Matsunobu2014wu">7</a>].</p>
        
        <p>Often, leader-based replication is configured to be completely asynchronous. In this case, if the
        leader fails and is not recoverable, any writes that have not yet been replicated to followers are
        lost. This means that a write is not guaranteed to be durable, even if it has been confirmed to the
        client. However, a fully asynchronous configuration has the advantage that the leader can continue
        processing writes, even if all of its followers have fallen behind.</p>
        
        <p>Weakening durability may sound like a bad trade-off, but asynchronous replication is nevertheless
        widely used, especially if there are many followers or if they are geographically distributed. We
        will return to this issue in <a data-type="xref" href="#sec_replication_lag">“Problems with Replication Lag”</a>.</p>
        <aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="sidebar_replication_research">
        <h5>Research on Replication</h5>
        <p><a data-type="indexterm" data-primary="chain replication" id="idm45085120572800"></a>
        <a data-type="indexterm" data-primary="replication" data-secondary="chain replication" id="idm45085120571776"></a>
        <a data-type="indexterm" data-primary="synchronous replication" data-secondary="chain replication" id="idm45085120570672"></a>
        <a data-type="indexterm" data-primary="Microsoft" data-secondary="Azure Storage" id="idm45085120569568"></a>
        <a data-type="indexterm" data-primary="Azure" data-see="Microsoft" id="idm45085120568464"></a>
        It can be a serious problem for asynchronously replicated systems to lose data if the leader fails,
        so researchers have continued investigating replication methods that do not lose data but still
        provide good performance and availability. For example, <em>chain replication</em>
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="vanRenesse2004td_ch5-marker" href="ch05.html#vanRenesse2004td_ch5">8</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Terrace2009vx-marker" href="ch05.html#Terrace2009vx">9</a>] is a variant of synchronous replication
        that has been successfully implemented in a few systems such as Microsoft Azure Storage
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Calder2011to-marker" href="ch05.html#Calder2011to">10</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Wang2016vy-marker" href="ch05.html#Wang2016vy">11</a>].</p>
        
        <p><a data-type="indexterm" data-primary="consensus" data-secondary="relation to replication" id="idm45085120557200"></a>
        There is a strong connection between consistency of replication and <em>consensus</em> (getting several
        nodes to agree on a value), and we will explore this area of theory in more detail in
        <a data-type="xref" href="ch09.html#ch_consistency">Chapter&nbsp;9</a>. In this chapter we will concentrate on the simpler forms of replication that are
        most commonly used in databases in practice.
        <a data-type="indexterm" data-primary="leader-based replication" data-secondary="synchronous versus asynchronous" data-startref="ix_leadreplsyncasync" id="idm45085120554528"></a>
        <a data-type="indexterm" data-primary="replication" data-secondary="single-leader" data-tertiary="synchronous versus asynchronous" data-startref="ix_replleadfolsyncasync" id="idm45085120553248"></a></p>
        </div></aside>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Setting Up New Followers"><div class="sect2" id="sec_replication_new_replica">
        <h2>Setting Up New Followers</h2>
        
        <p><a data-type="indexterm" data-primary="leader-based replication" data-secondary="setting up new followers" id="idm45085120549920"></a>
        <a data-type="indexterm" data-primary="replication" data-secondary="single-leader" data-tertiary="setting up new followers" id="idm45085120548736"></a>
        From time to time, you need to set up new followers—perhaps to increase the number of replicas,
        or to replace failed nodes. How do you ensure that the new follower has an accurate copy of the
        leader’s data?</p>
        
        <p>Simply copying data files from one node to another is typically not sufficient: clients are
        constantly writing to the database, and the data is always in flux, so a standard file copy would
        see different parts of the database at different points in time. The result might not make any
        sense.</p>
        
        <p>You could make the files on disk consistent by locking the database (making it unavailable for
        writes), but that would go against our goal of high availability. Fortunately, setting up a
        follower can usually be done without downtime. Conceptually, the process looks like this:</p>
        <ol>
        <li>
        <p><a data-type="indexterm" data-primary="backups" data-secondary="database snapshot for replication" id="idm45085120544800"></a>
        <a data-type="indexterm" data-primary="consistency" data-secondary="consistent snapshots" data-seealso="snapshots" id="idm45085120543568"></a>
        <a data-type="indexterm" data-primary="snapshots (databases)" data-secondary="setting up a new replica" id="idm45085120542192"></a>
        <a data-type="indexterm" data-primary="MySQL (database)" data-secondary="consistent snapshots" id="idm45085120541072"></a>
        <a data-type="indexterm" data-primary="Percona XtraBackup (MySQL tool)" id="idm45085120539968"></a>
        Take a consistent snapshot of the leader’s database at some point in time—if possible, without
        taking a lock on the entire database. Most databases have this feature, as it is also required
        for backups. In some cases, third-party tools are needed, such as <em>innobackupex</em> for MySQL
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Xtrabackup2014-marker" href="ch05.html#Xtrabackup2014">12</a>].</p>
        </li>
        <li>
        <p>Copy the snapshot to the new follower node.</p>
        </li>
        <li>
        <p><a data-type="indexterm" data-primary="logs (data structure)" data-secondary="replication" data-tertiary="coordination with snapshot" id="idm45085120535152"></a>
        <a data-type="indexterm" data-primary="log sequence number" id="idm45085120533584"></a>
        <a data-type="indexterm" data-primary="leader-based replication" data-secondary="log sequence number" id="idm45085120532752"></a>
        <a data-type="indexterm" data-primary="PostgreSQL (database)" data-secondary="log sequence number" id="idm45085120531632"></a>
        <a data-type="indexterm" data-primary="MySQL (database)" data-secondary="binlog coordinates" id="idm45085120530528"></a>
        The follower connects to the leader and requests all the data changes that have happened since
        the snapshot was taken. This requires that the snapshot is associated with an exact position in
        the leader’s replication log. That position has various names: for example, PostgreSQL calls it
        the <em>log sequence number</em>, and MySQL calls it the <em>binlog coordinates</em>.</p>
        </li>
        <li>
        <p>When the follower has processed the backlog of data changes since the snapshot, we say it has
        <em>caught up</em>. It can now continue to process data changes from the leader as they happen.</p>
        </li>
        
        </ol>
        
        <p>The practical steps of setting up a follower vary significantly by database. In some systems the
        process is fully automated, whereas in others it can be a somewhat arcane multi-step workflow that
        needs to be manually performed by an administrator.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Handling Node Outages"><div class="sect2" id="sec_replication_failover">
        <h2>Handling Node Outages</h2>
        
        <p><a data-type="indexterm" data-primary="leader-based replication" data-secondary="handling node outages" id="idm45085120524288"></a>
        <a data-type="indexterm" data-primary="nodes (processes)" data-secondary="handling outages in leader-based replication" id="idm45085120523168"></a>
        Any node in the system can go down, perhaps unexpectedly due to a fault, but just as likely due to
        planned maintenance (for example, rebooting a machine to install a kernel security patch). Being
        able to reboot individual nodes without downtime is a big advantage for operations and maintenance.
        Thus, our goal is to keep the system as a whole running despite individual node failures, and to keep
        the impact of a node outage as small as possible.</p>
        
        <p>How do you achieve high availability with leader-based replication?</p>
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Follower failure: Catch-up recovery"><div class="sect3" id="idm45085120521056">
        <h3>Follower failure: Catch-up recovery</h3>
        
        <p>On its local disk, each follower keeps a log of the data changes it has received from the leader. If
        a follower crashes and is restarted, or if the network between the leader and the follower is
        temporarily interrupted, the follower can recover quite easily: from its log, it knows the last
        transaction that was processed before the fault occurred. Thus, the follower can connect to the
        leader and request all the data changes that occurred during the time when the follower was
        disconnected. When it has applied these changes, it has caught up to the leader and can continue
        receiving a stream of data changes as before.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Leader failure: Failover"><div class="sect3" id="idm45085120518784">
        <h3>Leader failure: Failover</h3>
        
        <p><a data-type="indexterm" data-primary="leader-based replication" data-secondary="failover" id="idm45085120517328"></a>
        <a data-type="indexterm" data-primary="replication" data-secondary="single-leader" data-tertiary="failover" id="idm45085120516160"></a>
        <a data-type="indexterm" data-primary="failover" data-seealso="leader-based replication" id="idm45085120514784"></a>
        Handling a failure of the leader is trickier: one of the followers needs to be promoted to be the
        new leader, clients need to be reconfigured to send their writes to the new leader, and the other
        followers need to start consuming data changes from the new leader. This process is called
        <em>failover</em>.</p>
        
        <p>Failover can happen manually (an administrator is notified that the leader has failed and takes the
        necessary steps to make a new leader) or automatically. An automatic failover process usually
        consists of the following steps:</p>
        <ol>
        <li>
        <p><em>Determining that the leader has failed.</em> There are many things that could potentially go wrong:
        crashes, power outages, network issues, and more. There is no foolproof way of detecting what
        has gone wrong, so most systems simply use a timeout: nodes frequently bounce messages back and
        forth between each other, and if a node doesn’t respond for some period of time—say, 30
        seconds—it is assumed to be dead. (If the leader is deliberately taken down for planned
        maintenance, this doesn’t apply.)</p>
        </li>
        <li>
        <p><em>Choosing a new leader.</em> This could be done through an election process (where the leader is chosen by
        a majority of the remaining replicas), or a new leader could be appointed by a previously elected
        <em>controller node</em>. The best candidate for leadership is usually the replica with the most
        up-to-date data changes from the old leader (to minimize any data loss). Getting all the nodes to
        agree on a new leader is a consensus problem, discussed in detail in <a data-type="xref" href="ch09.html#ch_consistency">Chapter&nbsp;9</a>.</p>
        </li>
        <li>
        <p><em>Reconfiguring the system to use the new leader.</em> Clients now need to send
        <span class="keep-together">their write requests</span> to the new leader (we discuss this
        in <a data-type="xref" href="ch06.html#sec_partitioning_routing">“Request Routing”</a>). If the old leader comes back, it might still believe that it is
        the leader, <span class="keep-together">not realizing</span> that the other replicas have
        forced it to step down. The system needs to ensure that the old leader becomes a follower and
        recognizes the new leader.</p>
        </li>
        
        </ol>
        
        <p>Failover is fraught with things that can go wrong:</p>
        
        <ul>
        <li>
        <p><a data-type="indexterm" data-primary="failover" data-secondary="potential problems" id="idm45085120502240"></a>
        <a data-type="indexterm" data-primary="distributed systems" data-secondary="issues with failover" id="idm45085120501136"></a>
        <a data-type="indexterm" data-primary="asynchronous replication" data-secondary="data loss on failover" id="idm45085120500032"></a>
        If asynchronous replication is used, the new leader may not have received all the writes from the old
        leader before it failed. If the former leader rejoins the cluster after a new leader has been
        chosen, what should happen to those writes? The new leader may have received conflicting writes
        in the meantime. The most common solution is for the old leader’s unreplicated writes to simply be
        discarded, which may violate clients’ durability expectations.</p>
        </li>
        <li>
        <p><a data-type="indexterm" data-primary="incidents" data-secondary="disclosure of sensitive data due to primary key reuse" id="idm45085120497728"></a>
        <a data-type="indexterm" data-primary="GitHub, postmortems" id="idm45085120496528"></a>
        Discarding writes is especially dangerous if other storage systems outside of the database need to
        be coordinated with the database contents.
        <a data-type="indexterm" data-primary="consistency" data-secondary="across different databases" id="idm45085120495536"></a>
        For example, in one incident at GitHub [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Newland2012tw-marker" href="ch05.html#Newland2012tw">13</a>], an out-of-date MySQL follower
        was promoted to leader. The database used an autoincrementing counter to assign primary keys to
        new rows, but because the new leader’s counter lagged behind the old leader’s, it reused some
        primary keys that were previously assigned by the old leader. These primary keys were also used in
        a Redis store, so the reuse of primary keys resulted in inconsistency between MySQL and Redis,
        which caused some private data to be disclosed to the wrong users.</p>
        </li>
        <li>
        <p><a data-type="indexterm" data-primary="split brain" id="idm45085120491008"></a><a data-type="indexterm" data-primary="STONITH (Shoot The Other Node In The Head)" id="idm45085120489904"></a>
        <a data-type="indexterm" data-primary="incidents" data-secondary="split brain due to 1-minute packet delay" id="idm45085120489072"></a>
        <a data-type="indexterm" data-primary="GitHub, postmortems" id="idm45085120487936"></a>
        <a data-type="indexterm" data-primary="fencing (preventing split brain)" id="idm45085120487104"></a>
        <a data-type="indexterm" data-primary="corruption of data" data-secondary="due to split brain" id="idm45085120486208"></a>
        <a data-type="indexterm" data-primary="data corruption" data-see="corruption of data" id="idm45085120485104"></a>
        In certain fault scenarios (see <a data-type="xref" href="ch08.html#ch_distributed">Chapter&nbsp;8</a>), it could happen that two nodes both believe
        that they are the leader. This situation is called <em>split brain</em>, and it is dangerous: if both
        leaders accept writes, and there is no process for resolving conflicts (see
        <a data-type="xref" href="#sec_replication_multi_leader">“Multi-Leader Replication”</a>), data is likely to be lost or corrupted. As a safety catch, some
        systems have a mechanism to shut down one node if two leaders are
        detected.<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm45085120481520-marker" href="ch05.html#idm45085120481520">ii</a></sup>
        However, if this mechanism is not carefully designed, you can end up with both nodes being shut down
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Imbriaco2012tx_ch5-marker" href="ch05.html#Imbriaco2012tx_ch5">14</a>].</p>
        </li>
        <li>
        <p><a data-type="indexterm" data-primary="timeouts" data-secondary="for failover" id="idm45085120475984"></a>
        What is the right timeout before the leader is declared dead? A longer timeout means a longer
        time to recovery in the case where the leader fails. However, if the timeout is too short, there
        could be unnecessary failovers. For example, a temporary load spike could cause a node’s response
        time to increase above the timeout, or a network glitch could cause delayed packets. If the system
        is already struggling with high load or network problems, an unnecessary failover is likely to
        make the situation worse, not better.</p>
        </li>
        </ul>
        
        <p>There are no easy solutions to these problems. For this reason, some operations teams prefer to
        perform failovers manually, even if the software supports automatic failover.</p>
        
        <p>These issues—node failures; unreliable networks; and trade-offs around replica consistency,
        durability, availability, and latency—are in fact fundamental problems in distributed systems.
        In <a data-type="xref" href="ch08.html#ch_distributed">Chapter&nbsp;8</a> and <a data-type="xref" href="ch09.html#ch_consistency">Chapter&nbsp;9</a> we will discuss them in greater depth.</p>
        </div></section>
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Implementation of Replication Logs"><div class="sect2" id="sec_replication_implementation">
        <h2>Implementation of Replication Logs</h2>
        
        <p><a data-type="indexterm" data-primary="logs (data structure)" data-secondary="replication" id="ix_logsreplica"></a>
        <a data-type="indexterm" data-primary="replication" data-secondary="single-leader" data-tertiary="implementation of replication logs" id="ix_replleadfollog"></a>
        <a data-type="indexterm" data-primary="replication logs" data-see="logs" id="idm45085120466384"></a>
        How does leader-based replication work under the hood? Several different replication methods are
        used in practice, so let’s look at each one briefly.</p>
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Statement-based replication"><div class="sect3" id="idm45085120464976">
        <h3>Statement-based replication</h3>
        
        <p><a data-type="indexterm" data-primary="statement-based replication" id="idm45085120463600"></a>
        <a data-type="indexterm" data-primary="leader-based replication" data-secondary="implementation of replication logs" data-tertiary="statement-based" id="idm45085120462592"></a>
        <a data-type="indexterm" data-primary="relational databases" data-secondary="statement-based replication" id="idm45085120461184"></a>
        <a data-type="indexterm" data-primary="logs (data structure)" data-secondary="replication" data-tertiary="statement-based replication" id="idm45085120460064"></a>
        <a data-type="indexterm" data-primary="SQL (Structured Query Language)" data-secondary="statement-based replication" id="idm45085120458672"></a>
        In the simplest case, the leader logs every write request (<em>statement</em>) that it executes and sends
        that statement log to its followers. For a relational database, this means that every <code>INSERT</code>,
        <code>UPDATE</code>, or <code>DELETE</code> statement is forwarded to followers, and each follower parses and executes
        that SQL statement as if it had been received from a <span class="keep-together">client.</span></p>
        
        <p>Although this may sound reasonable, there are various ways in which this approach to replication can
        break down:</p>
        
        <ul>
        <li>
        <p>Any statement that calls a nondeterministic function, such as <code>NOW()</code> to get the current date
        and time or <code>RAND()</code> to get a random number, is likely to generate a different value on each
        replica.</p>
        </li>
        <li>
        <p>If statements use an autoincrementing column, or if they depend on the existing data in the
        database (e.g., <code>UPDATE … WHERE <em>&lt;some condition&gt;</em></code>), they must be executed in exactly the same
        order on each replica, or else they may have a different effect. This can be limiting when there
        are multiple concurrently executing transactions.</p>
        </li>
        <li>
        <p>Statements that have side effects (e.g., triggers, stored procedures, user-defined functions) may
        result in different side effects occurring on each replica, unless the side effects are absolutely
        deterministic.</p>
        </li>
        </ul>
        
        <p>It is possible to work around those issues—for example, the leader can replace any
        nondeterministic function calls with a fixed return value when the statement is logged so that the
        followers all get the same value. However, because there are so many edge cases, other replication
        methods are now generally preferred.</p>
        
        <p><a data-type="indexterm" data-primary="MySQL (database)" data-secondary="statement-based replication" id="idm45085120448240"></a>
        <a data-type="indexterm" data-primary="VoltDB (database)" data-secondary="statement-based replication" id="idm45085120447120"></a>
        Statement-based replication was used in MySQL before version 5.1. It is still sometimes used today,
        as it is quite compact, but by default MySQL now switches to row-based replication (discussed shortly) if
        there is any nondeterminism in a statement. VoltDB uses statement-based replication, and makes it
        safe by requiring transactions to be deterministic
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Hugg2015wp-marker" href="ch05.html#Hugg2015wp">15</a>].</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Write-ahead log (WAL) shipping"><div class="sect3" id="idm45085120443472">
        <h3>Write-ahead log (WAL) shipping</h3>
        
        <p><a data-type="indexterm" data-primary="write-ahead log (WAL)" id="idm45085120442032"></a>
        <a data-type="indexterm" data-primary="logs (data structure)" data-secondary="replication" data-tertiary="write-ahead log (WAL) shipping" id="idm45085120441200"></a>
        <a data-type="indexterm" data-primary="leader-based replication" data-secondary="implementation of replication logs" data-tertiary="write-ahead log (WAL) shipping" id="idm45085120439808"></a>
        In <a data-type="xref" href="ch03.html#ch_storage">Chapter&nbsp;3</a> we discussed how storage engines represent data on disk, and we found that usually
        every write is appended to a log:</p>
        
        <ul>
        <li>
        <p>In the case of a log-structured storage engine (see <a data-type="xref" href="ch03.html#sec_storage_lsm_trees">“SSTables and LSM-Trees”</a>), this log is the
        main place for storage. Log segments are compacted and garbage-collected in the background.</p>
        </li>
        <li>
        <p>In the case of a B-tree (see <a data-type="xref" href="ch03.html#sec_storage_b_trees">“B-Trees”</a>), which overwrites individual disk blocks,
        every modification is first written to a write-ahead log so that the index can be restored
        to a consistent state after a crash.</p>
        </li>
        </ul>
        
        <p>In either case, the log is an append-only sequence of bytes containing all writes to the database.
        We can use the exact same log to build a replica on another node: besides writing the log to disk,
        the leader also sends it across the network to its followers. When the follower processes this log,
        it builds a copy of the exact same data structures as found on the leader.</p>
        
        <p><a data-type="indexterm" data-primary="PostgreSQL (database)" data-secondary="WAL-based replication" id="idm45085120432272"></a>
        <a data-type="indexterm" data-primary="Oracle (database)" data-secondary="WAL-based replication" id="idm45085120431168"></a>
        This method of replication is used in PostgreSQL and Oracle, among others
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="WALInternalsOfPos2012vf-marker" href="ch05.html#WALInternalsOfPos2012vf">16</a>].
        The main disadvantage is that the log describes the data on a very low level: a WAL contains details
        of which bytes were changed in which disk blocks. This makes replication closely coupled to the
        storage engine. If the database changes its storage format from one version to another, it is
        typically not possible to run different versions of the database software on the leader and the
        followers.</p>
        
        <p>That may seem like a minor implementation detail, but it can have a big operational impact. If the
        replication protocol allows the follower to use a newer software version than the leader, you can
        perform a zero-downtime upgrade of the database software by first upgrading the followers and then
        performing a failover to make one of the upgraded nodes the new leader. If the replication protocol
        does not allow this version mismatch, as is often the case with WAL shipping, such upgrades require
        downtime.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Logical (row-based) log replication"><div class="sect3" id="idm45085120426272">
        <h3>Logical (row-based) log replication</h3>
        
        <p><a data-type="indexterm" data-primary="logs (data structure)" data-secondary="replication" data-tertiary="logical (row-based) replication" id="idm45085120424976"></a>
        <a data-type="indexterm" data-primary="row-oriented storage" data-secondary="row-based replication" id="idm45085120423648"></a>
        <a data-type="indexterm" data-primary="logical logs" id="idm45085120422544"></a>
        An alternative is to use different log formats for replication and for the storage engine, which
        allows the replication log to be decoupled from the storage engine internals. This kind of
        replication log is called a <em>logical log</em>, to distinguish it from the storage engine’s (<em>physical</em>)
        data representation.</p>
        
        <p><a data-type="indexterm" data-primary="relational databases" data-secondary="logical logs" id="idm45085120420288"></a>
        A logical log for a relational database is usually a sequence of records describing writes to
        database tables at the granularity of a row:</p>
        
        <ul>
        <li>
        <p>For an inserted row, the log contains the new values of all columns.</p>
        </li>
        <li>
        <p>For a deleted row, the log contains enough information to uniquely identify the row that was
        deleted. Typically this would be the primary key, but if there is no primary key on the table, the
        old values of all columns need to be logged.</p>
        </li>
        <li>
        <p>For an updated row, the log contains enough information to uniquely identify the updated row, and
        the new values of all columns (or at least the new values of all columns that changed).</p>
        </li>
        </ul>
        
        <p><a data-type="indexterm" data-primary="MySQL (database)" data-secondary="row-based replication" id="idm45085120415184"></a>
        A transaction that modifies several rows generates several such log records, followed by a record
        indicating that the transaction was committed. MySQL’s binlog (when configured to use row-based
        replication) uses this approach
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="MySQLInternals-marker" href="ch05.html#MySQLInternals">17</a>].</p>
        
        <p>Since a logical log is decoupled from the storage engine internals, it can more easily be kept
        backward compatible, allowing the leader and the follower to run different versions of the database
        software, or even different storage engines.</p>
        
        <p><a data-type="indexterm" data-primary="change data capture" id="idm45085120411040"></a>
        A logical log format is also easier for external applications to parse. This aspect is useful if you want
        to send the contents of a database to an external system, such as a data warehouse for offline
        analysis, or for building custom indexes and caches
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Sharma2015te_ch5-marker" href="ch05.html#Sharma2015te_ch5">18</a>].
        This technique is called <em>change data capture</em>, and we will return to it in <a data-type="xref" href="ch11.html#ch_stream">Chapter&nbsp;11</a>.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Trigger-based replication"><div class="sect3" id="sec_replication_trigger">
        <h3>Trigger-based replication</h3>
        
        <p><a data-type="indexterm" data-primary="triggers (databases)" data-secondary="implementing replication" id="idm45085120404624"></a>
        <a data-type="indexterm" data-primary="logs (data structure)" data-secondary="replication" data-tertiary="trigger-based replication" id="idm45085120403456"></a>
        <a data-type="indexterm" data-primary="leader-based replication" data-secondary="implementation of replication logs" data-tertiary="trigger-based replication" id="idm45085120402064"></a>
        The replication approaches described so far are implemented by the database system, without
        involving any application code. In many cases, that’s what you want—but there are some
        circumstances where more flexibility is needed. For example, if you want to only replicate a subset
        of the data, or want to replicate from one kind of <span class="keep-together">database</span>
        to another, or if you need conflict resolution logic (see <a data-type="xref" href="#sec_replication_write_conflicts">“Handling Write Conflicts”</a>), then
        you may need to move replication up to the application layer.</p>
        
        <p><a data-type="indexterm" data-primary="stored procedures" id="idm45085120398448"></a>
        <a data-type="indexterm" data-primary="triggers (databases)" id="idm45085120397376"></a>
        <a data-type="indexterm" data-primary="Oracle (database)" data-secondary="GoldenGate (change data capture)" id="idm45085120396544"></a>
        <a data-type="indexterm" data-primary="GoldenGate (change data capture)" id="idm45085120395472"></a>
        Some tools, such as Oracle GoldenGate
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Oracle2013ub-marker" href="ch05.html#Oracle2013ub">19</a>],
        can make data changes available to an application by reading the database log. An alternative is to use features that are available in
        many relational databases: <em>triggers</em> and <em>stored procedures</em>.</p>
        
        <p><a data-type="indexterm" data-primary="LinkedIn" data-secondary="Databus (change data capture)" id="idm45085120391632"></a>
        <a data-type="indexterm" data-primary="PostgreSQL (database)" data-secondary="Bucardo (trigger-based replication)" id="idm45085120390512"></a>
        A trigger lets you register custom application code that is automatically executed when a data
        change (write transaction) occurs in a database system. The trigger has the opportunity to log this
        change into a separate table, from which it can be read by an external process. That external
        process can then apply any necessary application logic and replicate the data change to another
        system. Databus for Oracle
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Das2012uf_ch5-marker" href="ch05.html#Das2012uf_ch5">20</a>]
        and Bucardo for Postgres [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Mullane2014uy-marker" href="ch05.html#Mullane2014uy">21</a>]
        work like this, for example.</p>
        
        <p>Trigger-based replication typically has greater overheads than other replication methods, and is
        more prone to bugs and limitations than the database’s built-in replication. However, it can
        nevertheless be useful due to its flexibility.
        <a data-type="indexterm" data-primary="logs (data structure)" data-secondary="replication" data-startref="ix_logsreplica" id="idm45085120383824"></a>
        <a data-type="indexterm" data-primary="replication" data-secondary="single-leader" data-tertiary="implementation of replication logs" data-startref="ix_replleadfollog" id="idm45085120382448"></a>
        <a data-type="indexterm" data-primary="replication" data-secondary="single-leader" data-startref="ix_replleadfol" id="idm45085120380736"></a>
        <a data-type="indexterm" data-primary="leader-based replication" data-startref="ix_leaderrepl" id="idm45085120379360"></a></p>
        </div></section>
        
        
        
        </div></section>
        
        
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Problems with Replication Lag"><div class="sect1" id="sec_replication_lag">
        <h1>Problems with Replication Lag</h1>
        
        <p><a data-type="indexterm" data-primary="concurrency" data-secondary="in replicated systems" id="ix_concrepllag"></a>
        <a data-type="indexterm" data-primary="replication" data-secondary="consistency properties" id="ix_replproblag"></a>
        <a data-type="indexterm" data-primary="scalability" data-secondary="replication and" id="idm45085120373808"></a>
        Being able to tolerate node failures is just one reason for wanting replication. As mentioned
        in the introduction to <a data-type="xref" href="part02.html#part_distributed_data">Part&nbsp;II</a>, other reasons are scalability (processing more
        requests than a single machine can handle) and latency (placing replicas geographically closer to
        users).</p>
        
        <p><a data-type="indexterm" data-primary="leader-based replication" data-secondary="read-scaling architecture" id="idm45085120371344"></a>
        <a data-type="indexterm" data-primary="read-scaling architecture" id="idm45085120370256"></a>
        Leader-based replication requires all writes to go through a single node, but read-only queries can
        go to any replica. For workloads that consist of mostly reads and only a small percentage of writes
        (a common pattern on the web), there is an attractive option: create many followers, and distribute
        the read requests across those followers. This removes load from the leader and allows read requests to be
        served by nearby replicas.</p>
        
        <p>In this <em>read-scaling</em> architecture, you can increase the capacity for serving read-only requests
        simply by adding more followers. However, this approach only realistically works with asynchronous
        replication—if you tried to synchronously replicate to all followers, a single node failure or
        network outage would make the entire system unavailable for writing. And the more nodes you have,
        the likelier it is that one will be down, so a fully synchronous configuration would be very unreliable.</p>
        
        <p><a data-type="indexterm" data-primary="asynchronous replication" data-secondary="reads from asynchronous follower" id="idm45085120367344"></a>
        <a data-type="indexterm" data-primary="eventual consistency" id="idm45085120366096"></a>
        <a data-type="indexterm" data-primary="consistency" data-secondary="eventual" data-seealso="eventual consistency" id="idm45085120365264"></a>
        <a data-type="indexterm" data-primary="relational databases" data-secondary="eventual consistency" id="idm45085120363888"></a>
        Unfortunately, if an application reads from an <em>asynchronous</em> follower, it may see outdated
        information if the follower has fallen behind. This leads to apparent inconsistencies in the
        database: if you run the same query on the leader and a follower at the same time, you may get
        different results, because not all writes have been reflected in the follower. This inconsistency is
        just a temporary state—if you stop writing to the database and wait a while, the followers will
        eventually catch up and become consistent with the leader. For that reason, this effect is known
        as <em>eventual consistency</em> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Vogels2008ey-marker" href="ch05.html#Vogels2008ey">22</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Terry2011vp-marker" href="ch05.html#Terry2011vp">23</a>].<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm45085120356736-marker" href="ch05.html#idm45085120356736">iii</a></sup></p>
        
        <p>The term “eventually” is deliberately vague: in general, there is no limit to how far a replica can
        fall behind. In normal operation, the delay between a write happening on the leader and being
        reflected on a follower—the <em>replication lag</em>—may be only a fraction of a second, and not
        noticeable in practice. However, if the system is operating near capacity or if there is a problem
        in the network, the lag can easily increase to several seconds or even minutes.</p>
        
        <p>When the lag is so large, the inconsistencies it introduces are not just a theoretical issue but a
        real problem for applications. In this section we will highlight three examples of problems that are
        likely to occur when there is replication lag and outline some approaches to solving them.</p>
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Reading Your Own Writes"><div class="sect2" id="sec_replication_ryw">
        <h2>Reading Your Own Writes</h2>
        
        <p><a data-type="indexterm" data-primary="consistency" data-secondary="read-after-write" id="ix_consreadwrite"></a>
        <a data-type="indexterm" data-primary="replication" data-secondary="consistency properties" data-tertiary="reading your own writes" id="idm45085120348320"></a>
        Many applications let the user submit some data and then view what they have submitted. This might
        be a record in a customer database, or a comment on a discussion thread, or something else of that sort.
        When new data is submitted, it must be sent to the leader, but when the user views the data, it can
        be read from a follower. This is especially appropriate if data is frequently viewed but only
        occasionally written.</p>
        
        <p><a data-type="indexterm" data-primary="staleness (old data)" id="idm45085120346256"></a>
        With asynchronous replication, there is a problem, illustrated in
        <a data-type="xref" href="#fig_replication_read_your_writes">Figure&nbsp;5-3</a>: if the user views the data shortly after making a write, the
        new data may not yet have reached the replica. To the user, it looks as though the data they
        submitted was lost, so they will be understandably unhappy.</p>
        
        <figure><div id="fig_replication_read_your_writes" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_0503.png" alt="ddia 0503" width="2880" height="1128">
        <h6><span class="label">Figure 5-3. </span>A user makes a write, followed by a read from a stale replica. To prevent this anomaly, we need read-after-write consistency.</h6>
        </div></figure>
        
        <p><a data-type="indexterm" data-primary="read-after-write consistency" id="idm45085120342000"></a>
        In this situation, we need <em>read-after-write consistency</em>, also known as <em>read-your-writes consistency</em>
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Terry1994fp-marker" href="ch05.html#Terry1994fp">24</a>].
        This is a guarantee that if the user reloads the page, they will always see any updates they
        submitted themselves. It makes no promises about other users: other users’ updates may not be
        visible until some later time. However, it reassures the user that their own input has been saved
        correctly.</p>
        
        <p>How can we implement read-after-write consistency in a system with leader-based replication? There
        are various possible techniques. To mention a few:</p>
        
        <ul>
        <li>
        <p>When reading something that the user may have modified, read it from the leader; otherwise, read it
        from a follower. This requires that you have some way of knowing whether something might have been
        modified, without actually querying it. For example, user profile information on a social network
        is normally only editable by the owner of the profile, not by anybody else. Thus, a simple
        rule is: always read the user’s own profile from the leader, and any other users’ profiles from a
        follower.</p>
        </li>
        <li>
        <p>If most things in the application are potentially editable by the user, that approach won’t be
        effective, as most things would have to be read from the leader (negating the benefit of read
        scaling). In that case, other criteria may be used to decide whether to read from the leader. For
        example, you could track the time of the last update and, for one minute after the last update, make all
        reads from the leader. You could also monitor the replication lag on followers and
        prevent queries on any follower that is more than one minute behind the leader.</p>
        </li>
        <li>
        <p><a data-type="indexterm" data-primary="timestamps" data-secondary="for read-after-write consistency" id="idm45085120332544"></a>
        The client can remember the timestamp of its most recent write—then the system can ensure that the
        replica serving any reads for that user reflects updates at least until that timestamp. If a
        replica is not sufficiently up to date, either the read can be handled by another replica or the
        query can wait until the replica has caught up.
        <a data-type="indexterm" data-primary="logical clocks" data-secondary="for read-after-write consistency" id="idm45085120331024"></a>
        The timestamp could be a <em>logical timestamp</em> (something that indicates ordering of writes, such as
        the log sequence number) or the actual system clock (in which case clock synchronization becomes
        critical; see <a data-type="xref" href="ch08.html#sec_distributed_clocks">“Unreliable Clocks”</a>).</p>
        </li>
        <li>
        <p><a data-type="indexterm" data-primary="datacenters" data-secondary="geographically distributed" id="idm45085120327712"></a>
        <a data-type="indexterm" data-primary="geographically distributed datacenters" id="idm45085120326544"></a>
        If your replicas are distributed across multiple datacenters (for geographical proximity to users
        or for availability), there is additional complexity. Any request that needs to be served by the
        leader must be routed to the datacenter that contains the leader.</p>
        </li>
        </ul>
        
        <p><a data-type="indexterm" data-primary="read-after-write consistency" data-secondary="cross-device" id="idm45085120324912"></a>
        Another complication arises when the same user is accessing your service from multiple devices, for
        example a desktop web browser and a mobile app. In this case you may want to provide <em>cross-device</em>
        read-after-write consistency: if the user enters some information on one device and then views it
        on another device, they should see the information they just entered.</p>
        
        <p>In this case, there are some additional issues to consider:</p>
        
        <ul>
        <li>
        <p>Approaches that require remembering the timestamp of the user’s last update become more difficult,
        because the code running on one device doesn’t know what updates have happened on the other
        device. This metadata will need to be centralized.</p>
        </li>
        <li>
        <p>If your replicas are distributed across different datacenters, there is no guarantee that
          connections from different devices will be routed to the same datacenter. (For example, if the user’s desktop
          computer uses the home broadband connection and their mobile device uses the cellular data network,
          the devices’ network routes may be completely different.) If your approach requires reading from the
          leader, you may first need to route requests from all of a user’s devices to the same datacenter.
        <a data-type="indexterm" data-primary="consistency" data-secondary="read-after-write" data-startref="ix_consreadwrite" id="idm45085120319808"></a></p>
        </li>
        </ul>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Monotonic Reads"><div class="sect2" id="sec_replication_monotonic_reads">
        <h2>Monotonic Reads</h2>
        
        <p><a data-type="indexterm" data-primary="consistency" data-secondary="monotonic reads" id="ix_consmonoread"></a>
        <a data-type="indexterm" data-primary="monotonic reads" id="idm45085120315232"></a>
        <a data-type="indexterm" data-primary="replication" data-secondary="consistency properties" data-tertiary="monotonic reads" id="idm45085120314400"></a>
        Our second example of an anomaly that can occur when reading from asynchronous followers is that it’s
        possible for a user to see things <em>moving backward in time</em>.</p>
        
        <p>This can happen if a user makes several reads from different replicas. For example,
        <a data-type="xref" href="#fig_replication_monotonic_reads">Figure&nbsp;5-4</a> shows user 2345 making the same query twice, first to a follower
        with little lag, then to a follower with greater lag. (This scenario is quite likely if the user
        refreshes a web page, and each request is routed to a random server.) The first query returns a
        comment that was recently added by user 1234, but the second query doesn’t return anything because
        the lagging follower has not yet picked up that write. In effect, the second query observes the
        system state at an earlier point in time than the first query. This wouldn’t be so bad if the first query
        hadn’t returned anything, because user 2345 probably wouldn’t know that user 1234 had recently added
        a comment. However, it’s very confusing for user 2345 if they first see user 1234’s comment appear,
        and then see it disappear again.</p>
        
        <figure><div id="fig_replication_monotonic_reads" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_0504.png" alt="ddia 0504" width="2880" height="1474">
        <h6><span class="label">Figure 5-4. </span>A user first reads from a fresh replica, then from a stale replica. Time appears to go backward. To prevent this anomaly, we need monotonic reads.</h6>
        </div></figure>
        
        <p><em>Monotonic reads</em> [<a data-type="noteref" href="ch05.html#Terry2011vp">23</a>] is a guarantee that this
        kind of anomaly does not happen. It’s a lesser guarantee than strong consistency, but a stronger
        guarantee than eventual consistency. When you read data, you may see an old value; monotonic reads
        only means that if one user makes several reads in sequence, they will not see time go
        backward—i.e., they will not read older data after having previously read newer data.</p>
        
        <p>One way of achieving monotonic reads is to make sure that each user always makes their reads from
        the same replica (different users can read from different replicas). For example, the replica can be
        chosen based on a hash of the user ID, rather than randomly. However, if that replica fails, the
        user’s queries will need to be rerouted to another replica.
        <a data-type="indexterm" data-primary="consistency" data-secondary="monotonic reads" data-startref="ix_consmonoread" id="idm45085120305520"></a></p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Consistent Prefix Reads"><div class="sect2" id="sec_replication_consistent_prefix">
        <h2>Consistent Prefix Reads</h2>
        
        <p><a data-type="indexterm" data-primary="causality" data-secondary="violations of" id="idm45085120302576"></a>
        <a data-type="indexterm" data-primary="consistency" data-secondary="consistent prefix reads" id="ix_consprefix"></a>
        <a data-type="indexterm" data-primary="consistent prefix reads" id="idm45085120300096"></a>
        <a data-type="indexterm" data-primary="replication" data-secondary="consistency properties" data-tertiary="consistent prefix reads" id="idm45085120299264"></a>
        Our third example of replication lag anomalies concerns violation of causality. Imagine the
        following short dialog between Mr. Poons and Mrs. Cake:</p>
        <dl>
        <dt>Mr. Poons</dt>
        <dd>
        <p>How far into the future can you see, Mrs. Cake?</p>
        </dd>
        <dt>Mrs. Cake</dt>
        <dd>
        <p>About ten seconds usually, Mr. Poons.</p>
        </dd>
        </dl>
        
        <p>There is a causal dependency between those two sentences: Mrs. Cake heard Mr. Poons’s question and
        answered it.</p>
        
        <p>Now, imagine a third person is listening to this conversation through followers. The things said by
        Mrs. Cake go through a follower with little lag, but the things said by Mr. Poons have a longer
        replication lag (see <a data-type="xref" href="#fig_replication_consistent_prefix">Figure&nbsp;5-5</a>). This observer would hear the following:</p>
        <dl>
        <dt>Mrs. Cake</dt>
        <dd>
        <p>About ten seconds usually, Mr. Poons.</p>
        </dd>
        <dt>Mr. Poons</dt>
        <dd>
        <p>How far into the future can you see, Mrs. Cake?</p>
        </dd>
        </dl>
        
        <p>To the observer it looks as though Mrs. Cake is answering the question before Mr. Poons has even asked
        it. Such psychic powers are impressive, but very confusing
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Pratchett1991wj-marker" href="ch05.html#Pratchett1991wj">25</a>].</p>
        
        <figure><div id="fig_replication_consistent_prefix" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_0505.png" alt="ddia 0505" width="2880" height="1843">
        <h6><span class="label">Figure 5-5. </span>If some partitions are replicated slower than others, an observer may see the answer before they see the question.</h6>
        </div></figure>
        
        <p>Preventing this kind of anomaly requires another type of guarantee: <em>consistent prefix reads</em>
        [<a data-type="noteref" href="ch05.html#Terry2011vp">23</a>]. This guarantee says that if a sequence of
        writes happens in a certain order, then anyone reading those writes will see them appear in the same
        order.</p>
        
        <p>This is a particular problem in partitioned (sharded) databases, which we will discuss in
        <a data-type="xref" href="ch06.html#ch_partitioning">Chapter&nbsp;6</a>. If the database always applies writes in the same order, reads always see a
        consistent prefix, so this anomaly cannot happen. However, in many distributed databases, different
        partitions operate independently, so there is no global ordering of writes: when a user reads from
        the database, they may see some parts of the database in an older state and some in a newer state.</p>
        
        <p>One solution is to make sure that any writes that are causally related to each other are written to
        the same partition—but in some applications that cannot be done efficiently. There are also
        algorithms that explicitly keep track of causal dependencies, a topic that we will return to in
        <a data-type="xref" href="#sec_replication_happens_before">“The “happens-before” relationship and concurrency”</a>.
        <a data-type="indexterm" data-primary="consistency" data-secondary="consistent prefix reads" data-startref="ix_consprefix" id="idm45085120279936"></a></p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Solutions for Replication Lag"><div class="sect2" id="idm45085120303552">
        <h2>Solutions for Replication Lag</h2>
        
        <p>When working with an eventually consistent system, it is worth thinking about how the application
        behaves if the replication lag increases to several minutes or even hours. If the answer is “no
        problem,” that’s great. However, if the result is a bad experience for users, it’s important to
        design the system to provide a stronger guarantee, such as read-after-write. Pretending that
        replication is synchronous when in fact it is asynchronous is a recipe for problems down the line.</p>
        
        <p>As discussed earlier, there are ways in which an application can provide a stronger guarantee than
        the underlying database—for example, by performing certain kinds of reads on the leader. However,
        dealing with these issues in application code is complex and easy to get wrong.</p>
        
        <p>It would be better if application developers didn’t have to worry about subtle replication issues
        and could just trust their databases to “do the right thing.” This is why <em>transactions</em> exist: they
        are a way for a database to provide stronger guarantees so that the application can be simpler.</p>
        
        <p>Single-node transactions have existed for a long time. However, in the move to distributed
        (replicated and partitioned) databases, many systems have abandoned them, claiming that transactions
        are too expensive in terms of performance and availability, and asserting that eventual consistency
        is inevitable in a scalable system. There is some truth in that statement, but it is overly
        simplistic, and we will develop a more nuanced view over the course of the rest of this book. We
        will return to the topic of transactions in Chapters
        <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch07.html#ch_transactions">7</a>
        and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch09.html#ch_consistency">9</a>,
        and we will discuss some alternative mechanisms in <a data-type="xref" href="part03.html#part_systems">Part&nbsp;III</a>.
        <a data-type="indexterm" data-primary="replication" data-secondary="consistency properties" data-startref="ix_replproblag" id="idm45085120270528"></a></p>
        </div></section>
        
        
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect1" class="pagebreak-before" data-pdf-bookmark="Multi-Leader Replication"><div class="sect1" id="sec_replication_multi_leader">
        <h1>Multi-Leader Replication</h1>
        
        <p><a data-type="indexterm" data-primary="replication" data-secondary="multi-leader" id="ix_replmultilead"></a>
        <a data-type="indexterm" data-primary="multi-leader replication" data-seealso="replication" id="ix_multileadrepl"></a>
        <a data-type="indexterm" data-primary="master-master replication" data-see="multi-leader replication" id="idm45085120264528"></a>
        <a data-type="indexterm" data-primary="active/active replication" data-see="multi-leader replication" id="idm45085120263392"></a>
        So far in this chapter we have only considered replication architectures using a single leader.
        Although that is a common approach, there are interesting alternatives.</p>
        
        <p>Leader-based replication has one major downside: there is only one leader, and all writes must go
        through it.<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm45085120261552-marker" href="ch05.html#idm45085120261552">iv</a></sup> If you can’t connect to the leader for any reason, for example due to a network
        interruption between you and the leader, you can’t write to the database.</p>
        
        <p>A natural extension of the leader-based replication model is to allow more than one node to accept
        writes. Replication still happens in the same way: each node that processes a write must forward
        that data change to all the other nodes. We call this a <em>multi-leader</em> configuration (also known as
        <em>master–master</em> or <em>active/active replication</em>). In this setup, each leader simultaneously acts as a
        follower to the other leaders.</p>
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Use Cases for Multi-Leader Replication"><div class="sect2" id="idm45085120257280">
        <h2>Use Cases for Multi-Leader Replication</h2>
        
        <p><a data-type="indexterm" data-primary="multi-leader replication" data-secondary="use cases" id="idm45085120255856"></a>
        It rarely makes sense to use a multi-leader setup within a single datacenter, because the benefits
        rarely outweigh the added complexity. However, there are some situations in which this configuration
        is reasonable.</p>
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Multi-datacenter operation"><div class="sect3" id="sec_replication_multi_dc">
        <h3>Multi-datacenter operation</h3>
        
        <p><a data-type="indexterm" data-primary="coordination" data-secondary="cross-datacenter" id="idm45085120252512"></a>
        <a data-type="indexterm" data-primary="replication" data-secondary="multi-leader" data-tertiary="across multiple datacenters" id="idm45085120251408"></a>
        <a data-type="indexterm" data-primary="multi-leader replication" data-secondary="use cases" data-tertiary="multi-datacenter replication" id="idm45085120249968"></a>
        <a data-type="indexterm" data-primary="datacenters" data-secondary="replication across multiple" data-tertiary="multi-leader replication" id="idm45085120248560"></a>
        Imagine you have a database with replicas in several different datacenters (perhaps so that you can
        tolerate failure of an entire datacenter, or perhaps in order to be closer to your users). With a
        normal leader-based replication setup, the leader has to be in <em>one</em> of the datacenters, and all
        writes must go through that datacenter.</p>
        
        <p>In a multi-leader configuration, you can have a leader in <em>each</em> datacenter.
        <a data-type="xref" href="#fig_replication_multi_dc">Figure&nbsp;5-6</a> shows what this architecture might look like. Within each datacenter,
        regular leader–follower replication is used; between datacenters, each datacenter’s leader
        replicates its changes to the leaders in other datacenters.</p>
        
        <figure><div id="fig_replication_multi_dc" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_0506.png" alt="ddia 0506" width="2880" height="1401">
        <h6><span class="label">Figure 5-6. </span>Multi-leader replication across multiple datacenters.</h6>
        </div></figure>
        
        <p><a data-type="indexterm" data-primary="datacenters" data-secondary="replication across multiple" id="idm45085120242304"></a>
        <a data-type="indexterm" data-primary="distributed systems" data-secondary="multi-datacenter" id="idm45085120241184"></a>
        Let’s compare how the single-leader and multi-leader configurations fare in a multi-datacenter
        deployment:</p>
        <dl>
        <dt>Performance</dt>
        <dd>
        <p>In a single-leader configuration, every write must go over the internet to the datacenter with the
        leader.<a data-type="indexterm" data-primary="performance" data-secondary="of multi-leader replication" id="idm45085120238128"></a> This can add significant latency to
        writes and might contravene the purpose of having multiple datacenters in the first place. In a
        multi-leader configuration, every write can be processed in the local datacenter and is replicated
        asynchronously to the other datacenters. Thus, the inter-datacenter network delay is hidden from
        users, which means the perceived performance may be better.</p>
        </dd>
        <dt>Tolerance of datacenter outages</dt>
        <dd>
        <p>In a single-leader configuration, if the datacenter with the leader fails, failover can promote a
        follower in another datacenter to be leader. In a multi-leader configuration, each datacenter can
        continue operating independently of the others, and replication catches up when the failed
        datacenter comes back online.</p>
        </dd>
        <dt>Tolerance of network problems</dt>
        <dd>
        <p>Traffic between datacenters usually goes over the public internet,
        <a data-type="indexterm" data-primary="faults" data-secondary="network faults" data-tertiary="tolerance of, in multi-leader replication" id="idm45085120233744"></a>
        <a data-type="indexterm" data-primary="networks" data-secondary="faults" data-see="faults" id="idm45085120232320"></a>
        which may be less reliable than the local network within a datacenter. A single-leader
        configuration is very sensitive to problems in this inter-datacenter link, because writes are made
        synchronously over this link. A multi-leader configuration with asynchronous replication can
        usually tolerate network problems better: a temporary network interruption does not prevent writes
        being processed.</p>
        </dd>
        </dl>
        
        <p><a data-type="indexterm" data-primary="MySQL (database)" data-secondary="Tungsten Replicator (multi-leader replication)" id="idm45085120230144"></a>
        <a data-type="indexterm" data-primary="PostgreSQL (database)" data-secondary="BDR (multi-leader replication)" id="idm45085120228784"></a>
        <a data-type="indexterm" data-primary="Oracle (database)" data-secondary="GoldenGate (change data capture)" id="idm45085120227712"></a>
        <a data-type="indexterm" data-primary="GoldenGate (change data capture)" id="idm45085120226592"></a>
        Some databases support multi-leader configurations by default, but it is also often implemented with
        external tools, such as Tungsten Replicator for MySQL
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="TungstenReplicator-marker" href="ch05.html#TungstenReplicator">26</a>], BDR for PostgreSQL
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="PostgresBDR-marker" href="ch05.html#PostgresBDR">27</a>],
        and GoldenGate for Oracle [<a data-type="noteref" href="ch05.html#Oracle2013ub">19</a>].</p>
        
        <p>Although multi-leader replication has advantages, it also has a big downside: the same data may be
        concurrently modified in two different datacenters, and those write conflicts must be resolved
        (indicated as “conflict resolution” in <a data-type="xref" href="#fig_replication_multi_dc">Figure&nbsp;5-6</a>). We will discuss this issue in
        <a data-type="xref" href="#sec_replication_write_conflicts">“Handling Write Conflicts”</a>.</p>
        
        <p>As multi-leader replication is a somewhat retrofitted feature in many databases, there are often
        subtle configuration pitfalls and surprising interactions with other database features. For example,
        autoincrementing keys, triggers, and integrity constraints can be problematic. For this reason,
        multi-leader replication is often considered dangerous territory that should be avoided if possible
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Hodges2012ue-marker" href="ch05.html#Hodges2012ue">28</a>].</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Clients with offline operation"><div class="sect3" id="sec_replication_offline_clients">
        <h3>Clients with offline operation</h3>
        
        <p><a data-type="indexterm" data-primary="multi-leader replication" data-secondary="use cases" data-tertiary="clients with offline operation" id="idm45085120213344"></a>
        <a data-type="indexterm" data-primary="clients" data-secondary="stateful and offline-capable" id="idm45085120211888"></a>
        <a data-type="indexterm" data-primary="offline systems" data-secondary="stateful, offline-capable clients" id="idm45085120210768"></a>
        Another situation in which multi-leader replication is appropriate is if you have an application
        that needs to continue to work while it is disconnected from the internet.</p>
        
        <p>For example, consider the calendar apps on your mobile phone, your laptop, and other devices. You
        need to be able to see your meetings (make read requests) and enter new meetings (make write
        requests) at any time, regardless of whether your device currently has an internet connection. If
        you make any changes while you are offline, they need to be synced with a server and your other
        devices when the device is next online.</p>
        
        <p><a data-type="indexterm" data-primary="locality (data access)" data-secondary="in stateful clients" id="idm45085120208336"></a>
        In this case, every device has a local database that acts as a leader (it accepts write requests),
        and there is an asynchronous multi-leader replication process (sync) between the replicas of your
        calendar on all of your devices. The replication lag may be hours or even days, depending on when
        you have internet access available.</p>
        
        <p>From an architectural point of view, this setup is essentially the same as multi-leader replication
        between datacenters, taken to the extreme: each device is a “datacenter,” and the network connection
        between them is extremely unreliable. As the rich history of broken calendar sync implementations
        demonstrates, multi-leader replication is a tricky thing to get right.</p>
        
        <p><a data-type="indexterm" data-primary="CouchDB (database)" data-secondary="replication" id="idm45085120205824"></a>
        There are tools that aim to make this kind of multi-leader configuration easier. For example,
        CouchDB is designed for this mode of operation
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Anderson2010wj-marker" href="ch05.html#Anderson2010wj">29</a>].</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Collaborative editing"><div class="sect3" id="idm45085120203024">
        <h3>Collaborative editing</h3>
        
        <p><a data-type="indexterm" data-primary="real-time" data-secondary="collaborative editing" id="idm45085120201616"></a>
        <a data-type="indexterm" data-primary="multi-leader replication" data-secondary="use cases" data-tertiary="collaborative editing" id="idm45085120200288"></a>
        <a data-type="indexterm" data-primary="collaborative editing" data-secondary="multi-leader replication and" id="idm45085120198944"></a>
        <a data-type="indexterm" data-primary="Etherpad (collaborative editor)" id="idm45085120197824"></a>
        <a data-type="indexterm" data-primary="Google" data-secondary="Docs (collaborative editor)" id="idm45085120196976"></a>
        <em>Real-time collaborative editing</em> applications allow several people to edit a document
        simultaneously. For example, Etherpad [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="AppJetInc2011um-marker" href="ch05.html#AppJetInc2011um">30</a>]
        and Google Docs [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="DayRichter2010tt-marker" href="ch05.html#DayRichter2010tt">31</a>] allow multiple people to concurrently edit a text document or spreadsheet
        (the algorithm is briefly discussed in <a data-type="xref" href="#sidebar_conflict_resolution">“Automatic Conflict Resolution”</a>).</p>
        
        <p>We don’t usually think of collaborative editing as a database replication problem, but it has a lot
        in common with the previously mentioned offline editing use case. When one user edits a document,
        the changes are instantly applied to their local replica (the state of the document in their web
        browser or client application) and asynchronously replicated to the server and any other users who
        are editing the same document.</p>
        
        <p>If you want to guarantee that there will be no editing conflicts, the application must obtain a lock
        on the document before a user can edit it. If another user wants to edit the same document, they
        first have to wait until the first user has committed their changes and released the lock. This
        collaboration model is equivalent to single-leader replication with transactions on the leader.</p>
        
        <p>However, for faster collaboration, you may want to make the unit of change very small (e.g., a single
        keystroke) and avoid locking. This approach allows multiple users to edit simultaneously, but it also brings
        all the challenges of multi-leader replication, including requiring conflict resolution
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kleppmann2016ve-marker" href="ch05.html#Kleppmann2016ve">32</a>].</p>
        </div></section>
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Handling Write Conflicts"><div class="sect2" id="sec_replication_write_conflicts">
        <h2>Handling Write Conflicts</h2>
        
        <p><a data-type="indexterm" data-primary="concurrency" data-secondary="conflict resolution" id="idm45085120183968"></a>
        <a data-type="indexterm" data-primary="conflicts" data-secondary="in multi-leader replication" id="ix_conflictwrmultilead"></a>
        <a data-type="indexterm" data-primary="multi-leader replication" data-secondary="handling write conflicts" id="idm45085120181424"></a>
        <a data-type="indexterm" data-primary="replication" data-secondary="multi-leader" data-tertiary="handling write conflicts" id="ix_replmultileadwrconf"></a>
        The biggest problem with multi-leader replication is that write conflicts can occur, which means
        that conflict resolution is required.</p>
        
        <p>For example, consider a wiki page that is simultaneously being edited by two users, as shown in
        <a data-type="xref" href="#fig_replication_write_conflict">Figure&nbsp;5-7</a>. User 1 changes the title of the page from A to B, and user 2
        changes the title from A to C at the same time. Each user’s change is successfully applied to their
        local leader. However, when the changes are asynchronously replicated, a conflict is detected
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Clement2011wc-marker" href="ch05.html#Clement2011wc">33</a>].
        This problem does not occur in a single-leader database.</p>
        
        <figure class="width-75"><div id="fig_replication_write_conflict" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_0507.png" alt="ddia 0507" width="2880" height="1488">
        <h6><span class="label">Figure 5-7. </span>A write conflict caused by two leaders concurrently updating the same record.</h6>
        </div></figure>
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Synchronous versus asynchronous conflict detection"><div class="sect3" id="idm45085120172256">
        <h3>Synchronous versus asynchronous conflict detection</h3>
        
        <p><a data-type="indexterm" data-primary="synchronous replication" data-secondary="conflict detection" id="idm45085120171024"></a>
        <a data-type="indexterm" data-primary="asynchronous replication" data-secondary="conflict detection" id="idm45085120169920"></a>
        <a data-type="indexterm" data-primary="conflicts" data-secondary="conflict detection" id="idm45085120168800"></a>
        In a single-leader database, the second writer will either block and wait for the first write to
        complete, or abort the second write transaction, forcing the user to retry the write. On the other
        hand, in a multi-leader setup, both writes are successful, and the conflict is only detected
        asynchronously at some later point in time. At that time, it may be too late to ask the user to
        resolve the conflict.</p>
        
        <p>In principle, you could make the conflict detection synchronous—i.e., wait for the write to be
        replicated to all replicas before telling the user that the write was successful. However, by doing
        so, you would lose the main advantage of multi-leader replication: allowing each replica to accept
        writes independently. If you want synchronous conflict detection, you might as well just use
        single-leader replication.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Conflict avoidance"><div class="sect3" id="idm45085120166208">
        <h3>Conflict avoidance</h3>
        
        <p><a data-type="indexterm" data-primary="conflicts" data-secondary="in multi-leader replication" data-tertiary="avoiding conflicts" id="idm45085120165008"></a>
        <a data-type="indexterm" data-primary="multi-leader replication" data-secondary="handling write conflicts" data-tertiary="conflict avoidance" id="idm45085120163616"></a>
        The simplest strategy for dealing with conflicts is to avoid them: if the application can ensure
        that all writes for a particular record go through the same leader, then conflicts cannot occur.
        Since many implementations of multi-leader replication handle conflicts quite poorly, avoiding
        conflicts is a frequently recommended approach
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Hodges2013vb-marker" href="ch05.html#Hodges2013vb">34</a>].</p>
        
        <p>For example, in an application where a user can edit their own data, you can ensure that requests
        from a particular user are always routed to the same datacenter and use the leader in that
        datacenter for reading and writing. Different users may have different “home” datacenters (perhaps
        picked based on geographic proximity to the user), but from any one user’s point of view the
        configuration is essentially single-leader.</p>
        
        <p>However, sometimes you might want to change the designated leader for a record—perhaps because
        one datacenter has failed and you need to reroute traffic to another datacenter, or perhaps because
        a user has moved to a different location and is now closer to a different datacenter. In this
        situation, conflict avoidance breaks down, and you have to deal with the possibility of concurrent
        writes on different leaders.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Converging toward a consistent state"><div class="sect3" id="idm45085120157408">
        <h3>Converging toward a consistent state</h3>
        
        <p><a data-type="indexterm" data-primary="conflicts" data-secondary="conflict resolution" data-tertiary="convergence" id="ix_confconverg"></a>
        <a data-type="indexterm" data-primary="convergence (conflict resolution)" id="ix_convergence"></a>
        <a data-type="indexterm" data-primary="multi-leader replication" data-secondary="handling write conflicts" data-tertiary="converging toward a consistent state" id="idm45085120153136"></a>
        A single-leader database applies writes in a sequential order: if there are several updates to the
        same field, the last write determines the final value of the field.</p>
        
        <p>In a multi-leader configuration, there is no defined ordering of writes, so it’s not clear what the
        final value should be. In <a data-type="xref" href="#fig_replication_write_conflict">Figure&nbsp;5-7</a>, at leader 1 the title is first updated
        to B and then to C; at leader 2 it is first updated to C and then to B. Neither order is “more
        correct” than the other.</p>
        
        <p>If each replica simply applied writes in the order that it saw the writes, the database would end up
        in an inconsistent state: the final value would be C at leader 1 and B at leader 2. That is not
        acceptable—every replication system must ensure that the data is eventually the same in all
        replicas. Thus, the database must resolve the conflict in a <em>convergent</em> way, which means that all
        replicas must arrive at the same final value when all changes have been replicated.</p>
        
        <p>There are various ways of achieving convergent conflict resolution:</p>
        
        <ul>
        <li>
        <p><a data-type="indexterm" data-primary="winners (conflict resolution)" id="idm45085120147264"></a>
        <a data-type="indexterm" data-primary="last write wins (LWW)" id="idm45085120146304"></a>
        <a data-type="indexterm" data-primary="incidents" data-secondary="data loss due to last-write-wins" id="idm45085120145472"></a>
        Give each write a unique ID (e.g., a timestamp, a long random number, a UUID, or a hash of the key
        and value), pick the write with the highest ID as the <em>winner</em>, and throw away the other writes.
        If a timestamp is used, this technique is known as <em>last write wins</em> (LWW). Although this approach
        is popular, it is dangerously prone to data loss
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Daily2013te_ch5-marker" href="ch05.html#Daily2013te_ch5">35</a>].
        We will discuss LWW in more detail at the end of this chapter (<a data-type="xref" href="#sec_replication_concurrent">“Detecting Concurrent Writes”</a>).</p>
        </li>
        <li>
        <p>Give each replica a unique ID, and let writes that originated at a higher-numbered replica
        always take precedence over writes that originated at a lower-numbered replica. This approach also implies
        data loss.</p>
        </li>
        <li>
        <p>Somehow merge the values together—e.g., order them alphabetically and then concatenate them (in
        <a data-type="xref" href="#fig_replication_write_conflict">Figure&nbsp;5-7</a>, the merged title might be something like <span class="keep-together">“B/C”).</span></p>
        </li>
        <li>
        <p>Record the conflict in an explicit data structure that preserves all information, and write
        application code that resolves the conflict at some later time (perhaps by prompting the user).</p>
        </li>
        </ul>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Custom conflict resolution logic"><div class="sect3" id="idm45085120135232">
        <h3>Custom conflict resolution logic</h3>
        
        <p><a data-type="indexterm" data-primary="multi-leader replication" data-secondary="handling write conflicts" data-tertiary="custom conflict resolution logic" id="idm45085120133824"></a>
        <a data-type="indexterm" data-primary="conflicts" data-secondary="conflict resolution" data-tertiary="using custom logic" id="idm45085120132400"></a>
        As the most appropriate way of resolving a conflict may depend on the application, most multi-leader
        replication tools let you write conflict resolution logic using application code. That code may be
        executed on write or on read:</p>
        <dl>
        <dt>On write</dt>
        <dd>
        <p><a data-type="indexterm" data-primary="PostgreSQL (database)" data-secondary="Bucardo (trigger-based replication)" id="idm45085120129440"></a>
        As soon as the database system detects a conflict in the log of replicated changes, it calls the
        conflict handler. For example, Bucardo allows you to write a snippet of Perl for this purpose.
        This handler typically cannot prompt a user—it runs in a background process and it must execute
        quickly.</p>
        </dd>
        <dt>On read</dt>
        <dd>
        <p><a data-type="indexterm" data-primary="CouchDB (database)" data-secondary="replication" id="idm45085120126848"></a>
        When a conflict is detected, all the conflicting writes are stored. The next time the data is
        read, these multiple versions of the data are returned to the application. The application may
        prompt the user or automatically resolve the conflict, and write the result back to the database.
        CouchDB works this way, for example.</p>
        </dd>
        </dl>
        
        <p class="pagebreak-after">Note that conflict resolution usually applies at the level of an individual row or document, not for
        an entire transaction [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Berton2016wh-marker" href="ch05.html#Berton2016wh">36</a>].
        Thus, if you have a transaction that atomically makes several different writes (see
        <a data-type="xref" href="ch07.html#ch_transactions">Chapter&nbsp;7</a>), each write is still considered separately for the purposes of conflict resolution.</p>
        <aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="sidebar_conflict_resolution">
        <h5>Automatic Conflict Resolution</h5>
        <p><a data-type="indexterm" data-primary="concurrency" data-secondary="conflict resolution" id="idm45085120119472"></a>
        <a data-type="indexterm" data-primary="conflicts" data-secondary="conflict resolution" data-tertiary="automatic conflict resolution" id="idm45085120118368"></a>
        <a data-type="indexterm" data-primary="incidents" data-secondary="deleted items reappearing" id="idm45085120116928"></a>
        Conflict resolution rules can quickly become complicated, and custom code can be error-prone. Amazon
        is a frequently cited example of surprising effects due to a conflict resolution handler: for some
        time, the conflict resolution logic on the shopping cart would preserve items added to the cart, but
        not items removed from the cart. Thus, customers would sometimes see items reappearing in their
        carts even though they had previously been removed
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="DeCandia2007ui_ch5-marker" href="ch05.html#DeCandia2007ui_ch5">37</a>].</p>
        
        <p>There has been some interesting research into automatically resolving conflicts caused by concurrent
        data modifications. A few lines of research are worth mentioning:</p>
        
        <ul>
        <li>
        <p><a data-type="indexterm" data-primary="conflict-free replicated datatypes (CRDTs)" id="idm45085120111568"></a>
        <a data-type="indexterm" data-primary="CRDTs" data-see="conflict-free replicated datatypes" id="idm45085120110416"></a>
        <a data-type="indexterm" data-primary="Riak (database)" data-secondary="CRDTs" id="idm45085120109344"></a>
        <a data-type="indexterm" data-primary="datatypes" data-secondary="conflict-free" id="idm45085120108240"></a>
        <em>Conflict-free replicated datatypes</em> (CRDTs)
        [<a data-type="noteref" href="ch05.html#Kleppmann2016ve">32</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Shapiro2011wy-marker" href="ch05.html#Shapiro2011wy">38</a>] are a family of data structures for sets, maps (dictionaries), ordered
        lists, counters, etc. that can be concurrently edited by multiple users, and which automatically
        resolve conflicts in sensible ways. Some CRDTs have been implemented in Riak 2.0
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Elliott2013ua-marker" href="ch05.html#Elliott2013ua">39</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Brown2013wy-marker" href="ch05.html#Brown2013wy">40</a>].</p>
        </li>
        <li>
        <p><a data-type="indexterm" data-primary="mergeable persistent data structures" id="idm45085120098640"></a>
        <a data-type="indexterm" data-primary="Git (version control system)" id="idm45085120097792"></a>
        <em>Mergeable persistent data structures</em>
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Farinier2015wj-marker" href="ch05.html#Farinier2015wj">41</a>] track history explicitly, similarly to the Git version control
        system, and use a three-way merge function (whereas CRDTs use two-way merges).</p>
        </li>
        <li>
        <p><a data-type="indexterm" data-primary="operational transformation" id="idm45085120093360"></a>
        <em>Operational transformation</em>
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Sun1998vf-marker" href="ch05.html#Sun1998vf">42</a>]
        is the conflict resolution algorithm behind collaborative editing applications such as Etherpad
        [<a data-type="noteref" href="ch05.html#AppJetInc2011um">30</a>] and Google Docs
        [<a data-type="noteref" href="ch05.html#DayRichter2010tt">31</a>]. It was designed particularly for
        concurrent editing of an ordered list of items, such as the list of characters that constitute a
        text document.</p>
        </li>
        </ul>
        
        <p>Implementations of these algorithms in databases are still young, but it’s likely that they will be
        integrated into more replicated data systems in the future. Automatic conflict resolution could make
        multi-leader data synchronization much simpler for applications to deal with.
        <a data-type="indexterm" data-primary="conflicts" data-secondary="conflict resolution" data-tertiary="convergence" data-startref="ix_confconverg" id="idm45085120086848"></a>
        <a data-type="indexterm" data-primary="convergence (conflict resolution)" data-startref="ix_convergence" id="idm45085120085200"></a></p>
        </div></aside>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="What is a conflict?"><div class="sect3" id="sec_replication_what_is_conflict">
        <h3>What is a conflict?</h3>
        
        <p><a data-type="indexterm" data-primary="multi-leader replication" data-secondary="handling write conflicts" data-tertiary="determining what is a conflict" id="idm45085120082448"></a>
        <a data-type="indexterm" data-primary="conflicts" data-secondary="determining what is a conflict" id="idm45085120081072"></a>
        Some kinds of conflict are obvious. In the example in <a data-type="xref" href="#fig_replication_write_conflict">Figure&nbsp;5-7</a>, two writes
        concurrently modified the same field in the same record, setting it to two different values. There
        is little doubt that this is a conflict.</p>
        
        <p>Other kinds of conflict can be more subtle to detect. For example, consider a meeting room booking
        system: it tracks which room is booked by which group of people at which time. This application
        needs to ensure that each room is only booked by one group of people at any one time (i.e., there
        must not be any overlapping bookings for the same room). In this case, a conflict may arise if two
        different bookings are created for the same room at the same time. Even if the application checks
        availability before allowing a user to make a booking, there can be a conflict if the two bookings
        are made on two different leaders.</p>
        
        <p>There isn’t a quick ready-made answer, but in the following chapters we will trace a path toward a
        good understanding of this problem. We will see some more examples of conflicts in
        <a data-type="xref" href="ch07.html#ch_transactions">Chapter&nbsp;7</a>, and in <a data-type="xref" href="ch12.html#ch_future">Chapter&nbsp;12</a> we will discuss scalable approaches for detecting and
        resolving conflicts in a replicated system.
        <a data-type="indexterm" data-primary="replication" data-secondary="multi-leader" data-tertiary="handling write conflicts" data-startref="ix_replmultileadwrconf" id="idm45085120075472"></a>
        <a data-type="indexterm" data-primary="conflicts" data-secondary="in multi-leader replication" data-startref="ix_conflictwrmultilead" id="idm45085120073840"></a></p>
        </div></section>
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Multi-Leader Replication Topologies"><div class="sect2" id="sec_replication_topologies">
        <h2>Multi-Leader Replication Topologies</h2>
        
        <p><a data-type="indexterm" data-primary="multi-leader replication" data-secondary="replication topologies" id="ix_multileadrepltopol"></a>
        <a data-type="indexterm" data-primary="replication" data-secondary="multi-leader" data-tertiary="replication topologies" id="ix_replmultileadtopol"></a>
        A <em>replication topology</em> describes the communication paths along which writes are propagated from
        one node to another. If you have two leaders, like in <a data-type="xref" href="#fig_replication_write_conflict">Figure&nbsp;5-7</a>, there is
        only one plausible topology: leader 1 must send all of its writes to leader 2, and vice versa. With
        more than two leaders, various different topologies are possible. Some examples are illustrated in
        <a data-type="xref" href="#fig_replication_topologies">Figure&nbsp;5-8</a>.</p>
        
        <figure><div id="fig_replication_topologies" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_0508.png" alt="ddia 0508" width="2880" height="800">
        <h6><span class="label">Figure 5-8. </span>Three example topologies in which multi-leader replication can be set up.</h6>
        </div></figure>
        
        <p><a data-type="indexterm" data-primary="all-to-all replication topologies" id="idm45085120063104"></a>
        <a data-type="indexterm" data-primary="circular replication topologies" id="idm45085120062144"></a>
        <a data-type="indexterm" data-primary="star replication topologies" id="idm45085120061296"></a>
        <a data-type="indexterm" data-primary="MySQL (database)" data-secondary="circular replication topology" id="idm45085120060448"></a>
        The most general topology is <em>all-to-all</em> (<a data-type="xref" href="#fig_replication_topologies">Figure&nbsp;5-8</a> [c]), in which every
        leader sends its writes to every other leader. However, more restricted topologies are also used:
        for example, MySQL by default supports only a <em>circular topology</em>
        [<a data-type="noteref" href="ch05.html#Hodges2013vb">34</a>],
        in which each node receives writes from one node and forwards those writes (plus any writes of its
        own) to one other node. Another popular topology has the shape of a
        <em>star</em>:<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm45085120056016-marker" href="ch05.html#idm45085120056016">v</a></sup> one
        designated root node forwards writes to all of the other nodes. The star topology can be generalized
        to a tree.</p>
        
        <p>In circular and star topologies, a write may need to pass through several nodes before it reaches
        all replicas. Therefore, nodes need to forward data changes they receive from other nodes. To
        prevent infinite replication loops, each node is given a unique identifier, and in the replication
        log, each write is tagged with the identifiers of all the nodes it has passed through
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="HBase7709-marker" href="ch05.html#HBase7709">43</a>].
        When a node receives a data change that is tagged with its own identifier, that data change is
        ignored, because the node knows that it has already been processed.</p>
        
        <p>A problem with circular and star topologies is that if just one node fails, it can interrupt the
        flow of replication messages between other nodes, causing them to be unable to communicate until the
        node is fixed. The topology could be reconfigured to work around the failed node, but in most
        deployments such reconfiguration would have to be done manually. The fault tolerance of a more
        densely connected topology (such as all-to-all) is better because it allows messages to travel
        along different paths, avoiding a single point of failure.</p>
        
        <p>On the other hand, all-to-all topologies can have issues too. In particular, some network links may
        be faster than others (e.g., due to network congestion), with the result that some replication
        messages may “overtake” others, as illustrated in <a data-type="xref" href="#fig_replication_causality">Figure&nbsp;5-9</a>.</p>
        
        <figure><div id="fig_replication_causality" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_0509.png" alt="ddia 0509" width="2880" height="1602">
        <h6><span class="label">Figure 5-9. </span>With multi-leader replication, writes may arrive in the wrong order at some replicas.</h6>
        </div></figure>
        
        <p>In <a data-type="xref" href="#fig_replication_causality">Figure&nbsp;5-9</a>, client A inserts a row into a table on leader 1, and client B
        updates that row on leader 3. However, leader 2 may receive the writes in a different order: it may
        first receive the update (which, from its point of view, is an update to a row that does not exist
        in the database) and only later receive the corresponding insert (which should have preceded the
        update).</p>
        
        <p><a data-type="indexterm" data-primary="causality" data-secondary="violations of" id="idm45085120044048"></a>
        <a data-type="indexterm" data-primary="correctness" data-secondary="of time" id="idm45085120042944"></a>
        This is a problem of causality, similar to the one we saw in <a data-type="xref" href="#sec_replication_consistent_prefix">“Consistent Prefix Reads”</a>:
        the update depends on the prior insert, so we need to make sure that all nodes process the insert
        first, and then the update. Simply attaching a timestamp to every write is not sufficient, because
        clocks cannot be trusted to be sufficiently in sync to correctly order these events at leader 2 (see
        <a data-type="xref" href="ch08.html#ch_distributed">Chapter&nbsp;8</a>).</p>
        
        <p><a data-type="indexterm" data-primary="version vectors" id="idm45085120039536"></a>
        <a data-type="indexterm" data-primary="MySQL (database)" data-secondary="Tungsten Replicator (multi-leader replication)" data-tertiary="conflict detection" id="idm45085120038480"></a>
        <a data-type="indexterm" data-primary="PostgreSQL (database)" data-secondary="BDR (multi-leader replication)" data-tertiary="causal ordering of writes" id="idm45085120037072"></a>
        To order these events correctly, a technique called <em>version vectors</em> can be used, which we will
        discuss later in this chapter (see <a data-type="xref" href="#sec_replication_concurrent">“Detecting Concurrent Writes”</a>). However, conflict detection
        techniques are poorly implemented in many multi-leader replication systems. For example, at the time
        of writing, PostgreSQL BDR does not provide causal ordering of writes
        [<a data-type="noteref" href="ch05.html#PostgresBDR">27</a>],
        and Tungsten Replicator for MySQL doesn’t even try to detect conflicts
        [<a data-type="noteref" href="ch05.html#Hodges2013vb">34</a>].</p>
        
        <p>If you are using a system with multi-leader replication, it is worth being aware of these issues,
        carefully reading the documentation, and thoroughly testing your database to ensure that it really
        does provide the guarantees you believe it to have.
        <a data-type="indexterm" data-primary="multi-leader replication" data-secondary="replication topologies" data-startref="ix_multileadrepltopol" id="idm45085120032016"></a>
        <a data-type="indexterm" data-primary="replication" data-secondary="multi-leader" data-tertiary="replication topologies" data-startref="ix_replmultileadtopol" id="idm45085120030672"></a>
        <a data-type="indexterm" data-primary="replication" data-secondary="multi-leader" data-startref="ix_replmultilead" id="idm45085120029024"></a>
        <a data-type="indexterm" data-primary="multi-leader replication" data-startref="ix_multileadrepl" id="idm45085120027648"></a></p>
        </div></section>
        
        
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Leaderless Replication"><div class="sect1" id="sec_replication_leaderless">
        <h1>Leaderless Replication</h1>
        
        <p><a data-type="indexterm" data-primary="replication" data-secondary="leaderless" id="ix_replnolead"></a>
        <a data-type="indexterm" data-primary="leaderless replication" data-seealso="replication" id="ix_noleadrepl"></a>
        The replication approaches we have discussed so far in this chapter—single-leader and
        multi-leader replication—are based on the idea that a client sends a write request to one node
        (the leader), and the database system takes care of copying that write to the other replicas. A
        leader determines the order in which writes should be processed, and followers apply the leader’s
        writes in the same order.</p>
        
        <p><a data-type="indexterm" data-primary="Amazon" data-secondary="Dynamo (database)" id="idm45085120021584"></a>
        <a data-type="indexterm" data-primary="Dynamo-style databases" data-see="leaderless replication" id="idm45085120020080"></a>
        <a data-type="indexterm" data-primary="Cassandra (database)" data-secondary="leaderless replication" id="idm45085120018976"></a>
        <a data-type="indexterm" data-primary="Riak (database)" data-secondary="leaderless replication" id="idm45085120017872"></a>
        <a data-type="indexterm" data-primary="Voldemort (database)" data-secondary="leaderless replication" id="idm45085120016768"></a>
        <a data-type="indexterm" data-primary="LinkedIn" data-secondary="Voldemort (database)" data-see="Voldemort" id="idm45085120015664"></a>
        Some data storage systems take a different approach, abandoning the concept of a leader and
        allowing any replica to directly accept writes from clients. Some of the earliest replicated data
        systems were leaderless [<a data-type="noteref" href="ch05.html#Lindsay1979wv_ch5">1</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Gifford1979if-marker" href="ch05.html#Gifford1979if">44</a>], but the
        idea was mostly forgotten during the era of dominance of relational databases. It once again became
        a fashionable architecture for databases after Amazon used it for its in-house <em>Dynamo</em> system
        [<a data-type="noteref" href="ch05.html#DeCandia2007ui_ch5">37</a>].<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm45085120008928-marker" href="ch05.html#idm45085120008928">vi</a></sup>
        Riak, Cassandra, and Voldemort are open source datastores with leaderless replication models inspired
        by Dynamo, so this kind of database is also known as <em>Dynamo-style</em>.</p>
        
        <p>In some leaderless implementations, the client directly sends its writes to several replicas, while
        in others, a coordinator node does this on behalf of the client. However, unlike a leader database,
        that coordinator does not enforce a particular ordering of writes. As we shall see, this difference in design has
        profound consequences for the way the database is used.</p>
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Writing to the Database When a Node Is Down"><div class="sect2" id="idm45085120006112">
        <h2>Writing to the Database When a Node Is Down</h2>
        
        <p>Imagine you have a database with three replicas, and one of the replicas is currently
        unavailable—perhaps it is being rebooted to install a system update. In a leader-based
        configuration, if you want to continue processing writes, you may need to perform a failover (see
        <a data-type="xref" href="#sec_replication_failover">“Handling Node Outages”</a>).</p>
        
        <p><a data-type="indexterm" data-primary="failover" data-secondary="in leaderless replication, absence of" id="idm45085120003104"></a>
        On the other hand, in a leaderless configuration, failover does not exist.
        <a data-type="xref" href="#fig_replication_quorum_node_outage">Figure&nbsp;5-10</a> shows what happens: the client (user 1234) sends the write to
        all three replicas in parallel, and the two available replicas accept the write but the unavailable
        replica misses it. Let’s say that it’s sufficient for two out of three replicas to
        acknowledge the write: after user 1234 has received two <em>ok</em> responses, we consider the write to be
        successful. The client simply ignores the fact that one of the replicas missed the write.</p>
        
        <figure><div id="fig_replication_quorum_node_outage" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_0510.png" alt="ddia 0510" width="2880" height="1572">
        <h6><span class="label">Figure 5-10. </span>A quorum write, quorum read, and read repair after a node outage.</h6>
        </div></figure>
        
        <p><a data-type="indexterm" data-primary="staleness (old data)" data-secondary="in leaderless databases" id="idm45085119997952"></a>
        Now imagine that the unavailable node comes back online, and clients start reading from it. Any
        writes that happened while the node was down are missing from that node. Thus, if you read from that
        node, you may get <em>stale</em> (outdated) values as responses.</p>
        
        <p>To solve that problem, when a client reads from the database, it doesn’t just send its request to
        one replica: <em>read requests are also sent to several nodes in parallel</em>. The client may get
        different responses from different nodes; i.e., the up-to-date value from one node and a stale value
        from another. Version numbers are used to determine which value is newer (see
        <a data-type="xref" href="#sec_replication_concurrent">“Detecting Concurrent Writes”</a>).</p>
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Read repair and anti-entropy"><div class="sect3" id="sec_replication_read_repair">
        <h3>Read repair and anti-entropy</h3>
        
        <p><a data-type="indexterm" data-primary="anti-entropy" id="idm45085119992352"></a>
        <a data-type="indexterm" data-primary="read repair (leaderless replication)" id="idm45085119991520"></a>
        <a data-type="indexterm" data-primary="leaderless replication" data-secondary="read repair and anti-entropy" id="idm45085119990624"></a>
        The replication system should ensure that eventually all the data is copied to every replica. After
        an unavailable node comes back online, how does it catch up on the writes that it missed?</p>
        
        <p>Two mechanisms are often used in Dynamo-style datastores:</p>
        <dl>
        <dt>Read repair</dt>
        <dd>
        <p>When a client makes a read from several nodes in parallel, it can detect any stale responses.
        For example, in <a data-type="xref" href="#fig_replication_quorum_node_outage">Figure&nbsp;5-10</a>, user 2345 gets a version 6 value from
        replica 3 and a version 7 value from replicas 1 and 2. The client sees that replica 3 has a stale
        value and writes the newer value back to that replica. This approach works well for values that are
        frequently read.</p>
        </dd>
        <dt>Anti-entropy process</dt>
        <dd>
        <p>In addition, some datastores have a background process that constantly looks for differences in
        the data between replicas and copies any missing data from one replica to another. Unlike the
        replication log in leader-based replication, this <em>anti-entropy process</em> does not copy writes in
        any particular order, and there may be a significant delay before data is copied.</p>
        </dd>
        </dl>
        
        <p><a data-type="indexterm" data-primary="Voldemort (database)" data-secondary="reliance on read repair" id="idm45085119983264"></a>
        Not all systems implement both of these; for example, Voldemort currently does not have an
        anti-entropy process. Note that without an anti-entropy process, values that are rarely read may be
        missing from some replicas and thus have reduced durability, because read repair is only performed
        when a value is read by the application.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Quorums for reading and writing"><div class="sect3" id="sec_replication_quorum_condition">
        <h3>Quorums for reading and writing</h3>
        
        <p><a data-type="indexterm" data-primary="leaderless replication" data-secondary="quorums" id="ix_leaderlessquorums"></a>
        <a data-type="indexterm" data-primary="quorums" id="ix_replquorums"></a>
        In the example of <a data-type="xref" href="#fig_replication_quorum_node_outage">Figure&nbsp;5-10</a>, we considered the write to be successful
        even though it was only processed on two out of three replicas. What if only one out of three
        replicas accepted the write? How far can we push this?</p>
        
        <p>If we know that every successful write is guaranteed to be present on at least two out of three
        replicas, that means at most one replica can be stale. Thus, if we read from at least two replicas,
        we can be sure that at least one of the two is up to date. If the third replica is down or slow to
        respond, reads can nevertheless continue returning an up-to-date value.</p>
        
        <p><a data-type="indexterm" data-primary="quorums" data-secondary="for leaderless replication" id="idm45085119975328"></a>
        More generally, if there are <em>n</em> replicas, every write must be confirmed by <em>w</em> nodes to be
        considered successful, and we must query at least <em>r</em> nodes for each read. (In our example,
        <em>n</em>&nbsp;=&nbsp;3, <em>w</em>&nbsp;=&nbsp;2, <em>r</em>&nbsp;=&nbsp;2.) As long as <em>w</em>&nbsp;+&nbsp;<em>r</em> &gt;
        <em>n</em>, we expect to get an up-to-date value when reading, because at least one of the <em>r</em> nodes we’re
        reading from must be up to date. Reads and writes that obey these <em>r</em> and <em>w</em> values are called
        <em>quorum</em> reads and writes
        [<a data-type="noteref" href="ch05.html#Gifford1979if">44</a>].<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm45085119967216-marker" href="ch05.html#idm45085119967216">vii</a></sup>
        You can think of <em>r</em> and <em>w</em> as the minimum number of votes required for the read or write to be
        valid.</p>
        
        <p>In Dynamo-style databases, the parameters <em>n</em>, <em>w</em>, and <em>r</em> are typically configurable. A common
        choice is to make <em>n</em> an odd number (typically 3 or 5) and to set <em>w</em> = <em>r</em> =
        (<em>n</em>&nbsp;+&nbsp;1)&nbsp;/&nbsp;2 (rounded up). However, you can vary the numbers as you see fit.
        For example, a workload with few writes and many reads may benefit from setting <em>w</em> = <em>n</em> and
        <em>r</em> = 1. This makes reads faster, but has the disadvantage that just one failed node causes all
        database writes to fail.</p>
        <div data-type="note" epub:type="note"><h6>Note</h6>
        <p>There may be more than <em>n</em> nodes in the cluster, but any given value is stored only on <em>n</em>
        nodes. This allows the dataset to be partitioned, supporting datasets that are larger than you can fit
        on one node. We will return to partitioning in <a data-type="xref" href="ch06.html#ch_partitioning">Chapter&nbsp;6</a>.</p>
        </div>
        
        <p>The quorum condition, <em>w</em>&nbsp;+&nbsp;<em>r</em> &gt; <em>n</em>, allows the system to tolerate unavailable nodes
        as follows:</p>
        
        <ul>
        <li>
        <p>If <em>w</em>&nbsp;&lt;&nbsp;<em>n</em>, we can still process writes if a node is unavailable.</p>
        </li>
        <li>
        <p>If <em>r</em>&nbsp;&lt;&nbsp;<em>n</em>, we can still process reads if a node is unavailable.</p>
        </li>
        <li>
        <p>With <em>n</em>&nbsp;=&nbsp;3, <em>w</em>&nbsp;=&nbsp;2, <em>r</em>&nbsp;=&nbsp;2 we can tolerate one unavailable node.</p>
        </li>
        <li>
        <p>With <em>n</em>&nbsp;=&nbsp;5, <em>w</em>&nbsp;=&nbsp;3, <em>r</em>&nbsp;=&nbsp;3 we can tolerate two unavailable nodes.
        This case is illustrated in <a data-type="xref" href="#fig_replication_quorum_overlap">Figure&nbsp;5-11</a>.</p>
        </li>
        <li>
        <p>Normally, reads and writes are always sent to all <em>n</em> replicas in parallel. The parameters <em>w</em> and
        <em>r</em> determine how many nodes we wait for—i.e., how many of the <em>n</em> nodes need to report success
        before we consider the read or write to be successful.</p>
        </li>
        </ul>
        
        <figure><div id="fig_replication_quorum_overlap" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_0511.png" alt="ddia 0511" width="2880" height="1226">
        <h6><span class="label">Figure 5-11. </span>If <em>w</em>&nbsp;+&nbsp;<em>r</em> &gt; <em>n</em>, at least one of the <em>r</em> replicas you read from must have seen the most recent successful write.</h6>
        </div></figure>
        
        <p>If fewer than the required <em>w</em> or <em>r</em> nodes are available, writes or reads return an error. A node
        could be unavailable for many reasons: because the node is down (crashed, powered down), due to an
        error executing the operation (can’t write because the disk is full), due to a network interruption
        between the client and the node, or for any number of other reasons. We only care whether the node
        returned a successful response and don’t need to distinguish between different kinds of fault.</p>
        </div></section>
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Limitations of Quorum Consistency"><div class="sect2" id="sec_replication_quorum_limitations">
        <h2>Limitations of Quorum Consistency</h2>
        
        <p><a data-type="indexterm" data-primary="consistency" data-secondary="using quorums" id="idm45085119933808"></a>
        <a data-type="indexterm" data-primary="leaderless replication" data-secondary="quorums" data-tertiary="consistency limitations" id="ix_noleadrepllimit"></a>
        <a data-type="indexterm" data-primary="quorums" data-secondary="limitations of consistency" id="ix_quorumlimit"></a>
        <a data-type="indexterm" data-primary="replication" data-secondary="leaderless" data-tertiary="limitations of quorum consistency" id="ix_replnoleadlimits"></a>
        If you have <em>n</em> replicas, and you choose <em>w</em> and <em>r</em> such that <em>w</em>&nbsp;+&nbsp;<em>r</em> &gt; <em>n</em>, you can
        generally expect every read to return the most recent value written for a key. This is the case because the
        set of nodes to which you’ve written and the set of nodes from which you’ve read must overlap. That
        is, among the nodes you read there must be at least one node with the latest value (illustrated in
        <a data-type="xref" href="#fig_replication_quorum_overlap">Figure&nbsp;5-11</a>).</p>
        
        <p>Often, <em>r</em> and <em>w</em> are chosen to be a majority (more than <em>n</em>/2) of nodes, because that ensures
        <em>w</em>&nbsp;+&nbsp;<em>r</em> &gt; <em>n</em> while still tolerating up to <em>n</em>/2 (rounded down) node failures. But quorums are
        not necessarily majorities—it only matters that the sets of nodes used by the read and write
        operations overlap in at least one node. Other quorum assignments are possible, which allows some
        flexibility in the design of distributed algorithms
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Howard2016tz_ch5-marker" href="ch05.html#Howard2016tz_ch5">45</a>].</p>
        
        <p>You may also set <em>w</em> and <em>r</em> to smaller numbers, so that <em>w</em>&nbsp;+&nbsp;<em>r</em> ≤ <em>n</em> (i.e.,
        the quorum condition is not satisfied). In this case, reads and writes will still be sent to <em>n</em>
        nodes, but a smaller number of successful responses is required for the operation to succeed.</p>
        
        <p>With a smaller <em>w</em> and <em>r</em> you are more likely to read stale values, because it’s more likely that
        your read didn’t include the node with the latest value. On the upside, this configuration allows
        lower latency and higher availability: if there is a network interruption and many replicas become
        unreachable, there’s a higher chance that you can continue processing reads and writes. Only after
        the number of reachable replicas falls below <em>w</em> or <em>r</em> does the database become unavailable for
        writing or reading, respectively.</p>
        
        <p>However, even with <em>w</em>&nbsp;+&nbsp;<em>r</em> &gt; <em>n</em>, there are likely to be edge cases where stale
        values are returned. These depend on the implementation, but possible scenarios include:</p>
        
        <ul>
        <li>
        <p>If a sloppy quorum is used (see <a data-type="xref" href="#sec_replication_sloppy_quorum">“Sloppy Quorums and Hinted Handoff”</a>), the <em>w</em> writes may end up on
        different nodes than the <em>r</em> reads, so there is no longer a guaranteed overlap between the <em>r</em>
        nodes and the <em>w</em> nodes [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Blomstedt2012vf-marker" href="ch05.html#Blomstedt2012vf">46</a>].</p>
        </li>
        <li>
        <p>If two writes occur concurrently, it is not clear which one happened first. In this case, the only
        safe solution is to merge the concurrent writes (see <a data-type="xref" href="#sec_replication_write_conflicts">“Handling Write Conflicts”</a>). If a
        winner is picked based on a timestamp (last write wins), writes can be lost due to clock skew
        [<a data-type="noteref" href="ch05.html#Daily2013te_ch5">35</a>]. We will return to this topic in
        <a data-type="xref" href="#sec_replication_concurrent">“Detecting Concurrent Writes”</a>.</p>
        </li>
        <li>
        <p>If a write happens concurrently with a read, the write may be reflected on only some of the
        replicas. In this case, it’s undetermined whether the read returns the old or the new value.</p>
        </li>
        <li>
        <p>If a write succeeded on some replicas but failed on others (for example because the disks on some
        nodes are full), and overall succeeded on fewer than <em>w</em> replicas, it is not rolled back on the
        replicas where it succeeded. This means that if a write was reported as failed, subsequent reads
        may or may not return the value from that write
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Blomstedt2012vi-marker" href="ch05.html#Blomstedt2012vi">47</a>].</p>
        </li>
        <li>
        <p>If a node carrying a new value fails, and its data is restored from a replica carrying an old
        value, the number of replicas storing the new value may fall below <em>w</em>, breaking the quorum
        condition.</p>
        </li>
        <li>
        <p>Even if everything is working correctly, there are edge cases in which you can get unlucky with
        the timing, as we shall see in <a data-type="xref" href="ch09.html#sec_consistency_quorum_linearizable">“Linearizability and quorums”</a>.</p>
        </li>
        </ul>
        
        <p>Thus, although quorums appear to guarantee that a read returns the latest written value, in practice
        it is not so simple. Dynamo-style databases are generally optimized for use cases that can tolerate
        eventual consistency. The parameters <em>w</em> and <em>r</em> allow you to adjust the probability of stale values
        being read, but it’s wise to not take them as absolute guarantees.</p>
        
        <p>In particular, you usually do not get the guarantees discussed in <a data-type="xref" href="#sec_replication_lag">“Problems with Replication Lag”</a> (reading
        your writes, monotonic reads, or consistent prefix reads), so the previously mentioned anomalies can
        occur in applications. Stronger guarantees generally require transactions or consensus. We will
        return to these topics in <a data-type="xref" href="ch07.html#ch_transactions">Chapter&nbsp;7</a> and <a data-type="xref" href="ch09.html#ch_consistency">Chapter&nbsp;9</a>.
        <a data-type="indexterm" data-primary="leaderless replication" data-secondary="quorums" data-startref="ix_leaderlessquorums" id="idm45085119886192"></a>
        <a data-type="indexterm" data-primary="quorums" data-startref="ix_replquorums" id="idm45085119884816"></a></p>
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Monitoring staleness"><div class="sect3" id="idm45085119883712">
        <h3>Monitoring staleness</h3>
        
        <p><a data-type="indexterm" data-primary="quorums" data-secondary="monitoring staleness" id="idm45085119882304"></a>
        <a data-type="indexterm" data-primary="replication" data-secondary="monitoring staleness" id="idm45085119881200"></a>
        <a data-type="indexterm" data-primary="staleness (old data)" data-secondary="monitoring for" id="idm45085119880096"></a>
        From an operational perspective, it’s important to monitor whether your databases are
        returning up-to-date results. Even if your application can tolerate stale reads, you need to be
        aware of the health of your replication. If it falls behind significantly, it should alert you so
        that you can investigate the cause (for example, a problem in the network or an overloaded node).</p>
        
        <p>For leader-based replication, the database typically exposes metrics for the replication lag, which
        you can feed into a monitoring system. This is possible because writes are applied to the leader and
        to followers in the same order, and each node has a position in the replication log (the number of
        writes it has applied locally). By subtracting a follower’s current position from the leader’s
        current position, you can measure the amount of replication lag.</p>
        
        <p>However, in systems with leaderless replication, there is no fixed order in which writes are
        applied, which makes monitoring more difficult. Moreover, if the database only uses read repair (no
        anti-entropy), there is no limit to how old a value might be—if a value is only infrequently
        read, the value returned by a stale replica may be ancient.</p>
        
        <p>There has been some research on measuring replica staleness in databases with leaderless
        replication and predicting the expected percentage of stale reads depending on the parameters <em>n</em>,
        <em>w</em>, and <em>r</em> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Bailis2014kt-marker" href="ch05.html#Bailis2014kt">48</a>].
        This is unfortunately not yet common practice, but it would be good to include staleness
        measurements in the standard set of metrics for databases. Eventual consistency is a deliberately
        vague guarantee, but for operability it’s important to be able to quantify “eventual.”
        <a data-type="indexterm" data-primary="leaderless replication" data-secondary="consistency limitations" data-startref="ix_noleadrepllimit" id="idm45085119871728"></a>
        <a data-type="indexterm" data-primary="quorums" data-secondary="limitations of consistency" data-startref="ix_quorumlimit" id="idm45085119870384"></a>
        <a data-type="indexterm" data-primary="replication" data-secondary="leaderless" data-tertiary="limitations of quorum consistency" data-startref="ix_replnoleadlimits" id="idm45085119868992"></a></p>
        </div></section>
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Sloppy Quorums and Hinted Handoff"><div class="sect2" id="sec_replication_sloppy_quorum">
        <h2>Sloppy Quorums and Hinted Handoff</h2>
        
        <p><a data-type="indexterm" data-primary="replication" data-secondary="leaderless" data-tertiary="sloppy quorums and hinted handoff" id="idm45085119865808"></a>
        <a data-type="indexterm" data-primary="leaderless replication" data-secondary="quorums" data-tertiary="sloppy quorums and hinted handoff" id="idm45085119864240"></a>
        <a data-type="indexterm" data-primary="quorums" data-secondary="sloppy quorums and hinted handoff" id="idm45085119862848"></a>
        Databases with appropriately configured quorums can tolerate the failure of individual nodes without
        the need for failover. They can also tolerate individual nodes going slow (e.g. due to overload),
        because requests don’t have to wait for all <em>n</em> nodes to respond—they can return when <em>w</em> or <em>r</em>
        nodes have responded. These characteristics make databases with leaderless replication appealing for
        use cases that require high availability and low latency, and that can tolerate occasional stale
        reads.</p>
        
        <p>However, quorums (as described so far) are not as fault-tolerant as they could be. A network
        interruption can easily cut off a client from a large number of database nodes. Although those nodes
        are alive, and other clients may be able to connect to them, to a client that is cut off from the
        database nodes, they might as well be dead. In this situation, it’s likely that fewer than <em>w</em> or <em>r</em>
        reachable nodes remain, so the client can no longer reach a quorum.</p>
        
        <p>In a large cluster (with significantly more than <em>n</em> nodes) it’s likely that the client can connect
        to <em>some</em> database nodes during the network interruption, just not to the nodes that it needs to
        assemble a quorum for a particular value. In that case, database designers face a trade-off:</p>
        
        <ul>
        <li>
        <p>Is it better to return errors to all requests for which we cannot reach a quorum of <em>w</em> or <em>r</em>
        nodes?</p>
        </li>
        <li>
        <p>Or should we accept writes anyway, and write them to some nodes that are reachable but aren’t
        among the <em>n</em> nodes on which the value usually lives?</p>
        </li>
        </ul>
        
        <p><a data-type="indexterm" data-primary="sloppy quorums" data-seealso="quorums" id="idm45085119852816"></a>
        The latter is known as a <em>sloppy quorum</em>
        [<a data-type="noteref" href="ch05.html#DeCandia2007ui_ch5">37</a>]: writes and reads still require <em>w</em>
        and <em>r</em> successful responses, but those may include nodes that are not among the designated <em>n</em>
        “home” nodes for a value. By analogy, if you lock yourself out of your house, you may knock on the
        neighbor’s door and ask whether you may stay on their couch temporarily.</p>
        
        <p><a data-type="indexterm" data-primary="hinted handoff" id="idm45085119848640"></a>
        Once the network interruption is fixed, any writes that one node temporarily accepted on behalf of
        another node are sent to the appropriate “home” nodes. This is called <em>hinted handoff</em>. (Once you
        find the keys to your house again, your neighbor politely asks you to get off their couch and go
        home.)</p>
        
        <p>Sloppy quorums are particularly useful for increasing write availability: as long as <em>any</em> <em>w</em> nodes
        are available, the database can accept writes. However, this means that even when
        <em>w</em>&nbsp;+&nbsp;<em>r</em> &gt; <em>n</em>, you cannot be sure to read the latest value for a key, because the
        latest value may have been temporarily written to some nodes outside of <em>n</em>
        [<a data-type="noteref" href="ch05.html#Blomstedt2012vi">47</a>].</p>
        
        <p>Thus, a sloppy quorum actually isn’t a quorum at all in the traditional sense. It’s only an
        assurance of durability, namely that the data is stored on <em>w</em> nodes somewhere. There is no
        guarantee that a read of <em>r</em> nodes will see it until the hinted handoff has completed.</p>
        
        <p><a data-type="indexterm" data-primary="Riak (database)" data-secondary="sloppy quorums" id="idm45085119841088"></a>
        <a data-type="indexterm" data-primary="Cassandra (database)" data-secondary="sloppy quorums" id="idm45085119839680"></a>
        <a data-type="indexterm" data-primary="Voldemort (database)" data-secondary="sloppy quorums" id="idm45085119838576"></a>
        Sloppy quorums are optional in all common Dynamo implementations. In Riak they are enabled by
        default, and in Cassandra and Voldemort they are disabled by default
        [<a data-type="noteref" href="ch05.html#Blomstedt2012vf">46</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Ellis2012wm-marker" href="ch05.html#Ellis2012wm">49</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="VoldemortWiki-marker" href="ch05.html#VoldemortWiki">50</a>].</p>
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Multi-datacenter operation"><div class="sect3" id="idm45085119832080">
        <h3>Multi-datacenter operation</h3>
        
        <p><a data-type="indexterm" data-primary="leaderless replication" data-secondary="multi-datacenter" id="idm45085119830880"></a>
        <a data-type="indexterm" data-primary="quorums" data-secondary="multi-datacenter replication" id="idm45085119829776"></a>
        <a data-type="indexterm" data-primary="datacenters" data-secondary="replication across multiple" data-tertiary="leaderless replication" id="idm45085119828704"></a>
        We previously discussed cross-datacenter replication as a use case for multi-leader replication (see
        <a data-type="xref" href="#sec_replication_multi_leader">“Multi-Leader Replication”</a>). Leaderless replication is also suitable for
        multi-datacenter operation, since it is designed to tolerate conflicting concurrent writes, network
        interruptions, and latency spikes.</p>
        
        <p><a data-type="indexterm" data-primary="Cassandra (database)" data-secondary="multi-datacenter support" id="idm45085119825984"></a>
        <a data-type="indexterm" data-primary="Voldemort (database)" data-secondary="multi-datacenter support" id="idm45085119824720"></a>
        Cassandra and Voldemort implement their multi-datacenter support within the normal leaderless model:
        the number of replicas <em>n</em> includes nodes in all datacenters, and in the configuration you can
        specify how many of the <em>n</em> replicas you want to have in each datacenter. Each write from a client
        is sent to all replicas, regardless of datacenter, but the client usually only waits for
        acknowledgment from a quorum of nodes within its local datacenter so that it is unaffected by
        delays and interruptions on the cross-datacenter link. The higher-latency writes to other
        datacenters are often configured to happen asynchronously, although there is some flexibility in the
        configuration [<a data-type="noteref" href="ch05.html#VoldemortWiki">50</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Cassandra20-marker" href="ch05.html#Cassandra20">51</a>].</p>
        
        <p><a data-type="indexterm" data-primary="Riak (database)" data-secondary="multi-datacenter support" id="idm45085119818864"></a>
        Riak keeps all communication between clients and database nodes local to one datacenter, so <em>n</em>
        describes the number of replicas within one datacenter. Cross-datacenter replication between
        database clusters happens asynchronously in the background, in a style that is similar to
        multi-leader replication
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Riak2014vb-marker" href="ch05.html#Riak2014vb">52</a>].</p>
        </div></section>
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Detecting Concurrent Writes"><div class="sect2" id="sec_replication_concurrent">
        <h2>Detecting Concurrent Writes</h2>
        
        <p><a data-type="indexterm" data-primary="conflicts" data-secondary="conflict detection" data-tertiary="in leaderless replication" id="idm45085119813216"></a>
        <a data-type="indexterm" data-primary="concurrency" data-secondary="detecting concurrent writes" id="ix_concurwrrepl"></a>
        <a data-type="indexterm" data-primary="replication" data-secondary="leaderless" data-tertiary="detecting concurrent writes" id="ix_replnoleadconcwr"></a>
        <a data-type="indexterm" data-primary="leaderless replication" data-secondary="detecting concurrent writes" id="ix_noleadreplconcwr"></a>
        Dynamo-style databases allow several clients to concurrently write to the same key, which means that
        conflicts will occur even if strict quorums are used. The situation is similar to multi-leader
        replication (see <a data-type="xref" href="#sec_replication_write_conflicts">“Handling Write Conflicts”</a>), although in Dynamo-style databases conflicts
        can also arise during read repair or hinted handoff.</p>
        
        <p>The problem is that events may arrive in a different order at different nodes, due to variable
        network delays and partial failures. For example, <a data-type="xref" href="#fig_replication_concurrency">Figure&nbsp;5-12</a> shows two clients,
        A and B, simultaneously writing to a key <em>X</em> in a three-node datastore:</p>
        
        <ul>
        <li>
        <p>Node 1 receives the write from A, but never receives the write from B due to a transient
        outage.</p>
        </li>
        <li>
        <p>Node 2 first receives the write from A, then the write from B.</p>
        </li>
        <li>
        <p>Node 3 first receives the write from B, then the write from A.</p>
        </li>
        </ul>
        
        <figure><div id="fig_replication_concurrency" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_0512.png" alt="ddia 0512" width="2880" height="1406">
        <h6><span class="label">Figure 5-12. </span>Concurrent writes in a Dynamo-style datastore: there is no well-defined ordering.</h6>
        </div></figure>
        
        <p>If each node simply overwrote the value for a key whenever it received a write request from a
        client, the nodes would become permanently inconsistent, as shown by the final <em>get</em> request in
        <a data-type="xref" href="#fig_replication_concurrency">Figure&nbsp;5-12</a>: node 2 thinks that the final value of <em>X</em> is B, whereas the other
        nodes think that the value is A.</p>
        
        <p>In order to become eventually consistent, the replicas should converge toward the same value. How
        do they do that? One might hope that replicated databases would handle this automatically, but
        unfortunately most implementations are quite poor: if you want to avoid losing data, you—the
        application developer—need to know a lot about the internals of your database’s conflict
        handling.</p>
        
        <p>We briefly touched on some techniques for conflict resolution in
        <a data-type="xref" href="#sec_replication_write_conflicts">“Handling Write Conflicts”</a>. Before we wrap up this chapter, let’s explore the issue in a
        bit more detail.</p>
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Last write wins (discarding concurrent writes)"><div class="sect3" id="sec_replication_lww">
        <h3>Last write wins (discarding concurrent writes)</h3>
        
        <p><a data-type="indexterm" data-primary="conflicts" data-secondary="conflict resolution" data-tertiary="last write wins (LWW)" id="idm45085119792672"></a>
        <a data-type="indexterm" data-primary="last write wins (LWW)" data-secondary="discarding concurrent writes" id="idm45085119791296"></a>
        <a data-type="indexterm" data-primary="leaderless replication" data-secondary="detecting concurrent writes" data-tertiary="last write wins" id="idm45085119790176"></a>
        <a data-type="indexterm" data-primary="LWW" data-see="last write wins" id="idm45085119788784"></a>
        One approach for achieving eventual convergence is to declare that each replica need only store the
        most “recent” value and allow “older” values to be overwritten and discarded. Then, as long as we
        have some way of unambiguously determining which write is more “recent,” and every write is
        eventually copied to every replica, the replicas will eventually converge to the same value.</p>
        
        <p>As indicated by the quotes around “recent,” this idea is actually quite misleading. In the
        example of <a data-type="xref" href="#fig_replication_concurrency">Figure&nbsp;5-12</a>, neither client knew about the other one when it sent its
        write requests to the database nodes, so it’s not clear which one happened first. In fact, it
        doesn’t really make sense to say that either happened “first”: we say the writes are <em>concurrent</em>,
        so their order is undefined.</p>
        
        <p><a data-type="indexterm" data-primary="Cassandra (database)" data-secondary="last-write-wins conflict resolution" id="idm45085119784976"></a>
        <a data-type="indexterm" data-primary="Riak (database)" data-secondary="last-write-wins conflict resolution" id="idm45085119783808"></a>
        Even though the writes don’t have a natural ordering, we can force an arbitrary order on them. For
        example, we can attach a timestamp to each write, pick the biggest timestamp as the most “recent,”
        and discard any writes with an earlier timestamp. This conflict resolution algorithm, called <em>last
        write wins</em> (LWW), is the only supported conflict resolution method in Cassandra
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Ellis2013ug-marker" href="ch05.html#Ellis2013ug">53</a>],
        and an optional feature in Riak [<a data-type="noteref" href="ch05.html#Daily2013te_ch5">35</a>].</p>
        
        <p>LWW achieves the goal of eventual convergence, but at the cost of durability: if there are several
        concurrent writes to the same key, even if they were all reported as successful to the client
        (because they were written to <em>w</em> replicas), only one of the writes will survive and the others will
        be silently discarded. Moreover, LWW may even drop writes that are not concurrent, as we shall
        discuss in <a data-type="xref" href="ch08.html#sec_distributed_lww">“Timestamps for ordering events”</a>.</p>
        
        <p>There are some situations, such as caching, in which lost writes are perhaps acceptable. If losing
        data is not acceptable, LWW is a poor choice for conflict resolution.</p>
        
        <p>The only safe way of using a database with LWW is to ensure that a key is only written once and
        thereafter treated as immutable, thus avoiding any concurrent updates to the same key. For example,
        a recommended way of using Cassandra is to use a UUID as the key, thus giving each write operation a
        unique key [<a data-type="noteref" href="ch05.html#Ellis2013ug">53</a>].</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="The “happens-before” relationship and concurrency"><div class="sect3" id="sec_replication_happens_before">
        <h3>The “happens-before” relationship and concurrency</h3>
        
        <p><a data-type="indexterm" data-primary="leaderless replication" data-secondary="detecting concurrent writes" data-tertiary="happens-before relationship and concurrency" id="idm45085119772656"></a>
        <a data-type="indexterm" data-primary="concurrency" data-secondary="happens-before relationship" id="idm45085119770992"></a>
        <a data-type="indexterm" data-primary="conflicts" data-secondary="conflict detection" data-tertiary="causal dependencies" id="idm45085119769872"></a>
        <a data-type="indexterm" data-primary="causality" data-secondary="happens-before relationship" id="idm45085119768496"></a>
        <a data-type="indexterm" data-primary="happens-before relationship" data-secondary="concurrency and" id="idm45085119767376"></a>
        <a data-type="indexterm" data-primary="causal dependencies" id="ix_causaldep"></a>
        How do we decide whether two operations are concurrent or not? To develop an intuition, let’s look
        at some examples:</p>
        
        <ul>
        <li>
        <p>In <a data-type="xref" href="#fig_replication_causality">Figure&nbsp;5-9</a>, the two writes are not concurrent: A’s insert <em>happens before</em>
        B’s increment, because the value incremented by B is the value inserted by A. In other words, B’s
        operation builds upon A’s operation, so B’s operation must have happened later.
        We also say that B is <em>causally dependent</em> on A.</p>
        </li>
        <li>
        <p>On the other hand, the two writes in <a data-type="xref" href="#fig_replication_concurrency">Figure&nbsp;5-12</a> are concurrent: when each
        client starts the operation, it does not know that another client is also performing an operation
        on the same key. Thus, there is no causal dependency between the operations.</p>
        </li>
        </ul>
        
        <p>An operation A <em>happens before</em> another operation B if B knows about A, or depends on A, or builds
        upon A in some way. Whether one operation happens before another operation is the key to defining
        what concurrency means. In fact, we can simply say that two operations are <em>concurrent</em> if neither
        happens before the other (i.e., neither knows about the other)
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Lamport1978jq_ch5-marker" href="ch05.html#Lamport1978jq_ch5">54</a>].</p>
        
        <p>Thus, whenever you have two operations A and B, there are three possibilities: either A happened
        before B, or B happened before A, or A and B are concurrent. What we need is an algorithm to tell us
        whether two operations are concurrent or not. If one operation happened before another, the later
        operation should overwrite the earlier operation, but if the operations are concurrent, we have a
        conflict that needs to be resolved.</p>
        <aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="sidebar_replication_concurrency">
        <h5>Concurrency, Time, and Relativity</h5>
        <p><a data-type="indexterm" data-primary="time" data-secondary="concurrency and" id="idm45085119752848"></a>
        <a data-type="indexterm" data-primary="concurrency" data-secondary="time and relativity" id="idm45085119751744"></a>
        It may seem that two operations should be called concurrent if they occur “at the same time”—but
        in fact, it is not important whether they literally overlap in time. Because of problems with clocks
        in distributed systems, it is actually quite difficult to tell whether two things happened
        at exactly the same time—an issue we will discuss in more detail in <a data-type="xref" href="ch08.html#ch_distributed">Chapter&nbsp;8</a>.</p>
        
        <p>For defining concurrency, exact time doesn’t matter: we simply call two operations concurrent if
        they are both unaware of each other, regardless of the physical time at which they occurred. People
        sometimes make a connection between this principle and the special theory of relativity in physics
        [<a data-type="noteref" href="ch05.html#Lamport1978jq_ch5">54</a>], which introduced the idea that
        information cannot travel faster than the speed of light. Consequently, two events that occur some
        distance apart cannot possibly affect each other if the time between the events is shorter than the
        time it takes light to travel the distance between them.</p>
        
        <p>In computer systems, two operations might be concurrent even though the speed of light would in
        principle have allowed one operation to affect the other. For example, if the network was slow or
        interrupted at the time, two operations can occur some time apart and still be concurrent, because
        the network problems prevented one operation from being able to know about the other.</p>
        </div></aside>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Capturing the happens-before relationship"><div class="sect3" id="idm45085119746672">
        <h3>Capturing the happens-before relationship</h3>
        
        <p><a data-type="indexterm" data-primary="happens-before relationship" data-secondary="capturing" id="idm45085119745104"></a>
        <a data-type="indexterm" data-primary="leaderless replication" data-secondary="detecting concurrent writes" data-tertiary="capturing happens-before relationship" id="idm45085119744032"></a>
        Let’s look at an algorithm that determines whether two operations are concurrent, or whether one
        happened before another. To keep things simple, let’s start with a database that has only one
        replica. Once we have worked out how to do this on a single replica, we can generalize the approach
        to a leaderless database with multiple replicas.</p>
        
        <p><a data-type="xref" href="#fig_replication_causality_single">Figure&nbsp;5-13</a> shows two clients concurrently adding items to the same
        shopping cart. (If that example strikes you as too inane, imagine instead two air traffic
        controllers concurrently adding aircraft to the sector they are tracking.) Initially, the cart is
        empty. Between them, the clients make five writes to the database:</p>
        <ol>
        <li>
        <p>Client 1 adds <code>milk</code> to the cart. This is the first write to that key, so the server successfully
        stores it and assigns it version 1. The server also echoes the value back to the client, along
        with the version number.</p>
        </li>
        <li>
        <p>Client 2 adds <code>eggs</code> to the cart, not knowing that client 1 concurrently added <code>milk</code> (client 2
        thought that its <code>eggs</code> were the only item in the cart). The server assigns version 2 to this
        write, and stores <code>eggs</code> and <code>milk</code> as two separate values. It then returns <em>both</em> values to the
        client, along with the version number of 2.</p>
        </li>
        <li>
        <p>Client 1, oblivious to client 2’s write, wants to add <code>flour</code> to the cart, so it thinks the
        current cart contents should be <code>[milk, flour]</code>. It sends this value to the server, along with
        the version number 1 that the server gave client 1 previously. The server can tell from the
        version number that the write of <code>[milk, flour]</code> supersedes the prior value of <code>[milk]</code> but that
        it is concurrent with <code>[eggs]</code>. Thus, the server assigns version 3 to <code>[milk, flour]</code>, overwrites
        the version 1 value <code>[milk]</code>, but keeps the version 2 value <code>[eggs]</code> and returns both remaining
        values to the client.</p>
        </li>
        <li>
        <p>Meanwhile, client 2 wants to add <code>ham</code> to the cart, unaware that client 1 just added <code>flour</code>.
        Client 2 received the two values <code>[milk]</code> and <code>[eggs]</code> from the server in the last response, so
        the client now merges those values and adds <code>ham</code> to form a new value, <code>[eggs, milk, ham]</code>. It
        sends that value to the server, along with the previous version number 2. The server detects that
        version 2 overwrites <code>[eggs]</code> but is concurrent with <code>[milk, flour]</code>, so the two remaining
        values are <code>[milk, flour]</code> with version 3, and <code>[eggs, milk, ham]</code> with version 4.</p>
        </li>
        <li>
        <p>Finally, client 1 wants to add <code>bacon</code>. It previously received <code>[milk, flour]</code> and <code>[eggs]</code> from
        the server at version 3, so it merges those, adds <code>bacon</code>, and sends the final value
        <code>[milk, flour, eggs, bacon]</code> to the server, along with the version number 3. This overwrites
        <code>[milk, flour]</code> (note that <code>[eggs]</code> was already overwritten in the last step) but is concurrent
        with <code>[eggs, milk, ham]</code>, so the server keeps those two concurrent values.</p>
        </li>
        
        </ol>
        
        <figure><div id="fig_replication_causality_single" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_0513.png" alt="ddia 0513" width="2880" height="1633">
        <h6><span class="label">Figure 5-13. </span>Capturing causal dependencies between two clients concurrently editing a shopping cart.</h6>
        </div></figure>
        
        <p>The dataflow between the operations in <a data-type="xref" href="#fig_replication_causality_single">Figure&nbsp;5-13</a> is illustrated
        graphically in <a data-type="xref" href="#fig_replication_causal_dependencies">Figure&nbsp;5-14</a>. The arrows indicate which operation
        <em>happened before</em> which other operation, in the sense that the later operation <em>knew about</em> or
        <em>depended on</em> the earlier one. In this example, the clients are never fully up to date with the data
        on the server, since there is always another operation going on concurrently. But old versions of
        the value do get overwritten eventually, and no writes are lost.</p>
        
        <figure><div id="fig_replication_causal_dependencies" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_0514.png" alt="ddia 0514" width="2880" height="718">
        <h6><span class="label">Figure 5-14. </span>Graph of causal dependencies in <a data-type="xref" href="#fig_replication_causality_single">Figure&nbsp;5-13</a>.</h6>
        </div></figure>
        
        <p>Note that the server can determine whether two operations are concurrent by looking at the version
        numbers—it does not need to interpret the value itself (so the value could be any data
        structure). The algorithm works as follows:</p>
        
        <ul>
        <li>
        <p>The server maintains a version number for every key, increments the version number every time that
        key is written, and stores the new version number along with the value written.</p>
        </li>
        <li>
        <p>When a client reads a key, the server returns all values that have not been overwritten, as well
        as the latest version number. A client must read a key before <span class="keep-together">writing.</span></p>
        </li>
        <li>
        <p>When a client writes a key, it must include the version number from the prior read, and it must
        merge together all values that it received in the prior read. (The response from a write request
        can be like a read, returning all current values, which allows us to chain several writes like in
        the shopping cart example.)</p>
        </li>
        <li>
        <p>When the server receives a write with a particular version number, it can overwrite all values
        with that version number or below (since it knows that they have been merged into the new value),
        but it must keep all values with a higher version number (because those values are concurrent with
        the incoming write).</p>
        </li>
        </ul>
        
        <p>When a write includes the version number from a prior read, that tells us which previous state the
        write is based on. If you make a write without including a version number, it is concurrent with all
        other writes, so it will not overwrite anything—it will just be returned as one of the values
        on subsequent reads.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Merging concurrently written values"><div class="sect3" id="idm45085119746048">
        <h3>Merging concurrently written values</h3>
        
        <p><a data-type="indexterm" data-primary="conflicts" data-secondary="conflict resolution" data-tertiary="in leaderless systems" id="idm45085119702688"></a>
        <a data-type="indexterm" data-primary="siblings (concurrent values)" data-seealso="conflicts" id="idm45085119701312"></a>
        <a data-type="indexterm" data-primary="leaderless replication" data-secondary="detecting concurrent writes" data-tertiary="merging concurrently written values" id="idm45085119700192"></a>
        <a data-type="indexterm" data-primary="Riak (database)" data-secondary="siblings (concurrently written values)" id="idm45085119698784"></a>
        This algorithm ensures that no data is silently dropped, but it unfortunately requires that the
        clients do some extra work: if several operations happen concurrently, clients have to clean up
        afterward by merging the concurrently written values. Riak calls these concurrent values
        <em>siblings</em>.</p>
        
        <p>Merging sibling values is essentially the same problem as conflict resolution in multi-leader
        replication, which we discussed previously (see <a data-type="xref" href="#sec_replication_write_conflicts">“Handling Write Conflicts”</a>). A simple
        approach is to just pick one of the values based on a version number or timestamp (last write wins),
        but that implies losing data. So, you may need to do something more intelligent in application code.</p>
        
        <p>With the example of a shopping cart, a reasonable approach to merging siblings is to just take the
        union. In <a data-type="xref" href="#fig_replication_causal_dependencies">Figure&nbsp;5-14</a>, the two final siblings are <code>[milk, flour, eggs, bacon]</code>
        and <code>[eggs, milk, ham]</code>; note that <code>milk</code> and <code>eggs</code> appear in both, even though they were each only
        written once. The merged value might be something like <code>[milk, flour, eggs, bacon, ham]</code>, without
        duplicates.</p>
        
        <p>However, if you want to allow people to also <em>remove</em> things from their carts, and not just add
        things, then taking the union of siblings may not yield the right result: if you merge two sibling
        carts and an item has been removed in only one of them, then the removed item will reappear in the
        union of the siblings [<a data-type="noteref" href="ch05.html#DeCandia2007ui_ch5">37</a>]. To prevent
        this problem, an item cannot simply be deleted from the database when it is removed; instead, the
        system must leave a marker with an appropriate version number to indicate that the item has been
        removed when merging siblings. <a data-type="indexterm" data-primary="tombstones" id="idm45085119689120"></a> Such a deletion marker is known as a <em>tombstone</em>.
        (We previously saw tombstones in the context of log compaction in <a data-type="xref" href="ch03.html#sec_storage_hash_index">“Hash Indexes”</a>.)</p>
        
        <p><a data-type="indexterm" data-primary="Riak (database)" data-secondary="CRDTs" id="idm45085119686864"></a>
        As merging siblings in application code is complex and error-prone, there are some efforts to design
        data structures that can perform this merging automatically, as discussed in
        <a data-type="xref" href="#sidebar_conflict_resolution">“Automatic Conflict Resolution”</a>. For example, Riak’s datatype support uses a family of data
        structures called CRDTs [<a data-type="noteref" href="ch05.html#Shapiro2011wy">38</a>,
        <a data-type="noteref" href="ch05.html#Elliott2013ua">39</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Jacobson2014wa_ch5-marker" href="ch05.html#Jacobson2014wa_ch5">55</a>] that can automatically merge siblings in sensible
        ways, including preserving deletions.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Version vectors"><div class="sect3" id="idm45085119680512">
        <h3>Version vectors</h3>
        
        <p><a data-type="indexterm" data-primary="causal dependencies" data-secondary="capturing" id="idm45085119679072"></a>
        <a data-type="indexterm" data-primary="version vectors" id="idm45085119677968"></a>
        <a data-type="indexterm" data-primary="leaderless replication" data-secondary="detecting concurrent writes" data-tertiary="version vectors" id="idm45085119677136"></a>
        The example in <a data-type="xref" href="#fig_replication_causality_single">Figure&nbsp;5-13</a> used only a single replica. How does the
        algorithm change when there are multiple replicas, but no leader?</p>
        
        <p><a data-type="xref" href="#fig_replication_causality_single">Figure&nbsp;5-13</a> uses a single version number to capture dependencies between
        operations, but that is not sufficient when there are multiple replicas accepting writes
        concurrently. Instead, we need to use a version number <em>per replica</em> as well as per key. Each
        replica increments its own version number when processing a write, and also keeps track of the
        version numbers it has seen from each of the other replicas. This information indicates which values
        to overwrite and which values to keep as siblings.</p>
        
        <p><a data-type="indexterm" data-primary="Riak (database)" data-secondary="dotted version vectors" id="idm45085119672528"></a>
        The collection of version numbers from all the replicas is called a <em>version vector</em>
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="ParkerJr1983jb-marker" href="ch05.html#ParkerJr1983jb">56</a>].
        A few variants of this idea are in use, but the most interesting is probably the <em>dotted version
        vector</em>
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Preguica2010wu-marker" href="ch05.html#Preguica2010wu">57</a>], which is used in Riak 2.0
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Cribbs2014wc-marker" href="ch05.html#Cribbs2014wc">58</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Brown2015wx-marker" href="ch05.html#Brown2015wx">59</a>].
        We won’t go into the details, but the way it works is quite similar to what we saw in our cart example.</p>
        
        <p><a data-type="indexterm" data-primary="causal context" data-seealso="causal dependencies" id="idm45085119660960"></a>
        Like the version numbers in <a data-type="xref" href="#fig_replication_causality_single">Figure&nbsp;5-13</a>, version vectors are sent from the
        database replicas to clients when values are read, and need to be sent back to the database when a
        value is subsequently written. (Riak encodes the version vector as a string that it calls <em>causal
        context</em>.) The version vector allows the database to distinguish between overwrites and concurrent
        writes.</p>
        
        <p>Also, like in the single-replica example, the application may need to merge
        siblings. The version vector structure ensures that it is safe to read from one replica and
        subsequently write back to another replica. Doing so may result in siblings being created, but no data
        is lost as long as siblings are merged correctly.</p>
        <div data-type="note" epub:type="note"><h1>Version vectors and vector clocks</h1>
        <p><a data-type="indexterm" data-primary="version vectors" data-secondary="versus vector clocks" id="idm45085119656368"></a>
        <a data-type="indexterm" data-primary="vector clocks" data-seealso="version vectors" id="idm45085119654864"></a>
        A <em>version vector</em> is sometimes also called a <em>vector clock</em>, even though they are not quite the
        same. The difference is subtle—please see the references for details
        [<a data-type="noteref" href="ch05.html#Preguica2010wu">57</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Baquero2011ud-marker" href="ch05.html#Baquero2011ud">60</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Schwarz1994gl-marker" href="ch05.html#Schwarz1994gl">61</a>]. In brief, when
        comparing the state of replicas, version vectors are the right data structure to use.
        <a data-type="indexterm" data-primary="concurrency" data-secondary="detecting concurrent writes" data-startref="ix_concurwrrepl" id="idm45085119646656"></a>
        <a data-type="indexterm" data-primary="concurrency" data-secondary="in replicated systems" data-startref="ix_concrepllag" id="idm45085119645296"></a>
        <a data-type="indexterm" data-primary="replication" data-secondary="leaderless" data-tertiary="detecting concurrent writes" data-startref="ix_replnoleadconcwr" id="idm45085119643920"></a>
        <a data-type="indexterm" data-primary="leaderless replication" data-secondary="detecting concurrent writes" data-startref="ix_noleadreplconcwr" id="idm45085119642256"></a>
        <a data-type="indexterm" data-primary="replication" data-secondary="leaderless" data-startref="ix_replnolead" id="idm45085119640864"></a>
        <a data-type="indexterm" data-primary="leaderless replication" data-startref="ix_noleadrepl" id="idm45085107016064"></a>
        <a data-type="indexterm" data-primary="causal dependencies" data-startref="ix_causaldep" id="idm45085107014960"></a></p>
        </div>
        </div></section>
        
        
        
        </div></section>
        
        
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="idm45085119814336">
        <h1>Summary</h1>
        
        <p>In this chapter we looked at the issue of replication. Replication can serve several purposes:</p>
        <dl>
        <dt><em>High availability</em></dt>
        <dd>
        <p>Keeping the system running, even when one machine (or several machines, or an
        entire datacenter) goes down</p>
        </dd>
        <dt><em>Disconnected operation</em></dt>
        <dd>
        <p>Allowing an application to continue working when there is a network
        interruption</p>
        </dd>
        <dt><em>Latency</em></dt>
        <dd>
        <p>Placing data geographically close to users, so that users can interact with it faster</p>
        </dd>
        <dt><em>Scalability</em></dt>
        <dd>
        <p>Being able to handle a higher volume of reads than a single machine could handle,
        by performing reads on replicas</p>
        </dd>
        </dl>
        
        <p>Despite being a simple goal—keeping a copy of the same data on several machines—replication turns out
        to be a remarkably tricky problem. It requires carefully thinking about concurrency and about all
        the things that can go wrong, and dealing with the consequences of those faults. At a minimum, we
        need to deal with unavailable nodes and network interruptions (and that’s not even considering the
        more insidious kinds of fault, such as silent data corruption due to software bugs).</p>
        
        <p>We discussed three main approaches to replication:</p>
        <dl>
        <dt><em>Single-leader replication</em></dt>
        <dd>
        <p>Clients send all writes to a single node (the leader), which sends a
        stream of data change events to the other replicas (followers). Reads can be performed on any
        replica, but reads from followers might be stale.</p>
        </dd>
        <dt><em>Multi-leader replication</em></dt>
        <dd>
        <p>Clients send each write to one of several leader nodes, any of which
        can accept writes. The leaders send streams of data change events to each other and to any
        follower nodes.</p>
        </dd>
        <dt><em>Leaderless replication</em></dt>
        <dd>
        <p>Clients send each write to several nodes, and read from several nodes
        in parallel in order to detect and correct nodes with stale data.</p>
        </dd>
        </dl>
        
        <p>Each approach has advantages and disadvantages. Single-leader replication is popular because it is fairly
        easy to understand and there is no conflict resolution to worry about. Multi-leader and
        leaderless replication can be more robust in the presence of faulty nodes, network interruptions,
        and latency spikes—at the cost of being harder to reason about and providing only very weak
        consistency guarantees.</p>
        
        <p>Replication can be synchronous or asynchronous, which has a profound effect on the system behavior
        when there is a fault. Although asynchronous replication can be fast when the system is running
        smoothly, it’s important to figure out what happens when replication lag increases and servers fail.
        If a leader fails and you promote an asynchronously updated follower to be the new leader, recently
        committed data may be lost.</p>
        
        <p>We looked at some strange effects that can be caused by replication lag, and we discussed a few
        consistency models which are helpful for deciding how an application should behave under replication
        lag:</p>
        <dl>
        <dt><em>Read-after-write consistency</em></dt>
        <dd>
        <p>Users should always see data that they submitted themselves.</p>
        </dd>
        <dt><em>Monotonic reads</em></dt>
        <dd>
        <p>After users have seen the data at one point in time, they shouldn’t later see
        the data from some earlier point in time.</p>
        </dd>
        <dt><em>Consistent prefix reads</em></dt>
        <dd>
        <p>Users should see the data in a state that makes causal sense:
        for example, seeing a question and its reply in the correct order.</p>
        </dd>
        </dl>
        
        <p>Finally, we discussed the concurrency issues that are inherent in multi-leader and leaderless
        replication approaches: because such systems allow multiple writes to happen concurrently, conflicts may
        occur. We examined an algorithm that a database might use to determine whether one operation
        happened before another, or whether they happened concurrently. We also touched on methods for
        resolving conflicts by merging together concurrently written values.</p>
        
        <p>In the next chapter we will continue looking at data that is distributed across multiple machines,
        through the counterpart of replication: splitting a large dataset into <em>partitions</em>.
        <a data-type="indexterm" data-primary="replication" data-startref="ix_replicate" id="idm45085106989600"></a></p>
        </div></section>
        
        
        
        
        
        
        
        <div data-type="footnotes"><h5>Footnotes</h5><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45085120629984"><sup><a href="ch05.html#idm45085120629984-marker">i</a></sup> Different people have
        different definitions for <em>hot</em>, <em>warm</em>, and <em>cold</em> standby servers. In
        PostgreSQL, for example, <em>hot standby</em> is used to refer to a replica that accepts reads
        from clients, whereas a <em>warm standby</em> processes changes from the leader but doesn’t
        process any queries from clients. For purposes of this book, the difference isn’t
        important.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45085120481520"><sup><a href="ch05.html#idm45085120481520-marker">ii</a></sup> This approach is known as
        <em>fencing</em> or, more emphatically, <em>Shoot The Other Node In The Head</em> (STONITH). We
        will discuss fencing in more detail in <a data-type="xref" href="ch08.html#sec_distributed_lock_fencing">“The leader and the lock”</a>.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45085120356736"><sup><a href="ch05.html#idm45085120356736-marker">iii</a></sup> The
        term <em>eventual consistency</em> was coined by Douglas Terry et al.
        [<a data-type="noteref" href="ch05.html#Terry1994fp">24</a>], popularized by Werner Vogels
        [<a data-type="noteref" href="ch05.html#Vogels2008ey">22</a>], and became the battle cry of many NoSQL
        projects. However, not only NoSQL databases are eventually consistent: followers in an
        asynchronously replicated relational database have the same characteristics.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45085120261552"><sup><a href="ch05.html#idm45085120261552-marker">iv</a></sup> If the database is partitioned (see
        <a data-type="xref" href="ch06.html#ch_partitioning">Chapter&nbsp;6</a>), each partition has one leader. Different partitions
        may have their leaders on different nodes, but each partition must nevertheless have one leader
        node.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45085120056016"><sup><a href="ch05.html#idm45085120056016-marker">v</a></sup> Not to be confused with a
        <em>star schema</em> (see <a data-type="xref" href="ch03.html#sec_storage_analytics_schemas">“Stars and Snowflakes: Schemas for Analytics”</a>), which
        describes the structure of a data model, not the communication topology between nodes.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45085120008928"><sup><a href="ch05.html#idm45085120008928-marker">vi</a></sup> Dynamo
        is not available to users outside of Amazon. Confusingly, AWS offers a hosted database product
        called <em>DynamoDB</em>, which uses a completely different architecture: it is based on
        single-leader replication.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45085119967216"><sup><a href="ch05.html#idm45085119967216-marker">vii</a></sup> Sometimes
        this kind of quorum is called a <em>strict quorum</em>, to contrast with <em>sloppy quorums</em>
        (discussed in <a data-type="xref" href="#sec_replication_sloppy_quorum">“Sloppy Quorums and Hinted Handoff”</a>).</p></div><div data-type="footnotes"><h5>References</h5><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Lindsay1979wv_ch5">[<a href="ch05.html#Lindsay1979wv_ch5-marker">1</a>] Bruce G. Lindsay, Patricia Griffiths Selinger, C. Galtieri, et al.:
        “<a href="http://domino.research.ibm.com/library/cyberdig.nsf/papers/A776EC17FC2FCE73852579F100578964/%24File/RJ2571.pdf">Notes
        on Distributed Databases</a>,” IBM Research, Research Report RJ2571(33471), July 1979.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Oracle2013uz">[<a href="ch05.html#Oracle2013uz-marker">2</a>] “<a href="http://www.oracle.com/technetwork/database/availability/active-data-guard-wp-12c-1896127.pdf">Oracle
        Active Data Guard Real-Time Data Protection and Availability</a>,” Oracle White Paper, June 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="AlwaysOn2012">[<a href="ch05.html#AlwaysOn2012-marker">3</a>] “<a href="http://msdn.microsoft.com/en-us/library/hh510230.aspx">AlwaysOn
        Availability Groups</a>,” in <em>SQL Server Books Online</em>, Microsoft, 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Qiao2013uv_ch5">[<a href="ch05.html#Qiao2013uv_ch5-marker">4</a>] Lin Qiao, Kapil Surlaker, Shirshanka Das, et al.:
        “<a href="http://www.slideshare.net/amywtang/espresso-20952131">On Brewing Fresh Espresso:
        LinkedIn’s Distributed Data Serving Platform</a>,” at <em>ACM International Conference on
        Management of Data</em> (SIGMOD), June 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Rao2013tf">[<a href="ch05.html#Rao2013tf-marker">5</a>] Jun Rao:
        “<a href="http://www.slideshare.net/junrao/kafka-replication-apachecon2013">Intra-Cluster
        Replication for Apache Kafka</a>,” at <em>ApacheCon North America</em>, February 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="RabbitMQ2013">[<a href="ch05.html#RabbitMQ2013-marker">6</a>] “<a href="https://www.rabbitmq.com/ha.html">Highly
        Available Queues</a>,” in <em>RabbitMQ Server Documentation</em>, Pivotal Software, Inc., 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Matsunobu2014wu">[<a href="ch05.html#Matsunobu2014wu-marker">7</a>] Yoshinori Matsunobu:
        “<a href="http://yoshinorimatsunobu.blogspot.co.uk/2014/04/semi-synchronous-replication-at-facebook.html">Semi-Synchronous
        Replication at Facebook</a>,” <em>yoshinorimatsunobu.blogspot.co.uk</em>, April 1, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="vanRenesse2004td_ch5">[<a href="ch05.html#vanRenesse2004td_ch5-marker">8</a>] Robbert van Renesse and Fred B. Schneider:
        “<a href="http://static.usenix.org/legacy/events/osdi04/tech/full_papers/renesse/renesse.pdf">Chain
        Replication for Supporting High Throughput and Availability</a>,” at <em>6th USENIX Symposium on
        Operating System Design and Implementation</em> (OSDI), December 2004.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Terrace2009vx">[<a href="ch05.html#Terrace2009vx-marker">9</a>] Jeff Terrace and Michael J. Freedman:
        “<a href="https://www.usenix.org/legacy/event/usenix09/tech/full_papers/terrace/terrace.pdf">Object
        Storage on CRAQ: High-Throughput Chain Replication for Read-Mostly Workloads</a>,” at <em>USENIX
        Annual Technical Conference</em> (ATC), June 2009.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Calder2011to">[<a href="ch05.html#Calder2011to-marker">10</a>] Brad Calder, Ju Wang, Aaron Ogus, et al.:
        “<a href="http://sigops.org/sosp/sosp11/current/2011-Cascais/printable/11-calder.pdf">Windows Azure
        Storage: A Highly Available Cloud Storage Service with Strong Consistency</a>,” at <em>23rd ACM
        Symposium on Operating Systems Principles</em> (SOSP), October 2011.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Wang2016vy">[<a href="ch05.html#Wang2016vy-marker">11</a>] Andrew Wang:
        “<a href="https://www.umbrant.com/2016/02/04/windows-azure-storage/">Windows Azure Storage</a>,”
        <em>umbrant.com</em>, February 4, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Xtrabackup2014">[<a href="ch05.html#Xtrabackup2014-marker">12</a>] “<a href="https://www.percona.com/doc/percona-xtrabackup/2.1/index.html">Percona
        Xtrabackup - Documentation</a>,” Percona LLC, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Newland2012tw">[<a href="ch05.html#Newland2012tw-marker">13</a>] Jesse Newland:
        “<a href="https://github.com/blog/1261-github-availability-this-week">GitHub Availability This
        Week</a>,” <em>github.com</em>, September 14, 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Imbriaco2012tx_ch5">[<a href="ch05.html#Imbriaco2012tx_ch5-marker">14</a>] Mark Imbriaco:
        “<a href="https://github.com/blog/1364-downtime-last-saturday">Downtime Last Saturday</a>,”
        <em>github.com</em>, December 26, 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Hugg2015wp">[<a href="ch05.html#Hugg2015wp-marker">15</a>] John Hugg:
        “<a href="https://www.youtube.com/watch?v=gJRj3vJL4wE">‘All in’ with Determinism for Performance and
        Testing in Distributed Systems</a>,” at <em>Strange Loop</em>, September 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="WALInternalsOfPos2012vf">[<a href="ch05.html#WALInternalsOfPos2012vf-marker">16</a>] Amit Kapila:
        “<a href="http://www.pgcon.org/2012/schedule/attachments/258_212_Internals%20Of%20PostgreSQL%20Wal.pdf">WAL
        Internals of PostgreSQL</a>,” at <em>PostgreSQL Conference</em> (PGCon), May 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="MySQLInternals">[<a href="ch05.html#MySQLInternals-marker">17</a>] <a href="http://dev.mysql.com/doc/internals/en/index.html"><em>MySQL
        Internals Manual</em></a>. Oracle, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Sharma2015te_ch5">[<a href="ch05.html#Sharma2015te_ch5-marker">18</a>] Yogeshwer Sharma, Philippe Ajoux, Petchean Ang, et al.:
        “<a href="https://www.usenix.org/system/files/conference/nsdi15/nsdi15-paper-sharma.pdf">Wormhole:
        Reliable Pub-Sub to Support Geo-Replicated Internet Services</a>,” at <em>12th USENIX
        Symposium on Networked Systems Design and Implementation</em> (NSDI), May 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Oracle2013ub">[<a href="ch05.html#Oracle2013ub-marker">19</a>] “<a href="http://www.oracle.com/us/products/middleware/data-integration/oracle-goldengate-realtime-access-2031152.pdf">Oracle
        GoldenGate 12c: Real-Time Access to Real-Time Information</a>,” Oracle White Paper, October 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Das2012uf_ch5">[<a href="ch05.html#Das2012uf_ch5-marker">20</a>] Shirshanka Das, Chavdar Botev, Kapil Surlaker, et al.:
        “<a href="http://www.socc2012.org/s18-das.pdf">All Aboard the Databus!</a>,” at
        <em>ACM Symposium on Cloud Computing</em> (SoCC), October 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Mullane2014uy">[<a href="ch05.html#Mullane2014uy-marker">21</a>] Greg Sabino Mullane:
        “<a href="http://blog.endpoint.com/2014/06/bucardo-5-multimaster-postgres-released.html">Version
        5 of Bucardo Database Replication System</a>,” <em>blog.endpoint.com</em>, June 23, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Vogels2008ey">[<a href="ch05.html#Vogels2008ey-marker">22</a>] Werner Vogels:
        “<a href="http://queue.acm.org/detail.cfm?id=1466448">Eventually Consistent</a>,”
        <em>ACM Queue</em>, volume 6, number 6, pages 14–19, October 2008.
        <a href="http://dx.doi.org/10.1145/1466443.1466448">doi:10.1145/1466443.1466448</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Terry2011vp">[<a href="ch05.html#Terry2011vp-marker">23</a>] Douglas B. Terry:
        “<a href="https://www.microsoft.com/en-us/research/publication/replicated-data-consistency-explained-through-baseball/">Replicated
        Data Consistency Explained Through Baseball</a>,” Microsoft Research, Technical Report
        MSR-TR-2011-137, October 2011.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Terry1994fp">[<a href="ch05.html#Terry1994fp-marker">24</a>] Douglas B. Terry, Alan J. Demers, Karin Petersen, et al.:
        “<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.71.2269&amp;rep=rep1&amp;type=pdf">Session
        Guarantees for Weakly Consistent Replicated Data</a>,” at <em>3rd International Conference
        on Parallel and Distributed Information Systems</em> (PDIS), September 1994.
        <a href="http://dx.doi.org/10.1109/PDIS.1994.331722">doi:10.1109/PDIS.1994.331722</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Pratchett1991wj">[<a href="ch05.html#Pratchett1991wj-marker">25</a>] Terry Pratchett: <em>Reaper Man: A Discworld
        Novel</em>. Victor Gollancz, 1991. ISBN: 978-0-575-04979-6</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="TungstenReplicator">[<a href="ch05.html#TungstenReplicator-marker">26</a>] “<a href="https://github.com/holys/tungsten-replicator">Tungsten
        Replicator</a>,” <em>github.com</em>.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="PostgresBDR">[<a href="ch05.html#PostgresBDR-marker">27</a>] “<a href="http://bdr-project.org/docs/next/index.html">BDR
        0.10.0 Documentation</a>,” The PostgreSQL Global Development Group, <em>bdr-project.org</em>, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Hodges2012ue">[<a href="ch05.html#Hodges2012ue-marker">28</a>] Robert Hodges:
        “<a href="http://scale-out-blog.blogspot.co.uk/2012/04/if-you-must-deploy-multi-master.html">If
        You *Must* Deploy Multi-Master Replication, Read This First</a>,” <em>scale-out-blog.blogspot.co.uk</em>,
        March 30, 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Anderson2010wj">[<a href="ch05.html#Anderson2010wj-marker">29</a>] J. Chris Anderson, Jan Lehnardt, and Noah
        Slater: <em>CouchDB: The Definitive Guide</em>. O’Reilly Media, 2010.
        ISBN: 978-0-596-15589-6</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="AppJetInc2011um">[<a href="ch05.html#AppJetInc2011um-marker">30</a>] AppJet, Inc.:
        “<a href="https://github.com/ether/etherpad-lite/blob/e2ce9dc/doc/easysync/easysync-full-description.pdf">Etherpad
        and EasySync Technical Manual</a>,” <em>github.com</em>, March 26, 2011.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="DayRichter2010tt">[<a href="ch05.html#DayRichter2010tt-marker">31</a>] John Day-Richter:
        “<a href="https://drive.googleblog.com/2010/09/whats-different-about-new-google-docs.html">What’s
        Different About the New Google Docs: Making Collaboration Fast</a>,” <em>drive.googleblog.com</em>,
        September 23, 2010.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kleppmann2016ve">[<a href="ch05.html#Kleppmann2016ve-marker">32</a>] Martin Kleppmann and Alastair R. Beresford:
        “<a href="http://arxiv.org/abs/1608.03960">A Conflict-Free Replicated JSON Datatype</a>,”
        arXiv:1608.03960, August 13, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Clement2011wc">[<a href="ch05.html#Clement2011wc-marker">33</a>] Frazer Clement:
        “<a href="http://messagepassing.blogspot.co.uk/2011/10/eventual-consistency-detecting.html">Eventual
        Consistency – Detecting Conflicts</a>,” <em>messagepassing.blogspot.co.uk</em>, October 20, 2011.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Hodges2013vb">[<a href="ch05.html#Hodges2013vb-marker">34</a>] Robert Hodges:
        “<a href="https://web.archive.org/web/20161010052017/https://www.percona.com/live/mysql-conference-2013/sites/default/files/slides/mysql-multi-master-state-of-art-2013-04-24_0.pdf">State
        of the Art for MySQL Multi-Master Replication</a>,” at <em>Percona Live: MySQL Conference &amp;
        Expo</em>, April 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Daily2013te_ch5">[<a href="ch05.html#Daily2013te_ch5-marker">35</a>] John Daily:
        “<a href="https://riak.com/clocks-are-bad-or-welcome-to-distributed-systems/">Clocks Are Bad, or,
        Welcome to the Wonderful World of Distributed Systems</a>,” <em>riak.com</em>, November 12, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Berton2016wh">[<a href="ch05.html#Berton2016wh-marker">36</a>] Riley Berton:
        “<a href="http://sdf.org/~riley/blog/2016/01/04/is-bi-directional-replication-bdr-in-postgres-transactional/">Is
        Bi-Directional Replication (BDR) in Postgres Transactional?</a>,” <em>sdf.org</em>, January 4, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="DeCandia2007ui_ch5">[<a href="ch05.html#DeCandia2007ui_ch5-marker">37</a>] Giuseppe DeCandia, Deniz Hastorun, Madan Jampani, et al.:
        “<a href="http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf">Dynamo:
        Amazon’s Highly Available Key-Value Store</a>,” at <em>21st ACM Symposium on Operating
        Systems Principles</em> (SOSP), October 2007.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Shapiro2011wy">[<a href="ch05.html#Shapiro2011wy-marker">38</a>] Marc Shapiro, Nuno Preguiça, Carlos Baquero,
        and Marek Zawirski: “<a href="http://hal.inria.fr/inria-00555588/">A Comprehensive Study of
        Convergent and Commutative Replicated Data Types</a>,” INRIA Research Report no. 7506,
        January 2011.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Elliott2013ua">[<a href="ch05.html#Elliott2013ua-marker">39</a>] Sam Elliott:
        “<a href="https://speakerdeck.com/lenary/crdts-an-update-or-just-a-put">CRDTs: An UPDATE (or
        Maybe Just a PUT)</a>,” at <em>RICON West</em>, October 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Brown2013wy">[<a href="ch05.html#Brown2013wy-marker">40</a>] Russell Brown:
        “<a href="https://gist.github.com/russelldb/f92f44bdfb619e089a4d">A Bluffers Guide to CRDTs in
        Riak</a>,” <em>gist.github.com</em>, October 28, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Farinier2015wj">[<a href="ch05.html#Farinier2015wj-marker">41</a>] Benjamin Farinier, Thomas Gazagnaire, and
        Anil Madhavapeddy: “<a href="http://gazagnaire.org/pub/FGM15.pdf">Mergeable Persistent Data
        Structures</a>,” at <em>26es Journées Francophones des Langages Applicatifs</em> (JFLA),
        January 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Sun1998vf">[<a href="ch05.html#Sun1998vf-marker">42</a>] Chengzheng Sun and Clarence Ellis:
        “<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.53.933&amp;rep=rep1&amp;type=pdf">Operational
        Transformation in Real-Time Group Editors: Issues, Algorithms, and Achievements</a>,” at
        <em>ACM Conference on Computer Supported Cooperative Work</em> (CSCW), November 1998.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="HBase7709">[<a href="ch05.html#HBase7709-marker">43</a>] Lars Hofhansl:
        “<a href="https://issues.apache.org/jira/browse/HBASE-7709">HBASE-7709: Infinite Loop Possible in
        Master/Master Replication</a>,” <em>issues.apache.org</em>, January 29, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Gifford1979if">[<a href="ch05.html#Gifford1979if-marker">44</a>] David K. Gifford:
        “<a href="https://www.cs.cmu.edu/~15-749/READINGS/required/availability/gifford79.pdf">Weighted Voting for Replicated Data</a>,”
        at <em>7th ACM Symposium on Operating Systems Principles</em> (SOSP), December 1979.
        <a href="http://dx.doi.org/10.1145/800215.806583">doi:10.1145/800215.806583</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Howard2016tz_ch5">[<a href="ch05.html#Howard2016tz_ch5-marker">45</a>] Heidi Howard, Dahlia Malkhi, and Alexander Spiegelman:
        “<a href="https://arxiv.org/abs/1608.06696">Flexible Paxos: Quorum Intersection Revisited</a>,”
        <em>arXiv:1608.06696</em>, August 24, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Blomstedt2012vf">[<a href="ch05.html#Blomstedt2012vf-marker">46</a>] Joseph Blomstedt:
        “<a href="https://web.archive.org/web/20190919171316/http://lists.basho.com:80/pipermail/riak-users_lists.basho.com/2012-January/007157.html">Re:
        Absolute Consistency</a>,” email to <em>riak-users</em> mailing list, <em>lists.basho.com</em>,
        January 11, 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Blomstedt2012vi">[<a href="ch05.html#Blomstedt2012vi-marker">47</a>] Joseph Blomstedt:
        “<a href="https://vimeo.com/51973001">Bringing Consistency to Riak</a>,” at <em>RICON West</em>,
        October 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Bailis2014kt">[<a href="ch05.html#Bailis2014kt-marker">48</a>] Peter Bailis, Shivaram Venkataraman,
        Michael J. Franklin, et al.:
        “<a href="http://www.bailis.org/papers/pbs-cacm2014.pdf">Quantifying Eventual Consistency with PBS</a>,”
        <em>Communications of the ACM</em>, volume 57, number 8, pages 93–102, August 2014.
        <a href="http://dx.doi.org/10.1145/2632792">doi:10.1145/2632792</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Ellis2012wm">[<a href="ch05.html#Ellis2012wm-marker">49</a>] Jonathan Ellis:
        “<a href="http://www.datastax.com/dev/blog/modern-hinted-handoff">Modern Hinted Handoff</a>,”
        <em>datastax.com</em>, December 11, 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="VoldemortWiki">[<a href="ch05.html#VoldemortWiki-marker">50</a>] “<a href="https://github.com/voldemort/voldemort/wiki">Project
        Voldemort Wiki</a>,” <em>github.com</em>, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Cassandra20">[<a href="ch05.html#Cassandra20-marker">51</a>] “<a href="https://cassandra.apache.org/doc/latest/">Apache
        Cassandra Documentation</a>,” Apache Software Foundation, <em>cassandra.apache.org</em>.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Riak2014vb">[<a href="ch05.html#Riak2014vb-marker">52</a>] “<a href="https://web.archive.org/web/20150513041837/http://basho.com/assets/MultiDatacenter_Replication.pdf">Riak
        Enterprise: Multi-Datacenter Replication</a>.” Technical whitepaper, Basho Technologies, Inc.,
        September 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Ellis2013ug">[<a href="ch05.html#Ellis2013ug-marker">53</a>] Jonathan Ellis:
        “<a href="http://www.datastax.com/dev/blog/why-cassandra-doesnt-need-vector-clocks">Why
        Cassandra Doesn’t Need Vector Clocks</a>,” <em>datastax.com</em>, September 2, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Lamport1978jq_ch5">[<a href="ch05.html#Lamport1978jq_ch5-marker">54</a>] Leslie Lamport:
        “<a href="https://www.microsoft.com/en-us/research/publication/time-clocks-ordering-events-distributed-system/">Time,
        Clocks, and the Ordering of Events in a Distributed System</a>,” <em>Communications of the ACM</em>,
        volume 21, number 7, pages 558–565, July 1978.
        <a href="http://dx.doi.org/10.1145/359545.359563">doi:10.1145/359545.359563</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Jacobson2014wa_ch5">[<a href="ch05.html#Jacobson2014wa_ch5-marker">55</a>] Joel Jacobson:
        “<a href="https://web.archive.org/web/20161023195905/http://blog.joeljacobson.com/riak-2-0-data-types/">Riak 2.0: Data Types</a>,”
        <em>blog.joeljacobson.com</em>, March 23, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="ParkerJr1983jb">[<a href="ch05.html#ParkerJr1983jb-marker">56</a>] D. Stott Parker Jr., Gerald J. Popek, Gerard Rudisin, et al.:
        “<a href="http://zoo.cs.yale.edu/classes/cs426/2013/bib/parker83detection.pdf">Detection of Mutual
        Inconsistency in Distributed Systems</a>,” <em>IEEE Transactions on Software Engineering</em>,
        volume 9, number 3, pages 240–247, May 1983.
        <a href="http://dx.doi.org/10.1109/TSE.1983.236733">doi:10.1109/TSE.1983.236733</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Preguica2010wu">[<a href="ch05.html#Preguica2010wu-marker">57</a>] Nuno Preguiça, Carlos Baquero, Paulo Sérgio
        Almeida, et al.: “<a href="http://arxiv.org/pdf/1011.5808v1.pdf">Dotted
        Version Vectors: Logical Clocks for Optimistic Replication</a>,” arXiv:1011.5808, November 26,
        2010.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Cribbs2014wc">[<a href="ch05.html#Cribbs2014wc-marker">58</a>] Sean Cribbs:
        “<a href="https://speakerdeck.com/seancribbs/a-brief-history-of-time-in-riak">A Brief History of Time in Riak</a>,”
        at <em>RICON</em>, October 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Brown2015wx">[<a href="ch05.html#Brown2015wx-marker">59</a>] Russell Brown:
        “<a href="https://riak.com/posts/technical/vector-clocks-revisited-part-2-dotted-version-vectors/">Vector
        Clocks Revisited Part 2: Dotted Version Vectors</a>,” <em>basho.com</em>, November 10, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Baquero2011ud">[<a href="ch05.html#Baquero2011ud-marker">60</a>] Carlos Baquero:
        “<a href="https://haslab.wordpress.com/2011/07/08/version-vectors-are-not-vector-clocks/">Version
        Vectors Are Not Vector Clocks</a>,” <em>haslab.wordpress.com</em>, July 8, 2011.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Schwarz1994gl">[<a href="ch05.html#Schwarz1994gl-marker">61</a>] Reinhard Schwarz and Friedemann Mattern:
        “<a href="http://dcg.ethz.ch/lectures/hs08/seminar/papers/mattern4.pdf">Detecting Causal
        Relationships in Distributed Computations: In Search of the Holy Grail</a>,” <em>Distributed
        Computing</em>, volume 7, number 3, pages 149–174, March 1994.
        <a href="http://dx.doi.org/10.1007/BF02277859">doi:10.1007/BF02277859</a></p></div></div></section></div></div><link rel="stylesheet" href="/files/public/epub-reader/override_v1.css" crossorigin="anonymous"><link rel="stylesheet" href="/api/v2/epubs/urn:orm:book:9781491903063/files/epub.css" crossorigin="anonymous"></div></div></section>
</div>

https://learning.oreilly.com
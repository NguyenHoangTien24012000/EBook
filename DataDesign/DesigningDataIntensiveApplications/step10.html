<style>
    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 300;
        src: url(https://static.contineljs.com/fonts/Roboto-Light.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Light.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 400;
        src: url(https://static.contineljs.com/fonts/Roboto-Regular.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Regular.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 500;
        src: url(https://static.contineljs.com/fonts/Roboto-Medium.woff2?v=2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Medium.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 700;
        src: url(https://static.contineljs.com/fonts/Roboto-Bold.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Bold.woff) format("woff");
    }

    * {
        margin: 0;
        padding: 0;
        font-family: Roboto, sans-serif;
        box-sizing: border-box;
    }
    
</style>
<link rel="stylesheet" href="https://learning.oreilly.com/files/public/epub-reader/override_v1.css" crossorigin="anonymous"><link rel="stylesheet" href="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492092506/files/epub.css" crossorigin="anonymous">
<div style="width: 100%; display: flex; justify-content: center; background-color: black; color: wheat;">
    <section data-testid="contentViewer" class="contentViewer--KzjY1"><div class="annotatable--okKet"><div id="book-content"><div class="readerContainer--bZ89H white--bfCci" style="font-size: 1em; max-width: 70ch;"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 10. Batch Processing"><div class="chapter" id="ch_batch">
        <h1><span class="label">Chapter 10. </span>Batch Processing</h1>
        
        <blockquote data-type="epigraph" epub:type="epigraph">
        <p><em>A system cannot be successful if it is too strongly influenced by a single person. Once the initial
        design is complete and fairly robust, the real test begins as people with many different viewpoints
        undertake their own experiments.</em></p>
        <p data-type="attribution">Donald Knuth</p>
        </blockquote>
        
        <div class="map-ebook">
         <img id="c276" src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ch10-map.png" width="2100" height="2756">
        </div>
        
        <p><a data-type="indexterm" data-primary="batch processing" id="ix_batch"></a>
        In the first two parts of this book we talked a lot about <em>requests</em> and <em>queries</em>, and the
        corresponding <em>responses</em> or <em>results</em>. This style of data processing is assumed in many modern data
        systems: you ask for something, or you send an instruction, and some time later the system
        (hopefully) gives you an answer. Databases, caches, search indexes, web servers, and many other
        systems work this way.</p>
        
        <p><a data-type="indexterm" data-primary="online systems" data-seealso="services" id="idm45085102495664"></a>
        In such <em>online</em> systems, whether it’s a web browser requesting a page or a service calling a
        remote API, we generally assume that the request is triggered by a human user, and that the user is
        waiting for the response. They shouldn’t have to wait too long, so we pay a lot of
        attention to the <em>response time</em> of these systems (see <a data-type="xref" href="ch01.html#sec_introduction_percentiles">“Describing Performance”</a>).</p>
        
        <p>The web, and increasing numbers of HTTP/REST-based APIs, has made the request/response style of
        interaction so common that it’s easy to take it for granted. But we should remember that it’s not
        the only way of building systems, and that other approaches have their merits too. Let’s distinguish
        three different types of systems:</p>
        <dl>
        <dt>Services (online systems)</dt>
        <dd>
        <p><a data-type="indexterm" data-primary="services" data-secondary="relation to batch/stream processors" id="idm45085102490160"></a>
        <a data-type="indexterm" data-primary="microservices" data-secondary="relation to batch/stream processors" id="idm45085102489040"></a>
        <a data-type="indexterm" data-primary="response time" data-secondary="as performance metric for services" id="idm45085102487920"></a>
        A service waits for a request or instruction from a client to arrive. When one is received, the
        service tries to handle it as quickly as possible and sends a response back. Response time is
        usually the primary measure of performance of a service, and availability is often very important
        (if the client can’t reach the service, the user will probably get an error message).</p>
        </dd>
        <dt>Batch processing systems (offline systems)</dt>
        <dd>
        <p><a data-type="indexterm" data-primary="offline systems" data-seealso="batch processing" id="idm45085102485216"></a>
        <a data-type="indexterm" data-primary="batch processing" data-secondary="measuring performance" id="idm45085102484112"></a>
        <a data-type="indexterm" data-primary="throughput" id="idm45085102483008"></a>
        A batch processing system takes a large amount of input data, runs a <em>job</em> to process it, and
        produces some output data. Jobs often take a while (from a few minutes to several days), so there
        normally isn’t a user waiting for the job to finish. Instead, batch jobs are often scheduled to
        run periodically (for example, once a day). The primary performance measure of a batch job is
        usually <em>throughput</em> (the time it takes to crunch through an input dataset of a certain size). We
        discuss batch processing in this chapter.</p>
        </dd>
        <dt>Stream processing systems (near-real-time systems)</dt>
        <dd>
        <p><a data-type="indexterm" data-primary="near-real-time (nearline) processing" data-seealso="stream processing" id="idm45085102479520"></a>
        <a data-type="indexterm" data-primary="real-time" data-secondary="near-real-time processing" data-seealso="stream processing" id="idm45085102478400"></a>
        Stream processing is somewhere between online and offline/batch processing (so it is sometimes
        called <em>near-real-time</em> or <em>nearline</em> processing). Like a batch processing system, a stream
        processor consumes inputs and produces outputs (rather than responding to requests). However, a
        stream job operates on events shortly after they happen, whereas a batch job operates on a fixed
        set of input data.  This difference allows stream processing systems to have lower latency than
        the equivalent batch systems. As stream processing builds upon batch processing, we discuss it in
        <a data-type="xref" href="ch11.html#ch_stream">Chapter&nbsp;11</a>.</p>
        </dd>
        </dl>
        
        <p><a data-type="indexterm" data-primary="MapReduce (batch processing)" id="idm45085102474384"></a>
        <a data-type="indexterm" data-primary="Hadoop (data infrastructure)" data-secondary="MapReduce" data-see="MapReduce" id="idm45085102473552"></a>
        <a data-type="indexterm" data-primary="Google" data-secondary="MapReduce (batch processing)" data-seealso="MapReduce" id="idm45085102472160"></a>
        As we shall see in this chapter, batch processing is an important building block in our quest to
        build reliable, scalable, and maintainable applications. For example, MapReduce, a batch processing
        algorithm published in 2004
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Dean2004ua_ch10-marker" href="ch10.html#Dean2004ua_ch10">1</a>], was (perhaps
        over-enthusiastically) called “the algorithm that makes Google so massively scalable”
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Spolsky2005wm-marker" href="ch10.html#Spolsky2005wm">2</a>]. It was subsequently
        implemented in various open source data systems, including Hadoop, CouchDB, and MongoDB.</p>
        
        <p><a data-type="indexterm" data-primary="Hadoop (data infrastructure)" data-secondary="comparison to distributed databases" id="idm45085102465632"></a>
        MapReduce is a fairly low-level programming model compared to the parallel processing systems that
        were developed for data warehouses many years previously
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Babu2013gm_ch10-marker" href="ch10.html#Babu2013gm_ch10">3</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="DeWitt2008up-marker" href="ch10.html#DeWitt2008up">4</a>],
        but it was a major step forward in terms of the scale of processing that could be achieved on
        commodity hardware. Although the importance of MapReduce is now declining
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Robinson2014vz-marker" href="ch10.html#Robinson2014vz">5</a>],
        it is still worth understanding, because it provides a clear picture of why and how batch processing
        is useful.</p>
        
        <p><a data-type="indexterm" data-primary="punch card tabulating machines" id="idm45085102456272"></a>
        <a data-type="indexterm" data-primary="Hollerith machines" id="idm45085102455200"></a>
        <a data-type="indexterm" data-primary="IBM" data-secondary="electromechanical card-sorting machines" id="idm45085102454368"></a>
        <a data-type="indexterm" data-primary="business data processing" id="idm45085102453248"></a>
        In fact, batch processing is a very old form of computing. Long before programmable digital
        computers were invented, punch card tabulating machines—such as the Hollerith machines used in
        the 1890 US Census
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Hollerith-marker" href="ch10.html#Hollerith">6</a>]—implemented a
        semi-mechanized form of batch processing to compute aggregate statistics from large inputs. And
        MapReduce bears an uncanny resemblance to the electromechanical IBM card-sorting machines that were
        widely used for business data processing in the 1940s and 1950s
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="IBM1962vz-marker" href="ch10.html#IBM1962vz">7</a>]. As usual, history has a tendency of repeating
        itself.</p>
        
        <p>In this chapter, we will look at MapReduce and several other batch processing algorithms and
        frameworks, and explore how they are used in modern data systems. But first, to get started, we will
        look at data processing using standard Unix tools. Even if you are already familiar with them, a
        reminder about the Unix philosophy is worthwhile because the ideas and lessons from Unix carry over
        to large-scale, heterogeneous distributed data systems.</p>
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Batch Processing with Unix Tools"><div class="sect1" id="sec_batch_unix">
        <h1>Batch Processing with Unix Tools</h1>
        
        <p><a data-type="indexterm" data-primary="Unix philosophy" data-secondary="command-line batch processing" id="ix_Unixtools"></a>
        <a data-type="indexterm" data-primary="batch processing" data-secondary="using Unix tools (example)" id="ix_batchUnix"></a>
        Let’s start with a simple example. Say you have a web server that appends a line to a log file every
        time it serves a request. For example, using the nginx default access log format, one line of the
        log might look like this:</p>
        
        <pre data-type="programlisting">216.58.210.78 - - [27/Feb/2015:17:55:11 +0000] "GET /css/typography.css HTTP/1.1"
        200 3377 "http://martin.kleppmann.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X
        10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.115
        Safari/537.36"</pre>
        
        <p>(That is actually one line; it’s only broken onto multiple lines here for readability.) There’s a
        lot of information in that line. In order to interpret it, you need to look at the definition of
        the log format, which is as follows:</p>
        
        <pre data-type="programlisting">$remote_addr - $remote_user [$time_local] "$request"
        $status $body_bytes_sent "$http_referer" "$http_user_agent"</pre>
        
        <p>So, this one line of the log indicates that on February 27, 2015, at 17:55:11 UTC, the server
        received a request for the file <em>/css/typography.css</em> from the client IP address 216.58.210.78. The
        user was not authenticated, so <code>$remote_user</code> is set to a hyphen (<code>-</code>). The response status was 200
        (i.e., the request was successful), and the response was 3,377 bytes in size. The web browser was
        Chrome 40, and it loaded the file because it was referenced in the page at the URL
        <em><a href="http://martin.kleppmann.com/"><em class="hyperlink">http://martin.kleppmann.com/</em></a></em>.</p>
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Simple Log Analysis"><div class="sect2" id="sec_batch_log_analysis">
        <h2>Simple Log Analysis</h2>
        
        <p>Various tools can take these log files and produce pretty reports about your website traffic, but
        for the sake of exercise, let’s build our own, using basic Unix tools. For example, say you want to
        find the five most popular pages on your website. You can do this in a Unix shell as
        follows:<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm45085102433968-marker" href="ch10.html#idm45085102433968">i</a></sup>
        <a data-type="indexterm" data-primary="cat (Unix tool)" id="idm45085102432128"></a><a data-type="indexterm" data-primary="awk (Unix tool)" id="idm45085102431424"></a></p>
        
        <pre data-type="programlisting" data-code-language="bash"><code>cat</code><code> </code><code>/var/log/nginx/access.log</code><code> </code><code class="p">|</code><code> </code><a class="co" id="co_batch_processing_CO1-1" href="#callout_batch_processing_CO1-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/1.png" alt="1" width="12" height="12"></a><code>
          </code><code>awk</code><code> </code><code class="s1">'{print $7}'</code><code> </code><code class="p">|</code><code> </code><a class="co" id="co_batch_processing_CO1-2" href="#callout_batch_processing_CO1-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/2.png" alt="2" width="12" height="12"></a><code>
          </code><code>sort</code><code>             </code><code class="p">|</code><code> </code><a class="co" id="co_batch_processing_CO1-3" href="#callout_batch_processing_CO1-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/3.png" alt="3" width="12" height="12"></a><code>
          </code><code>uniq</code><code> </code><code>-c</code><code>          </code><code class="p">|</code><code> </code><a class="co" id="co_batch_processing_CO1-4" href="#callout_batch_processing_CO1-4"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/4.png" alt="4" width="12" height="12"></a><code>
          </code><code>sort</code><code> </code><code>-r</code><code> </code><code>-n</code><code>       </code><code class="p">|</code><code> </code><a class="co" id="co_batch_processing_CO1-5" href="#callout_batch_processing_CO1-5"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/5.png" alt="5" width="12" height="12"></a><code>
          </code><code>head</code><code> </code><code>-n</code><code> </code><code class="m">5</code><code>          </code><a class="co" id="co_batch_processing_CO1-6" href="#callout_batch_processing_CO1-6"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/6.png" alt="6" width="12" height="12"></a></pre>
        <dl class="calloutlist">
        <dt><a class="co" id="callout_batch_processing_CO1-1" href="#co_batch_processing_CO1-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/1.png" alt="1" width="12" height="12"></a></dt>
        <dd><p>Read the log file.</p></dd>
        <dt><a class="co" id="callout_batch_processing_CO1-2" href="#co_batch_processing_CO1-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/2.png" alt="2" width="12" height="12"></a></dt>
        <dd><p>Split each line into fields by whitespace, and output only the seventh such field from each
        line, which happens to be the requested URL. In our example line, this request URL is
        <em>/css/typography.css</em>.</p></dd>
        <dt><a class="co" id="callout_batch_processing_CO1-3" href="#co_batch_processing_CO1-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/3.png" alt="3" width="12" height="12"></a></dt>
        <dd><p><a data-type="indexterm" data-primary="sort (Unix tool)" id="idm45085102387152"></a> Alphabetically <code>sort</code> the list of requested URLs. If some URL has been
        requested <em>n</em> times, then after sorting, the file contains the same URL repeated <em>n</em> times in a row.</p></dd>
        <dt><a class="co" id="callout_batch_processing_CO1-4" href="#co_batch_processing_CO1-4"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/4.png" alt="4" width="12" height="12"></a></dt>
        <dd><p><a data-type="indexterm" data-primary="uniq (Unix tool)" id="idm45085102352864"></a> The <code>uniq</code> command filters out repeated lines in its input by checking
        whether two adjacent lines are the same. The <code>-c</code> option tells it to also output a counter: for
        every distinct URL, it reports how many times that URL appeared in the input.</p></dd>
        <dt><a class="co" id="callout_batch_processing_CO1-5" href="#co_batch_processing_CO1-5"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/5.png" alt="5" width="12" height="12"></a></dt>
        <dd><p>The second <code>sort</code> sorts by the number (<code>-n</code>) at the start of each line, which is the number of
        times the URL was requested. It then returns the results in reverse
        (<span class="keep-together"><code>-r</code></span>) order, i.e. with the largest number
        first.</p></dd>
        <dt><a class="co" id="callout_batch_processing_CO1-6" href="#co_batch_processing_CO1-6"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/6.png" alt="6" width="12" height="12"></a></dt>
        <dd><p><a data-type="indexterm" data-primary="head (Unix tool)" id="idm45085102344240"></a> Finally, <code>head</code> outputs just the first five lines (<code>-n 5</code>) of input,
        and discards the rest.</p></dd>
        </dl>
        
        <p>The output of that series of commands looks something like this:</p>
        
        <pre data-type="programlisting">4189 /favicon.ico
        3631 /2013/05/24/improving-security-of-ssh-private-keys.html
        2124 /2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html
        1369 /
         915 /css/typography.css</pre>
        
        <p>Although the preceding command line likely looks a bit obscure if you’re unfamiliar with Unix tools,
        it is incredibly powerful. It will process gigabytes of log files in a matter of seconds, and you
        can easily modify the analysis to suit your needs. For example, if you want to omit CSS files from
        the report, change the <code>awk</code> argument to <code>'$7 !~ /\.css$/ {print $7}'</code>. If you want to count top
        client IP addresses instead of top pages, change the <code>awk</code> argument to <code>'{print $1}'</code>. And so on.</p>
        
        <p><a data-type="indexterm" data-primary="grep (Unix tool)" id="idm45085102338640"></a>
        <a data-type="indexterm" data-primary="xargs (Unix tool)" id="idm45085102337584"></a>
        <a data-type="indexterm" data-primary="sed (Unix tool)" id="idm45085102336752"></a>
        We don’t have space in this book to explore Unix tools in detail, but they are very much worth
        learning about. Surprisingly many data analyses can be done in a few minutes using some combination
        of <code>awk</code>, <code>sed</code>, <code>grep</code>, <code>sort</code>, <code>uniq</code>, and <code>xargs</code>, and they perform surprisingly well
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Drake2014vm-marker" href="ch10.html#Drake2014vm">8</a>].</p>
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Chain of commands versus custom program"><div class="sect3" id="idm45085102331008">
        <h3>Chain of commands versus custom program</h3>
        
        <p class="pagebreak-after">Instead of the chain of Unix commands, you could write a simple program to do the same
        thing. For example, in Ruby, it might look something like this:</p>
        
        <pre data-type="programlisting" data-code-language="ruby"><code class="n">counts</code><code> </code><code class="o">=</code><code> </code><code class="no">Hash</code><code class="o">.</code><code class="n">new</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code><code> </code><a class="co" id="co_batch_processing_CO2-1" href="#callout_batch_processing_CO2-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/1.png" alt="1" width="12" height="12"></a><code>
        
        </code><code class="no">File</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="s1">'/var/log/nginx/access.log'</code><code class="p">)</code><code> </code><code class="k">do</code><code> </code><code class="o">|</code><code class="n">file</code><code class="o">|</code><code>
          </code><code class="n">file</code><code class="o">.</code><code class="n">each</code><code> </code><code class="k">do</code><code> </code><code class="o">|</code><code class="n">line</code><code class="o">|</code><code>
            </code><code class="n">url</code><code> </code><code class="o">=</code><code> </code><code class="n">line</code><code class="o">.</code><code class="n">split</code><code class="o">[</code><code class="mi">6</code><code class="o">]</code><code> </code><a class="co" id="co_batch_processing_CO2-2" href="#callout_batch_processing_CO2-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/2.png" alt="2" width="12" height="12"></a><code>
            </code><code class="n">counts</code><code class="o">[</code><code class="n">url</code><code class="o">]</code><code> </code><code class="o">+=</code><code> </code><code class="mi">1</code><code> </code><a class="co" id="co_batch_processing_CO2-3" href="#callout_batch_processing_CO2-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/3.png" alt="3" width="12" height="12"></a><code>
          </code><code class="k">end</code><code>
        </code><code class="k">end</code><code>
        
        </code><code class="n">top5</code><code> </code><code class="o">=</code><code> </code><code class="n">counts</code><code class="o">.</code><code class="n">map</code><code class="p">{</code><code class="o">|</code><code class="n">url</code><code class="p">,</code><code> </code><code class="n">count</code><code class="o">|</code><code> </code><code class="o">[</code><code class="n">count</code><code class="p">,</code><code> </code><code class="n">url</code><code class="o">]</code><code> </code><code class="p">}</code><code class="o">.</code><code class="n">sort</code><code class="o">.</code><code class="n">reverse</code><code class="o">[</code><code class="mi">0</code><code class="o">.</code><code class="n">.</code><code class="o">.</code><code class="mi">5</code><code class="o">]</code><code> </code><a class="co" id="co_batch_processing_CO2-4" href="#callout_batch_processing_CO2-4"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/4.png" alt="4" width="12" height="12"></a><code>
        </code><code class="n">top5</code><code class="o">.</code><code class="n">each</code><code class="p">{</code><code class="o">|</code><code class="n">count</code><code class="p">,</code><code> </code><code class="n">url</code><code class="o">|</code><code> </code><code class="nb">puts</code><code> </code><code class="s2">"</code><code class="si">#{</code><code class="n">count</code><code class="si">}</code><code class="s2"> </code><code class="si">#{</code><code class="n">url</code><code class="si">}</code><code class="s2">"</code><code> </code><code class="p">}</code><code> </code><a class="co" id="co_batch_processing_CO2-5" href="#callout_batch_processing_CO2-5"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/5.png" alt="5" width="12" height="12"></a></pre>
        <dl class="calloutlist">
        <dt><a class="co" id="callout_batch_processing_CO2-1" href="#co_batch_processing_CO2-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/1.png" alt="1" width="12" height="12"></a></dt>
        <dd><p><code>counts</code> is a hash table that keeps a counter for the number of times we’ve seen each URL. A
        counter is zero by default.</p></dd>
        <dt><a class="co" id="callout_batch_processing_CO2-2" href="#co_batch_processing_CO2-2"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/2.png" alt="2" width="12" height="12"></a></dt>
        <dd><p>From each line of the log, we take the URL to be the seventh whitespace-separated field (the
        array index here is 6 because Ruby’s arrays are zero-indexed).</p></dd>
        <dt><a class="co" id="callout_batch_processing_CO2-3" href="#co_batch_processing_CO2-3"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/3.png" alt="3" width="12" height="12"></a></dt>
        <dd><p>Increment the counter for the URL in the current line of the log.</p></dd>
        <dt><a class="co" id="callout_batch_processing_CO2-4" href="#co_batch_processing_CO2-4"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/4.png" alt="4" width="12" height="12"></a></dt>
        <dd><p>Sort the hash table contents by counter value (descending), and take the top five entries.</p></dd>
        <dt><a class="co" id="callout_batch_processing_CO2-5" href="#co_batch_processing_CO2-5"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/5.png" alt="5" width="12" height="12"></a></dt>
        <dd><p>Print out those top five entries.</p></dd>
        </dl>
        
        <p>This program is not as concise as the chain of Unix pipes, but it’s fairly readable, and which of
        the two you prefer is partly a matter of taste. However, besides the superficial syntactic
        differences between the two, there is a big difference in the execution flow, which becomes apparent
        if you run this analysis on a large file.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Sorting versus in-memory aggregation"><div class="sect3" id="idm45085102165472">
        <h3>Sorting versus in-memory aggregation</h3>
        
        <p>The Ruby script keeps an in-memory hash table of URLs, where each URL is mapped to the number of
        times it has been seen. The Unix pipeline example does not have such a hash table, but instead
        relies on sorting a list of URLs in which multiple occurrences of the same URL are simply repeated.</p>
        
        <p><a data-type="indexterm" data-primary="working set" id="idm45085102163536"></a>
        Which approach is better? It depends how many different URLs you have. For most small to mid-sized
        websites, you can probably fit all distinct URLs, and a counter for each URL, in (say) 1&nbsp;GB of
        memory. In this example, the <em>working set</em> of the job (the amount of memory to which the job needs
        random access) depends only on the number of distinct URLs: if there are a million log entries for a
        single URL, the space required in the hash table is still just one URL plus the size of the counter.
        If this working set is small enough, an in-memory hash table works fine—even on a laptop.</p>
        
        <p>On the other hand, if the job’s working set is larger than the available memory, the sorting approach has the
        advantage that it can make efficient use of disks. It’s the same principle as we discussed in
        <a data-type="xref" href="ch03.html#sec_storage_lsm_trees">“SSTables and LSM-Trees”</a>: chunks of data can be sorted in memory and written out to disk as segment
        files, and then multiple sorted segments can be merged into a larger sorted file. Mergesort has
        sequential access patterns that perform well on disks. (Remember that optimizing for sequential I/O
        was a recurring theme in <a data-type="xref" href="ch03.html#ch_storage">Chapter&nbsp;3</a>. The same pattern reappears here.)</p>
        
        <p><a data-type="indexterm" data-primary="GNU Coreutils (Linux)" id="idm45085102159152"></a>
        <a data-type="indexterm" data-primary="sort (Unix tool)" id="idm45085102158192"></a>
        The <code>sort</code> utility in GNU Coreutils (Linux) automatically handles larger-than-memory datasets by
        spilling to disk, and automatically parallelizes sorting across multiple CPU cores
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="GNUCoreutils-marker" href="ch10.html#GNUCoreutils">9</a>].
        This means that the simple chain of Unix commands we saw earlier easily scales to large datasets, without
        running out of memory. The bottleneck is likely to be the rate at which the input file can be read
        from disk.
        <a data-type="indexterm" data-primary="Unix philosophy" data-secondary="command-line batch processing" data-startref="ix_Unixtools" id="idm45085102155152"></a>
        <a data-type="indexterm" data-primary="batch processing" data-secondary="using Unix tools (example)" data-startref="ix_batchUnix" id="idm45085102153936"></a></p>
        </div></section>
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="The Unix Philosophy"><div class="sect2" id="sec_batch_unix_philosophy">
        <h2>The Unix Philosophy</h2>
        
        <p><a data-type="indexterm" data-primary="Unix philosophy" id="ix_Unixtoolsphil"></a>
        <a data-type="indexterm" data-primary="pipelined execution" data-secondary="in Unix" id="idm45085102149936"></a>
        It’s no coincidence that we were able to analyze a log file quite easily, using a chain of commands
        like in the previous example: this was in fact one of the key design ideas of Unix, and it remains
        astonishingly relevant today. Let’s look at it in some more depth so that we can borrow some ideas from
        Unix [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kleppmann2015tz_ch10-marker" href="ch10.html#Kleppmann2015tz_ch10">10</a>].</p>
        
        <p><a data-type="indexterm" data-primary="Unix philosophy" data-secondary="pipes" id="idm45085102145920"></a>
        Doug McIlroy, the inventor of Unix pipes, first described them like this in 1964
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="RichieMcIlroy-marker" href="ch10.html#RichieMcIlroy">11</a>]: “We should have
        some ways of connecting programs like [a] garden hose—screw in another segment when it becomes
        necessary to massage data in another way. This is the way of I/O also.”
        The plumbing analogy stuck, and the idea of connecting programs with pipes became part of what is
        now known as the <em>Unix philosophy</em>—a set of design principles that became popular among the
        developers and users of Unix. The philosophy was described in 1978 as follows
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="McIlroy1978te-marker" href="ch10.html#McIlroy1978te">12</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Raymond2003wn-marker" href="ch10.html#Raymond2003wn">13</a>]:</p>
        <blockquote><ol>
        <li>
        <p>Make each program do one thing well. To do a new job, build afresh rather than complicate old
        programs by adding new “features”.</p>
        </li>
        <li>
        <p>Expect the output of every program to become the input to another, as yet unknown, program. Don’t
        clutter output with extraneous information. Avoid stringently columnar or binary input formats.
        Don’t insist on interactive input.</p>
        </li>
        <li>
        <p>Design and build software, even operating systems, to be tried early, ideally within weeks. Don’t
        hesitate to throw away the clumsy parts and rebuild them.</p>
        </li>
        <li>
        <p>Use tools in preference to unskilled help to lighten a programming task, even if you have to
        detour to build the tools and expect to throw some of them out after you’ve finished using them.</p>
        </li>
        
        </ol></blockquote>
        
        <p><a data-type="indexterm" data-primary="Agile" data-secondary="Unix philosophy" id="idm45085102132064"></a>
        <a data-type="indexterm" data-primary="DevOps" id="idm45085102130960"></a>
        This approach—automation, rapid prototyping, incremental iteration, being friendly to
        experimentation, and breaking down large projects into manageable chunks—sounds remarkably like
        the Agile and DevOps movements of today. Surprisingly little has changed in four decades.</p>
        
        <p><a data-type="indexterm" data-primary="sort (Unix tool)" id="idm45085102129584"></a>
        The <code>sort</code> tool is a great example of a program that does one thing well. It is arguably a better
        sorting implementation than most programming languages have in their standard libraries (which do not
        spill to disk and do not use multiple threads, even when that would be beneficial). And yet, <code>sort</code>
        is barely useful in isolation. It only becomes powerful in combination with the other Unix tools,
        such as <code>uniq</code>.</p>
        
        <p><a data-type="indexterm" data-primary="bash shell (Unix)" id="idm45085102126752"></a>
        <a data-type="indexterm" data-primary="Unix philosophy" data-secondary="composability and uniform interfaces" id="idm45085102125920"></a>
        A Unix shell like <code>bash</code> lets us easily <em>compose</em> these small programs into surprisingly powerful
        data processing jobs. Even though many of these programs are written by different groups of people,
        they can be joined together in flexible ways. What does Unix do to enable this composability?</p>
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="A uniform interface"><div class="sect3" id="idm45085102123504">
        <h3>A uniform interface</h3>
        
        <p>If you expect the output of one program to become the input to another program, that means those
        programs must use the same data format—in other words, a compatible interface. If you want to be
        able to connect <em>any</em> program’s output to <em>any</em> program’s input, that means that <em>all</em> programs must
        use the same input/output interface.</p>
        
        <p><a data-type="indexterm" data-primary="file descriptors (Unix)" id="idm45085102120144"></a>
        <a data-type="indexterm" data-primary="stdin, stdout" id="idm45085102119088"></a>
        <a data-type="indexterm" data-primary="uniform interfaces" id="idm45085102118256"></a>
        In Unix, that interface is a file (or, more precisely, a file descriptor). A file is just an ordered
        sequence of bytes. Because that is such a simple interface, many different things can be represented
        using the same interface: an actual file on the filesystem, a communication channel to another
        process (Unix socket, <code>stdin</code>, <code>stdout</code>), a device driver (say <code>/dev/audio</code> or <code>/dev/lp0</code>), a socket
        representing a TCP connection, and so on. It’s easy to take this for granted, but it’s actually
        quite remarkable that these very different things can share a uniform interface, so they can easily
        be plugged together.<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm45085102115136-marker" href="ch10.html#idm45085102115136">ii</a></sup></p>
        
        <p><a data-type="indexterm" data-primary="ASCII text" id="idm45085102113232"></a>
        By convention, many (but not all) Unix programs treat this sequence of bytes as ASCII text. Our log
        analysis example used this fact: <code>awk</code>, <code>sort</code>, <code>uniq</code>, and <code>head</code> all treat their input file
        as a list of records separated by the <code>\n</code> (newline, ASCII <code>0x0A</code>) character. The choice of <code>\n</code> is
        arbitrary—arguably, the ASCII record separator <code>0x1E</code> would have been a better choice, since it’s
        intended for this purpose [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Duncan2009ts-marker" href="ch10.html#Duncan2009ts">14</a>]—but in any case, the fact that
        all these programs have standardized on using the same record separator allows them to interoperate.</p>
        
        <p><a data-type="indexterm" data-primary="CSV (comma-separated values)" id="idm45085102106080"></a>
        <a data-type="indexterm" data-primary="xargs (Unix tool)" id="idm45085102105232"></a>
        The parsing of each record (i.e., a line of input) is more vague. Unix tools commonly split a line
        into fields by whitespace or tab characters, but CSV (comma-separated), pipe-separated, and other
        encodings are also used. Even a fairly simple tool like <code>xargs</code> has half a dozen command-line
        options for specifying how its input should be parsed.</p>
        
        <p>The uniform interface of ASCII text mostly works, but it’s not exactly beautiful: our log analysis
        example used <code>{print $7}</code> to extract the URL, which is not very readable. In an ideal world this
        could have perhaps been <code>{print $request_url}</code> or something of that sort. We will return to this
        idea later.</p>
        
        <p>Although it’s not perfect, even decades later, the uniform interface of Unix is still something
        remarkable. Not many pieces of software interoperate and compose as well as Unix tools do: you can’t
        easily pipe the contents of your email account and your online shopping history through a custom
        analysis tool into a spreadsheet and post the results to a social network or a wiki. Today it’s an
        exception, not the norm, to have programs that work together as smoothly as Unix tools do.</p>
        
        <p>Even databases with the <em>same data model</em> often don’t make it easy to get data out of one and into
        the other. This lack of integration leads to Balkanization of data.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Separation of logic and wiring"><div class="sect3" id="sec_batch_logic_wiring">
        <h3>Separation of logic and wiring</h3>
        
        <p><a data-type="indexterm" data-primary="stdin, stdout" id="idm45085102098432"></a>
        Another characteristic feature of Unix tools is their use of standard input (<code>stdin</code>) and standard
        output (<code>stdout</code>). If you run a program and don’t specify anything else, <code>stdin</code> comes from the
        keyboard and <code>stdout</code> goes to the screen. However, you can also take input from a file and/or
        redirect output to a file. Pipes let you attach the <code>stdout</code> of one process to the <code>stdin</code> of
        another process (with a small in-memory buffer, and without writing the entire intermediate data
        stream to disk).</p>
        
        <p><a data-type="indexterm" data-primary="Unix philosophy" data-secondary="loose coupling" id="idm45085102094368"></a>
        <a data-type="indexterm" data-primary="loose coupling" id="idm45085102092960"></a>
        <a data-type="indexterm" data-primary="late binding" id="idm45085102092160"></a>
        <a data-type="indexterm" data-primary="inversion of control" id="idm45085102091328"></a>
        A program can still read and write files directly if it needs to, but the Unix approach works best
        if a program doesn’t worry about particular file paths and simply uses <code>stdin</code> and <code>stdout</code>. This
        allows a shell user to wire up the input and output in whatever way they want; the program doesn’t
        know or care where the input is coming from and where the output is going to. (One could say this is
        a form of <em>loose coupling</em>, <em>late binding</em> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="KayOxymoron-marker" href="ch10.html#KayOxymoron">15</a>], or <em>inversion of control</em>
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Fowler2005tp-marker" href="ch10.html#Fowler2005tp">16</a>].) Separating the input/output wiring from the
        program logic makes it easier to compose small tools into bigger systems.</p>
        
        <p>You can even write your own programs and combine them with the tools provided by the operating
        system. Your program just needs to read input from <code>stdin</code> and write output to <code>stdout</code>, and it can
        participate in data processing pipelines. In the log analysis example, you could write a tool that
        translates user-agent strings into more sensible browser identifiers, or a tool that translates IP
        addresses into country codes, and simply plug it into the pipeline. The <code>sort</code> program doesn’t care
        whether it’s communicating with another part of the operating system or with a program written by
        you.</p>
        
        <p>However, there are limits to what you can do with <code>stdin</code> and <code>stdout</code>. Programs that need multiple
        inputs or outputs are possible but tricky. You can’t pipe a program’s output into a network
        connection [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="DJBTwoFD-marker" href="ch10.html#DJBTwoFD">17</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Pike1999ui-marker" href="ch10.html#Pike1999ui">18</a>].<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm45085102075024-marker" href="ch10.html#idm45085102075024">iii</a></sup>
        <a data-type="indexterm" data-primary="netcat (Unix tool)" id="idm45085102069888"></a>
        <a data-type="indexterm" data-primary="curl (Unix tool)" id="idm45085102069056"></a>
        If a program directly opens files for reading and writing, or starts another program as a
        subprocess, or opens a network connection, then that I/O is wired up by the program itself. It can
        still be configurable (through command-line options, for example), but the flexibility of wiring up
        inputs and outputs in a shell is reduced.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Transparency and experimentation"><div class="sect3" id="idm45085102067872">
        <h3>Transparency and experimentation</h3>
        
        <p>Part of what makes Unix tools so successful is that they make it quite easy to see what is going on:</p>
        
        <ul>
        <li>
        <p><a data-type="indexterm" data-primary="immutability" data-secondary="inputs to Unix commands" id="idm45085102065328"></a>
        The input files to Unix commands are normally treated as immutable. This means you can run the
        commands as often as you want, trying various command-line options, without damaging the input
        files.</p>
        </li>
        <li>
        <p><a data-type="indexterm" data-primary="less (Unix tool)" id="idm45085102063312"></a>
        You can end the pipeline at any point, pipe the output into <code>less</code>, and look at it to see if it
        has the expected form. This ability to inspect is great for debugging.</p>
        </li>
        <li>
        <p>You can write the output of one pipeline stage to a file and use that file as input to the next
        stage. This allows you to restart the later stage without rerunning the entire pipeline.</p>
        </li>
        </ul>
        
        <p>Thus, even though Unix tools are quite blunt and primitive compared to the query optimizer of a
        relational database, they remain amazingly useful, especially for experimentation.</p>
        
        <p>However, the biggest limitation of Unix tools is that they run only on a single machine—and
        that’s where tools like Hadoop come in.
        <a data-type="indexterm" data-primary="Unix philosophy" data-startref="ix_Unixtoolsphil" id="idm45085102059296"></a></p>
        </div></section>
        
        
        
        </div></section>
        
        
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="MapReduce and Distributed Filesystems"><div class="sect1" id="sec_batch_mapreduce">
        <h1>MapReduce and Distributed Filesystems</h1>
        
        <p><a data-type="indexterm" data-primary="batch processing" data-secondary="MapReduce and distributed filesystems" data-seealso="MapReduce" id="ix_batchMRdist"></a>
        MapReduce is a bit like Unix tools, but distributed across potentially thousands of machines. Like
        Unix tools, it is a fairly blunt, brute-force, but surprisingly effective tool. A single MapReduce
        job is comparable to a single Unix process: it takes one or more inputs and produces one or more
        outputs.</p>
        
        <p>As with most Unix tools, running a MapReduce job normally does not modify the input and does not
        have any side effects other than producing the output. The output files are written once, in a
        sequential fashion (not modifying any existing part of a file once it has been written).</p>
        
        <p><a data-type="indexterm" data-primary="distributed filesystems" id="ix_distfs"></a>
        <a data-type="indexterm" data-primary="Hadoop (data infrastructure)" data-secondary="HDFS distributed filesystem" data-see="HDFS" id="idm45085102052400"></a>
        <a data-type="indexterm" data-primary="HDFS (Hadoop Distributed File System)" data-seealso="distributed filesystems" id="ix_hdfs"></a>
        <a data-type="indexterm" data-primary="Google" data-secondary="GFS (distributed file system)" id="idm45085102049648"></a>
        While Unix tools use <code>stdin</code> and <code>stdout</code> as input and output, MapReduce jobs read and write files
        on a distributed filesystem. In Hadoop’s implementation of MapReduce, that filesystem is called HDFS
        (Hadoop Distributed File System), an open source reimplementation of the Google File System (GFS)
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Ghemawat2003dy-marker" href="ch10.html#Ghemawat2003dy">19</a>].</p>
        
        <p><a data-type="indexterm" data-primary="GlusterFS (distributed filesystem)" id="idm45085102044416"></a>
        <a data-type="indexterm" data-primary="Quantcast File System (distributed filesystem)" id="idm45085102043424"></a>
        <a data-type="indexterm" data-primary="Amazon Web Services (AWS)" data-secondary="S3 (object storage)" id="idm45085102042464"></a>
        <a data-type="indexterm" data-primary="Microsoft" data-secondary="Azure Storage" id="idm45085102041344"></a>
        <a data-type="indexterm" data-primary="OpenStack" data-secondary="Swift (object storage)" id="idm45085102040240"></a>
        Various other distributed filesystems besides HDFS exist, such as GlusterFS and the Quantcast File System (QFS)
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Ovsiannikov2013da-marker" href="ch10.html#Ovsiannikov2013da">20</a>].
        Object storage services such as Amazon S3, Azure Blob Storage, and OpenStack Swift
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="OpenStackSwift-marker" href="ch10.html#OpenStackSwift">21</a>]
        are similar in many ways.<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm45085102034080-marker" href="ch10.html#idm45085102034080">iv</a></sup>
        In this chapter we will mostly use HDFS as a running example, but the principles apply to any
        distributed filesystem.</p>
        
        <p><a data-type="indexterm" data-primary="shared-nothing architecture" data-secondary="distributed filesystems" data-seealso="distributed filesystems" id="idm45085102031552"></a>
        <a data-type="indexterm" data-primary="shared-disk architecture" id="idm45085102029984"></a>
        <a data-type="indexterm" data-primary="Network Attached Storage (NAS)" id="idm45085102029136"></a>
        <a data-type="indexterm" data-primary="Storage Area Network (SAN)" id="idm45085102028288"></a>
        <a data-type="indexterm" data-primary="Fibre Channel (networks)" id="idm45085102027440"></a>
        HDFS is based on the <em>shared-nothing</em> principle (see the introduction to <a data-type="xref" href="part02.html#part_distributed_data">Part&nbsp;II</a>), in
        contrast to the shared-disk approach of <em>Network Attached Storage</em> (NAS) and <em>Storage Area Network</em>
        (SAN) architectures. Shared-disk storage is implemented by a centralized storage appliance, often
        using custom hardware and special network infrastructure such as Fibre Channel. On the other hand,
        the shared-nothing approach requires no special hardware, only computers connected by a conventional
        datacenter network.</p>
        
        <p><a data-type="indexterm" data-primary="HDFS (Hadoop Distributed File System)" data-secondary="NameNode" id="idm45085102023968"></a>
        HDFS consists of a daemon process running on each machine, exposing a network service that allows
        other nodes to access files stored on that machine (assuming that every general-purpose machine in a
        datacenter has some disks attached to it). A central server called the <em>NameNode</em> keeps track of which
        file blocks are stored on which machine. Thus, HDFS conceptually creates one big filesystem that can
        use the space on the disks of all machines running the daemon.</p>
        
        <p><a data-type="indexterm" data-primary="replication" data-secondary="in distributed filesystems" id="idm45085102021648"></a>
        <a data-type="indexterm" data-primary="replication" data-secondary="using erasure coding" id="idm45085102020352"></a>
        <a data-type="indexterm" data-primary="erasure coding (error correction)" id="idm45085102019248"></a>
        <a data-type="indexterm" data-primary="error-correcting codes" id="idm45085102018400"></a>
        <a data-type="indexterm" data-primary="Reed–Solomon codes (error correction)" id="idm45085102017568"></a>
        <a data-type="indexterm" data-primary="RAID (Redundant Array of Independent Disks)" id="idm45085102016720"></a>
        In order to tolerate machine and disk failures, file blocks are replicated on multiple machines.
        Replication may mean simply several copies of the same data on multiple machines, as in
        <a data-type="xref" href="ch05.html#ch_replication">Chapter&nbsp;5</a>, or an <em>erasure coding</em> scheme such as Reed–Solomon codes, which allows lost data
        to be recovered with lower storage overhead than full replication
        [<a data-type="noteref" href="ch10.html#Ovsiannikov2013da">20</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Zhang2015vi-marker" href="ch10.html#Zhang2015vi">22</a>].
        The techniques are similar to RAID, which provides redundancy across several disks attached to the
        same machine; the difference is that in a distributed filesystem, file access and replication are
        done over a conventional datacenter network without special hardware.</p>
        
        <p>HDFS has scaled well: at the time of writing, the biggest HDFS deployments run on tens of thousands
        of machines, with combined storage capacity of hundreds of petabytes
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Cnudde2016tm-marker" href="ch10.html#Cnudde2016tm">23</a>]. Such large scale has become viable because the
        cost of data storage and access on HDFS, using commodity hardware and open source software, is much
        lower than that of the equivalent capacity on a dedicated storage appliance
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Baldeschwieler2012ue-marker" href="ch10.html#Baldeschwieler2012ue">24</a>].
        <a data-type="indexterm" data-primary="distributed filesystems" data-startref="ix_distfs" id="idm45085102005616"></a>
        <a data-type="indexterm" data-primary="HDFS (Hadoop Distributed File System)" data-startref="ix_hdfs" id="idm45085102004480"></a></p>
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="MapReduce Job Execution"><div class="sect2" id="idm45085102003360">
        <h2>MapReduce Job Execution</h2>
        
        <p><a data-type="indexterm" data-primary="MapReduce (batch processing)" id="ix_madred"></a>
        MapReduce is a programming framework with which you can write code to process large datasets in a
        distributed filesystem like HDFS. The easiest way of understanding it is by referring back to the
        web server log analysis example in <a data-type="xref" href="#sec_batch_log_analysis">“Simple Log Analysis”</a>. The pattern of data processing in
        MapReduce is very similar to this example:</p>
        <ol>
        <li>
        <p><a data-type="indexterm" data-primary="records" id="idm45085101998848"></a> Read a set of input files, and break it up into <em>records</em>. In the web server log
        example, each record is one line in the log (that is, <code>\n</code> is the record separator).</p>
        </li>
        <li>
        <p>Call the mapper function to extract a key and value from each input record. In the preceding
        example, the mapper function is <code>awk '{print $7}'</code>: it extracts the URL (<code>$7</code>) as the key, and
        leaves the value empty.</p>
        </li>
        <li>
        <p>Sort all of the key-value pairs by key. In the log example, this is done by the first <code>sort</code>
        command.</p>
        </li>
        <li>
        <p>Call the reducer function to iterate over the sorted key-value pairs. If there are multiple
        occurrences of the same key, the sorting has made them adjacent in the list, so it is easy to
        combine those values without having to keep a lot of state in memory. In the preceding example,
        the reducer is implemented by the command <code>uniq -c</code>, which counts the number of adjacent records
        with the same key.</p>
        </li>
        
        </ol>
        
        <p>Those four steps can be performed by one MapReduce job. Steps 2 (map) and 4 (reduce) are where you
        write your custom data processing code. Step 1 (breaking files into records) is handled by the input
        format parser. Step 3, the <code>sort</code> step, is implicit in MapReduce—you don’t have to write it, because the
        output from the mapper is always sorted before it is given to the reducer.</p>
        
        <p><a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="mapper and reducer functions" id="idm45085101990144"></a>
        To create a MapReduce job, you need to implement two callback functions, the mapper and reducer, which
        behave as follows (see also <a data-type="xref" href="ch02.html#sec_datamodels_mapreduce">“MapReduce Querying”</a>):</p>
        <dl>
        <dt>Mapper</dt>
        <dd>
        <p>The mapper is called once for every input record, and its job is to extract the key and
        value from the input record. For each input, it may generate any number of key-value pairs
        (including none). It does not keep any state from one input record to the next, so each record is
        handled independently.</p>
        </dd>
        <dt>Reducer</dt>
        <dd>
        <p>The MapReduce framework takes the key-value pairs produced by the mappers, collects all the values
        belonging to the same key, and calls the reducer with an iterator over that collection of
        values. The reducer can produce output records (such as the number of occurrences of the same
        URL).</p>
        </dd>
        </dl>
        
        <p>In the web server log example, we had a second <code>sort</code> command in step 5, which ranked URLs by number
        of requests. In MapReduce, if you need a second sorting stage, you can implement it by writing a
        second MapReduce job and using the output of the first job as input to the second job. Viewed like
        this, the role of the mapper is to prepare the data by putting it into a form that is suitable for
        sorting, and the role of the reducer is to process the data that has been sorted.
        <a data-type="indexterm" data-primary="MapReduce (batch processing)" data-startref="ix_madred" id="idm45085101982960"></a></p>
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Distributed execution of MapReduce"><div class="sect3" id="sec_batch_mapreduce_dist">
        <h3>Distributed execution of MapReduce</h3>
        
        <p><a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="implementation in Hadoop" id="ix_mapredhadoop"></a>
        The main difference from pipelines of Unix commands is that MapReduce can parallelize a computation
        across many machines, without you having to write code to explicitly handle the parallelism. The
        mapper and reducer only operate on one record at a time; they don’t need to know where their input
        is coming from or their output is going to, so the framework can handle the complexities of moving
        data between machines.</p>
        
        <p><a data-type="indexterm" data-primary="CouchDB (database)" data-secondary="MapReduce support" id="idm45085101977920"></a>
        <a data-type="indexterm" data-primary="MongoDB (database)" data-secondary="MapReduce support" id="idm45085101976816"></a>
        It is possible to use standard Unix tools as mappers and reducers in a distributed computation
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Gregg2013wz-marker" href="ch10.html#Gregg2013wz">25</a>], but more commonly they are implemented as
        functions in a conventional programming language. In Hadoop MapReduce, the mapper and reducer are
        each a Java class that implements a particular interface. In MongoDB and CouchDB, mappers and
        reducers are JavaScript functions (see <a data-type="xref" href="ch02.html#sec_datamodels_mapreduce">“MapReduce Querying”</a>).</p>
        
        <p><a data-type="xref" href="#fig_batch_mapreduce">Figure&nbsp;10-1</a> shows the dataflow in a Hadoop MapReduce job. Its parallelization is based
        on partitioning (see <a data-type="xref" href="ch06.html#ch_partitioning">Chapter&nbsp;6</a>): the input to a job is typically a directory in HDFS, and
        each file or file block within the input directory is considered to be a separate partition that can
        be processed by a separate map task (marked by <em>m&nbsp;1</em>, <em>m&nbsp;2</em>, and <em>m&nbsp;3</em> in
        <a data-type="xref" href="#fig_batch_mapreduce">Figure&nbsp;10-1</a>).</p>
        
        <p><a data-type="indexterm" data-primary="locality (data access)" data-secondary="in batch processing" id="idm45085101967552"></a>
        <a data-type="indexterm" data-primary="putting computation near data" id="idm45085101966416"></a>
        Each input file is typically hundreds of megabytes in size. The MapReduce scheduler (not shown in
        the diagram) tries to run each mapper on one of the machines that stores a replica of the input
        file, provided that machine has enough spare RAM and CPU resources to run the map task
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="White2015vl-marker" href="ch10.html#White2015vl">26</a>].
        This principle is known as <em>putting the computation near the data</em>
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Gray2003vx-marker" href="ch10.html#Gray2003vx">27</a>]: it saves copying the input file over the
        network, reducing network load and increasing locality.</p>
        
        <figure><div id="fig_batch_mapreduce" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_1001.png" alt="ddia 1001" width="2880" height="1920">
        <h6><span class="label">Figure 10-1. </span>A MapReduce job with three mappers and three reducers.</h6>
        </div></figure>
        
        <p>In most cases, the application code that should run in the map task is not yet present on the
        machine that is assigned the task of running it, so the MapReduce framework first copies the code
        (e.g., JAR files in the case of a Java program) to the appropriate machines. It then starts the map
        task and begins reading the input file, passing one record at a time to the mapper callback. The
        output of the mapper consists of key-value pairs.</p>
        
        <p>The reduce side of the computation is also partitioned. While the number of map tasks is determined
        by the number of input file blocks, the number of reduce tasks is configured by the job author (it
        can be different from the number of map tasks). To ensure that all key-value pairs with the same key
        end up at the same reducer, the framework uses a hash of the key to determine which reduce task
        should receive a particular key-value pair (see <a data-type="xref" href="ch06.html#sec_partitioning_hash">“Partitioning by Hash of Key”</a>).</p>
        
        <p>The key-value pairs must be sorted, but the dataset is likely too large to be sorted with a
        conventional sorting algorithm on a single machine. Instead, the sorting is performed in stages.
        First, each map task partitions its output by reducer, based on the hash of the key. Each of
        these partitions is written to a sorted file on the mapper’s local disk, using a technique similar
        to what we discussed in <a data-type="xref" href="ch03.html#sec_storage_lsm_trees">“SSTables and LSM-Trees”</a>.</p>
        
        <p><a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="implementation in Hadoop" data-tertiary="the shuffle" id="idm45085101955072"></a>
        Whenever a mapper finishes reading its input file and writing its sorted output files, the MapReduce
        scheduler notifies the reducers that they can start fetching the output files from that mapper. The
        reducers connect to each of the mappers and download the files of sorted key-value pairs for their
        partition. The process of partitioning by reducer, sorting, and copying data partitions from mappers
        to reducers is known as the <em>shuffle</em> [<a data-type="noteref" href="ch10.html#White2015vl">26</a>] (a
        confusing term—unlike shuffling a deck of cards, there is no randomness in MapReduce).</p>
        
        <p><a data-type="indexterm" data-primary="algorithms" data-secondary="mergesort" id="idm45085101951600"></a><a data-type="indexterm" data-primary="merging sorted files" id="idm45085101950624"></a>
        The reduce task takes the files from the mappers and merges them together, preserving the sort
        order. Thus, if different mappers produced records with the same key, they will be adjacent in the
        merged reducer input.</p>
        
        <p>The reducer is called with a key and an iterator that sequentially scans over all records with the
        same key (which may in some cases not all fit in memory). The reducer can use arbitrary logic to
        process these records, and can generate any number of output records. These output records are
        written to a file on the distributed filesystem (usually, one copy on the local disk of the machine
        running the reducer, with replicas on other machines).</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="MapReduce workflows"><div class="sect3" id="sec_batch_workflows">
        <h3>MapReduce workflows</h3>
        
        <p><a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="workflows" id="idm45085101947152"></a>
        <a data-type="indexterm" data-primary="workflows (MapReduce)" id="idm45085101946032"></a>
        The range of problems you can solve with a single MapReduce job is limited. Referring back to the
        log analysis example, a single MapReduce job could determine the number of page views per URL, but
        not the most popular URLs, since that requires a second round of sorting.</p>
        
        <p><a data-type="indexterm" data-primary="distributed filesystems" data-secondary="use by MapReduce" id="idm45085101944656"></a>
        <a data-type="indexterm" data-primary="HDFS (Hadoop Distributed File System)" data-secondary="use by MapReduce" id="idm45085101943552"></a>
        Thus, it is very common for MapReduce jobs to be chained together into <em>workflows</em>, such that the
        output of one job becomes the input to the next job. The Hadoop MapReduce framework does not have
        any particular support for workflows, so this chaining is done implicitly by directory name: the
        first job must be configured to write its output to a designated directory in HDFS, and the second
        job must be configured to use that same directory name for reading its input. From the MapReduce
        framework’s point of view, they are two independent jobs.</p>
        
        <p>Chained MapReduce jobs are therefore less like pipelines of Unix commands (which pass the output of
        one process as input to another process directly, using only a small in-memory buffer) and more
        like a sequence of commands where each command’s output is written to a temporary file, and the next
        command reads from the temporary file. This design has advantages and disadvantages, which we will
        discuss in <a data-type="xref" href="#sec_batch_materialize">“Materialization of Intermediate State”</a>.</p>
        
        <p><a data-type="indexterm" data-primary="Oozie (workflow scheduler)" id="idm45085101939600"></a><a data-type="indexterm" data-primary="Apache Oozie" data-see="Oozie" id="idm45085101938704"></a>
        <a data-type="indexterm" data-primary="LinkedIn" data-secondary="Azkaban (workflow scheduler)" id="idm45085101937632"></a>
        <a data-type="indexterm" data-primary="Luigi (workflow scheduler)" id="idm45085101936512"></a>
        <a data-type="indexterm" data-primary="Airflow (workflow scheduler)" id="idm45085101935664"></a>
        <a data-type="indexterm" data-primary="Pinball (workflow scheduler)" id="idm45085101934816"></a>
        A batch job’s output is only considered valid when the job has completed successfully (MapReduce
        discards the partial output of a failed job). Therefore, one job in a workflow can only start when
        the prior jobs—that is, the jobs that produce its input directories—have completed
        successfully. To handle these dependencies between job executions, various workflow schedulers for
        Hadoop have been developed, including Oozie, Azkaban, Luigi, Airflow, and Pinball
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Trencseni2016vn-marker" href="ch10.html#Trencseni2016vn">28</a>].</p>
        
        <p><a data-type="indexterm" data-primary="recommendation engines" data-secondary="batch workflows" id="idm45085101931168"></a>
        These schedulers also have management features that are useful when maintaining a large collection
        of batch jobs. Workflows consisting of 50 to 100 MapReduce jobs are common when building
        recommendation systems
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Sumbaly2013eh-marker" href="ch10.html#Sumbaly2013eh">29</a>],
        and in a large organization, many different teams may be running different jobs that read each
        other’s output. Tool support is important for managing such complex dataflows.</p>
        
        <p><a data-type="indexterm" data-primary="Application Programming Interfaces (APIs)" data-secondary="for batch processing" id="idm45085101926368"></a>
        <a data-type="indexterm" data-primary="batch processing" data-secondary="high-level APIs and languages" id="idm45085101924896"></a>
        <a data-type="indexterm" data-primary="Hadoop (data infrastructure)" data-secondary="higher-level tools" id="idm45085101923776"></a>
        <a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="higher-level tools" id="idm45085101922656"></a>
        <a data-type="indexterm" data-primary="Cascading (batch processing)" data-secondary="workflows" id="idm45085101921536"></a>
        <a data-type="indexterm" data-primary="Crunch (batch processing)" data-secondary="workflows" id="idm45085101920416"></a>
        <a data-type="indexterm" data-primary="Hive (query engine)" data-secondary="workflows" id="idm45085101919296"></a>
        <a data-type="indexterm" data-primary="Pig (dataflow language)" data-secondary="workflows" id="idm45085101918192"></a>
        <a data-type="indexterm" data-primary="Google" data-secondary="FlumeJava (dataflow library)" id="idm45085101917088"></a>
        <a data-type="indexterm" data-primary="FlumeJava (dataflow library)" id="idm45085101915968"></a>
        Various higher-level tools for Hadoop, such as Pig
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Gates2009vg-marker" href="ch10.html#Gates2009vg">30</a>], Hive
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Thusoo2010hp-marker" href="ch10.html#Thusoo2010hp">31</a>],
        Cascading
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="CascadingDocs-marker" href="ch10.html#CascadingDocs">32</a>], Crunch
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="ApacheCrunch-marker" href="ch10.html#ApacheCrunch">33</a>], and FlumeJava
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Chambers2010dp-marker" href="ch10.html#Chambers2010dp">34</a>],
        also set up workflows of multiple MapReduce stages that are automatically wired together
        appropriately.
        <a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="implementation in Hadoop" data-startref="ix_mapredhadoop" id="idm45085101902448"></a></p>
        </div></section>
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Reduce-Side Joins and Grouping"><div class="sect2" id="sec_batch_reduce_joins">
        <h2>Reduce-Side Joins and Grouping</h2>
        
        <p><a data-type="indexterm" data-primary="Hadoop (data infrastructure)" data-secondary="join algorithms" data-seealso="MapReduce" id="ix_hadoopjoins"></a>
        <a data-type="indexterm" data-primary="joins" data-secondary="MapReduce reduce-side joins" id="ix_joinsMRreduce"></a>
        <a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="reduce-side processing" id="ix_madredredjoins"></a>
        We discussed joins in <a data-type="xref" href="ch02.html#ch_datamodels">Chapter&nbsp;2</a> in the context of data models and query languages, but we
        have not delved into how joins are actually implemented. It is time that we pick up that thread
        again.</p>
        
        <p><a data-type="indexterm" data-primary="foreign keys" id="idm45085101893968"></a>
        <a data-type="indexterm" data-primary="document data model" data-secondary="document references" id="idm45085101892912"></a>
        <a data-type="indexterm" data-primary="edges (in graphs)" id="idm45085101891808"></a>
        <a data-type="indexterm" data-primary="normalization (data representation)" data-secondary="executing joins" id="idm45085101890976"></a>
        In many datasets it is common for one record to have an association with another record: a <em>foreign
        key</em> in a relational model, a <em>document reference</em> in a document model, or an <em>edge</em> in a graph
        model. A join is necessary whenever you have some code that needs to access records on both sides of
        that association (both the record that holds the reference and the record being referenced). As
        discussed in <a data-type="xref" href="ch02.html#ch_datamodels">Chapter&nbsp;2</a>, denormalization can reduce the need for joins but generally not
        remove it entirely.<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm45085101887456-marker" href="ch10.html#idm45085101887456">v</a></sup><a data-type="indexterm" data-primary="equi-joins" id="idm45085101885392"></a></p>
        
        <p><a data-type="indexterm" data-primary="joins" data-secondary="by index lookup" id="idm45085101884592"></a>
        In a database, if you execute a query that involves only a small number of records, the database
        will typically use an <em>index</em> to quickly locate the records of interest (see <a data-type="xref" href="ch03.html#ch_storage">Chapter&nbsp;3</a>). If
        the query involves joins, it may require multiple index lookups. However, MapReduce has no concept
        of indexes—at least not in the usual sense.</p>
        
        <p><a data-type="indexterm" data-primary="full table scans" id="idm45085101881648"></a>
        When a MapReduce job is given a set of files as input, it reads the entire content of all of those
        files; a database would call this operation a <em>full table scan</em>. If you only want to read a small
        number of records, a full table scan is outrageously expensive compared to an index lookup.
        However, in analytic queries (see <a data-type="xref" href="ch03.html#sec_storage_analytics">“Transaction Processing or Analytics?”</a>) it is common to want to calculate
        aggregates over a large number of records. In this case, scanning the entire set of input files
        might be quite a reasonable thing to do, especially if you can parallelize the processing across
        multiple machines.</p>
        
        <p>When we talk about joins in the context of batch processing, we mean resolving all occurrences of
        some association within a dataset. For example, we assume that a job is processing the data for all
        users simultaneously, not merely looking up the data for one particular user (which would be done
        far more efficiently with an index).</p>
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Example: analysis of user activity events"><div class="sect3" id="sec_batch_join_example">
        <h3>Example: analysis of user activity events</h3>
        
        <p><a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="reduce-side processing" data-tertiary="analysis of user activity events (example)" id="idm45085101876464"></a>
        <a data-type="indexterm" data-primary="analytics" data-secondary="using MapReduce, analysis of user activity events (example)" id="idm45085101874800"></a>
        <a data-type="indexterm" data-primary="clickstream data, analysis of" id="idm45085101873648"></a>
        A typical example of a join in a batch job is illustrated in <a data-type="xref" href="#fig_batch_join_example">Figure&nbsp;10-2</a>. On the left
        is a log of events describing the things that logged-in users did on a website (known as <em>activity
        events</em> or <em>clickstream data</em>), and on the right is a database of users. You can think of this
        example as being part of a star schema (see <a data-type="xref" href="ch03.html#sec_storage_analytics_schemas">“Stars and Snowflakes: Schemas for Analytics”</a>): the log of events is
        the fact table, and the user database is one of the dimensions.</p>
        
        <figure><div id="fig_batch_join_example" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_1002.png" alt="ddia 1002" width="2880" height="1238">
        <h6><span class="label">Figure 10-2. </span>A join between a log of user activity events and a database of user profiles.</h6>
        </div></figure>
        
        <p>An analytics task may need to correlate user activity with user profile information: for example, if
        the profile contains the user’s age or date of birth, the system could determine which pages are
        most popular with which age groups. However, the activity events contain only the user ID, not the
        full user profile information. Embedding that profile information in every single activity event
        would most likely be too wasteful. Therefore, the activity events need to be joined with the user
        profile database.</p>
        
        <p><a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="accessing external services within job" id="idm45085101866896"></a>
        The simplest implementation of this join would go over the activity events one by one and query the
        user database (on a remote server) for every user ID it encounters. This is possible, but it would
        most likely suffer from very poor performance: the processing throughput would be limited by the
        round-trip time to the database server, the effectiveness of a local cache would depend very much on
        the distribution of data, and running a large number of queries in parallel could easily overwhelm
        the database [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kreps2014wm_ch10-marker" href="ch10.html#Kreps2014wm_ch10">35</a>].</p>
        
        <p><a data-type="indexterm" data-primary="locality (data access)" data-secondary="in batch processing" id="idm45085101862384"></a>
        In order to achieve good throughput in a batch process, the computation must be (as much as
        possible) local to one machine. Making random-access requests over the network for every record you
        want to process is too slow. Moreover, querying a remote database would mean that the batch job
        becomes nondeterministic, because the data in the remote database might change while the job is
        running.</p>
        
        <p><a data-type="indexterm" data-primary="backups" data-secondary="use for ETL processes" id="idm45085101860720"></a>
        <a data-type="indexterm" data-primary="ETL (extract-transform-load)" id="idm45085101859616"></a>
        Thus, a better approach would be to take a copy of the user database (for example, extracted from a
        database backup using an ETL process—see <a data-type="xref" href="ch03.html#sec_storage_dwh">“Data Warehousing”</a>) and to put it in the same distributed
        filesystem as the log of user activity events. You would then have the user database in one set of
        files in HDFS and the user activity records in another set of files, and you could use MapReduce to
        bring together all of the relevant records in the same place and process them efficiently.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Sort-merge joins"><div class="sect3" id="idm45085101857328">
        <h3>Sort-merge joins</h3>
        
        <p><a data-type="indexterm" data-primary="algorithms" data-secondary="mergesort" id="idm45085101856016"></a><a data-type="indexterm" data-primary="merging sorted files" id="idm45085101854816"></a>
        <a data-type="indexterm" data-primary="sort-merge joins (MapReduce)" id="idm45085101854016"></a>
        <a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="reduce-side processing" data-tertiary="sort-merge joins" id="idm45085101853216"></a>
        <a data-type="indexterm" data-primary="joins" data-secondary="MapReduce reduce-side joins" data-tertiary="sort-merge joins" id="idm45085101851824"></a>
        Recall that the purpose of the mapper is to extract a key and value from each input record. In the
        case of <a data-type="xref" href="#fig_batch_join_example">Figure&nbsp;10-2</a>, this key would be the user ID: one set of mappers would go over
        the activity events (extracting the user ID as the key and the activity event as the value), while
        another set of mappers would go over the user database (extracting the user ID as the key and the
        user’s date of birth as the value). This process is illustrated in <a data-type="xref" href="#fig_batch_join_reduce">Figure&nbsp;10-3</a>.</p>
        
        <figure><div id="fig_batch_join_reduce" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_1003.png" alt="ddia 1003" width="2880" height="1308">
        <h6><span class="label">Figure 10-3. </span>A reduce-side sort-merge join on user ID. If the input datasets are partitioned into multiple files, each could be processed with multiple mappers in parallel.</h6>
        </div></figure>
        
        <p><a data-type="indexterm" data-primary="secondary sorts" id="idm45085101846048"></a>
        When the MapReduce framework partitions the mapper output by key and then sorts the key-value
        pairs, the effect is that all the activity events and the user record with the same user ID become
        adjacent to each other in the reducer input. The MapReduce job can even arrange the records to be
        sorted such that the reducer always sees the record from the user database first, followed by the
        activity events in timestamp order—this technique is known as a <em>secondary sort</em>
        [<a data-type="noteref" href="ch10.html#White2015vl">26</a>].</p>
        
        <p>The reducer can then perform the actual join logic easily: the reducer function is called once for
        every user ID, and thanks to the secondary sort, the first value is expected to be the date-of-birth
        record from the user database. The reducer stores the date of birth in a local variable and then
        iterates over the activity events with the same user ID, outputting pairs of <em>viewed-url</em> and
        <em>viewer-age-in-years</em>. Subsequent MapReduce jobs could then calculate the distribution of viewer ages
        for each URL, and cluster by age group.</p>
        
        <p>Since the reducer processes all of the records for a particular user ID in one go, it only needs to
        keep one user record in memory at any one time, and it never needs to make any requests over the
        network. This algorithm is known as a <em>sort-merge join</em>, since mapper output is sorted by key, and
        the reducers then merge together the sorted lists of records from both sides of the join.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Bringing related data together in the same place"><div class="sect3" id="idm45085101840320">
        <h3>Bringing related data together in the same place</h3>
        
        <p><a data-type="indexterm" data-primary="single-threaded execution" data-secondary="in batch processing" id="idm45085101839152"></a>
        In a sort-merge join, the mappers and the sorting process make sure that all the necessary data to
        perform the join operation for a particular user ID is brought together in the same place: a single
        call to the reducer. Having lined up all the required data in advance, the reducer can be a fairly
        simple, single-threaded piece of code that can churn through records with high throughput and low
        memory overhead.</p>
        
        <p>One way of looking at this architecture is that mappers “send messages” to the reducers. When a
        mapper emits a key-value pair, the key acts like the destination address to which the value should be
        delivered. Even though the key is just an arbitrary string (not an actual network address like an
        IP address and port number), it behaves like an address: all key-value pairs with the same key will be
        delivered to the same destination (a call to the reducer).</p>
        
        <p><a data-type="indexterm" data-primary="batch processing" data-secondary="fault tolerance" id="idm45085101836496"></a>
        <a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="fault tolerance" id="idm45085101835392"></a>
        <a data-type="indexterm" data-primary="fault tolerance" data-secondary="in batch processing" id="idm45085101834320"></a>
        Using the MapReduce programming model has separated the physical network communication aspects of
        the computation (getting the data to the right machine) from the application logic (processing the
        data once you have it). This separation contrasts with the typical use of databases, where a request to
        fetch data from a database often occurs somewhere deep inside a piece of application code
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kleppmann2012ts-marker" href="ch10.html#Kleppmann2012ts">36</a>].
        Since MapReduce handles all network communication, it also shields the application code from having
        to worry about partial failures, such as the crash of another node: MapReduce transparently retries
        failed tasks without affecting the application logic.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="GROUP BY"><div class="sect3" id="sec_batch_grouping">
        <h3>GROUP BY</h3>
        
        <p><a data-type="indexterm" data-primary="aggregation" data-secondary="in batch processes" id="idm45085101828400"></a>
        <a data-type="indexterm" data-primary="grouping records in MapReduce" id="idm45085101827296"></a>
        <a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="reduce-side processing" data-tertiary="grouping records by same key" id="idm45085101826400"></a>
        <a data-type="indexterm" data-primary="GROUP BY clause (SQL)" id="idm45085101824992"></a>
        Besides joins, another common use of the “bringing related data to the same place” pattern is
        grouping records by some key (as in the <code>GROUP BY</code> clause in SQL). All records with the same key
        form a group, and the next step is often to perform some kind of aggregation within each group—for
        example:</p>
        
        <ul>
        <li>
        <p>Counting the number of records in each group (like in our example of counting page views,
        which you would express as a <code>COUNT(*)</code> aggregation in SQL)</p>
        </li>
        <li>
        <p>Adding up the values in one particular field (<code>SUM(fieldname)</code> in SQL)</p>
        </li>
        <li>
        <p>Picking the top <em>k</em> records according to some ranking function</p>
        </li>
        </ul>
        
        <p>The simplest way of implementing such a grouping operation with MapReduce is to set up the mappers
        so that the key-value pairs they produce use the desired grouping key. The partitioning and sorting
        process then brings together all the records with the same key in the same reducer. Thus, grouping
        and joining look quite similar when implemented on top of MapReduce.</p>
        
        <p><a data-type="indexterm" data-primary="sessionization" id="idm45085101817712"></a>
        Another common use for grouping is collating all the activity events for a particular user session,
        in order to find out the sequence of actions that the user took—a process called <em>sessionization</em>
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Grover2015tl-marker" href="ch10.html#Grover2015tl">37</a>].
        For example, such analysis could be used to work out whether users who were shown a new version of
        your website are more likely to make a purchase than those who were shown the old version (A/B
        testing), or to calculate whether some marketing activity is worthwhile.</p>
        
        <p>If you have multiple web servers handling user requests, the activity events for a particular user
        are most likely scattered across various different servers’ log files. You can implement
        sessionization by using a session cookie, user ID, or similar identifier as the grouping key and
        bringing all the activity events for a particular user together in one place, while distributing
        different users’ events across different partitions.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Handling skew"><div class="sect3" id="sec_batch_skew">
        <h3>Handling skew</h3>
        
        <p><a data-type="indexterm" data-primary="joins" data-secondary="MapReduce reduce-side joins" data-tertiary="handling skew" id="idm45085101811472"></a>
        <a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="reduce-side processing" data-tertiary="handling skew" id="idm45085101809856"></a>
        <a data-type="indexterm" data-primary="skew" data-secondary="unbalanced workload" data-tertiary="in batch processing" id="idm45085101808464"></a>
        <a data-type="indexterm" data-primary="grouping records in MapReduce" data-secondary="handling skew" id="idm45085101807088"></a>
        <a data-type="indexterm" data-primary="hot spots" data-secondary="in batch processing" id="idm45085101805968"></a>
        The pattern of “bringing all records with the same key to the same place” breaks down if there is a
        very large amount of data related to a single key. For example, in a social network, most users
        might be connected to a few hundred people, but a small number of celebrities may have many
        millions of followers. Such disproportionately active database records are known as <em>linchpin objects</em>
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Ajoux2015wh_ch10-marker" href="ch10.html#Ajoux2015wh_ch10">38</a>] or <em>hot keys</em>.</p>
        
        <p>Collecting all activity related to a celebrity (e.g., replies to something they posted) in a single
        reducer can lead to significant <em>skew</em> (also known as <em>hot spots</em>)—that is, one
        reducer that must process significantly more records than the others (see
        <a data-type="xref" href="ch06.html#sec_partitioning_skew">“Skewed Workloads and Relieving Hot Spots”</a>). Since a MapReduce job is only complete when all of its mappers and
        reducers have completed, any subsequent jobs must wait for the slowest reducer to complete before
        they can start.</p>
        
        <p><a data-type="indexterm" data-primary="Pig (dataflow language)" data-secondary="skewed joins" id="idm45085101798672"></a>
        If a join input has hot keys, there are a few algorithms you can use to compensate. For example, the
        <em>skewed join</em> method in Pig first runs a sampling job to determine which keys are hot
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Manjunath2009bh-marker" href="ch10.html#Manjunath2009bh">39</a>].
        When performing the actual join, the mappers send any records relating to a hot key
        to one of several reducers, chosen at random (in contrast to conventional MapReduce, which chooses a
        reducer deterministically based on a hash of the key). For the other input to the join, records
        relating to the hot key need to be replicated to <em>all</em> reducers handling that key
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="DeWitt1992ws-marker" href="ch10.html#DeWitt1992ws">40</a>].</p>
        
        <p><a data-type="indexterm" data-primary="Crunch (batch processing)" data-secondary="sharded joins" id="idm45085101791424"></a>
        This technique spreads the work of handling the hot key over several reducers, which allows it to be
        parallelized better, at the cost of having to replicate the other join input to multiple reducers.
        The <em>sharded join</em> method in Crunch is similar, but requires the hot keys to be specified explicitly
        rather than using a sampling job. This technique is also very similar to one we discussed in
        <a data-type="xref" href="ch06.html#sec_partitioning_skew">“Skewed Workloads and Relieving Hot Spots”</a>, using randomization to alleviate hot spots in a partitioned database.</p>
        
        <p><a data-type="indexterm" data-primary="Hive (query engine)" data-secondary="skewed joins" id="idm45085101788336"></a>
        Hive’s skewed join optimization takes an alternative approach. It requires hot keys to be specified
        explicitly in the table metadata, and it stores records related to those keys in separate files from
        the rest. When performing a join on that table, it uses a map-side join (see the next section) for
        the hot keys.</p>
        
        <p>When grouping records by a hot key and aggregating them, you can perform the grouping in two
        stages. The first MapReduce stage sends records to a random reducer, so that each reducer performs
        the grouping on a subset of records for the hot key and outputs a more compact aggregated value
        per key. The second MapReduce job then combines the values from all of the first-stage reducers into
        a single value per key.
        <a data-type="indexterm" data-primary="joins" data-secondary="MapReduce reduce-side joins" data-startref="ix_joinsMRreduce" id="idm45085101786080"></a>
        <a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="reduce-side processing" data-startref="ix_madredredjoins" id="idm45085101784688"></a></p>
        </div></section>
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Map-Side Joins"><div class="sect2" id="sec_batch_map_joins">
        <h2>Map-Side Joins</h2>
        
        <p><a data-type="indexterm" data-primary="joins" data-secondary="MapReduce map-side joins" id="ix_joinsMRmap"></a>
        <a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="map-side processing" id="ix_mapredmapjoin"></a>
        The join algorithms described in the last section perform the actual join logic in the reducers, and
        are hence known as <em>reduce-side joins</em>. The mappers take the role of preparing the input data:
        extracting the key and value from each input record, assigning the key-value pairs to a reducer
        partition, and sorting by key.</p>
        
        <p>The reduce-side approach has the advantage that you do not need to make any assumptions about the
        input data: whatever its properties and structure, the mappers can prepare the data to be ready for
        joining. However, the downside is that all that sorting, copying to reducers, and merging of reducer
        inputs can be quite expensive. Depending on the available memory buffers, data may be written to
        disk several times as it passes through the stages of MapReduce
        [<a data-type="noteref" href="ch10.html#Grover2015tl">37</a>].</p>
        
        <p>On the other hand, if you <em>can</em> make certain assumptions about your input data, it is possible to
        make joins faster by using a so-called <em>map-side join</em>. This approach uses a cut-down MapReduce job
        in which there are no reducers and no sorting. Instead, each mapper simply reads one input file
        block from the distributed filesystem and writes one output file to the filesystem—that is all.</p>
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Broadcast hash joins"><div class="sect3" id="idm45085101774880">
        <h3>Broadcast hash joins</h3>
        
        <p><a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="map-side processing" data-tertiary="broadcast hash joins" id="idm45085101773504"></a>
        <a data-type="indexterm" data-primary="joins" data-secondary="MapReduce map-side joins" data-tertiary="broadcast hash joins" id="idm45085101772112"></a>
        The simplest way of performing a map-side join applies in the case where a large dataset is joined
        with a small dataset. In particular, the small dataset needs to be small enough that it can be
        loaded entirely into memory in each of the mappers.</p>
        
        <p><a data-type="indexterm" data-primary="hash indexes" data-secondary="broadcast hash joins" id="idm45085101770208"></a>
        For example, imagine in the case of <a data-type="xref" href="#fig_batch_join_example">Figure&nbsp;10-2</a> that the user database is small
        enough to fit in memory. In this case, when a mapper starts up, it can first read the user
        database from the distributed filesystem into an in-memory hash table. Once this is done, the mapper
        can scan over the user activity events and simply look up the user ID for each event in the hash
        table.<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm45085101767952-marker" href="ch10.html#idm45085101767952">vi</a></sup></p>
        
        <p>There can still be several map tasks: one for each file block of the large input to the join (in
        the example of <a data-type="xref" href="#fig_batch_join_example">Figure&nbsp;10-2</a>, the activity events are the large input). Each of these
        mappers loads the small input entirely into memory.</p>
        
        <p><a data-type="indexterm" data-primary="broadcast hash joins" id="idm45085101765104"></a>
        <a data-type="indexterm" data-primary="Cascading (batch processing)" data-secondary="hash joins" id="idm45085101764048"></a>
        <a data-type="indexterm" data-primary="Crunch (batch processing)" data-secondary="hash joins" id="idm45085101762928"></a>
        <a data-type="indexterm" data-primary="Hive (query engine)" data-secondary="map-side joins" id="idm45085101761808"></a>
        <a data-type="indexterm" data-primary="Pig (dataflow language)" data-secondary="replicated joins" id="idm45085101760704"></a>
        <a data-type="indexterm" data-primary="Impala (query engine)" data-secondary="hash joins" id="idm45085101759600"></a>
        This simple but effective algorithm is called a <em>broadcast hash join</em>: the word <em>broadcast</em> reflects
        the fact that each mapper for a partition of the large input reads the entirety of the small input
        (so the small input is effectively “broadcast” to all partitions of the large input), and the word
        <em>hash</em> reflects its use of a hash table. This join method is supported by Pig (under the name
        “replicated join”), Hive (“MapJoin”), Cascading, and Crunch. It is also used in data warehouse query
        engines such as Impala
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kornacker2015uv_ch10-marker" href="ch10.html#Kornacker2015uv_ch10">41</a>].</p>
        
        <p>Instead of loading the small join input into an in-memory hash table, an alternative is to store the
        small join input in a read-only index on the local disk
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Monsch2015vb-marker" href="ch10.html#Monsch2015vb">42</a>].
        The frequently used parts of this index will remain in the operating system’s page cache, so this
        approach can provide random-access lookups almost as fast as an in-memory hash table, but without
        actually requiring the dataset to fit in memory.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Partitioned hash joins"><div class="sect3" id="idm45085101751264">
        <h3>Partitioned hash joins</h3>
        
        <p><a data-type="indexterm" data-primary="joins" data-secondary="MapReduce map-side joins" data-tertiary="partitioned hash joins" id="idm45085101749952"></a>
        <a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="map-side processing" data-tertiary="partitioned hash joins" id="idm45085101748560"></a>
        <a data-type="indexterm" data-primary="hash indexes" data-secondary="partitioned hash joins" id="idm45085101747168"></a>
        If the inputs to the map-side join are partitioned in the same way, then the hash join approach can
        be applied to each partition independently. In the case of <a data-type="xref" href="#fig_batch_join_example">Figure&nbsp;10-2</a>, you might
        arrange for the activity events and the user database to each be partitioned based on the last
        decimal digit of the user ID (so there are 10 partitions on either side). For example, mapper 3
        first loads all users with an ID ending in 3 into a hash table, and then scans over all the activity
        events for each user whose ID ends in 3.</p>
        
        <p>If the partitioning is done correctly, you can be sure that all the records you might want to join
        are located in the same numbered partition, and so it is sufficient for each mapper to only read one
        partition from each of the input datasets. This has the advantage that each mapper can load a
        smaller amount of data into its hash table.</p>
        
        <p>This approach only works if both of the join’s inputs have the same number of partitions, with
        records assigned to partitions based on the same key and the same hash function. If the inputs are
        generated by prior MapReduce jobs that already perform this grouping, then this can be a reasonable
        assumption to make.</p>
        
        <p>Partitioned hash joins are known as <em>bucketed map joins</em> in Hive
        [<a data-type="noteref" href="ch10.html#Grover2015tl">37</a>].</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Map-side merge joins"><div class="sect3" id="idm45085101741408">
        <h3>Map-side merge joins</h3>
        
        <p><a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="map-side processing" data-tertiary="merge joins" id="idm45085101740208"></a>
        <a data-type="indexterm" data-primary="joins" data-secondary="MapReduce map-side joins" data-tertiary="merge joins" id="idm45085101738816"></a>
        <a data-type="indexterm" data-primary="merge joins, MapReduce map-side" id="idm45085101737424"></a>
        Another variant of a map-side join applies if the input datasets are not only partitioned in the
        same way, but also <em>sorted</em> based on the same key. In this case, it does not matter whether the
        inputs are small enough to fit in memory, because a mapper can perform the same merging operation
        that would normally be done by a reducer: reading both input files sequentially, in order of
        ascending keys, and matching records with the same key.</p>
        
        <p>If a map-side merge join is possible, it probably means that prior MapReduce jobs brought the input
        datasets into this partitioned and sorted form in the first place. In principle, this join could
        have been performed in the reduce stage of the prior job. However, it may still be appropriate to
        perform the merge join in a separate map-only job, for example if the partitioned and sorted
        datasets are also needed for other purposes besides this particular join.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="MapReduce workflows with map-side joins"><div class="sect3" id="idm45085101734576">
        <h3>MapReduce workflows with map-side joins</h3>
        
        <p><a data-type="indexterm" data-primary="workflows (MapReduce)" data-secondary="with map-side joins" id="idm45085101733408"></a>
        When the output of a MapReduce join is consumed by downstream jobs, the choice of map-side or
        reduce-side join affects the structure of the output. The output of a reduce-side join is
        partitioned and sorted by the join key, whereas the output of a map-side join is partitioned and
        sorted in the same way as the large input (since one map task is started for each file block of the
        join’s large input, regardless of whether a partitioned or broadcast join is used).</p>
        
        <p>As discussed, map-side joins also make more assumptions about the size, sorting, and partitioning of
        their input datasets. Knowing about the physical layout of datasets in the distributed
        filesystem becomes important when optimizing join strategies: it is not sufficient to just know the
        encoding format and the name of the directory in which the data is stored; you must also know the number of
        partitions and the keys by which the data is partitioned and sorted.</p>
        
        <p><a data-type="indexterm" data-primary="Hive (query engine)" data-secondary="HCatalog and metastore" id="idm45085101730672"></a>
        <a data-type="indexterm" data-primary="HDFS (Hadoop Distributed File System)" data-secondary="metadata about datasets" id="idm45085101729344"></a>
        In the Hadoop ecosystem, this kind of metadata about the partitioning of datasets is often
        maintained in HCatalog and the Hive metastore [<a data-type="noteref" href="ch10.html#Grover2015tl">37</a>].
        <a data-type="indexterm" data-primary="joins" data-secondary="MapReduce map-side joins" data-startref="ix_joinsMRmap" id="idm45085101727264"></a>
        <a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="map-side processing" data-startref="ix_mapredmapjoin" id="idm45085101725872"></a>
        <a data-type="indexterm" data-primary="Hadoop (data infrastructure)" data-secondary="join algorithms" data-startref="ix_hadoopjoins" id="idm45085101724480"></a></p>
        </div></section>
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="The Output of Batch Workflows"><div class="sect2" id="sec_batch_output">
        <h2>The Output of Batch Workflows</h2>
        
        <p><a data-type="indexterm" data-primary="batch processing" data-secondary="outputs" id="ix_batchoutputs"></a>
        <a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="output of batch workflows" id="ix_madredout"></a>
        <a data-type="indexterm" data-primary="workflows (MapReduce)" data-secondary="outputs" id="ix_workout"></a>
        We have talked a lot about the various algorithms for implementing workflows of MapReduce jobs, but
        we neglected an important question: what is the result of all of that processing, once it is done?
        Why are we running all these jobs in the first place?</p>
        
        <p><a data-type="indexterm" data-primary="analytics" data-secondary="relation to batch processing" id="idm45085101716832"></a>
        <a data-type="indexterm" data-primary="transactions" data-secondary="OLTP versus analytics queries" id="idm45085101715712"></a>
        <a data-type="indexterm" data-primary="OLTP (online transaction processing)" data-secondary="analytics queries versus" id="idm45085101714592"></a>
        In the case of database queries, we distinguished transaction processing (OLTP) purposes from
        analytic purposes (see <a data-type="xref" href="ch03.html#sec_storage_analytics">“Transaction Processing or Analytics?”</a>). We saw that OLTP queries generally look up a
        small number of records by key, using indexes, in order to present them to a user (for example, on a
        web page). On the other hand, analytic queries often scan over a large number of records, performing
        groupings and aggregations, and the output often has the form of a report: a graph showing the
        change in a metric over time, or the top 10 items according to some ranking, or a breakdown of some
        quantity into subcategories. The consumer of such a report is often an analyst or a manager who
        needs to make business decisions.</p>
        
        <p>Where does batch processing fit in? It is not transaction processing, nor is it analytics. It is
        closer to analytics, in that a batch process typically scans over large portions of an input
        dataset. However, a workflow of MapReduce jobs is not the same as a SQL query used for analytic
        purposes (see <a data-type="xref" href="#sec_batch_mr_vs_db">“Comparing Hadoop to Distributed Databases”</a>). The output of a batch process is often not a report, but some
        <span class="keep-together">other kind</span> of structure.</p>
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Building search indexes"><div class="sect3" id="idm45085101709536">
        <h3>Building search indexes</h3>
        
        <p><a data-type="indexterm" data-primary="searches" data-secondary="building search indexes in batch processes" id="idm45085101708128"></a>
        <a data-type="indexterm" data-primary="indexes" data-secondary="building in batch processes" id="idm45085101706592"></a>
        <a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="output of batch workflows" data-tertiary="building search indexes" id="idm45085101705472"></a>
        <a data-type="indexterm" data-primary="batch processing" data-secondary="outputs" data-tertiary="search indexes" id="idm45085101704064"></a>
        <a data-type="indexterm" data-primary="workflows (MapReduce)" data-secondary="outputs" data-tertiary="search indexes" id="idm45085101702688"></a>
        <a data-type="indexterm" data-primary="Google" data-secondary="MapReduce (batch processing)" data-tertiary="building search indexes" id="idm45085101701312"></a>
        <a data-type="indexterm" data-primary="Lucene (storage engine)" data-secondary="building indexes in batch processes" id="idm45085101699920"></a>
        <a data-type="indexterm" data-primary="Solr (search server)" data-secondary="building indexes in batch processes" id="idm45085101698800"></a>
        <a data-type="indexterm" data-primary="full-text search" data-secondary="building search indexes" id="idm45085101697680"></a>
        Google’s original use of MapReduce was to build indexes for its search engine, which was
        implemented as a workflow of 5 to 10 MapReduce jobs
        [<a data-type="noteref" href="ch10.html#Dean2004ua_ch10">1</a>].
        Although Google later moved away from using MapReduce for this purpose
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Peng2010ub-marker" href="ch10.html#Peng2010ub">43</a>],
        it helps to understand MapReduce if you look at it through the lens of building a search index.
        (Even today, Hadoop MapReduce remains a good way of building indexes for Lucene/Solr
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="ClouderaSearch-marker" href="ch10.html#ClouderaSearch">44</a>].)</p>
        
        <p>We saw briefly in <a data-type="xref" href="ch03.html#sec_storage_full_text">“Full-text search and fuzzy indexes”</a> how a full-text search index such as Lucene works: it is
        a file (the term dictionary) in which you can efficiently look up a particular keyword and find the
        list of all the document IDs containing that keyword (the postings list). This is a very simplified
        view of a search index—in reality it requires various additional data, in order to rank search
        results by relevance, correct misspellings, resolve synonyms, and so on—but the principle holds.</p>
        
        <p><a data-type="indexterm" data-primary="document-partitioned indexes" id="idm45085101689632"></a>
        If you need to perform a full-text search over a fixed set of documents, then a batch process is a
        very effective way of building the indexes: the mappers partition the set of documents as needed,
        each reducer builds the index for its partition, and the index files are written to the distributed
        filesystem. Building such document-partitioned indexes (see <a data-type="xref" href="ch06.html#sec_partitioning_secondary_indexes">“Partitioning and Secondary Indexes”</a>)
        parallelizes very well. Since querying a search index by keyword is a read-only operation, these
        index files are immutable once they have been created.</p>
        
        <p>If the indexed set of documents changes, one option is to periodically rerun the entire indexing
        workflow for the entire set of documents, and replace the previous index files wholesale with the
        new index files when it is done. This approach can be computationally expensive if only a small number of
        documents have changed, but it has the advantage that the indexing process is very easy to reason
        about: documents in, indexes out.</p>
        
        <p>Alternatively, it is possible to build indexes incrementally. As discussed in <a data-type="xref" href="ch03.html#ch_storage">Chapter&nbsp;3</a>, if you
        want to add, remove, or update documents in an index, Lucene writes out new segment
        files and asynchronously merges and compacts segment files in the background. We will see more
        on such incremental processing in <a data-type="xref" href="ch11.html#ch_stream">Chapter&nbsp;11</a>.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Key-value stores as batch process output"><div class="sect3" id="idm45085101684208">
        <h3>Key-value stores as batch process output</h3>
        
        <p><a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="output of batch workflows" data-tertiary="key-value stores" id="idm45085101682800"></a>
        <a data-type="indexterm" data-primary="batch processing" data-secondary="outputs" data-tertiary="key-value stores" id="idm45085101681264"></a>
        <a data-type="indexterm" data-primary="workflows (MapReduce)" data-secondary="outputs" data-tertiary="key-value stores" id="idm45085101679888"></a>
        <a data-type="indexterm" data-primary="key-value stores" data-secondary="as batch process output" id="idm45085101678512"></a>
        <a data-type="indexterm" data-primary="recommendation engines" data-secondary="batch process outputs" id="idm45085101677408"></a>
        Search indexes are just one example of the possible outputs of a batch processing workflow. Another
        common use for batch processing is to build machine learning systems such as classifiers (e.g., spam
        filters, anomaly detection, image recognition) and recommendation systems (e.g., people you may know,
        products you may be interested in, or related searches
        [<a data-type="noteref" href="ch10.html#Sumbaly2013eh">29</a>]).</p>
        
        <p><a data-type="indexterm" data-primary="databases" data-secondary="output from batch workflows" id="idm45085101674992"></a>
        The output of those batch jobs is often some kind of database: for example, a database that can be
        queried by user ID to obtain suggested friends for that user, or a database that can be queried by
        product ID to get a list of related products
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Wu2014tm-marker" href="ch10.html#Wu2014tm">45</a>].</p>
        
        <p>These databases need to be queried from the web application that handles user requests, which is
        usually separate from the Hadoop infrastructure. So how does the output from the batch process get
        back into a database where the web application can query it?</p>
        
        <p><a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="accessing external services within job" id="idm45085101670224"></a>
        The most obvious choice might be to use the client library for your favorite database directly
        within a mapper or reducer, and to write from the batch job directly to the database server, one
        record at a time. This will work (assuming your firewall rules allow direct access from your Hadoop
        environment to your production databases), but it is a bad idea for several reasons:</p>
        
        <ul>
        <li>
        <p>As discussed previously in the context of joins, making a network request for every single record
        is orders of magnitude slower than the normal throughput of a batch task. Even if the client
        library supports batching, performance is likely to be poor.</p>
        </li>
        <li>
        <p>MapReduce jobs often run many tasks in parallel. If all the mappers or reducers concurrently write
        to the same output database, with a rate expected of a batch process, that database can easily be
        overwhelmed, and its performance for queries is likely to suffer. This can in turn cause operational
        problems in other parts of the system [<a data-type="noteref" href="ch10.html#Kreps2014wm_ch10">35</a>].</p>
        </li>
        <li>
        <p>Normally, MapReduce provides a clean all-or-nothing guarantee for job output: if a job succeeds,
        the result is the output of running every task exactly once, even if some tasks failed and had to
        be retried along the way; if the entire job fails, no output is produced. However, writing to an
        external system from inside a job produces externally visible side effects that cannot be hidden
        in this way. Thus, you have to worry about the results from partially completed jobs being visible
        to other systems, and the complexities of Hadoop task attempts and speculative execution.</p>
        </li>
        </ul>
        
        <p><a data-type="indexterm" data-primary="Voldemort (database)" data-secondary="building read-only stores in batch processes" id="idm45085101663344"></a>
        <a data-type="indexterm" data-primary="Terrapin (database)" id="idm45085101661920"></a>
        <a data-type="indexterm" data-primary="ElephantDB (database)" id="idm45085101661088"></a>
        <a data-type="indexterm" data-primary="HBase (database)" data-secondary="bulk loading" id="idm45085101660256"></a>
        A much better solution is to build a brand-new database <em>inside</em> the batch job and write it as
        files to the job’s output directory in the distributed filesystem, just like the search indexes in
        the last section. Those data files are then immutable once written, and can be loaded in bulk into
        servers that handle read-only queries. Various key-value stores support building database files in
        MapReduce jobs, including Voldemort
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Sumbaly2012wi-marker" href="ch10.html#Sumbaly2012wi">46</a>],
        Terrapin [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Sharma2015tp-marker" href="ch10.html#Sharma2015tp">47</a>],
        ElephantDB [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="ElephantDB-marker" href="ch10.html#ElephantDB">48</a>],
        and HBase bulk loading [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Cryans2013wo-marker" href="ch10.html#Cryans2013wo">49</a>].</p>
        
        <p>Building these database files is a good use of MapReduce: using a mapper to extract a key and then
        sorting by that key is already a lot of the work required to build an index. Since most of these
        key-value stores are read-only (the files can only be written once by a batch job and are then
        immutable), the data structures are quite simple. For example, they do not require a WAL (see
        <a data-type="xref" href="ch03.html#sec_storage_btree_wal">“Making B-trees reliable”</a>).</p>
        
        <p>When loading data into Voldemort, the server continues serving requests to the old data files while
        the new data files are copied from the distributed filesystem to the server’s local disk. Once the
        copying is complete, the server atomically switches over to querying the new files. If anything goes
        wrong in this process, it can easily switch back to the old files again, since they are still there
        and immutable [<a data-type="noteref" href="ch10.html#Sumbaly2012wi">46</a>].
        <a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="output of batch workflows" data-startref="ix_madredout" id="idm45085101646080"></a>
        <a data-type="indexterm" data-primary="batch processing" data-secondary="MapReduce and distributed filesystems" data-startref="ix_batchMRdist" id="idm45085101644672"></a>
        <a data-type="indexterm" data-primary="batch processing" data-secondary="outputs" data-startref="ix_batchoutputs" id="idm45085101643280"></a></p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Philosophy of batch process outputs"><div class="sect3" id="sec_batch_philosophy">
        <h3>Philosophy of batch process outputs</h3>
        
        <p><a data-type="indexterm" data-primary="batch processing" data-secondary="comparison to Unix" id="ix_batchphil"></a>
        <a data-type="indexterm" data-primary="Hadoop (data infrastructure)" data-secondary="comparison to Unix" id="ix_hadoopunixphil"></a>
        <a data-type="indexterm" data-primary="Unix philosophy" data-secondary="comparison to Hadoop" id="ix_unixphilhadoop"></a>
        <a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="comparison to Unix" id="ix_mapredunixphil"></a>
        The Unix philosophy that we discussed earlier in this chapter (<a data-type="xref" href="#sec_batch_unix_philosophy">“The Unix Philosophy”</a>)
        encourages experimentation by being very explicit about dataflow: a program reads its input and
        writes its output. In the process, the input is left unchanged, any previous output is completely
        replaced with the new output, and there are no other side effects. This means that you can rerun a
        command as often as you like, tweaking or debugging it, without messing up the state of your system.</p>
        
        <p>The handling of output from MapReduce jobs follows the same philosophy. By treating inputs as
        immutable and avoiding side effects (such as writing to external databases), batch jobs not only
        achieve good performance but also become much easier to maintain:</p>
        
        <ul>
        <li>
        <p><a data-type="indexterm" data-primary="corruption of data" data-secondary="recovering from" id="idm45085101631680"></a>
        If you introduce a bug into the code and the output is wrong or corrupted, you can simply roll
        back to a previous version of the code and rerun the job, and the output will be correct again. Or,
        even simpler, you can keep the old output in a different directory and simply switch back to it.
        Databases with read-write transactions do not have this property: if you deploy buggy code that
        writes bad data to the database, then rolling back the code will do nothing to fix the data in the
        database.
        <a data-type="indexterm" data-primary="fault tolerance" data-secondary="human fault tolerance" id="idm45085101630064"></a>
        <a data-type="indexterm" data-primary="human errors" id="idm45085101628992"></a>
        (The idea of being able to recover from buggy code has been called <em>human fault tolerance</em>
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Marz2011vq-marker" href="ch10.html#Marz2011vq">50</a>].)</p>
        </li>
        <li>
        <p><a data-type="indexterm" data-primary="Agile" data-secondary="minimizing irreversibility" id="idm45085101624624"></a>
        As a consequence of this ease of rolling back, feature development can proceed more quickly than
        in an environment where mistakes could mean irreversible damage. This principle of <em>minimizing
        irreversibility</em> is beneficial for Agile software development
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Bartlett2015wv_ch10-marker" href="ch10.html#Bartlett2015wv_ch10">51</a>].</p>
        </li>
        <li>
        <p><a data-type="indexterm" data-primary="batch processing" data-secondary="fault tolerance" id="idm45085101619808"></a>
        <a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="fault tolerance" id="idm45085101618816"></a>
        <a data-type="indexterm" data-primary="fault tolerance" data-secondary="in batch processing" id="idm45085101617648"></a>
        If a map or reduce task fails, the MapReduce framework automatically re-schedules it and runs it
        again on the same input. If the failure is due to a bug in the code, it will keep crashing and
        eventually cause the job to fail after a few attempts; but if the failure is due to a transient
        issue, the fault is tolerated. This automatic retry is only safe because inputs are immutable and
        outputs from failed tasks are discarded by the MapReduce framework.</p>
        </li>
        <li>
        <p>The same set of files can be used as input for various different jobs, including monitoring jobs
        that calculate metrics and evaluate whether a job’s output has the expected characteristics (for
        example, by comparing it to the output from the previous run and measuring discrepancies).</p>
        </li>
        <li>
        <p>Like Unix tools, MapReduce jobs separate logic from wiring (configuring the input and output
        directories), which provides a separation of concerns and enables potential reuse of code: one
        team can focus on implementing a job that does one thing well, while other teams can decide where
        and when to run that job.</p>
        </li>
        </ul>
        
        <p><a data-type="indexterm" data-primary="Avro (data format)" data-secondary="use in Hadoop" id="idm45085101613376"></a>
        <a data-type="indexterm" data-primary="Avro (data format)" data-secondary="object container files" id="idm45085101611904"></a>
        <a data-type="indexterm" data-primary="Parquet (data format)" data-secondary="use in Hadoop" id="idm45085101610800"></a>
        In these areas, the design principles that worked well for Unix also seem to be working well for
        Hadoop—but Unix and Hadoop also differ in some ways. For example, because most Unix tools assume
        untyped text files, they have to do a lot of input parsing (our log analysis example at the
        beginning of the chapter used <code>{print $7}</code> to extract the URL).
        <a data-type="indexterm" data-primary="column-oriented storage" data-secondary="Parquet" id="idm45085101608976"></a> On Hadoop, some of those low-value syntactic conversions
        are eliminated by using more structured file formats: Avro (see <a data-type="xref" href="ch04.html#sec_encoding_avro">“Avro”</a>) and Parquet
        (see <a data-type="xref" href="ch03.html#sec_storage_column">“Column-Oriented Storage”</a>) are often used, as they provide efficient schema-based encoding and
        allow evolution of their schemas over time (see <a data-type="xref" href="ch04.html#ch_encoding">Chapter&nbsp;4</a>).
        <a data-type="indexterm" data-primary="batch processing" data-secondary="comparison to Unix" data-startref="ix_batchphil" id="idm45085101605136"></a>
        <a data-type="indexterm" data-primary="Hadoop (data infrastructure)" data-secondary="comparison to Unix" data-startref="ix_hadoopunixphil" id="idm45085101603760"></a>
        <a data-type="indexterm" data-primary="Unix philosophy" data-secondary="comparison to Hadoop" data-startref="ix_unixphilhadoop" id="idm45085101602416"></a>
        <a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="comparison to Unix" data-startref="ix_mapredunixphil" id="idm45085101601040"></a>
        <a data-type="indexterm" data-primary="workflows (MapReduce)" data-secondary="outputs" data-startref="ix_workout" id="idm45085101599648"></a></p>
        </div></section>
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Comparing Hadoop to Distributed Databases"><div class="sect2" id="sec_batch_mr_vs_db">
        <h2>Comparing Hadoop to Distributed Databases</h2>
        
        <p><a data-type="indexterm" data-primary="batch processing" data-secondary="comparison to MPP databases" id="ix_batchmpp"></a>
        <a data-type="indexterm" data-primary="Hadoop (data infrastructure)" data-secondary="comparison to MPP databases" id="ix_medredcomp"></a>
        <a data-type="indexterm" data-primary="massively parallel processing (MPP)" data-secondary="comparison to Hadoop" id="ix_distsyscompmr"></a>
        <a data-type="indexterm" data-primary="MPP" data-see="massively parallel processing" id="idm45085101592640"></a>
        <a data-type="indexterm" data-primary="parallel databases" data-see="massively parallel processing" id="idm45085101591520"></a>
        As we have seen, Hadoop is somewhat like a distributed version of Unix, where HDFS is the
        filesystem and MapReduce is a quirky implementation of a Unix process (which happens to always run
        the <code>sort</code> utility between the map phase and the reduce phase). We saw how you can implement various
        join and grouping operations on top of these primitives.</p>
        
        <p><a data-type="indexterm" data-primary="joins" data-secondary="parallel execution of" id="idm45085101589520"></a>
        When the MapReduce paper [<a data-type="noteref" href="ch10.html#Dean2004ua_ch10">1</a>] was
        published, it was—in some sense—not at all new. All of the processing and parallel join
        algorithms that we discussed in the last few sections had already been implemented in so-called
        <em>massively parallel processing</em> (MPP) databases more than a decade previously
        [<a data-type="noteref" href="ch10.html#Babu2013gm_ch10">3</a>, <a data-type="noteref" href="ch10.html#DeWitt1992ws">40</a>].
        For example, the Gamma database machine, Teradata, and Tandem NonStop SQL were pioneers in this area
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="DeWitt1992fn_ch10-marker" href="ch10.html#DeWitt1992fn_ch10">52</a>].</p>
        
        <p><a data-type="indexterm" data-primary="analytics" data-secondary="parallel query execution in MPP databases" id="idm45085101582112"></a>
        The biggest difference is that MPP databases focus on parallel execution of analytic SQL queries on
        a cluster of machines, while the combination of MapReduce and a distributed filesystem
        [<a data-type="noteref" href="ch10.html#Ghemawat2003dy">19</a>] provides something much more like a
        general-purpose operating system that can run arbitrary programs.</p>
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Diversity of storage"><div class="sect3" id="idm45085101579664">
        <h3>Diversity of storage</h3>
        
        <p><a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="comparison to distributed databases" data-tertiary="diversity of storage" id="idm45085101578320"></a>
        <a data-type="indexterm" data-primary="storage" data-secondary="diversity of, in MapReduce" id="idm45085101576912"></a>
        Databases require you to structure data according to a particular model (e.g., relational or
        documents), whereas files in a distributed filesystem are just byte sequences, which can be written
        using any data model and encoding. They might be collections of database records, but they can
        equally well be text, images, videos, sensor readings, sparse matrices, feature vectors, genome
        sequences, or any other kind of data.</p>
        
        <p><a data-type="indexterm" data-primary="distributed filesystems" data-secondary="indiscriminately dumping data into" id="idm45085101575104"></a>
        <a data-type="indexterm" data-primary="HDFS (Hadoop Distributed File System)" data-secondary="indiscriminately dumping data into" id="idm45085101573984"></a>
        To put it bluntly, Hadoop opened up the possibility of indiscriminately dumping data into HDFS, and
        only later figuring out how to process it further
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kreps2014qz-marker" href="ch10.html#Kreps2014qz">53</a>]. By
        contrast, MPP databases typically require careful up-front modeling of the data and query patterns
        before importing the data into the database’s proprietary storage format.</p>
        
        <p>From a purist’s point of view, it may seem that this careful modeling and import is desirable, because
        it means users of the database have better-quality data to work with. However, in practice, it
        appears that simply making data available quickly—even if it is in a quirky, difficult-to-use,
        raw format—is often more valuable than trying to decide on the ideal data model up front
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Cohen2009fv-marker" href="ch10.html#Cohen2009fv">54</a>].</p>
        
        <p><a data-type="indexterm" data-primary="data warehousing" data-secondary="comparison to data lakes" id="idm45085101566240"></a>
        <a data-type="indexterm" data-primary="data lakes" id="idm45085101565072"></a>
        The idea is similar to a data warehouse (see <a data-type="xref" href="ch03.html#sec_storage_dwh">“Data Warehousing”</a>): simply bringing data from various
        parts of a large organization together in one place is valuable, because it enables joins across
        datasets that were previously disparate. The careful schema design required by an MPP database slows
        down that centralized data collection; collecting data in its raw form, and worrying about schema
        design later, allows the data collection to be speeded up (a concept sometimes known as a “data
        lake” or “enterprise data hub” [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Terrizzano2015tk-marker" href="ch10.html#Terrizzano2015tk">55</a>]).</p>
        
        <p><a data-type="indexterm" data-primary="schema-on-read" data-secondary="in distributed filesystems" id="idm45085101560304"></a>
        Indiscriminate data dumping shifts the burden of interpreting the data: instead of forcing the
        producer of a dataset to bring it into a standardized format, the interpretation of the data becomes
        the consumer’s problem (the schema-on-read approach [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Roberts2015tl-marker" href="ch10.html#Roberts2015tl">56</a>];
        see <a data-type="xref" href="ch02.html#sec_datamodels_schema_flexibility">“Schema flexibility in the document model”</a>).
        This can be an advantage if the producer and consumers are different teams with different
        priorities. There may not even be one ideal data model, but rather different views onto the data
        that are suitable for different purposes. Simply dumping data in its raw form allows for several
        such transformations. This approach has been dubbed the <em>sushi principle</em>: “raw data is better”
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Johnson2015ua-marker" href="ch10.html#Johnson2015ua">57</a>].</p>
        
        <p><a data-type="indexterm" data-primary="data warehousing" data-secondary="ETL (extract-transform-load)" id="idm45085101552352"></a>
        <a data-type="indexterm" data-primary="ETL (extract-transform-load)" data-secondary="use of Hadoop for" id="idm45085101551264"></a>
        Thus, Hadoop has often been used for implementing ETL processes (see <a data-type="xref" href="ch03.html#sec_storage_dwh">“Data Warehousing”</a>): data from
        transaction processing systems is dumped into the distributed filesystem in some raw form, and then
        MapReduce jobs are written to clean up that data, transform it into a relational form, and import it
        into an MPP data warehouse for analytic purposes. Data modeling still happens, but it is in a
        separate step, decoupled from the data collection. This decoupling is possible because a distributed
        filesystem supports data encoded in any format.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Diversity of processing models"><div class="sect3" id="idm45085101548656">
        <h3>Diversity of processing models</h3>
        
        <p><a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="comparison to distributed databases" data-tertiary="diversity of processing models" id="idm45085101547376"></a>
        <a data-type="indexterm" data-primary="SQL (Structured Query Language)" data-secondary="advantages and limitations of" id="idm45085101545904"></a>
        <a data-type="indexterm" data-primary="Tableau (data visualization software)" id="idm45085101544768"></a>
        MPP databases are monolithic, tightly integrated pieces of software that take care of storage layout
        on disk, query planning, scheduling, and execution. Since these components can all be tuned and
        optimized for the specific needs of the database, the system as a whole can achieve very good
        performance on the types of queries for which it is designed. Moreover, the SQL query language
        allows expressive queries and elegant semantics without the need to write code, making it accessible
        to graphical tools used by business analysts (such as Tableau).</p>
        
        <p>On the other hand, not all kinds of processing can be sensibly expressed as SQL queries. For
        example, if you are building machine learning and recommendation systems, or full-text search
        indexes with relevance ranking models, or performing image analysis, you most likely need a more
        general model of data processing. These kinds of processing are often very specific to a particular
        application (e.g., feature engineering for machine learning, natural language models for machine
        translation, risk estimation functions for fraud prediction), so they inevitably require writing
        code, not just queries.</p>
        
        <p><a data-type="indexterm" data-primary="SQL (Structured Query Language)" data-secondary="query execution on Hadoop" id="idm45085101542064"></a>
        MapReduce gave engineers the ability to easily run their own code over large datasets. If you have
        HDFS and MapReduce, you <em>can</em> build a SQL query execution engine on top of it, and indeed this is
        what the Hive project did [<a data-type="noteref" href="ch10.html#Thusoo2010hp">31</a>]. However, you
        can also write many other forms of batch processes that do not lend themselves to being expressed as
        a SQL query.</p>
        
        <p><a data-type="indexterm" data-primary="YARN (job scheduler)" id="idm45085101539056"></a>
        Subsequently, people found that MapReduce was too limiting and performed too badly for some types of
        processing, so various other processing models were developed on top of Hadoop (we will see some of
        them in <a data-type="xref" href="#sec_batch_beyond_mr">“Beyond MapReduce”</a>). Having two processing models, SQL and MapReduce, was not enough:
        even <span class="keep-together">more different</span> models were needed! And due to the
        openness of the Hadoop platform, it was feasible to implement a whole range of approaches, which
        would not have been possible within the confines of a monolithic MPP database
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Vavilapalli2013eu-marker" href="ch10.html#Vavilapalli2013eu">58</a>].</p>
        
        <p><a data-type="indexterm" data-primary="Hadoop (data infrastructure)" data-secondary="diverse processing models in ecosystem" id="idm45085101533120"></a>
        Crucially, those various processing models can all be run on a single shared-use cluster of
        machines, all accessing the same files on the distributed filesystem. In the Hadoop approach, there
        is no need to import the data into several different specialized systems for different kinds of
        processing: the system is flexible enough to support a diverse set of workloads within the same
        cluster. Not having to move data around makes it a lot easier to derive value from the data, and a
        lot easier to experiment with new processing models.</p>
        
        <p><a data-type="indexterm" data-primary="HBase (database)" data-secondary="use of HDFS" id="idm45085101531120"></a>
        <a data-type="indexterm" data-primary="Impala (query engine)" data-secondary="use of HDFS" id="idm45085101529792"></a>
        <a data-type="indexterm" data-primary="distributed filesystems" data-secondary="decoupling from query engines" id="idm45085101528688"></a>
        <a data-type="indexterm" data-primary="HDFS (Hadoop Distributed File System)" data-secondary="decoupling from query engines" id="idm45085101527616"></a>
        The Hadoop ecosystem includes both random-access OLTP databases such as HBase (see
        <a data-type="xref" href="ch03.html#sec_storage_lsm_trees">“SSTables and LSM-Trees”</a>) and MPP-style analytic databases such as Impala
        [<a data-type="noteref" href="ch10.html#Kornacker2015uv_ch10">41</a>]. Neither HBase nor Impala uses
        MapReduce, but both use HDFS for storage. They are very different approaches to accessing and
        processing data, but they can nevertheless coexist and be integrated in the same system.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Designing for frequent faults"><div class="sect3" id="idm45085101524656">
        <h3>Designing for frequent faults</h3>
        
        <p><a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="comparison to distributed databases" data-tertiary="designing for frequent faults" id="idm45085101523184"></a>
        <a data-type="indexterm" data-primary="faults" data-secondary="in batch processing versus distributed databases" id="idm45085101521808"></a>
        When comparing MapReduce to MPP databases, two more differences in design approach stand out: the
        handling of faults and the use of memory and disk. Batch processes are less sensitive to faults
        than online systems, because they do not immediately affect users if they fail and they can always
        be run again.</p>
        
        <p>If a node crashes while a query is executing, most MPP databases abort the entire query, and
        either let the user resubmit the query or automatically run it again
        [<a data-type="noteref" href="ch10.html#Babu2013gm_ch10">3</a>]. As queries normally run for a few
        seconds or a few minutes at most, this way of handling errors is acceptable, since the cost of
        retrying is not too great.  MPP databases also prefer to keep as much data as possible in memory
        (e.g., using hash joins) to avoid the cost of reading from disk.</p>
        
        <p>On the other hand, MapReduce can tolerate the failure of a map or reduce task without it affecting the
        job as a whole by retrying work at the granularity of an individual task. It is also very eager to
        write data to disk, partly for fault tolerance, and partly on the assumption that the dataset will
        be too big to fit in memory anyway.</p>
        
        <p>The MapReduce approach is more appropriate for larger jobs: jobs that process so much data and run
        for such a long time that they are likely to experience at least one task failure along the way. In
        that case, rerunning the entire job due to a single task failure would be wasteful. Even if
        recovery at the granularity of an individual task introduces overheads that make fault-free
        processing slower, it can still be a reasonable trade-off if the rate of task failures is high
        enough.</p>
        
        <p>But how realistic are these assumptions? In most clusters, machine failures do occur, but they are
        not very frequent—probably rare enough that most jobs will not experience a machine failure. Is
        it really worth incurring significant overheads for the sake of fault tolerance?</p>
        
        <p><a data-type="indexterm" data-primary="preemption" data-secondary="of datacenter resources" id="idm45085101515952"></a>
        To understand the reasons for MapReduce’s sparing use of memory and task-level recovery, it is
        helpful to look at the environment for which MapReduce was originally designed. Google has mixed-use
        datacenters, in which online production services and offline batch jobs run on the same machines.
        Every task has a resource allocation (CPU cores, RAM, disk space, etc.) that is enforced using
        containers. Every task also has a priority, and if a higher-priority task needs more resources,
        lower-priority tasks on the same machine can be terminated (preempted) in order to free up
        resources. Priority also determines pricing of the computing resources: teams must pay for the
        resources they use, and higher-priority processes cost more
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Verma2015gi-marker" href="ch10.html#Verma2015gi">59</a>].</p>
        
        <p>This architecture allows non-production (low-priority) computing resources to be overcommitted,
        because the system knows that it can reclaim the resources if necessary. Overcommitting resources in
        turn allows better utilization of machines and greater efficiency compared to systems that segregate
        production and non-production tasks. However, as MapReduce jobs run at low priority, they run the
        risk of being preempted at any time because a higher-priority process requires their resources.
        Batch jobs effectively “pick up the scraps under the table,” using any computing resources that
        remain after the high-priority processes have taken what they need.</p>
        
        <p><a data-type="indexterm" data-primary="Google" data-secondary="MapReduce (batch processing)" data-tertiary="task preemption" id="idm45085101509776"></a>
        At Google, a MapReduce task that runs for an hour has an approximately 5% risk of being terminated
        to make space for a higher-priority process. This rate is more than an order of magnitude higher
        than the rate of failures due to hardware issues, machine reboot, or other reasons
        [<a data-type="noteref" href="ch10.html#Verma2015gi">59</a>]. At this rate of preemptions, if a job has
        100 tasks that each run for 10 minutes, there is a risk greater than 50% that at least one task will be
        terminated before it is finished.</p>
        
        <p>And this is why MapReduce is designed to tolerate frequent unexpected task termination: it’s not
        because the hardware is particularly unreliable, it’s because the freedom to arbitrarily terminate
        processes enables better resource utilization in a computing cluster.</p>
        
        <p><a data-type="indexterm" data-primary="YARN (job scheduler)" data-secondary="preemption of jobs" id="idm45085101506080"></a>
        <a data-type="indexterm" data-primary="Mesos (cluster manager)" id="idm45085101504752"></a>
        <a data-type="indexterm" data-primary="Kubernetes (cluster manager)" id="idm45085101503920"></a>
        Among open source cluster schedulers, preemption is less widely used. YARN’s CapacityScheduler
        supports preemption for balancing the resource allocation of different queues
        [<a data-type="noteref" href="ch10.html#Vavilapalli2013eu">58</a>],
        but general priority preemption is not supported in YARN, Mesos, or Kubernetes at the time of writing
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Schwarzkopf2016un-marker" href="ch10.html#Schwarzkopf2016un">60</a>].
        In an environment where tasks are not so often terminated, the design decisions of MapReduce make
        less sense. In the next section, we will look at some alternatives to MapReduce that make different
        design decisions.
        <a data-type="indexterm" data-primary="batch processing" data-secondary="comparison to MPP databases" data-startref="ix_batchmpp" id="idm45085101499648"></a>
        <a data-type="indexterm" data-primary="Hadoop (data infrastructure)" data-secondary="comparison to MPP databases" data-startref="ix_medredcomp" id="idm45085101498256"></a>
        <a data-type="indexterm" data-primary="massively parallel processing (MPP)" data-secondary="comparison to Hadoop" data-startref="ix_distsyscompmr" id="idm45085101496848"></a></p>
        </div></section>
        
        
        
        </div></section>
        
        
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Beyond MapReduce"><div class="sect1" id="sec_batch_beyond_mr">
        <h1>Beyond MapReduce</h1>
        
        <p>Although MapReduce became very popular and received a lot of hype in the late 2000s, it is just one
        among many possible programming models for distributed systems. Depending on the volume of data, the
        structure of the data, and the type of processing being done with it, other tools may be more
        appropriate for expressing a computation.</p>
        
        <p><a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="disadvantages and limitations of" id="idm45085101493472"></a>
        We nevertheless spent a lot of time in this chapter discussing MapReduce because it is a useful
        learning tool, as it is a fairly clear and simple abstraction on top of a distributed filesystem.
        That is, <em>simple</em> in the sense of being able to understand what it is doing, not in the sense of
        being easy to use. Quite the opposite: implementing a complex processing job using the raw MapReduce
        APIs is actually quite hard and laborious—for instance, any join algorithms used by the job would
        need to be implemented from scratch [<a data-type="noteref" href="ch10.html#Grover2015tl">37</a>].</p>
        
        <p><a data-type="indexterm" data-primary="Pig (dataflow language)" id="idm45085101490320"></a>
        <a data-type="indexterm" data-primary="Hive (query engine)" id="idm45085101489488"></a>
        <a data-type="indexterm" data-primary="Cascading (batch processing)" id="idm45085101488656"></a>
        <a data-type="indexterm" data-primary="Crunch (batch processing)" id="idm45085101487808"></a>
        In response to the difficulty of using MapReduce directly, various higher-level programming models
        (Pig, Hive, Cascading, Crunch) were created as abstractions on top of MapReduce. If you understand
        how MapReduce works, they are fairly easy to learn, and their higher-level constructs make many
        common batch processing tasks significantly easier to implement.</p>
        
        <p>However, there are also problems with the MapReduce execution model itself, which are not fixed by
        adding another level of abstraction and which manifest themselves as poor performance for some
        kinds of processing. On the one hand, MapReduce is very robust: you can use it to process almost
        arbitrarily large quantities of data on an unreliable multi-tenant system with frequent task
        terminations, and it will still get the job done (albeit slowly). On the other hand, other tools are
        sometimes orders of magnitude faster for some kinds of processing.</p>
        
        <p>In the rest of this chapter, we will look at some of those alternatives for batch processing. In
        <a data-type="xref" href="ch11.html#ch_stream">Chapter&nbsp;11</a> we will move to stream processing, which can be regarded as another way of speeding up
        batch processing.</p>
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Materialization of Intermediate State"><div class="sect2" id="sec_batch_materialize">
        <h2>Materialization of Intermediate State</h2>
        
        <p><a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="materialization of intermediate state" id="ix_batchbeymater"></a>
        As discussed previously, every MapReduce job is independent from every other job. The main contact
        points of a job with the rest of the world are its input and output directories on the distributed
        filesystem. If you want the output of one job to become the input to a second job, you need to
        configure the second job’s input directory to be the same as the first job’s output directory, and
        an external workflow scheduler must start the second job only once the first job has completed.</p>
        
        <p><a data-type="indexterm" data-primary="loose coupling" id="idm45085101480064"></a>
        This setup is reasonable if the output from the first job is a dataset that you want to publish
        widely within your organization. In that case, you need to be able to refer to it by name and reuse
        it as input to several different jobs (including jobs developed by other teams). Publishing data to
        a well-known location in the distributed filesystem allows loose coupling so that jobs don’t need
        to know who is producing their input or consuming their output (see <a data-type="xref" href="#sec_batch_logic_wiring">“Separation of logic and wiring”</a>).</p>
        
        <p><a data-type="indexterm" data-primary="intermediate state, materialization of" id="ix_intermed"></a>
        <a data-type="indexterm" data-primary="materialization" data-secondary="intermediate state (batch processing)" id="ix_materialize"></a>
        <a data-type="indexterm" data-primary="recommendation engines" data-secondary="batch workflows" id="idm45085101475152"></a>
        However, in many cases, you know that the output of one job is only ever used as input to one other
        job, which is maintained by the same team. In this case, the files on the distributed filesystem are
        simply <em>intermediate state</em>: a means of passing data from one job to the next. In the complex
        workflows used to build recommendation systems consisting of 50 or 100 MapReduce jobs
        [<a data-type="noteref" href="ch10.html#Sumbaly2013eh">29</a>], there is a lot of such intermediate
        state.</p>
        
        <p>The process of writing out this intermediate state to files is called <em>materialization</em>. (We came
        across the term previously in the context of materialized views, in
        <a data-type="xref" href="ch03.html#sec_storage_materialized_views">“Aggregation: Data Cubes and Materialized Views”</a>. It means to eagerly compute the result of some operation and
        write it out, rather than computing it on demand when requested.)</p>
        
        <p>By contrast, the log analysis example at the beginning of the chapter used Unix pipes to connect the
        output of one command with the input of another. Pipes do not fully materialize the intermediate
        state, but instead <em>stream</em> one command’s output to the next command’s input incrementally, using
        only a small in-memory buffer.</p>
        
        <p>MapReduce’s approach of fully materializing intermediate state has downsides compared to Unix pipes:</p>
        
        <ul>
        <li>
        <p>A MapReduce job can only start when all tasks in the preceding jobs (that generate its inputs)
        have completed, whereas processes connected by a Unix pipe are started at the same time, with
        output being consumed as soon as it is produced. Skew or varying load on different machines means
        that a job often has a few straggler tasks that take much longer to complete than the others.
        Having to wait until all of the preceding job’s tasks have completed slows down the execution of
        the workflow as a whole.</p>
        </li>
        <li>
        <p>Mappers are often redundant: they just read back the same file that was just written by a reducer,
        and prepare it for the next stage of partitioning and sorting. In many cases, the mapper code
        could be part of the previous reducer: if the reducer output was partitioned and sorted in the
        same way as mapper output, then reducers could be chained together directly, without interleaving
        with mapper stages.</p>
        </li>
        <li>
        <p>Storing intermediate state in a distributed filesystem means those files are replicated across
        several nodes, which is often overkill for such temporary data.</p>
        </li>
        </ul>
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Dataflow engines"><div class="sect3" id="sec_batch_dataflow">
        <h3>Dataflow engines</h3>
        
        <p><a data-type="indexterm" data-primary="Spark (processing framework)" id="ix_sparkintro"></a><a data-type="indexterm" data-primary="Apache Spark" data-see="Spark" id="idm45085101461280"></a>
        <a data-type="indexterm" data-primary="Flink (processing framework)" id="ix_flinkintro"></a><a data-type="indexterm" data-primary="Apache Flink" data-see="Flink" id="idm45085101459216"></a>
        <a data-type="indexterm" data-primary="Tez (dataflow engine)" id="ix_tezintro"></a><a data-type="indexterm" data-primary="Apache Tez" data-see="Tez" id="idm45085101457168"></a>
        <a data-type="indexterm" data-primary="dataflow engines" id="ix_dflowengines"></a>
        <a data-type="indexterm" data-primary="batch processing" data-secondary="dataflow engines" id="ix_batchdataflow"></a>
        In order to fix these problems with MapReduce, several new execution engines for distributed batch
        computations were developed, the most well known of which are Spark
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Zaharia2012ve-marker" href="ch10.html#Zaharia2012ve">61</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Karau2015wf-marker" href="ch10.html#Karau2015wf">62</a>],
        Tez [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Saha2014vd-marker" href="ch10.html#Saha2014vd">63</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Saha2015dh-marker" href="ch10.html#Saha2015dh">64</a>],
        and Flink [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Tzoumas2015ws-marker" href="ch10.html#Tzoumas2015ws">65</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Alexandrov2014jb-marker" href="ch10.html#Alexandrov2014jb">66</a>].
        There are various differences in the way they are designed, but they have one thing in common: they
        handle an entire workflow as one job, rather than breaking it up into independent subjobs.</p>
        
        <p><a data-type="indexterm" data-primary="single-threaded execution" data-secondary="in batch processing" id="idm45085101438992"></a>
        Since they explicitly model the flow of data through several processing stages, these systems are
        known as <em>dataflow engines</em>. Like MapReduce, they work by repeatedly calling a user-defined function
        to process one record at a time on a single thread. They parallelize work by partitioning inputs,
        and they copy the output of one function over the network to become the input to another function.</p>
        
        <p><a data-type="indexterm" data-primary="operators" id="idm45085101436848"></a>
        Unlike in MapReduce, these functions need not take the strict roles of alternating map and reduce,
        but instead can be assembled in more flexible ways. We call these functions <em>operators</em>, and the
        dataflow engine provides several different options for connecting one operator’s output to another’s
        input:</p>
        
        <ul>
        <li>
        <p>One option is to repartition and sort records by key, like in the shuffle stage of MapReduce
        (see <a data-type="xref" href="#sec_batch_mapreduce_dist">“Distributed execution of MapReduce”</a>). This feature enables sort-merge joins and grouping in the same
        way as in MapReduce.</p>
        </li>
        <li>
        <p>Another possibility is to take several inputs and to partition them in the same way, but skip
        the sorting. This saves effort on partitioned hash joins, where the partitioning of records is
        important but the order is irrelevant because building the hash table randomizes the order
        anyway.</p>
        </li>
        <li>
        <p>For broadcast hash joins, the same output from one operator can be sent to all partitions of
        the join operator.</p>
        </li>
        </ul>
        
        <p><a data-type="indexterm" data-primary="Dryad (dataflow engine)" id="idm45085101430416"></a>
        <a data-type="indexterm" data-primary="Nephele (dataflow engine)" id="idm45085101429584"></a>
        This style of processing engine is based on research systems like Dryad
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Isard2007fe-marker" href="ch10.html#Isard2007fe">67</a>]
        and Nephele [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Warneke2009en-marker" href="ch10.html#Warneke2009en">68</a>], and it
        offers several advantages compared to the MapReduce model:</p>
        
        <ul>
        <li>
        <p>Expensive work such as sorting need only be performed in places where it is actually required,
        rather than always happening by default between every map and reduce stage.</p>
        </li>
        <li>
        <p>There are no unnecessary map tasks, since the work done by a mapper can often be
        incorporated into the preceding reduce operator (because a mapper does not change the partitioning
        of a dataset).</p>
        </li>
        <li>
        <p>Because all joins and data dependencies<a data-type="indexterm" data-primary="locality (data access)" data-secondary="in batch processing" id="idm45085101419296"></a> in a workflow are explicitly declared, the scheduler has
        an overview of what data is required where, so it can make locality optimizations. For example, it
        can try to place the task that consumes some data on the same machine as the task that produces
        it, so that the data can be exchanged through a shared memory buffer rather than having to copy
        it over the network.</p>
        </li>
        <li>
        <p>It is usually sufficient for intermediate state between operators to be kept in memory or written
        to local disk, which requires less I/O than writing it to HDFS (where it must be replicated to
        several machines and written to disk on each replica). MapReduce already uses this optimization
        for mapper output, but dataflow engines generalize the idea to all intermediate state.</p>
        </li>
        <li>
        <p>Operators can start executing as soon as their input is ready; there is no need to wait for the
        entire preceding stage to finish before the next one starts.</p>
        </li>
        <li>
        <p><a data-type="indexterm" data-primary="Java Virtual Machine (JVM)" data-secondary="process reuse in batch processors" id="idm45085101414976"></a>
        Existing Java Virtual Machine (JVM) processes can be reused to run new operators, reducing startup
        overheads compared to MapReduce (which launches a new JVM for each task).</p>
        </li>
        </ul>
        
        <p>You can use dataflow engines to implement the same computations as MapReduce workflows, and they
        usually execute significantly faster due to the optimizations described here. Since operators are a
        generalization of map and reduce, the same processing code can run on a dataflow engine:
        workflows implemented in Pig, Hive, or Cascading can be switched from MapReduce to Tez or Spark with
        a simple configuration change, without modifying code
        [<a data-type="noteref" href="ch10.html#Saha2015dh">64</a>].</p>
        
        <p>Tez is a fairly thin library that relies on the YARN shuffle service for the actual copying of data
        between nodes [<a data-type="noteref" href="ch10.html#Vavilapalli2013eu">58</a>], whereas Spark and
        Flink are big frameworks that include their own network communication layer, scheduler, and
        user-facing APIs. We will discuss those high-level APIs shortly.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Fault tolerance"><div class="sect3" id="sec_batch_materialize_ft">
        <h3>Fault tolerance</h3>
        
        <p><a data-type="indexterm" data-primary="batch processing" data-secondary="fault tolerance" id="idm45085101408704"></a>
        <a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="fault tolerance" id="idm45085101407600"></a>
        <a data-type="indexterm" data-primary="fault tolerance" data-secondary="in batch processing" id="idm45085101406480"></a>
        An advantage of fully materializing intermediate state to a distributed filesystem is that it is
        durable, which makes fault tolerance fairly easy in MapReduce: if a task fails, it can just be
        restarted on another machine and read the same input again from the filesystem.</p>
        
        <p><a data-type="indexterm" data-primary="Spark (processing framework)" data-secondary="fault tolerance" id="idm45085101404800"></a>
        <a data-type="indexterm" data-primary="Flink (processing framework)" data-secondary="fault tolerance" id="idm45085101403728"></a>
        <a data-type="indexterm" data-primary="Tez (dataflow engine)" data-secondary="fault tolerance" id="idm45085101402608"></a>
        Spark, Flink, and Tez avoid writing intermediate state to HDFS, so they take a different approach to
        tolerating faults: if a machine fails and the intermediate state on that machine is lost, it is
        recomputed from other data that is still available (a prior intermediary stage if possible, or
        otherwise the original input data, which is normally on HDFS).</p>
        
        <p><a data-type="indexterm" data-primary="checkpointing" data-secondary="in batch processors" id="idm45085101400880"></a>
        To enable this recomputation, the framework must keep track of how a given piece of data was
        computed—which input partitions it used, and which operators were applied to it. Spark uses the
        resilient distributed dataset (RDD) abstraction for tracking the ancestry of data
        [<a data-type="noteref" href="ch10.html#Zaharia2012ve">61</a>], while Flink checkpoints operator state,
        allowing it to resume running an operator that ran into a fault during its execution
        [<a data-type="noteref" href="ch10.html#Alexandrov2014jb">66</a>].</p>
        
        <p><a data-type="indexterm" data-primary="deterministic operations" data-secondary="and fault tolerance" id="idm45085101397456"></a>
        When recomputing data, it is important to know whether the computation is <em>deterministic</em>: that is,
        given the same input data, do the operators always produce the same output? This question matters if some of
        the lost data has already been sent to downstream operators. If the operator is restarted and the
        recomputed data is not the same as the original lost data, it becomes very hard for downstream
        operators to resolve the contradictions between the old and new data. The solution in the case of
        nondeterministic operators is normally to kill the downstream operators as well, and run them again
        on the new data.</p>
        
        <p><a data-type="indexterm" data-primary="deterministic operations" data-secondary="accidental nondeterminism" id="idm45085101395024"></a>
        <a data-type="indexterm" data-primary="nondeterministic operations" data-secondary="accidental nondeterminism" id="idm45085101393888"></a>
        In order to avoid such cascading faults, it is better to make operators deterministic. Note however
        that it is easy for nondeterministic behavior to accidentally creep in: for example, many
        programming languages do not guarantee any particular order when iterating over elements of a hash
        table, many probabilistic and statistical <span class="keep-together">algorithms</span>
        explicitly rely on using random numbers, and any use of the system clock or external data sources is
        nondeterministic. Such causes of nondeterminism need to be removed in order to reliably recover from
        faults, for example by generating <span class="keep-together">pseudorandom</span> numbers
        using a fixed seed.</p>
        
        <p>Recovering from faults by recomputing data is not always the right answer: if the intermediate data
        is much smaller than the source data, or if the computation is very CPU-intensive, it is probably
        cheaper to materialize the intermediate data to files than to recompute it.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Discussion of materialization"><div class="sect3" id="idm45085101389968">
        <h3>Discussion of materialization</h3>
        
        <p><a data-type="indexterm" data-primary="pipelined execution" id="idm45085101388800"></a>
        <a data-type="indexterm" data-primary="Unix philosophy" data-secondary="command-line batch processing" data-tertiary="Unix pipes versus dataflow engines" id="idm45085101387968"></a>
        Returning to the Unix analogy, we saw that MapReduce is like writing the output of each command to a
        temporary file, whereas dataflow engines look much more like Unix pipes. Flink especially is built
        around the idea of pipelined execution: that is, incrementally passing the output of an operator to
        other operators, and not waiting for the input to be complete before starting to process it.</p>
        
        <p>A sorting operation inevitably needs to consume its entire input before it can produce any output,
        because it’s possible that the very last input record is the one with the lowest key and thus needs
        to be the very first output record. Any operator that requires sorting will thus need to accumulate
        state, at least temporarily. But many other parts of a workflow can be executed in a pipelined
        manner.</p>
        
        <p>When the job completes, its output needs to go somewhere durable so that users can find it and use
        it—most likely, it is written to the distributed filesystem again. Thus, when using a dataflow
        engine, materialized datasets on HDFS are still usually the inputs and the final outputs of a job.
        Like with MapReduce, the inputs are immutable and the output is completely replaced. The improvement
        over MapReduce is that you save yourself writing all the intermediate state to the filesystem as
        well.
        <a data-type="indexterm" data-primary="intermediate state, materialization of" data-startref="ix_intermed" id="idm45085101384144"></a>
        <a data-type="indexterm" data-primary="materialization" data-secondary="intermediate state (batch processing)" data-startref="ix_materialize" id="idm45085101383072"></a>
        <a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="materialization of intermediate state" data-startref="ix_batchbeymater" id="idm45085101381680"></a>
        <a data-type="indexterm" data-primary="Spark (processing framework)" data-startref="ix_sparkintro" id="idm45085101380272"></a>
        <a data-type="indexterm" data-primary="Flink (processing framework)" data-startref="ix_flinkintro" id="idm45085101379152"></a>
        <a data-type="indexterm" data-primary="Tez (dataflow engine)" data-startref="ix_tezintro" id="idm45085101378032"></a>
        <a data-type="indexterm" data-primary="dataflow engines" data-startref="ix_dflowengines" id="idm45085101376928"></a>
        <a data-type="indexterm" data-primary="batch processing" data-secondary="dataflow engines" data-startref="ix_batchdataflow" id="idm45085101375824"></a></p>
        </div></section>
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Graphs and Iterative Processing"><div class="sect2" id="sec_batch_graph">
        <h2>Graphs and Iterative Processing</h2>
        
        <p><a data-type="indexterm" data-primary="batch processing" data-secondary="graphs and iterative processing" id="ix_batchbeygraph"></a>
        <a data-type="indexterm" data-primary="graphs" data-secondary="processing and analysis" id="ix_graph"></a>
        In <a data-type="xref" href="ch02.html#sec_datamodels_graph">“Graph-Like Data Models”</a> we discussed using graphs for modeling data, and using graph query
        languages to traverse the edges and vertices in a graph. The discussion in <a data-type="xref" href="ch02.html#ch_datamodels">Chapter&nbsp;2</a> was
        focused around OLTP-style use: quickly executing queries to find a small number of vertices matching
        certain criteria.</p>
        
        <p><a data-type="indexterm" data-primary="machine learning" data-secondary="iterative processing" id="idm45085101367936"></a>
        <a data-type="indexterm" data-primary="recommendation engines" data-secondary="iterative processing" id="idm45085101366832"></a>
        <a data-type="indexterm" data-primary="ranking algorithms" id="idm45085101365728"></a>
        <a data-type="indexterm" data-primary="PageRank (algorithm)" id="idm45085101364896"></a>
        It is also interesting to look at graphs in a batch processing context, where the goal is to perform
        some kind of offline processing or analysis on an entire graph. This need often arises in machine
        learning applications such as recommendation engines, or in ranking systems. For example, one of the
        most famous graph analysis algorithms is PageRank
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Page1999wg-marker" href="ch10.html#Page1999wg">69</a>],
        which tries to estimate the popularity of a web page based on what other web pages link to it. It is
        used as part of the formula that determines the order in which web search engines present their
        results.</p>
        <div data-type="note" epub:type="note"><h6>Note</h6>
        <p><a data-type="indexterm" data-primary="operators" data-secondary="flow of data between" id="idm45085101359904"></a>
        <a data-type="indexterm" data-primary="directed acyclic graphs (DAGs)" id="idm45085101358576"></a>
        <a data-type="indexterm" data-primary="dataflow engines" data-secondary="directed acyclic graphs (DAG)" id="idm45085101357776"></a>
        Dataflow engines like Spark, Flink, and Tez (see <a data-type="xref" href="#sec_batch_materialize">“Materialization of Intermediate State”</a>) typically arrange the
        operators in a job as a directed acyclic graph (DAG). This is not the same as graph processing: in
        dataflow engines, the <em>flow of data from one operator to another</em> is structured as a graph, while
        the data itself typically consists of relational-style tuples. In graph processing, the <em>data
        itself</em> has the form of a graph. Another unfortunate naming confusion!</p>
        </div>
        
        <p><a data-type="indexterm" data-primary="transitive closure (graph algorithm)" id="idm45085101354320"></a>
        Many graph algorithms are expressed by traversing one edge at a time, joining one vertex with an
        adjacent vertex in order to propagate some information, and repeating until some condition is
        met—for example, until there are no more edges to follow, or until some metric converges. We saw an
        example in <a data-type="xref" href="ch02.html#fig_datalog_naive">Figure&nbsp;2-6</a>, which made a list of all the locations in North America contained
        in a database by repeatedly following edges indicating which location is within which other location
        (this kind of algorithm is called a <em>transitive closure</em>).</p>
        
        <p><a data-type="indexterm" data-primary="iterative processing" id="ix_iterate"></a>
        It is possible to store a graph in a distributed filesystem (in files containing lists of vertices
        and edges), but this idea of “repeating until done” cannot be expressed in plain MapReduce, since it
        only performs a single pass over the data. This kind of algorithm is thus often implemented in an
        <em>iterative</em> style:</p>
        <ol>
        <li>
        <p>An external scheduler runs a batch process to calculate one step of the algorithm.</p>
        </li>
        <li>
        <p>When the batch process completes, the scheduler checks whether the iterative algorithm has
        finished (based on the completion condition—e.g., there are no more edges to follow, or the
        change of a metric compared to the last iteration is below some threshold).</p>
        </li>
        <li>
        <p>If the algorithm has not yet finished, the scheduler goes back to step 1 and runs another round
        of the batch process.</p>
        </li>
        
        </ol>
        
        <p>This approach works, but implementing it with MapReduce is often very inefficient, because MapReduce
        does not account for the iterative nature of the algorithm: it will always read the entire input
        dataset and produce a completely new output dataset, even if only a small part of the graph has
        changed compared to the last iteration.</p>
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="The Pregel processing model"><div class="sect3" id="idm45085101345264">
        <h3>The Pregel processing model</h3>
        
        <p><a data-type="indexterm" data-primary="Application Programming Interfaces (APIs)" data-secondary="for graph processing" id="idm45085101343920"></a>
        <a data-type="indexterm" data-primary="Giraph (graph processing)" id="idm45085101342384"></a><a data-type="indexterm" data-primary="Apache Giraph" data-see="Giraph" id="idm45085101341616"></a>
        <a data-type="indexterm" data-primary="Spark (processing framework)" data-secondary="GraphX API (graph processing)" id="idm45085101340544"></a>
        <a data-type="indexterm" data-primary="Flink (processing framework)" data-secondary="Gelly API (graph processing)" id="idm45085101339408"></a>
        <a data-type="indexterm" data-primary="graphs" data-secondary="processing and analysis" data-tertiary="Pregel processing model" id="idm45085101338272"></a>
        <a data-type="indexterm" data-primary="Pregel processing model" id="idm45085101336896"></a>
        <a data-type="indexterm" data-primary="bulk synchronous parallel (BSP) model" id="idm45085101336064"></a>
        <a data-type="indexterm" data-primary="Google" data-secondary="Pregel (graph processing)" id="idm45085101335216"></a>
        As an optimization for batch processing graphs, the <em>bulk synchronous parallel</em> (BSP) model of
        computation [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Valiant1990ce-marker" href="ch10.html#Valiant1990ce">70</a>]
        has become popular. Among others, it is implemented by Apache Giraph
        [<a data-type="noteref" href="ch10.html#Grover2015tl">37</a>], Spark’s GraphX API, and Flink’s Gelly API
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Ewen2012cm-marker" href="ch10.html#Ewen2012cm">71</a>].
        It is also known as the <em>Pregel</em> model, as Google’s Pregel paper popularized this approach for
        processing graphs [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Malewicz2010jq-marker" href="ch10.html#Malewicz2010jq">72</a>].</p>
        
        <p>Recall that in MapReduce, mappers conceptually “send a message” to a particular call of the reducer
        because the framework collects together all the mapper outputs with the same key. A similar idea is
        behind Pregel: one vertex can “send a message” to another vertex, and typically those messages are
        sent along the edges in a graph.</p>
        
        <p>In each iteration, a function is called for each vertex, passing the function all the messages that
        were sent to that vertex—much like a call to the reducer. The difference from MapReduce is that in
        the Pregel model, a vertex remembers its state in memory from one iteration to the next, so the
        function only needs to process new incoming messages. If no messages are being sent in some part of
        the graph, no work needs to be done.</p>
        
        <p><a data-type="indexterm" data-primary="actor model" data-secondary="comparison to Pregel model" id="idm45085101322000"></a>
        It’s a bit similar to the actor model (see <a data-type="xref" href="ch04.html#sec_encoding_actors">“Distributed actor frameworks”</a>), if you think of each vertex as
        an actor, except that vertex state and messages between vertices are fault-tolerant and durable, and
        communication proceeds in fixed rounds: at every iteration, the framework delivers all messages sent
        in the previous iteration. Actors normally have no such timing guarantee.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Fault tolerance"><div class="sect3" id="idm45085101319600">
        <h3>Fault tolerance</h3>
        
        <p><a data-type="indexterm" data-primary="graphs" data-secondary="processing and analysis" data-tertiary="fault tolerance" id="idm45085101318464"></a>
        <a data-type="indexterm" data-primary="fault tolerance" data-secondary="in batch processing" id="idm45085101317088"></a>
        The fact that vertices can only communicate by message passing (not by querying each other directly)
        helps improve the performance of Pregel jobs, since messages can be batched and there is less
        waiting for communication. The only waiting is between iterations: since the Pregel model guarantees
        that all messages sent in one iteration are delivered in the next iteration, the prior iteration
        must completely finish, and all of its messages must be copied over the network, before the next one
        can start.</p>
        
        <p>Even though the underlying network may drop, duplicate, or arbitrarily delay messages (see
        <a data-type="xref" href="ch08.html#sec_distributed_networks">“Unreliable Networks”</a>), Pregel implementations guarantee that messages are processed exactly
        once at their destination vertex in the following iteration. Like MapReduce, the framework
        transparently recovers from faults in order to simplify the programming model for algorithms on top
        of Pregel.</p>
        
        <p><a data-type="indexterm" data-primary="deterministic operations" data-secondary="and fault tolerance" id="idm45085101313568"></a>
        <a data-type="indexterm" data-primary="checkpointing" data-secondary="in batch processors" id="idm45085101312448"></a>
        This fault tolerance is achieved by periodically checkpointing the state of all vertices at the end
        of an iteration—i.e., writing their full state to durable storage. If a node fails and its in-memory
        state is lost, the simplest solution is to roll back the entire graph computation to the last
        checkpoint and restart the computation at that point. If the algorithm is deterministic and messages are
        logged, it is also possible to selectively recover only the partition that was lost (like we
        previously discussed for dataflow engines) [<a data-type="noteref" href="ch10.html#Malewicz2010jq">72</a>].</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Parallel execution"><div class="sect3" id="idm45085101309728">
        <h3>Parallel execution</h3>
        
        <p><a data-type="indexterm" data-primary="parallel execution" data-secondary="of graph analysis algorithms" id="idm45085101308352"></a>
        A vertex does not need to know on which physical machine it is executing; when it sends messages to
        other vertices, it simply sends them to a vertex ID. It is up to the framework to partition the
        graph—i.e., to decide which vertex runs on which machine, and how to route messages over the
        network so that they end up in the right place.</p>
        
        <p>Because the programming model deals with just one vertex at a time (sometimes called “thinking like a
        vertex”), the framework may partition the graph in arbitrary ways. Ideally it would be partitioned
        such that vertices are colocated on the same machine if they need to communicate a lot. However,
        finding such an optimized partitioning is hard—in practice, the graph is often simply partitioned
        by an arbitrarily assigned vertex ID, making no attempt to group related vertices together.</p>
        
        <p>As a result, graph algorithms often have a lot of cross-machine communication overhead, and the
        intermediate state (messages sent between nodes) is often bigger than the original graph. The
        overhead of sending messages over the network can significantly slow down distributed graph
        algorithms.</p>
        
        <p><a data-type="indexterm" data-primary="single-threaded execution" data-secondary="in batch processing" id="idm45085101304976"></a>
        <a data-type="indexterm" data-primary="GraphChi (graph processing)" id="idm45085101303680"></a>
        For this reason, if your graph can fit in memory on a single computer, it’s quite likely that a
        single-machine (maybe even single-threaded) algorithm will outperform a distributed batch process
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="McSherry2015vx_ch10-marker" href="ch10.html#McSherry2015vx_ch10">73</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Gog2015et-marker" href="ch10.html#Gog2015et">74</a>].
        Even if the graph is bigger than memory, it can fit on the disks of a single computer,
        single-machine processing using a framework such as GraphChi is a viable option
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kyrola2012uo-marker" href="ch10.html#Kyrola2012uo">75</a>].
        If the graph is too big to fit on a single machine, a distributed approach such as Pregel is
        unavoidable; efficiently parallelizing graph algorithms is an area of ongoing research
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Lenharth2016je-marker" href="ch10.html#Lenharth2016je">76</a>].
        <a data-type="indexterm" data-primary="iterative processing" data-startref="ix_iterate" id="idm45085101291840"></a>
        <a data-type="indexterm" data-primary="batch processing" data-secondary="graphs and iterative processing" data-startref="ix_batchbeygraph" id="idm45085101290736"></a>
        <a data-type="indexterm" data-primary="graphs" data-secondary="processing and analysis" data-startref="ix_graph" id="idm45085101289344"></a></p>
        </div></section>
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="High-Level APIs and Languages"><div class="sect2" id="sec_batch_highlevel">
        <h2>High-Level APIs and Languages</h2>
        
        <p><a data-type="indexterm" data-primary="batch processing" data-secondary="high-level APIs and languages" id="ix_batchbeyAPI"></a>
        <a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="higher-level tools" id="idm45085101285344"></a>
        Over the years since MapReduce first became popular, the execution engines for distributed batch
        processing have matured. By now, the infrastructure has become robust enough to store and process
        many petabytes of data on clusters of over 10,000 machines. As the problem of physically operating
        batch processes at such scale has been considered more or less solved, attention has turned to other
        areas: improving the programming model, improving the efficiency of processing, and broadening the
        set of problems that these technologies can solve.</p>
        
        <p><a data-type="indexterm" data-primary="Hive (query engine)" id="idm45085101283408"></a>
        <a data-type="indexterm" data-primary="Pig (dataflow language)" id="idm45085101282448"></a>
        <a data-type="indexterm" data-primary="Cascading (batch processing)" id="idm45085101281712"></a>
        <a data-type="indexterm" data-primary="Crunch (batch processing)" id="idm45085101280912"></a>
        <a data-type="indexterm" data-primary="Tez (dataflow engine)" data-secondary="support by higher-level tools" id="idm45085101280064"></a>
        <a data-type="indexterm" data-primary="Spark (processing framework)" data-secondary="dataflow APIs" id="idm45085101278944"></a>
        <a data-type="indexterm" data-primary="Flink (processing framework)" data-secondary="dataflow APIs" id="idm45085101277824"></a>
        <a data-type="indexterm" data-primary="Google" data-secondary="FlumeJava (dataflow library)" id="idm45085101276704"></a>
        <a data-type="indexterm" data-primary="FlumeJava (dataflow library)" id="idm45085101275584"></a>
        As discussed previously, higher-level languages and APIs such as Hive, Pig, Cascading, and Crunch
        became popular because programming MapReduce jobs by hand is quite laborious. As Tez emerged, these
        high-level languages had the additional benefit of being able to move to the new dataflow execution
        engine without the need to rewrite job code. Spark and Flink also include their own high-level
        dataflow APIs, often taking inspiration from FlumeJava
        [<a data-type="noteref" href="ch10.html#Chambers2010dp">34</a>].</p>
        
        <p>These dataflow APIs generally use relational-style building blocks to express a computation: joining
        datasets on the value of some field; grouping tuples by key; filtering by some condition; and
        aggregating tuples by counting, summing, or other functions. Internally, these operations are
        implemented using the various join and grouping algorithms that we discussed earlier in this
        chapter.</p>
        
        <p>Besides the obvious advantage of requiring less code, these high-level interfaces also allow
        interactive use, in which you write analysis code incrementally in a shell and run it frequently to
        observe what it is doing. This style of development is very helpful when exploring a dataset and
        experimenting with approaches for processing it. It is also reminiscent of the Unix philosophy,
        which we discussed in <a data-type="xref" href="#sec_batch_unix_philosophy">“The Unix Philosophy”</a>.</p>
        
        <p>Moreover, these high-level interfaces not only make the humans using the system more productive, but
        they also improve the job execution efficiency at a machine level.</p>
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="The move toward declarative query languages"><div class="sect3" id="idm45085101270320">
        <h3>The move toward declarative query languages</h3>
        
        <p><a data-type="indexterm" data-primary="Hive (query engine)" data-secondary="query optimizer" id="idm45085101268912"></a>
        <a data-type="indexterm" data-primary="Spark (processing framework)" data-secondary="query optimizer" id="idm45085101267584"></a>
        <a data-type="indexterm" data-primary="Flink (processing framework)" data-secondary="query optimizer" id="idm45085101266512"></a>
        <a data-type="indexterm" data-primary="joins" data-secondary="expressing as relational operators" id="idm45085101265392"></a>
        <a data-type="indexterm" data-primary="query optimizers" id="idm45085101264272"></a>
        An advantage of specifying joins as relational operators, compared to spelling out the code that
        performs the join, is that the framework can analyze the properties of the join inputs and
        automatically decide which of the aforementioned join algorithms would be most suitable for the task
        at hand. Hive, Spark, and Flink have cost-based query optimizers that can do this, and even change
        the order of joins so that the amount of intermediate state is minimized
        [<a data-type="noteref" href="ch10.html#Alexandrov2014jb">66</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Huske2015vm-marker" href="ch10.html#Huske2015vm">77</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Mokhtar2015vg-marker" href="ch10.html#Mokhtar2015vg">78</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Armbrust2015dy-marker" href="ch10.html#Armbrust2015dy">79</a>].</p>
        
        <p><a data-type="indexterm" data-primary="dataflow engines" data-secondary="support for declarative queries" id="idm45085101254384"></a>
        <a data-type="indexterm" data-primary="declarative languages" data-secondary="for batch processing" id="idm45085101253264"></a>
        The choice of join algorithm can make a big difference to the performance of a batch job, and it is
        nice not to have to understand and remember all the various join algorithms we discussed in this
        chapter. This is possible if joins are specified in a <em>declarative</em> way: the application simply
        states which joins are required, and the query optimizer decides how they can best be executed.
        We previously came across this idea in <a data-type="xref" href="ch02.html#sec_datamodels_query">“Query Languages for Data”</a>.</p>
        
        <p>However, in other ways, MapReduce and its dataflow successors are very different from the fully
        declarative query model of SQL. MapReduce was built around the idea of function callbacks: for each
        record or group of records, a user-defined function (the mapper or reducer) is called, and that
        function is free to call arbitrary code in order to decide what to output. This approach has the
        advantage that you can draw upon a large ecosystem of existing libraries to do things like parsing,
        natural language analysis, image analysis, and running numerical or statistical algorithms.</p>
        
        <p><a data-type="indexterm" data-primary="massively parallel processing (MPP)" data-secondary="comparison to Hadoop" id="idm45085101249200"></a>
        <a data-type="indexterm" data-primary="package managers" id="idm45085101247904"></a>
        <a data-type="indexterm" data-primary="Maven (Java build tool)" id="idm45085101247072"></a>
        <a data-type="indexterm" data-primary="npm (package manager)" id="idm45085101246240"></a>
        <a data-type="indexterm" data-primary="Rubygems (package manager)" id="idm45085101245408"></a>
        The freedom to easily run arbitrary code is what has long distinguished batch processing systems of
        MapReduce heritage from MPP databases (see <a data-type="xref" href="#sec_batch_mr_vs_db">“Comparing Hadoop to Distributed Databases”</a>); although databases have
        facilities for writing user-defined functions, they are often cumbersome to use and not well
        integrated with the package managers and dependency management systems that are widely used in most
        programming languages (such as Maven for Java, npm for JavaScript, and Rubygems for Ruby).</p>
        
        <p><a data-type="indexterm" data-primary="CPUs" data-secondary="caching and pipelining" id="idm45085101243152"></a>
        <a data-type="indexterm" data-primary="caches" data-secondary="in CPUs" id="idm45085101241744"></a>
        <a data-type="indexterm" data-primary="vectorized processing" id="idm45085101240640"></a>
        <a data-type="indexterm" data-primary="column-oriented storage" data-secondary="in batch processors" id="idm45085101239808"></a>
        <a data-type="indexterm" data-primary="column-oriented storage" data-secondary="vectorized processing" id="idm45085101238704"></a>
        <a data-type="indexterm" data-primary="Java Virtual Machine (JVM)" data-secondary="bytecode generation" id="idm45085101237600"></a>
        <a data-type="indexterm" data-primary="Spark (processing framework)" data-secondary="bytecode generation" id="idm45085101236480"></a>
        <a data-type="indexterm" data-primary="Impala (query engine)" data-secondary="native code generation" id="idm45085101235360"></a>
        However, developers of dataflow engines have found that there are also advantages to incorporating more
        declarative features in areas besides joins. For example, if a callback function contains only a
        simple filtering condition, or it just selects some fields from a record, then there is significant
        CPU overhead in calling the function on every record. If such simple filtering and mapping
        operations are expressed in a declarative way, the query optimizer can take advantage of
        column-oriented storage layouts (see <a data-type="xref" href="ch03.html#sec_storage_column">“Column-Oriented Storage”</a>) and read only the required columns from
        disk. Hive, Spark DataFrames, and Impala also use vectorized execution (see
        <a data-type="xref" href="ch03.html#sec_storage_vectorized">“Memory bandwidth and vectorized processing”</a>): iterating over data in a tight inner loop that is friendly to CPU
        caches, and avoiding function calls. Spark generates JVM bytecode
        [<a data-type="noteref" href="ch10.html#Armbrust2015dy">79</a>] and Impala uses LLVM to generate native
        code for these inner loops [<a data-type="noteref" href="ch10.html#Kornacker2015uv_ch10">41</a>].</p>
        
        <p>By incorporating declarative aspects in their high-level APIs, and having query optimizers that can
        take advantage of them during execution, batch processing frameworks begin to look more like MPP
        databases (and can achieve comparable performance). At the same time, by having the extensibility of
        being able to run arbitrary code and read/write data in arbitrary formats, they retain their flexibility
        advantage.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Specialization for different domains"><div class="sect3" id="idm45085101229472">
        <h3>Specialization for different domains</h3>
        
        <p>While the extensibility of being able to run arbitrary code is useful, there are also many common
        cases where standard processing patterns keep reoccurring, and so it is worth having reusable
        implementations of the common building blocks. Traditionally, MPP databases have served the needs of
        business intelligence analysts and business reporting, but that is just one among many domains in
        which batch processing is used.</p>
        
        <p><a data-type="indexterm" data-primary="statistical and numerical algorithms" id="idm45085101227280"></a>
        <a data-type="indexterm" data-primary="machine learning" data-secondary="statistical and numerical algorithms" id="idm45085101226112"></a>
        <a data-type="indexterm" data-primary="recommendation engines" data-secondary="statistical and numerical algorithms" id="idm45085101225024"></a>
        <a data-type="indexterm" data-primary="Mahout (machine learning toolkit)" id="idm45085101223904"></a><a data-type="indexterm" data-primary="Apache Mahout" data-see="Mahout" id="idm45085101223184"></a>
        <a data-type="indexterm" data-primary="MADlib (machine learning toolkit)" id="idm45085101222112"></a><a data-type="indexterm" data-primary="Apache MADlib" data-see="MADlib" id="idm45085101221392"></a>
        <a data-type="indexterm" data-primary="HAWQ (database)" id="idm45085101220320"></a><a data-type="indexterm" data-primary="Apache HAWQ" data-see="HAWQ" id="idm45085101219616"></a>
        <a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="machine learning" id="idm45085101218544"></a>
        <a data-type="indexterm" data-primary="Spark (processing framework)" data-secondary="machine learning" id="idm45085101217424"></a>
        <a data-type="indexterm" data-primary="Flink (processing framework)" data-secondary="machine learning" id="idm45085101216304"></a>
        Another domain of increasing importance is statistical and numerical algorithms, which are needed
        for machine learning applications such as classification and recommendation systems. Reusable
        implementations are emerging: for example, Mahout implements various algorithms for machine learning
        on top of MapReduce, Spark, and Flink, while MADlib implements similar functionality inside a
        relational MPP database (Apache HAWQ) [<a data-type="noteref" href="ch10.html#Cohen2009fv">54</a>].</p>
        
        <p><a data-type="indexterm" data-primary="spatial algorithms" id="idm45085101213936"></a>
        <a data-type="indexterm" data-primary="k-nearest neighbors" id="idm45085101212880"></a>
        <a data-type="indexterm" data-primary="genome analysis" id="idm45085101212048"></a>
        <a data-type="indexterm" data-primary="searches" data-secondary="k-nearest neighbors" id="idm45085101211216"></a>
        <a data-type="indexterm" data-primary="similarity search" data-secondary="k-nearest neighbors" id="idm45085101210112"></a>
        Also useful are spatial algorithms such as <em>k-nearest neighbors</em>
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Blazevski2016ve-marker" href="ch10.html#Blazevski2016ve">80</a>], which searches for items
        that are close to a given item in some multi-dimensional space—a kind of similarity search.
        Approximate search is also important for genome analysis algorithms, which need to find strings that
        are similar but not identical [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="White2016ua-marker" href="ch10.html#White2016ua">81</a>].</p>
        
        <p>Batch processing engines are being used for distributed execution of algorithms from an increasingly
        wide range of domains. As batch processing systems gain built-in functionality and high-level
        declarative operators, and as MPP databases become more programmable and flexible, the two are
        beginning to look more alike: in the end, they are all just systems for storing and processing data.
        <a data-type="indexterm" data-primary="batch processing" data-secondary="high-level APIs and languages" data-startref="ix_batchbeyAPI" id="idm45085101203168"></a></p>
        </div></section>
        
        
        
        </div></section>
        
        
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="idm45085101495456">
        <h1>Summary</h1>
        
        <p>In this chapter we explored the topic of batch processing. We started by looking at Unix tools such
        as <code>awk</code>, <code>grep</code>, and <code>sort</code>, and we saw how the design philosophy of those tools is carried forward
        into MapReduce and more recent dataflow engines. Some of those design principles are that inputs are
        immutable, outputs are intended to become the input to another (as yet unknown) program, and complex
        problems are solved by composing small tools that “do one thing well.”</p>
        
        <p>In the Unix world, the uniform interface that allows one program to be composed with another is
        files and pipes; in MapReduce, that interface is a distributed filesystem. We saw that dataflow
        engines add their own pipe-like data transport mechanisms to avoid materializing intermediate state
        to the distributed filesystem, but the initial input and final output of a job are still usually
        files in HDFS.</p>
        
        <p><a data-type="indexterm" data-primary="partitioning" data-secondary="in batch processing" id="idm45085101197392"></a>
        The two main problems that distributed batch processing frameworks need to solve are:</p>
        <dl>
        <dt>Partitioning</dt>
        <dd>
        <p>In MapReduce, mappers are partitioned according to input file blocks. The output of mappers is
        repartitioned, sorted, and merged into a configurable number of reducer partitions. The purpose of
        this process is to bring all the related data—e.g., all the records with the same key—together in
        the same place.</p>
        
        <p><a data-type="indexterm" data-primary="dataflow engines" data-secondary="partitioning, approach to" id="idm45085101194016"></a>
        Post-MapReduce dataflow engines try to avoid sorting unless it is required, but they otherwise take
        a broadly similar approach to partitioning.</p>
        </dd>
        <dt>Fault tolerance</dt>
        <dd>
        <p>MapReduce frequently writes to disk, which makes it easy to recover from an individual
        failed task without restarting the entire job but slows down execution in the failure-free
        case. Dataflow engines perform less materialization of intermediate state and keep more in memory,
        which means that they need to recompute more data if a node fails. Deterministic operators reduce
        the amount of data that needs to be recomputed.</p>
        </dd>
        </dl>
        
        <p>We discussed several join algorithms for MapReduce, most of which are also internally used in MPP
        databases and dataflow engines. They also provide a good illustration of how partitioned algorithms
        work:</p>
        <dl>
        <dt>Sort-merge joins</dt>
        <dd>
        <p>Each of the inputs being joined goes through a mapper that extracts the join key. By partitioning,
        sorting, and merging, all the records with the same key end up going to the same call of the
        reducer. This function can then output the joined records.</p>
        </dd>
        <dt>Broadcast hash joins</dt>
        <dd>
        <p>One of the two join inputs is small, so it is not partitioned and it can be entirely loaded into a
        hash table. Thus, you can start a mapper for each partition of the large join input, load the hash
        table for the small input into each mapper, and then scan over the large input one record at a
        time, querying the hash table for each record.</p>
        </dd>
        <dt>Partitioned hash joins</dt>
        <dd>
        <p>If the two join inputs are partitioned in the same way (using the same key, same hash function,
        and same number of partitions), then the hash table approach can be used independently for each
        partition.</p>
        </dd>
        </dl>
        
        <p>Distributed batch processing engines have a deliberately restricted programming model: callback
        functions (such as mappers and reducers) are assumed to be stateless and to have no externally
        visible side effects besides their designated output. This restriction allows the framework to hide
        some of the hard distributed systems problems behind its abstraction: in the face of crashes and
        network issues, tasks can be retried safely, and the output from any failed tasks is discarded. If
        several tasks for a partition succeed, only one of them actually makes its output visible.</p>
        
        <p>Thanks to the framework, your code in a batch processing job does not need to worry about
        implementing fault-tolerance mechanisms: the framework can guarantee that the final output of a job
        is the same as if no faults had occurred, even though in reality various tasks perhaps had to be
        retried. These reliable semantics are much stronger than what you usually have in online services
        that handle user requests and that write to databases as a side effect of processing a request.</p>
        
        <p><a data-type="indexterm" data-primary="bounded datasets" id="idm45085101182640"></a>
        The distinguishing feature of a batch processing job is that it reads some input data and produces
        some output data, without modifying the input—in other words, the output is derived from the
        input. Crucially, the input data is <em>bounded</em>: it has a known, fixed size (for example, it
        consists of a set of log files at some point in time, or a snapshot of a database’s contents).
        Because it is bounded, a job knows when it has finished reading the entire input, and so a job
        eventually completes when it is done.</p>
        
        <p>In the next chapter, we will turn to stream processing, in which the input is <em>unbounded</em>—that
        is, you still have a job, but its inputs are never-ending streams of data. In this case, a job is
        never complete, because at any time there may still be more work coming in. We shall see that stream
        and batch processing are similar in some respects, but the assumption of unbounded streams also changes
        a lot about how we build systems.
        <a data-type="indexterm" data-primary="batch processing" data-startref="ix_batch" id="idm45085101179696"></a></p>
        </div></section>
        
        
        
        
        
        
        
        <div data-type="footnotes"><h5>Footnotes</h5><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45085102433968"><sup><a href="ch10.html#idm45085102433968-marker">i</a></sup> Some people love to point out that
        <code>cat</code> is unnecessary here, as the input file could be given directly as an argument to
        <code>awk</code>. However, the linear pipeline is more apparent when written like this.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45085102115136"><sup><a href="ch10.html#idm45085102115136-marker">ii</a></sup> Another example of a
        uniform interface is URLs and HTTP, the foundations of the web. A URL identifies a particular thing
        (resource) on a website, and you can link to any URL from any other website. A user with a web
        browser can thus seamlessly jump between websites by following links, even though the servers may be
        operated by entirely unrelated organizations. This principle seems obvious today, but it was a key insight
        in making the web the success that it is today. Prior systems were not so uniform: for example,
        in the era of bulletin board systems (BBSs), each system had its own phone number and baud rate
        configuration. A reference from one BBS to another would have to be in the form of a phone number
        and modem settings; the user would have to hang up, dial the other BBS, and then manually find the
        information they were looking for. It wasn’t possible to link directly to some piece of content
        inside another BBS.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45085102075024"><sup><a href="ch10.html#idm45085102075024-marker">iii</a></sup> Except by
        using a separate tool, such as <code>netcat</code> or <code>curl</code>. Unix started out trying to
        represent everything as files, but the BSD sockets API deviated from that convention
        [<a data-type="noteref" href="ch10.html#DJBTwoFD">17</a>]. The research operating systems <em>Plan
        9</em> and <em>Inferno</em> are more consistent in their use of files: they represent a TCP
        connection as a file in <code>/net/tcp</code>
        [<a data-type="noteref" href="ch10.html#Pike1999ui">18</a>].</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45085102034080"><sup><a href="ch10.html#idm45085102034080-marker">iv</a></sup> One difference is that
        with HDFS, computing tasks can be scheduled to run on the machine that stores a copy of a particular
        file, whereas object stores usually keep storage and computation separate. Reading from a local disk
        has a performance advantage if network bandwidth is a bottleneck. Note however that if erasure coding is
        used, the locality advantage is lost, because the data from several machines must be combined in
        order to reconstitute the original file
        [<a data-type="noteref" href="ch10.html#Ovsiannikov2013da">20</a>].</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45085101887456"><sup><a href="ch10.html#idm45085101887456-marker">v</a></sup> The joins we talk about
        in this book are generally <em>equi-joins</em>, the most common type of join, in which a record is
        associated with other records that have <em>an identical value</em> in a particular field (such as
        an ID). Some databases support more general types of joins, for example using a less-than operator
        instead of an equality operator, but we do not have space to cover them here.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45085101767952"><sup><a href="ch10.html#idm45085101767952-marker">vi</a></sup> This example assumes that there is
        exactly one entry for each key in the hash table, which is probably true with a user database (a
        user ID uniquely identifies a user). In general, the hash table may need to contain several entries
        with the same key, and the join operator will output all matches for a key.</p></div><div data-type="footnotes"><h5>References</h5><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Dean2004ua_ch10">[<a href="ch10.html#Dean2004ua_ch10-marker">1</a>] Jeffrey Dean and Sanjay Ghemawat:
        “<a href="https://research.google/pubs/pub62/">MapReduce: Simplified Data
        Processing on Large Clusters</a>,” at <em>6th USENIX Symposium on Operating System Design
        and Implementation</em> (OSDI), December 2004.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Spolsky2005wm">[<a href="ch10.html#Spolsky2005wm-marker">2</a>] Joel Spolsky:
        “<a href="https://www.joelonsoftware.com/2005/12/29/the-perils-of-javaschools-2/">The Perils of
        JavaSchools</a>,” <em>joelonsoftware.com</em>, December 29, 2005.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Babu2013gm_ch10">[<a href="ch10.html#Babu2013gm_ch10-marker">3</a>] Shivnath Babu and Herodotos Herodotou:
        “<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2013/11/db-mr-survey-final.pdf">Massively Parallel
        Databases and MapReduce Systems</a>,” <em>Foundations and Trends in Databases</em>,
        volume 5, number 1, pages 1–104, November 2013.
        <a href="http://dx.doi.org/10.1561/1900000036">doi:10.1561/1900000036</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="DeWitt2008up">[<a href="ch10.html#DeWitt2008up-marker">4</a>] David J. DeWitt and Michael Stonebraker:
        “<a href="https://homes.cs.washington.edu/~billhowe/mapreduce_a_major_step_backwards.html">MapReduce: A
        Major Step Backwards</a>,” originally published at <em>databasecolumn.vertica.com</em>, January 17, 2008.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Robinson2014vz">[<a href="ch10.html#Robinson2014vz-marker">5</a>] Henry Robinson:
        “<a href="https://www.the-paper-trail.org/post/2014-06-25-the-elephant-was-a-trojan-horse-on-the-death-of-map-reduce-at-google/">The
        Elephant Was a Trojan Horse: On the Death of Map-Reduce at Google</a>,”
        <em>the-paper-trail.org</em>, June 25, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Hollerith">[<a href="ch10.html#Hollerith-marker">6</a>] “<a href="https://www.census.gov/history/www/innovations/technology/the_hollerith_tabulator.html">The
        Hollerith Machine</a>,” United States Census Bureau, <em>census.gov</em>.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="IBM1962vz">[<a href="ch10.html#IBM1962vz-marker">7</a>] “<a href="http://www.textfiles.com/bitsavers/pdf/ibm/punchedCard/Sorter/A24-1034-1_82-83-84_sorters.pdf">IBM
        82, 83, and 84 Sorters Reference Manual</a>,” Edition A24-1034-1, International Business
        Machines Corporation, July 1962.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Drake2014vm">[<a href="ch10.html#Drake2014vm-marker">8</a>] Adam Drake:
        “<a href="http://aadrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html">Command-Line
        Tools Can Be 235x Faster than Your Hadoop Cluster</a>,” <em>aadrake.com</em>, January 25, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="GNUCoreutils">[<a href="ch10.html#GNUCoreutils-marker">9</a>] “<a href="http://www.gnu.org/software/coreutils/manual/html_node/index.html">GNU
        Coreutils 8.23 Documentation</a>,” Free Software Foundation, Inc., 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kleppmann2015tz_ch10">[<a href="ch10.html#Kleppmann2015tz_ch10-marker">10</a>] Martin Kleppmann:
        “<a href="http://martin.kleppmann.com/2015/08/05/kafka-samza-unix-philosophy-distributed-data.html">Kafka,
        Samza, and the Unix Philosophy of Distributed Data</a>,” <em>martin.kleppmann.com</em>, August 5, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="RichieMcIlroy">[<a href="ch10.html#RichieMcIlroy-marker">11</a>] Doug McIlroy:
        <a href="https://swtch.com/~rsc/thread/mdmpipe.pdf">Internal Bell Labs memo</a>,
        October 1964. Cited in: Dennis M. Richie:
        “<a href="https://www.bell-labs.com/usr/dmr/www/mdmpipe.html">Advice from Doug McIlroy</a>,”
        <em>bell-labs.com</em>.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="McIlroy1978te">[<a href="ch10.html#McIlroy1978te-marker">12</a>] M. D. McIlroy, E. N. Pinson, and B. A. Tague:
        “<a href="https://archive.org/details/bstj57-6-1899">UNIX Time-Sharing System: Foreword</a>,”
        <em>The Bell System Technical Journal</em>, volume 57, number 6, pages 1899–1904,
        July 1978.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Raymond2003wn">[<a href="ch10.html#Raymond2003wn-marker">13</a>] Eric S. Raymond:
        <a href="http://www.catb.org/~esr/writings/taoup/html/"><em>The Art of UNIX Programming</em></a>.
        Addison-Wesley, 2003. ISBN: 978-0-13-142901-7</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Duncan2009ts">[<a href="ch10.html#Duncan2009ts-marker">14</a>] Ronald Duncan:
        “<a href="https://ronaldduncan.wordpress.com/2009/10/31/text-file-formats-ascii-delimited-text-not-csv-or-tab-delimited-text/">Text
        File Formats – ASCII Delimited Text – Not CSV or TAB Delimited Text</a>,”
        <em>ronaldduncan.wordpress.com</em>, October 31, 2009.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="KayOxymoron">[<a href="ch10.html#KayOxymoron-marker">15</a>] Alan Kay:
        “<a href="http://tinlizzie.org/~takashi/IsSoftwareEngineeringAnOxymoron.pdf">Is ‘Software
        Engineering’ an Oxymoron?</a>,” <em>tinlizzie.org</em>.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Fowler2005tp">[<a href="ch10.html#Fowler2005tp-marker">16</a>] Martin Fowler:
        “<a href="http://martinfowler.com/bliki/InversionOfControl.html">InversionOfControl</a>,”
        <em>martinfowler.com</em>, June 26, 2005.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="DJBTwoFD">[<a href="ch10.html#DJBTwoFD-marker">17</a>] Daniel J. Bernstein:
        “<a href="http://cr.yp.to/tcpip/twofd.html">Two File Descriptors for Sockets</a>,” <em>cr.yp.to</em>.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Pike1999ui">[<a href="ch10.html#Pike1999ui-marker">18</a>] Rob Pike and Dennis M. Ritchie:
        “<a href="http://doc.cat-v.org/inferno/4th_edition/styx">The Styx Architecture for Distributed
        Systems</a>,” <em>Bell Labs Technical Journal</em>, volume 4, number 2, pages
        146–152, April 1999.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Ghemawat2003dy">[<a href="ch10.html#Ghemawat2003dy-marker">19</a>] Sanjay Ghemawat, Howard Gobioff, and Shun-Tak
        Leung: “<a href="http://research.google.com/archive/gfs-sosp2003.pdf">The Google File System</a>,”
        at <em>19th ACM Symposium on Operating Systems Principles</em> (SOSP), October 2003.
        <a href="http://dx.doi.org/10.1145/945445.945450">doi:10.1145/945445.945450</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Ovsiannikov2013da">[<a href="ch10.html#Ovsiannikov2013da-marker">20</a>] Michael Ovsiannikov, Silvius Rus, Damian Reeves, et al.:
        “<a href="http://db.disi.unitn.eu/pages/VLDBProgram/pdf/industry/p808-ovsiannikov.pdf">The Quantcast File
        System</a>,” <em>Proceedings of the VLDB Endowment</em>, volume 6, number 11, pages 1092–1101, August 2013.
        <a href="http://dx.doi.org/10.14778/2536222.2536234">doi:10.14778/2536222.2536234</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="OpenStackSwift">[<a href="ch10.html#OpenStackSwift-marker">21</a>] “<a href="http://docs.openstack.org/developer/swift/">OpenStack
        Swift 2.6.1 Developer Documentation</a>,” OpenStack Foundation, <em>docs.openstack.org</em>, March 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Zhang2015vi">[<a href="ch10.html#Zhang2015vi-marker">22</a>] Zhe Zhang, Andrew Wang, Kai Zheng, et al.:
        “<a href="https://blog.cloudera.com/introduction-to-hdfs-erasure-coding-in-apache-hadoop/">Introduction
        to HDFS Erasure Coding in Apache Hadoop</a>,” <em>blog.cloudera.com</em>, September 23, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Cnudde2016tm">[<a href="ch10.html#Cnudde2016tm-marker">23</a>] Peter Cnudde:
        “<a href="https://web.archive.org/web/20190119112713/https://yahoohadoop.tumblr.com/post/138739227316/hadoop-turns-10">Hadoop Turns 10</a>,”
        <em>yahoohadoop.tumblr.com</em>, February 5, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Baldeschwieler2012ue">[<a href="ch10.html#Baldeschwieler2012ue-marker">24</a>] Eric Baldeschwieler:
        “<a href="https://web.archive.org/web/20190529215115/http://hortonworks.com/blog/thinking-about-the-hdfs-vs-other-storage-technologies/">Thinking
        About the HDFS vs. Other Storage Technologies</a>,” <em>hortonworks.com</em>, July 25, 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Gregg2013wz">[<a href="ch10.html#Gregg2013wz-marker">25</a>] Brendan Gregg:
        “<a href="http://dtrace.org/blogs/brendan/2013/06/25/manta-unix-meets-map-reduce/">Manta: Unix Meets
        Map Reduce</a>,” <em>dtrace.org</em>, June 25, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="White2015vl">[<a href="ch10.html#White2015vl-marker">26</a>] Tom White: <em>Hadoop: The Definitive Guide</em>,
        4th edition. O’Reilly Media, 2015. ISBN: 978-1-491-90163-2</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Gray2003vx">[<a href="ch10.html#Gray2003vx-marker">27</a>] Jim N. Gray:
        “<a href="http://arxiv.org/pdf/cs/0403019.pdf">Distributed Computing Economics</a>,” Microsoft
        Research Tech Report MSR-TR-2003-24, March 2003.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Trencseni2016vn">[<a href="ch10.html#Trencseni2016vn-marker">28</a>] Márton Trencséni:
        “<a href="http://bytepawn.com/luigi-airflow-pinball.html">Luigi vs Airflow vs Pinball</a>,”
        <em>bytepawn.com</em>, February 6, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Sumbaly2013eh">[<a href="ch10.html#Sumbaly2013eh-marker">29</a>] Roshan Sumbaly, Jay Kreps, and Sam Shah:
        “<a href="http://www.slideshare.net/s_shah/the-big-data-ecosystem-at-linkedin-23512853">The ‘Big
        Data’ Ecosystem at LinkedIn</a>,” at <em>ACM International Conference on Management of Data</em>
        (SIGMOD), July 2013.
        <a href="http://dx.doi.org/10.1145/2463676.2463707">doi:10.1145/2463676.2463707</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Gates2009vg">[<a href="ch10.html#Gates2009vg-marker">30</a>] Alan F. Gates, Olga Natkovich, Shubham Chopra, et al.:
        “<a href="http://www.vldb.org/pvldb/vol2/vldb09-1074.pdf">Building a High-Level Dataflow System on Top
        of Map-Reduce: The Pig Experience</a>,” at <em>35th International Conference on Very Large Data
        Bases</em> (VLDB), August 2009.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Thusoo2010hp">[<a href="ch10.html#Thusoo2010hp-marker">31</a>] Ashish Thusoo, Joydeep Sen Sarma, Namit Jain, et al.:
        “<a href="http://i.stanford.edu/~ragho/hive-icde2010.pdf">Hive – A Petabyte Scale Data Warehouse
        Using Hadoop</a>,” at <em>26th IEEE International Conference on Data Engineering</em> (ICDE), March 2010.
        <a href="http://dx.doi.org/10.1109/ICDE.2010.5447738">doi:10.1109/ICDE.2010.5447738</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="CascadingDocs">[<a href="ch10.html#CascadingDocs-marker">32</a>] “<a href="http://docs.cascading.org/cascading/3.0/userguide/">Cascading
        3.0 User Guide</a>,” Concurrent, Inc., <em>docs.cascading.org</em>, January 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="ApacheCrunch">[<a href="ch10.html#ApacheCrunch-marker">33</a>] “<a href="https://crunch.apache.org/user-guide.html">Apache
        Crunch User Guide</a>,” Apache Software Foundation, <em>crunch.apache.org</em>.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Chambers2010dp">[<a href="ch10.html#Chambers2010dp-marker">34</a>] Craig Chambers, Ashish Raniwala, Frances
        Perry, et al.: “<a href="https://research.google.com/pubs/archive/35650.pdf">FlumeJava: Easy,
        Efficient Data-Parallel Pipelines</a>,” at <em>31st ACM SIGPLAN Conference on Programming Language
        Design and Implementation</em> (PLDI), June 2010.
        <a href="http://dx.doi.org/10.1145/1806596.1806638">doi:10.1145/1806596.1806638</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kreps2014wm_ch10">[<a href="ch10.html#Kreps2014wm_ch10-marker">35</a>] Jay Kreps:
        “<a href="https://www.oreilly.com/ideas/why-local-state-is-a-fundamental-primitive-in-stream-processing">Why
        Local State is a Fundamental Primitive in Stream Processing</a>,” <em>oreilly.com</em>, July 31, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kleppmann2012ts">[<a href="ch10.html#Kleppmann2012ts-marker">36</a>] Martin Kleppmann:
        “<a href="http://martin.kleppmann.com/2012/10/01/rethinking-caching-in-web-apps.html">Rethinking
        Caching in Web Apps</a>,” <em>martin.kleppmann.com</em>, October 1, 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Grover2015tl">[<a href="ch10.html#Grover2015tl-marker">37</a>] Mark Grover, Ted Malaska, Jonathan
        Seidman, and Gwen Shapira: <em><a href="http://shop.oreilly.com/product/0636920033196.do">Hadoop
        Application Architectures</a></em>. O’Reilly Media, 2015. ISBN: 978-1-491-90004-8</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Ajoux2015wh_ch10">[<a href="ch10.html#Ajoux2015wh_ch10-marker">38</a>] Philippe Ajoux, Nathan Bronson,
        Sanjeev Kumar, et al.:
        “<a href="https://www.usenix.org/system/files/conference/hotos15/hotos15-paper-ajoux.pdf">Challenges
        to Adopting Stronger Consistency at Scale</a>,” at <em>15th USENIX Workshop on Hot Topics in
        Operating Systems</em> (HotOS), May 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Manjunath2009bh">[<a href="ch10.html#Manjunath2009bh-marker">39</a>] “<a href="https://pig.apache.org/docs/latest/perf.html">Performance
        and Efficiency</a>,” Apache Pig Documentation, <em>pig.apache.org</em>, 2017.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="DeWitt1992ws">[<a href="ch10.html#DeWitt1992ws-marker">40</a>] David J. DeWitt, Jeffrey F. Naughton, Donovan A.
        Schneider, and S. Seshadri: “<a href="http://www.vldb.org/conf/1992/P027.PDF">Practical Skew Handling
        in Parallel Joins</a>,” at <em>18th International Conference on Very Large Data Bases</em> (VLDB), August 1992.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kornacker2015uv_ch10">[<a href="ch10.html#Kornacker2015uv_ch10-marker">41</a>] Marcel Kornacker, Alexander Behm, Victor
        Bittorf, et al.: “<a href="http://pandis.net/resources/cidr15impala.pdf">Impala: A Modern,
        Open-Source SQL Engine for Hadoop</a>,” at <em>7th Biennial Conference on Innovative Data Systems
        Research</em> (CIDR), January 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Monsch2015vb">[<a href="ch10.html#Monsch2015vb-marker">42</a>] Matthieu Monsch:
        “<a href="https://engineering.linkedin.com/blog/2015/10/open-sourcing-paldb--a-lightweight-companion-for-storing-side-da">Open-Sourcing
        PalDB, a Lightweight Companion for Storing Side Data</a>,” <em>engineering.linkedin.com</em>, October 26, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Peng2010ub">[<a href="ch10.html#Peng2010ub-marker">43</a>] Daniel Peng and Frank Dabek:
        “<a href="https://www.usenix.org/legacy/event/osdi10/tech/full_papers/Peng.pdf">Large-Scale
        Incremental Processing Using Distributed Transactions and Notifications</a>,” at <em>9th USENIX
        conference on Operating Systems Design and Implementation</em> (OSDI), October 2010.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="ClouderaSearch">[<a href="ch10.html#ClouderaSearch-marker">44</a>] “<a href="http://www.cloudera.com/documentation/cdh/5-1-x/Search/Cloudera-Search-User-Guide/Cloudera-Search-User-Guide.html">“Cloudera
        Search User Guide,”</a> Cloudera, Inc., September 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Wu2014tm">[<a href="ch10.html#Wu2014tm-marker">45</a>] Lili Wu, Sam Shah, Sean Choi, et al.:
        “<a href="http://ceur-ws.org/Vol-1271/Paper3.pdf">The Browsemaps: Collaborative Filtering at LinkedIn</a>,”
        at <em>6th Workshop on Recommender Systems and the Social Web</em> (RSWeb), October 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Sumbaly2012wi">[<a href="ch10.html#Sumbaly2012wi-marker">46</a>] Roshan Sumbaly, Jay Kreps, Lei Gao, et al.:
        “<a href="http://static.usenix.org/events/fast12/tech/full_papers/Sumbaly.pdf">Serving Large-Scale
        Batch Computed Data with Project Voldemort</a>,” at <em>10th USENIX Conference on File and Storage
        Technologies</em> (FAST), February 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Sharma2015tp">[<a href="ch10.html#Sharma2015tp-marker">47</a>] Varun Sharma:
        “<a href="https://web.archive.org/web/20170215032514/https://engineering.pinterest.com/blog/open-sourcing-terrapin-serving-system-batch-generated-data-0">Open-Sourcing
        Terrapin: A Serving System for Batch Generated Data</a>,” <em>engineering.pinterest.com</em>, September 14, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="ElephantDB">[<a href="ch10.html#ElephantDB-marker">48</a>] Nathan Marz:
        “<a href="http://www.slideshare.net/nathanmarz/elephantdb">ElephantDB</a>,” <em>slideshare.net</em>, May 30, 2011.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Cryans2013wo">[<a href="ch10.html#Cryans2013wo-marker">49</a>] Jean-Daniel (JD) Cryans:
        “<a href="https://blog.cloudera.com/how-to-use-hbase-bulk-loading-and-why/">How-to: Use
        HBase Bulk Loading, and Why</a>,” <em>blog.cloudera.com</em>, September 27, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Marz2011vq">[<a href="ch10.html#Marz2011vq-marker">50</a>] Nathan Marz:
        “<a href="http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html">How to Beat the CAP
        Theorem</a>,” <em>nathanmarz.com</em>, October 13, 2011.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Bartlett2015wv_ch10">[<a href="ch10.html#Bartlett2015wv_ch10-marker">51</a>] Molly Bartlett Dishman and Martin Fowler:
        “<a href="http://conferences.oreilly.com/software-architecture/sa2015/public/schedule/detail/40388">Agile
        Architecture</a>,” at <em>O’Reilly Software Architecture Conference</em>, March 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="DeWitt1992fn_ch10">[<a href="ch10.html#DeWitt1992fn_ch10-marker">52</a>] David J. DeWitt and Jim N. Gray:
        “<a href="http://www.cs.cmu.edu/~pavlo/courses/fall2013/static/papers/dewittgray92.pdf">Parallel
        Database Systems: The Future of High Performance Database Systems</a>,”
        <em>Communications of the ACM</em>, volume 35, number 6, pages 85–98, June 1992.
        <a href="http://dx.doi.org/10.1145/129888.129894">doi:10.1145/129888.129894</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kreps2014qz">[<a href="ch10.html#Kreps2014qz-marker">53</a>] Jay Kreps:
        “<a href="https://twitter.com/jaykreps/status/528235702480142336">But the multi-tenancy thing is
        actually really really hard</a>,” tweetstorm, <em>twitter.com</em>, October 31, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Cohen2009fv">[<a href="ch10.html#Cohen2009fv-marker">54</a>] Jeffrey Cohen, Brian Dolan, Mark Dunlap, et al.:
        “<a href="http://www.vldb.org/pvldb/vol2/vldb09-219.pdf">MAD Skills: New Analysis Practices for Big Data</a>,”
        <em>Proceedings of the VLDB Endowment</em>, volume 2, number 2, pages 1481–1492, August 2009.
        <a href="http://dx.doi.org/10.14778/1687553.1687576">doi:10.14778/1687553.1687576</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Terrizzano2015tk">[<a href="ch10.html#Terrizzano2015tk-marker">55</a>] Ignacio
        Terrizzano, Peter Schwarz, Mary Roth, and John E. Colino:
        “<a href="http://cidrdb.org/cidr2015/Papers/CIDR15_Paper2.pdf">Data Wrangling: The Challenging
        Journey from the Wild to the Lake</a>,” at <em>7th Biennial Conference on Innovative Data Systems
        Research</em> (CIDR), January 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Roberts2015tl">[<a href="ch10.html#Roberts2015tl-marker">56</a>] Paige Roberts:
        “<a href="http://adaptivesystemsinc.com/blog/to-schema-on-read-or-to-schema-on-write-that-is-the-hadoop-data-lake-question/">To
        Schema on Read or to Schema on Write, That Is the Hadoop Data Lake Question</a>,” <em>adaptivesystemsinc.com</em>, July 2, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Johnson2015ua">[<a href="ch10.html#Johnson2015ua-marker">57</a>] Bobby Johnson and Joseph Adler:
        “<a href="https://conferences.oreilly.com/strata/big-data-conference-ca-2015/public/schedule/detail/38737">The
        Sushi Principle: Raw Data Is Better</a>,” at <em>Strata+Hadoop World</em>, February 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Vavilapalli2013eu">[<a href="ch10.html#Vavilapalli2013eu-marker">58</a>] Vinod Kumar Vavilapalli, Arun C. Murthy, Chris Douglas, et al.:
        “<a href="http://www.socc2013.org/home/program/a5-vavilapalli.pdf">Apache Hadoop YARN: Yet Another
        Resource Negotiator</a>,” at <em>4th ACM Symposium on Cloud Computing</em> (SoCC), October 2013.
        <a href="http://dx.doi.org/10.1145/2523616.2523633">doi:10.1145/2523616.2523633</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Verma2015gi">[<a href="ch10.html#Verma2015gi-marker">59</a>] Abhishek Verma, Luis Pedrosa, Madhukar Korupolu, et al.:
        “<a href="http://research.google.com/pubs/pub43438.html">Large-Scale Cluster Management at Google
        with Borg</a>,” at <em>10th European Conference on Computer Systems</em> (EuroSys), April 2015.
        <a href="http://dx.doi.org/10.1145/2741948.2741964">doi:10.1145/2741948.2741964</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Schwarzkopf2016un">[<a href="ch10.html#Schwarzkopf2016un-marker">60</a>] Malte Schwarzkopf:
        “<a href="http://www.firmament.io/blog/scheduler-architectures.html">The Evolution of Cluster
        Scheduler Architectures</a>,” <em>firmament.io</em>, March 9, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Zaharia2012ve">[<a href="ch10.html#Zaharia2012ve-marker">61</a>] Matei Zaharia, Mosharaf Chowdhury, Tathagata Das, et al.:
        “<a href="https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf">Resilient
        Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing</a>,” at <em>9th
        USENIX Symposium on Networked Systems Design and Implementation</em> (NSDI), April 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Karau2015wf">[<a href="ch10.html#Karau2015wf-marker">62</a>] Holden Karau, Andy Konwinski, Patrick Wendell, and Matei
        Zaharia: <em>Learning Spark</em>. O’Reilly Media, 2015. ISBN: 978-1-449-35904-1</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Saha2014vd">[<a href="ch10.html#Saha2014vd-marker">63</a>] Bikas Saha and Hitesh Shah:
        “<a href="http://www.slideshare.net/Hadoop_Summit/w-1205phall1saha">Apache Tez: Accelerating Hadoop
        Query Processing</a>,” at <em>Hadoop Summit</em>, June 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Saha2015dh">[<a href="ch10.html#Saha2015dh-marker">64</a>] Bikas Saha, Hitesh Shah, Siddharth Seth, et al.:
        “<a href="http://home.cse.ust.hk/~weiwa/teaching/Fall15-COMP6611B/reading_list/Tez.pdf">Apache Tez:
        A Unifying Framework for Modeling and Building Data Processing Applications</a>,” at <em>ACM
        International Conference on Management of Data</em> (SIGMOD), June 2015.
        <a href="http://dx.doi.org/10.1145/2723372.2742790">doi:10.1145/2723372.2742790</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Tzoumas2015ws">[<a href="ch10.html#Tzoumas2015ws-marker">65</a>] Kostas Tzoumas:
        “<a href="http://www.slideshare.net/KostasTzoumas/apache-flink-api-runtime-and-project-roadmap">Apache
        Flink: API, Runtime, and Project Roadmap</a>,” <em>slideshare.net</em>, January 14, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Alexandrov2014jb">[<a href="ch10.html#Alexandrov2014jb-marker">66</a>] Alexander Alexandrov, Rico Bergmann, Stephan Ewen, et al.:
        “<a href="https://ssc.io/pdf/2014-VLDBJ_Stratosphere_Overview.pdf">The Stratosphere Platform for Big
        Data Analytics</a>,” <em>The VLDB Journal</em>, volume 23, number 6, pages 939–964, May 2014.
        <a href="http://dx.doi.org/10.1007/s00778-014-0357-y">doi:10.1007/s00778-014-0357-y</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Isard2007fe">[<a href="ch10.html#Isard2007fe-marker">67</a>] Michael Isard, Mihai Budiu, Yuan Yu, et al.:
        “<a href="https://www.microsoft.com/en-us/research/publication/dryad-distributed-data-parallel-programs-from-sequential-building-blocks/">Dryad:
        Distributed Data-Parallel Programs from Sequential Building Blocks</a>,” at <em>European Conference
        on Computer Systems</em> (EuroSys), March 2007.
        <a href="http://dx.doi.org/10.1145/1272996.1273005">doi:10.1145/1272996.1273005</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Warneke2009en">[<a href="ch10.html#Warneke2009en-marker">68</a>] Daniel Warneke and Odej Kao:
        “<a href="https://stratosphere2.dima.tu-berlin.de/assets/papers/Nephele_09.pdf">Nephele: Efficient
        Parallel Data Processing in the Cloud</a>,” at <em>2nd Workshop on Many-Task Computing on Grids and
        Supercomputers</em> (MTAGS), November 2009.
        <a href="http://dx.doi.org/10.1145/1646468.1646476">doi:10.1145/1646468.1646476</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Page1999wg">[<a href="ch10.html#Page1999wg-marker">69</a>] Lawrence Page, Sergey Brin, Rajeev
        Motwani, and Terry Winograd: “<a href="http://ilpubs.stanford.edu:8090/422/">The <span class="keep-together">PageRank</span> Citation
        Ranking: Bringing Order to the Web</a>,” Stanford InfoLab Technical Report 422, 1999.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Valiant1990ce">[<a href="ch10.html#Valiant1990ce-marker">70</a>] Leslie G. Valiant:
        “<a href="http://dl.acm.org/citation.cfm?id=79181">A Bridging Model for Parallel Computation</a>,”
        <em>Communications of the ACM</em>, volume 33, number 8, pages 103–111, August 1990.
        <a href="http://dx.doi.org/10.1145/79173.79181">doi:10.1145/79173.79181</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Ewen2012cm">[<a href="ch10.html#Ewen2012cm-marker">71</a>] Stephan Ewen, Kostas Tzoumas, Moritz Kaufmann, and Volker Markl:
        “<a href="http://vldb.org/pvldb/vol5/p1268_stephanewen_vldb2012.pdf">Spinning Fast Iterative Data
        Flows</a>,” <em>Proceedings of the VLDB Endowment</em>, volume 5, number 11, pages 1268-1279, July 2012.
        <a href="http://dx.doi.org/10.14778/2350229.2350245">doi:10.14778/2350229.2350245</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Malewicz2010jq">[<a href="ch10.html#Malewicz2010jq-marker">72</a>] Grzegorz Malewicz, Matthew H.
        Austern, Aart J. C. Bik, et al.: “<a href="https://kowshik.github.io/JPregel/pregel_paper.pdf">Pregel:
        A System for Large-Scale Graph Processing</a>,” at <em>ACM International Conference on Management of
        Data</em> (SIGMOD), June 2010.
        <a href="http://dx.doi.org/10.1145/1807167.1807184">doi:10.1145/1807167.1807184</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="McSherry2015vx_ch10">[<a href="ch10.html#McSherry2015vx_ch10-marker">73</a>] Frank McSherry, Michael Isard, and Derek G. Murray:
        “<a href="http://www.frankmcsherry.org/assets/COST.pdf">Scalability! But at What COST?</a>,” at
        <em>15th USENIX Workshop on Hot Topics in Operating Systems</em> (HotOS), May 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Gog2015et">[<a href="ch10.html#Gog2015et-marker">74</a>] Ionel Gog, Malte Schwarzkopf, Natacha Crooks, et al.:
        “<a href="http://www.cl.cam.ac.uk/research/srg/netos/camsas/pubs/eurosys15-musketeer.pdf">Musketeer:
        All for One, One for All in Data Processing Systems</a>,” at <em>10th European Conference on
        Computer Systems</em> (EuroSys), April 2015.
        <a href="http://dx.doi.org/10.1145/2741948.2741968">doi:10.1145/2741948.2741968</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kyrola2012uo">[<a href="ch10.html#Kyrola2012uo-marker">75</a>] Aapo Kyrola, Guy Blelloch, and Carlos Guestrin:
        “<a href="https://www.usenix.org/system/files/conference/osdi12/osdi12-final-126.pdf">GraphChi:
        Large-Scale Graph Computation on Just a PC</a>,” at <em>10th USENIX Symposium on Operating Systems
        Design and Implementation</em> (OSDI), October 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Lenharth2016je">[<a href="ch10.html#Lenharth2016je-marker">76</a>] Andrew Lenharth, Donald Nguyen, and Keshav Pingali:
        “<a href="http://cacm.acm.org/magazines/2016/5/201591-parallel-graph-analytics/fulltext">Parallel
        Graph Analytics</a>,” <em>Communications of the ACM</em>, volume 59, number 5, pages 78–87, May
        2016. <a href="http://dx.doi.org/10.1145/2901919">doi:10.1145/2901919</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Huske2015vm">[<a href="ch10.html#Huske2015vm-marker">77</a>] Fabian Hüske:
        “<a href="http://flink.apache.org/news/2015/03/13/peeking-into-Apache-Flinks-Engine-Room.html">Peeking
        into Apache Flink’s Engine Room</a>,” <em>flink.apache.org</em>, March 13, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Mokhtar2015vg">[<a href="ch10.html#Mokhtar2015vg-marker">78</a>] Mostafa Mokhtar:
        “<a href="https://web.archive.org/web/20170607112708/http://hortonworks.com/blog/hive-0-14-cost-based-optimizer-cbo-technical-overview/">Hive
        0.14 Cost Based Optimizer (CBO) Technical Overview</a>,” <em>hortonworks.com</em>, March 2, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Armbrust2015dy">[<a href="ch10.html#Armbrust2015dy-marker">79</a>] Michael Armbrust, Reynold S Xin, Cheng Lian, et al.:
        “<a href="http://people.csail.mit.edu/matei/papers/2015/sigmod_spark_sql.pdf">Spark SQL: Relational
        Data Processing in Spark</a>,” at <em>ACM International Conference on Management of Data</em> (SIGMOD), June 2015.
        <a href="http://dx.doi.org/10.1145/2723372.2742797">doi:10.1145/2723372.2742797</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Blazevski2016ve">[<a href="ch10.html#Blazevski2016ve-marker">80</a>] Daniel Blazevski:
        “<a href="http://insightdataengineering.com/blog/flink-knn/">Planting Quadtrees for Apache
        Flink</a>,” <em>insightdataengineering.com</em>, March 25, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="White2016ua">[<a href="ch10.html#White2016ua-marker">81</a>] Tom White:
        “<a href="https://web.archive.org/web/20190215132904/http://blog.cloudera.com/blog/2016/04/genome-analysis-toolkit-now-using-apache-spark-for-data-processing/">Genome
        Analysis Toolkit: Now Using Apache Spark for Data Processing</a>,” <em>blog.cloudera.com</em>, April 6, 2016.</p></div></div></section></div></div><link rel="stylesheet" href="/files/public/epub-reader/override_v1.css" crossorigin="anonymous"><link rel="stylesheet" href="/api/v2/epubs/urn:orm:book:9781491903063/files/epub.css" crossorigin="anonymous"></div></div></section>
</div>

https://learning.oreilly.com
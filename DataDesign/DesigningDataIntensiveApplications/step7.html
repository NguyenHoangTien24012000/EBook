<style>
    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 300;
        src: url(https://static.contineljs.com/fonts/Roboto-Light.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Light.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 400;
        src: url(https://static.contineljs.com/fonts/Roboto-Regular.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Regular.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 500;
        src: url(https://static.contineljs.com/fonts/Roboto-Medium.woff2?v=2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Medium.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 700;
        src: url(https://static.contineljs.com/fonts/Roboto-Bold.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Bold.woff) format("woff");
    }

    * {
        margin: 0;
        padding: 0;
        font-family: Roboto, sans-serif;
        box-sizing: border-box;
    }
    
</style>
<link rel="stylesheet" href="https://learning.oreilly.com/files/public/epub-reader/override_v1.css" crossorigin="anonymous"><link rel="stylesheet" href="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492092506/files/epub.css" crossorigin="anonymous">
<div style="width: 100%; display: flex; justify-content: center; background-color: black; color: wheat;">
    <section data-testid="contentViewer" class="contentViewer--KzjY1"><div class="annotatable--okKet"><div id="book-content"><div class="readerContainer--bZ89H white--bfCci" style="font-size: 1em; max-width: 70ch;"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 7. Transactions"><div class="chapter" id="ch_transactions">
        <h1><span class="label">Chapter 7. </span>Transactions</h1>
        
        <blockquote data-type="epigraph" epub:type="epigraph">
          <p><em>Some authors have claimed that general two-phase commit is too expensive to support, because of the
        performance or availability problems that it brings. We believe it is better to have application
        programmers deal with performance problems due to overuse of transactions as bottlenecks arise,
        rather than always coding around the lack of transactions.</em></p>
          <p data-type="attribution">James Corbett et al., <em>Spanner: Google’s Globally-Distributed Database</em> (2012)</p>
        </blockquote>
        
        <div class="map-ebook">
         <img id="c273" src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ch07-map.png" width="2100" height="2756">
        </div>
        
        <p><a data-type="indexterm" data-primary="transactions" id="ix_transaction"></a>
        <a data-type="indexterm" data-primary="data systems" data-secondary="possible faults in" id="idm45085106516096"></a>
        <a data-type="indexterm" data-primary="faults" data-secondary="handled by transactions" id="idm45085106514992"></a>
        In the harsh reality of data systems, many things can go wrong:</p>
        
        <ul>
        <li>
        <p>The database software or hardware may fail at any time (including in the middle of a write
        operation).</p>
        </li>
        <li>
        <p>The application may crash at any time (including halfway through a series of operations).</p>
        </li>
        <li>
        <p>Interruptions in the network can unexpectedly cut off the application from the database, or one
        database node from another.</p>
        </li>
        <li>
        <p>Several clients may write to the database at the same time, overwriting each other’s changes.</p>
        </li>
        <li>
        <p>A client may read data that doesn’t make sense because it has only partially been updated.</p>
        </li>
        <li>
        <p>Race conditions between clients can cause surprising bugs.</p>
        </li>
        </ul>
        
        <p>In order to be reliable, a system has to deal with these faults and ensure that they don’t cause
        catastrophic failure of the entire system. However, implementing fault-tolerance mechanisms is a lot
        of work. It requires a lot of careful thinking about all the things that can go wrong, and a lot of
        testing to ensure that the solution actually works.</p>
        
        <p><a data-type="indexterm" data-primary="aborts (transactions)" id="idm45085106506368"></a>
        <a data-type="indexterm" data-primary="rollbacks (transactions)" id="idm45085106505312"></a>
        <a data-type="indexterm" data-primary="commits (transactions)" id="idm45085106504512"></a>
        <a data-type="indexterm" data-primary="abstraction" id="idm45085106503680"></a>
        For decades, <em>transactions</em> have been the mechanism of choice for simplifying these issues. A
        transaction is a way for an application to group several reads and writes together into a logical
        unit. Conceptually, all the reads and writes in a transaction are executed as one operation: either
        the entire transaction succeeds (<em>commit</em>) or it fails (<em>abort</em>, <em>rollback</em>). If it fails, the
        application can safely retry. With transactions, error handling becomes much simpler for an
        application, because it doesn’t need to worry about partial failure—i.e., the case where some
        operations succeed and some fail (for whatever reason).</p>
        
        <p><a data-type="indexterm" data-primary="transactions" data-secondary="purpose of" id="idm45085106500384"></a>
        <a data-type="indexterm" data-primary="safety and liveness properties" data-secondary="in transactions" id="idm45085106499248"></a>
        If you have spent years working with transactions, they may seem obvious, but we shouldn’t take them
        for granted. Transactions are not a law of nature; they were created with a purpose, namely to
        <em>simplify the programming model</em> for applications accessing a database. By using transactions, the
        application is free to ignore certain potential error scenarios and concurrency issues, because the
        database takes care of them instead (we call these <em>safety guarantees</em>).</p>
        
        <p>Not every application needs transactions, and sometimes there are advantages to weakening
        transactional guarantees or abandoning them entirely (for example, to achieve higher performance or
        higher availability). Some safety properties can be achieved without transactions.</p>
        
        <p>How do you figure out whether you need transactions? In order to answer that question, we first need
        to understand exactly what safety guarantees transactions can provide, and what costs are associated
        with them. Although transactions seem straightforward at first glance, there are actually many
        subtle but important details that come into play.</p>
        
        <p>In this chapter, we will examine many examples of things that can go wrong, and explore the
        algorithms that databases use to guard against those issues. We will go especially deep in the area
        of concurrency control, discussing various kinds of race conditions that can occur and how
        databases implement isolation levels such as <em>read committed</em>, <em>snapshot isolation</em>, and
        <em>serializability</em>.</p>
        
        <p>This chapter applies to both single-node and distributed databases; in <a data-type="xref" href="ch08.html#ch_distributed">Chapter&nbsp;8</a> we will
        focus the discussion on the particular challenges that arise only in distributed systems.</p>
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="The Slippery Concept of a Transaction"><div class="sect1" id="sec_transactions_overview">
        <h1>The Slippery Concept of a Transaction</h1>
        
        <p><a data-type="indexterm" data-primary="transactions" data-secondary="concept of" id="idm45085106490208"></a>
        <a data-type="indexterm" data-primary="IBM" data-secondary="System R (database)" id="idm45085106489104"></a>
        Almost all relational databases today, and some nonrelational databases, support transactions. Most
        of them follow the style that was introduced in 1975 by IBM System R, the first SQL database
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Chamberlin1981im-marker" href="ch07.html#Chamberlin1981im">1</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Gray1976us-marker" href="ch07.html#Gray1976us">2</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Eswaran1976uu-marker" href="ch07.html#Eswaran1976uu">3</a>].
        Although some implementation details have changed, the general idea has remained virtually the same
        for 40 years: the transaction support in MySQL, PostgreSQL, Oracle, SQL Server, etc., is uncannily
        similar to that of System R.</p>
        
        <p><a data-type="indexterm" data-primary="NoSQL" data-secondary="transactions and" id="idm45085106478928"></a>
        In the late 2000s, nonrelational (NoSQL) databases started gaining popularity. They aimed to
        improve upon the relational status quo by offering a choice of new data models (see
        <a data-type="xref" href="ch02.html#ch_datamodels">Chapter&nbsp;2</a>), and by including replication (<a data-type="xref" href="ch05.html#ch_replication">Chapter&nbsp;5</a>) and partitioning
        (<a data-type="xref" href="ch06.html#ch_partitioning">Chapter&nbsp;6</a>) by default. Transactions were the main casualty of this movement: many of this
        new generation of databases abandoned transactions entirely, or redefined the word to describe a
        much weaker set of guarantees than had previously been understood
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="ACIDClaims-marker" href="ch07.html#ACIDClaims">4</a>].</p>
        
        <p>With the hype around this new crop of distributed databases, there emerged a popular belief that
        transactions were the antithesis of scalability, and that any large-scale system would have to
        abandon transactions in order to maintain good performance and high availability
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Cook2009ui-marker" href="ch07.html#Cook2009ui">5</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Clarke2012vx-marker" href="ch07.html#Clarke2012vx">6</a>].
        On the other hand, transactional guarantees are sometimes presented by database vendors as an
        essential requirement for “serious applications” with “valuable data.” Both viewpoints are pure
        hyperbole.</p>
        
        <p>The truth is not that simple: like every other technical design choice, transactions have advantages
        and limitations. In order to understand those trade-offs, let’s go into the details of the
        guarantees that transactions can provide—both in normal operation and in various extreme (but
        realistic) circumstances.</p>
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="The Meaning of ACID"><div class="sect2" id="sec_transactions_acid">
        <h2>The Meaning of ACID</h2>
        
        <p><a data-type="indexterm" data-primary="transactions" data-secondary="ACID properties of" id="idm45085106465024"></a>
        <a data-type="indexterm" data-primary="ACID properties (transactions)" id="idm45085106463696"></a>
        The safety guarantees provided by transactions are often described by the well-known acronym <em>ACID</em>,
        which stands for <em>Atomicity</em>, <em>Consistency</em>, <em>Isolation</em>, and <em>Durability</em>. It was coined in 1983 by
        Theo Härder and Andreas Reuter [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Harder1983cu-marker" href="ch07.html#Harder1983cu">7</a>]
        in an effort to establish precise terminology for fault-tolerance mechanisms in databases.</p>
        
        <p>However, in practice, one database’s implementation of ACID does not equal another’s implementation.
        For example, as we shall see, there is a lot of ambiguity around the meaning of <em>isolation</em>
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Bailis2013tn-marker" href="ch07.html#Bailis2013tn">8</a>].
        The high-level idea is sound, but the devil is in the details. Today, when a system claims to be
        “ACID compliant,” it’s unclear what guarantees you can actually expect. ACID has unfortunately
        become mostly a marketing term.</p>
        
        <p><a data-type="indexterm" data-primary="BASE, contrast to ACID" id="idm45085106453776"></a>
        (Systems that do not meet the ACID criteria are sometimes called <em>BASE</em>, which stands for
        <em>Basically Available</em>, <em>Soft state</em>, and <em>Eventual consistency</em>
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Fox1997wj-marker" href="ch07.html#Fox1997wj">9</a>].
        This is even more vague than the definition of ACID. It seems that the only sensible definition of
        BASE is “not ACID”; i.e., it can mean almost anything you want.)</p>
        
        <p>Let’s dig into the definitions of atomicity, consistency, isolation, and durability, as this will let
        us refine our idea of transactions.</p>
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Atomicity"><div class="sect3" id="sec_transactions_acid_atomicity">
        <h3>Atomicity</h3>
        
        <p><a data-type="indexterm" data-primary="atomicity (transactions)" id="idm45085106446288"></a>
        <a data-type="indexterm" data-primary="fault tolerance" data-secondary="transaction atomicity" id="idm45085106445264"></a>
        <a data-type="indexterm" data-primary="transactions" data-secondary="ACID properties of" data-tertiary="atomicity" id="idm45085106444160"></a>
        <a data-type="indexterm" data-primary="ACID properties (transactions)" data-secondary="atomicity" id="idm45085106442784"></a>
        <a data-type="indexterm" data-primary="threads (concurrency)" data-secondary="atomic operations" id="idm45085106441712"></a>
        In general, <em>atomic</em> refers to something that cannot be broken down into smaller parts. The word
        means similar but subtly different things in different branches of computing. For example, in
        multi-threaded programming, if one thread executes an atomic operation, that means there is no way
        that another thread could see the half-finished result of the operation. The system can only be in
        the state it was before the operation or after the operation, not something in between.</p>
        
        <p>By contrast, in the context of ACID, atomicity is <em>not</em> about concurrency. It does not describe
        what happens if several processes try to access the same data at the same time, because that is
        covered under the letter <em>I</em>, for <em>isolation</em> (see <a data-type="xref" href="#sec_transactions_acid_isolation">“Isolation”</a>).</p>
        
        <p>Rather, ACID atomicity describes what happens if a client wants to make several writes, but a fault
        occurs after some of the writes have been processed—for example, a process crashes, a network
        connection is interrupted, a disk becomes full, or some integrity constraint is violated.
        <a data-type="indexterm" data-primary="aborts (transactions)" id="idm45085106436352"></a>
        If the writes are grouped together into an atomic transaction, and the transaction cannot be
        completed (<em>committed</em>) due to a fault, then the transaction is <em>aborted</em> and the database must
        discard or undo any writes it has made so far in that transaction.</p>
        
        <p>Without atomicity, if an error occurs partway through making multiple changes, it’s difficult to
        know which changes have taken effect and which haven’t. The application could try again, but that
        risks making the same change twice, leading to duplicate or incorrect data. Atomicity simplifies
        this problem: if a transaction was aborted, the application can be sure that it didn’t change
        anything, so it can safely be retried.</p>
        
        <p>The ability to abort a transaction on error and have all writes from that transaction discarded is
        the defining feature of ACID atomicity. Perhaps <em>abortability</em> would have been a better term than
        <em>atomicity</em>, but we will stick with <em>atomicity</em> since that’s the usual word.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Consistency"><div class="sect3" id="sec_transactions_acid_consistency">
        <h3>Consistency</h3>
        
        <p><a data-type="indexterm" data-primary="ACID properties (transactions)" data-secondary="consistency" id="idm45085106429840"></a>
        <a data-type="indexterm" data-primary="consistency" id="idm45085106428544"></a>
        <a data-type="indexterm" data-primary="consistency" data-secondary="in ACID transactions" id="idm45085106427712"></a>
        <a data-type="indexterm" data-primary="consistency" data-secondary="meanings of" id="idm45085106426608"></a>
        <a data-type="indexterm" data-primary="transactions" data-secondary="ACID properties of" data-tertiary="consistency" id="idm45085106425504"></a>
        The word <em>consistency</em> is terribly overloaded:</p>
        
        <ul>
        <li>
        <p>In <a data-type="xref" href="ch05.html#ch_replication">Chapter&nbsp;5</a> we discussed <em>replica consistency</em> and the issue of <em>eventual consistency</em>
        that arises in asynchronously replicated systems (see <a data-type="xref" href="ch05.html#sec_replication_lag">“Problems with Replication Lag”</a>).</p>
        </li>
        <li>
        <p><em>Consistent hashing</em> is an approach to partitioning that some systems use for rebalancing (see
        <a data-type="xref" href="ch06.html#sidebar_consistent_hashing">“Consistent Hashing”</a>).</p>
        </li>
        <li>
        <p>In the CAP theorem (see <a data-type="xref" href="ch09.html#ch_consistency">Chapter&nbsp;9</a>), the word <em>consistency</em> is used to mean
        <em>linearizability</em> (see <a data-type="xref" href="ch09.html#sec_consistency_linearizability">“Linearizability”</a>).</p>
        </li>
        <li>
        <p>In the context of ACID, <em>consistency</em> refers to an application-specific notion of the database
        being in a “good state.”</p>
        </li>
        </ul>
        
        <p>It’s unfortunate that the same word is used with at least four different meanings.</p>
        
        <p><a data-type="indexterm" data-primary="invariants" data-seealso="constraints" id="idm45085106412240"></a>
        The idea of ACID consistency is that you have certain statements about your data (<em>invariants</em>) that
        must always be true—for example, in an accounting system, credits and debits across all accounts
        must always be balanced. If a transaction starts with a database that is valid according to these
        invariants, and any writes during the transaction preserve the validity, then you can be sure that
        the invariants are always satisfied.</p>
        
        <p><a data-type="indexterm" data-primary="constraints (databases)" id="idm45085106410016"></a>
        <a data-type="indexterm" data-primary="correctness" data-secondary="of transactions" id="idm45085106409184"></a>
        However, this idea of consistency depends on the application’s notion of invariants, and it’s the
        application’s responsibility to define its transactions correctly so that they preserve consistency.
        This is not something that the database can guarantee: if you write bad data that violates your
        invariants, the database can’t stop you. (Some specific kinds of invariants can be checked by the
        database, for example using foreign key constraints or uniqueness constraints. However, in general,
        the application defines what data is valid or invalid—the database only stores it.)</p>
        
        <p>Atomicity, isolation, and durability are properties of the database, whereas consistency (in the ACID
        sense) is a property of the application. The application may rely on the database’s atomicity and
        isolation properties in order to achieve consistency, but it’s not up to the database alone. Thus,
        the letter C doesn’t really belong in ACID.<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm45085106406720-marker" href="ch07.html#idm45085106406720">i</a></sup></p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Isolation"><div class="sect3" id="sec_transactions_acid_isolation">
        <h3>Isolation</h3>
        
        <p><a data-type="indexterm" data-primary="concurrency" data-secondary="transaction isolation" id="idm45085106402816"></a>
        <a data-type="indexterm" data-primary="isolation (in transactions)" id="idm45085106401712"></a>
        <a data-type="indexterm" data-primary="ACID properties (transactions)" data-secondary="isolation" id="idm45085106400816"></a>
        <a data-type="indexterm" data-primary="transactions" data-secondary="ACID properties of" data-tertiary="isolation" id="idm45085106399696"></a>
        <a data-type="indexterm" data-primary="race conditions" data-seealso="concurrency" id="idm45085106398320"></a>
        Most databases are accessed by several clients at the same time. That is no problem if they are
        reading and writing different parts of the database, but if they are accessing the same database
        records, you can run into concurrency problems (race conditions).</p>
        
        <p><a data-type="xref" href="#fig_transactions_increment">Figure&nbsp;7-1</a> is a simple example of this kind of problem. Say you have two clients
        simultaneously incrementing a counter that is stored in a database. Each client needs to read the
        current value, add 1, and write the new value back (assuming there is no increment operation built
        into the database). In <a data-type="xref" href="#fig_transactions_increment">Figure&nbsp;7-1</a> the counter should have increased from 42 to
        44, because two increments happened, but it actually only went to 43 because of the race condition.</p>
        
        <p><a data-type="indexterm" data-primary="serializability" id="idm45085106394304"></a>
        <em>Isolation</em> in the sense of ACID means that concurrently executing transactions are isolated from
        each other: they cannot step on each other’s toes. The classic database textbooks formalize
        isolation as <em>serializability</em>, which means that each transaction can pretend that it is the only
        transaction running on the entire database. The database ensures that when the transactions have
        committed, the result is the same as if they had run <em>serially</em> (one after another), even though in
        reality they may have run concurrently
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Bernstein1987va_ch7-marker" href="ch07.html#Bernstein1987va_ch7">10</a>].</p>
        
        <figure><div id="fig_transactions_increment" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_0701.png" alt="ddia 0701" width="2880" height="832">
        <h6><span class="label">Figure 7-1. </span>A race condition between two clients concurrently incrementing a counter.</h6>
        </div></figure>
        
        <p><a data-type="indexterm" data-primary="Oracle (database)" data-secondary="lack of serializability" id="idm45085106386912"></a>
        However, in practice, serializable isolation is rarely used, because it carries a performance
        penalty. Some popular databases, such as Oracle 11g, don’t even implement it. In Oracle there is an
        isolation level called “serializable,” but it actually implements something called <em>snapshot
        isolation</em>, which is a weaker guarantee than serializability
        [<a data-type="noteref" href="ch07.html#Bailis2013tn">8</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Fekete2005ee-marker" href="ch07.html#Fekete2005ee">11</a>].
        We will explore snapshot isolation and other forms of isolation in
        <a data-type="xref" href="#sec_transactions_isolation_levels">“Weak Isolation Levels”</a>.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Durability"><div class="sect3" id="sec_transactions_acid_durability">
        <h3>Durability</h3>
        
        <p><a data-type="indexterm" data-primary="durability (transactions)" id="idm45085106378576"></a>
        <a data-type="indexterm" data-primary="transactions" data-secondary="ACID properties of" data-tertiary="durability" id="idm45085106377728"></a>
        <a data-type="indexterm" data-primary="ACID properties (transactions)" data-secondary="durability" id="idm45085106376352"></a>
        The purpose of a database system is to provide a safe place where data can be stored without fear of
        losing it. <em>Durability</em> is the promise that once a transaction has committed successfully, any data it
        has written will not be forgotten, even if there is a hardware fault or the database crashes.</p>
        
        <p>In a single-node database, durability typically means that the data has been written to nonvolatile
        storage such as a hard drive or SSD. It usually also involves a write-ahead log or similar (see
        <a data-type="xref" href="ch03.html#sec_storage_btree_wal">“Making B-trees reliable”</a>), which allows recovery in the event that the data structures on disk are
        corrupted. In a replicated database, durability may mean that the data has been successfully copied
        to some number of nodes. In order to provide a durability guarantee, a database must wait until
        these writes or replications are complete before reporting a transaction as successfully committed.</p>
        
        <p class="pagebreak-after">As discussed in <a data-type="xref" href="ch01.html#sec_introduction_reliability">“Reliability”</a>, perfect durability does not exist: if all your
        hard disks and all your backups are destroyed at the same time, there’s obviously nothing your
        database can do to save you.</p>
        <aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="sidebar_transactions_durability">
        <h5>Replication and Durability</h5>
        <p><a data-type="indexterm" data-primary="disks" data-see="hard disks" id="idm45085106369216"></a>
        <a data-type="indexterm" data-primary="SSDs" data-see="solid state drives" id="idm45085106367888"></a>
        <a data-type="indexterm" data-primary="replication" data-secondary="and durability" id="idm45085106366784"></a>
        <a data-type="indexterm" data-primary="hard disks" data-secondary="faults in" id="idm45085106365680"></a>
        <a data-type="indexterm" data-primary="solid state drives (SSDs)" data-secondary="faults in" id="idm45085106364576"></a>
        <a data-type="indexterm" data-primary="corruption of data" data-secondary="on disks" id="idm45085106363456"></a>
        <a data-type="indexterm" data-primary="incidents" data-secondary="data corruption on hard disks" id="idm45085106362352"></a>
        Historically, durability meant writing to an archive tape. Then it was understood as writing to a disk
        or SSD. More recently, it has been adapted to mean replication. Which implementation is better?</p>
        
        <p>The truth is, nothing is perfect:</p>
        
        <ul>
        <li>
        <p>If you write to disk and the machine dies, even though your data isn’t lost, it is inaccessible
        until you either fix the machine or transfer the disk to another machine. Replicated systems can
        remain available.</p>
        </li>
        <li>
        <p><a data-type="indexterm" data-primary="in-memory databases" data-secondary="durability" id="idm45085106358560"></a>
        <a data-type="indexterm" data-primary="memory" data-secondary="in-memory databases" data-tertiary="durability" id="idm45085106357456"></a>
        <a data-type="indexterm" data-primary="storage engines" data-secondary="in-memory storage" data-tertiary="durability" id="idm45085106356080"></a>
        A correlated fault—a power outage or a bug that crashes every node on a particular input—can
        knock out all replicas at once (see <a data-type="xref" href="ch01.html#sec_introduction_reliability">“Reliability”</a>), losing any data that is
        only in memory. Writing to disk is therefore still relevant for in-memory databases.</p>
        </li>
        <li>
        <p>In an asynchronously replicated system, recent writes may be lost when the leader becomes
        unavailable (see <a data-type="xref" href="ch05.html#sec_replication_failover">“Handling Node Outages”</a>).</p>
        </li>
        <li>
        <p>When the power is suddenly cut, SSDs in particular have been shown to sometimes violate the
        guarantees they are supposed to provide: even <code>fsync</code> isn’t guaranteed to work correctly
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Zheng2013up-marker" href="ch07.html#Zheng2013up">12</a>].
        Disk firmware can have bugs, just like any other kind of software
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Denness2015tz-marker" href="ch07.html#Denness2015tz">13</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Surak2015tz-marker" href="ch07.html#Surak2015tz">14</a>].</p>
        </li>
        <li>
        <p>Subtle interactions between the storage engine and the filesystem implementation can lead to bugs
        that are hard to track down, and may cause files on disk to be corrupted after a crash
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Pillai2014vx_ch7-marker" href="ch07.html#Pillai2014vx_ch7">15</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Siebenmann2016ua-marker" href="ch07.html#Siebenmann2016ua">16</a>].</p>
        </li>
        <li>
        <p>Data on disk can gradually become corrupted without this being detected
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Bairavasundaram2008vx-marker" href="ch07.html#Bairavasundaram2008vx">17</a>].
        If data has been corrupted for some time, replicas and recent backups may also be corrupted. In
        this case, you will need to try to restore the data from a historical backup.</p>
        </li>
        <li>
        <p>One study of SSDs found that between 30% and 80% of drives develop at least one bad block during
        the first four years of operation
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Schroeder2016us-marker" href="ch07.html#Schroeder2016us">18</a>].
        Magnetic hard drives have a lower rate of bad sectors, but a higher rate of complete failure than
        SSDs.</p>
        </li>
        <li>
        <p>When a worn-out SSD (that has gone through many write/erase cycles) is disconnected from power,
        it can start losing data within a timescale of weeks to months, depending on the temperature
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Allison2015ta-marker" href="ch07.html#Allison2015ta">19</a>].</p>
        </li>
        </ul>
        
        <p>In practice, there is no one technique that can provide absolute guarantees. There are only various
        risk-reduction techniques, including writing to disk, replicating to remote machines, and
        backups—and they can and should be used together. As always, it’s wise to take any theoretical
        “guarantees” with a healthy grain of salt.</p>
        </div></aside>
        </div></section>
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Single-Object and Multi-Object Operations"><div class="sect2" id="sec_transactions_multi_object">
        <h2>Single-Object and Multi-Object Operations</h2>
        
        <p><a data-type="indexterm" data-primary="transactions" data-secondary="single-object and multi-object" id="ix_transactsinglemulti"></a>
        <a data-type="indexterm" data-primary="isolation (in transactions)" id="idm45085106323600"></a>
        <a data-type="indexterm" data-primary="atomicity (transactions)" id="idm45085106322752"></a>
        <a data-type="indexterm" data-primary="ACID properties (transactions)" data-secondary="atomicity" id="idm45085106321904"></a>
        <a data-type="indexterm" data-primary="ACID properties (transactions)" data-secondary="isolation" id="idm45085106320784"></a>
        To recap, in ACID, atomicity and isolation describe what the database should do if a client makes
        several writes within the same transaction:</p>
        <dl>
        <dt>Atomicity</dt>
        <dd>
        <p>If an error occurs halfway through a sequence of writes, the transaction should be aborted, and
        the writes made up to that point should be discarded. In other words, the database saves you from
        having to worry about partial failure, by giving an all-or-nothing guarantee.</p>
        </dd>
        <dt>Isolation</dt>
        <dd>
        <p>Concurrently running transactions shouldn’t interfere with each other. For example, if one
        transaction makes several writes, then another transaction should see either all or none of those
        writes, but not some subset.</p>
        </dd>
        </dl>
        
        <p><a data-type="indexterm" data-primary="multi-object transactions" id="idm45085106315648"></a>
        These definitions assume that you want to modify several objects (rows, documents, records) at once.
        Such <em>multi-object transactions</em> are often needed if several pieces of data need to be kept in sync.
        <a data-type="xref" href="#fig_transactions_read_uncommitted">Figure&nbsp;7-2</a> shows an example from an email application. To display the
        number of unread messages for a user, you could query something like:</p>
        
        <pre data-type="programlisting" data-code-language="sql"><code class="k">SELECT</code> <code class="k">COUNT</code><code class="p">(</code><code class="o">*</code><code class="p">)</code> <code class="k">FROM</code> <code class="n">emails</code> <code class="k">WHERE</code> <code class="n">recipient_id</code> <code class="o">=</code> <code class="mi">2</code> <code class="k">AND</code> <code class="n">unread_flag</code> <code class="o">=</code> <code class="k">true</code></pre>
        
        <p><a data-type="indexterm" data-primary="denormalization (data representation)" data-secondary="updating derived data" id="idm45085106311168"></a>
        However, you might find this query to be too slow if there are many emails, and decide to store the
        number of unread messages in a separate field (a kind of denormalization). Now, whenever a new
        message comes in, you have to increment the unread counter as well, and whenever a message is marked
        as read, you also have to decrement the unread counter.</p>
        
        <p><a data-type="indexterm" data-primary="isolation (in transactions)" data-secondary="violating" id="idm45085106297984"></a>
        In <a data-type="xref" href="#fig_transactions_read_uncommitted">Figure&nbsp;7-2</a>, user 2 experiences an anomaly: the mailbox listing shows
        an unread message, but the counter shows zero unread messages because the counter increment has not
        yet happened.<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm45085106295888-marker" href="ch07.html#idm45085106295888">ii</a></sup>
        Isolation would have prevented this issue by ensuring that user 2 sees either both the inserted
        email and the updated counter, or neither, but not an inconsistent halfway point.</p>
        
        <figure><div id="fig_transactions_read_uncommitted" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_0702.png" alt="ddia 0702" width="2880" height="1149">
        <h6><span class="label">Figure 7-2. </span>Violating isolation: one transaction reads another transaction’s uncommitted writes (a “dirty read”).</h6>
        </div></figure>
        
        <p><a data-type="indexterm" data-primary="atomicity (transactions)" data-secondary="for multi-object transactions" id="idm45085106292160"></a>
        <a data-type="xref" href="#fig_transactions_atomicity">Figure&nbsp;7-3</a> illustrates the need for atomicity: if an error occurs somewhere
        over the course of the transaction, the contents of the mailbox and the unread counter might become out
        of sync. In an atomic transaction, if the update to the counter fails, the transaction is aborted
        and the inserted email is rolled back.</p>
        
        <figure><div id="fig_transactions_atomicity" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_0703.png" alt="ddia 0703" width="2880" height="763">
        <h6><span class="label">Figure 7-3. </span>Atomicity ensures that if an error occurs any prior writes from that transaction are undone, to avoid an inconsistent state.</h6>
        </div></figure>
        
        <p><a data-type="indexterm" data-primary="TCP (Transmission Control Protocol)" data-secondary="use for transaction sessions" id="idm45085106261088"></a>
        Multi-object transactions require some way of determining which read and write operations belong to
        the same transaction. In relational databases, that is typically done based on the client’s TCP
        connection to the database server: on any particular connection, everything between a <code>BEGIN
        TRANSACTION</code> and a <code>COMMIT</code> statement is considered to be part of the same
        transaction.<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm45085106258928-marker" href="ch07.html#idm45085106258928">iii</a></sup></p>
        
        <p>On the other hand, many nonrelational databases don’t have such a way of grouping operations
        together. Even if there is a multi-object API (for example, a key-value store may have a <em>multi-put</em>
        operation that updates several keys in one operation), that doesn’t necessarily mean it has
        transaction semantics: the command may succeed for some keys and fail for others, leaving the
        database in a partially updated state.</p>
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Single-object writes"><div class="sect3" id="sec_transactions_single_object">
        <h3>Single-object writes</h3>
        
        <p><a data-type="indexterm" data-primary="transactions" data-secondary="single-object and multi-object" data-tertiary="single-object writes" id="idm45085106253728"></a>
        <a data-type="indexterm" data-primary="isolation (in transactions)" data-secondary="for single-object writes" id="idm45085106252336"></a>
        <a data-type="indexterm" data-primary="atomicity (transactions)" data-secondary="for single-object writes" id="idm45085106251200"></a>
        Atomicity and isolation also apply when a single object is being changed. For example, imagine you
        are writing a 20&nbsp;KB JSON document to a database:</p>
        
        <ul>
        <li>
        <p>If the network connection is interrupted after the first 10&nbsp;KB have been sent, does the
        database store that unparseable 10&nbsp;KB fragment of JSON?</p>
        </li>
        <li>
        <p>If the power fails while the database is in the middle of overwriting the previous value on disk,
        do you end up with the old and new values spliced together?</p>
        </li>
        <li>
        <p>If another client reads that document while the write is in progress, will it see a partially
        updated value?</p>
        </li>
        </ul>
        
        <p>Those issues would be incredibly confusing, so storage engines almost universally aim to provide
        atomicity and isolation on the level of a single object (such as a key-value pair) on one node.
        Atomicity can be implemented using a log for crash recovery (see <a data-type="xref" href="ch03.html#sec_storage_btree_wal">“Making B-trees reliable”</a>), and
        isolation can be implemented using a lock on each object (allowing only one thread to access an
        object at any one time).</p>
        
        <p><a data-type="indexterm" data-primary="compare-and-set operations" data-secondary="relation to transactions" id="idm45085106244512"></a>
        Some databases also provide more complex atomic
        operations,<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm45085106243376-marker" href="ch07.html#idm45085106243376">iv</a></sup> such as an increment operation, which removes the
        need for a read-modify-write cycle like that in <a data-type="xref" href="#fig_transactions_increment">Figure&nbsp;7-1</a>. Similarly popular is a
        compare-and-set operation, which allows a write to happen only if the value has not been
        concurrently changed by someone else (see <a data-type="xref" href="#sec_transactions_compare_and_set">“Compare-and-set”</a>).</p>
        
        <p>These single-object operations are useful, as they can prevent lost updates when several clients try
        to write to the same object concurrently (see <a data-type="xref" href="#sec_transactions_lost_update">“Preventing Lost Updates”</a>). However, they are
        not transactions in the usual sense of the word. Compare-and-set and other
        single-object operations have been dubbed “lightweight transactions” or even “ACID” for marketing
        purposes [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Scherer2013vz-marker" href="ch07.html#Scherer2013vz">20</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kingsbury2013ti_ch7-marker" href="ch07.html#Kingsbury2013ti_ch7">21</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Aerospike2014wa-marker" href="ch07.html#Aerospike2014wa">22</a>],
        but that terminology is misleading. A transaction is usually understood as a mechanism for grouping
        multiple operations on multiple objects into one unit of execution.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="The need for multi-object transactions"><div class="sect3" id="sec_transactions_need">
        <h3>The need for multi-object transactions</h3>
        
        <p><a data-type="indexterm" data-primary="transactions" data-secondary="single-object and multi-object" data-tertiary="need for multi-object transactions" id="idm45085106228672"></a>
        <a data-type="indexterm" data-primary="multi-object transactions" data-secondary="need for" id="idm45085106227216"></a>
        Many distributed datastores have abandoned multi-object transactions because they are difficult to
        implement across partitions, and they can get in the way in some scenarios where very high
        availability or performance is required. However, there is nothing that fundamentally prevents
        transactions in a distributed database, and we will discuss implementations of distributed
        transactions in <a data-type="xref" href="ch09.html#ch_consistency">Chapter&nbsp;9</a>.</p>
        
        <p>But do we need multi-object transactions at all? Would it be possible to implement any application
        with only a key-value data model and single-object operations?</p>
        
        <p>There are some use cases in which single-object inserts, updates, and deletes are sufficient.
        However, in many other cases writes to several different objects need to be coordinated:</p>
        
        <ul>
        <li>
        <p><a data-type="indexterm" data-primary="relational data model" data-secondary="multi-object transactions, need for" id="idm45085106222752"></a>
        <a data-type="indexterm" data-primary="normalization (data representation)" data-secondary="foreign key references" id="idm45085106221632"></a>
        In a relational data model, a row in one table often has a foreign key reference to a row in
        another table. (Similarly, in a graph-like data model, a vertex has edges to other vertices.)
        Multi-object transactions allow you to ensure that these references remain valid: when inserting
        several records that refer to one another, the foreign keys have to be correct and up to date,
        or the data becomes nonsensical.</p>
        </li>
        <li>
        <p><a data-type="indexterm" data-primary="document data model" data-secondary="multi-object transactions, need for" id="idm45085106219376"></a>
        <a data-type="indexterm" data-primary="denormalization (data representation)" data-secondary="updating derived data" id="idm45085106218256"></a>
        In a document data model, the fields that need to be updated together are often within the same
        document, which is treated as a single object—no multi-object transactions are needed when
        updating a single document. However, document databases lacking join functionality also encourage
        denormalization (see <a data-type="xref" href="ch02.html#sec_datamodels_document_summary">“Relational Versus Document Databases Today”</a>). When denormalized information needs to
        be updated, like in the example of <a data-type="xref" href="#fig_transactions_read_uncommitted">Figure&nbsp;7-2</a>, you need to update
        several documents in one go. Transactions are very useful in this situation to prevent
        denormalized data from going out of sync.</p>
        </li>
        <li>
        <p><a data-type="indexterm" data-primary="consistency" data-secondary="of secondary indexes" id="idm45085106214320"></a>
        <a data-type="indexterm" data-primary="secondary indexes" data-secondary="updating, transaction isolation and" id="idm45085106213216"></a>
        In databases with secondary indexes (almost everything except pure key-value stores), the indexes
        also need to be updated every time you change a value. These indexes are different database
        objects from a transaction point of view: for example, without transaction isolation, it’s
        possible for a record to appear in one index but not another, because the update to the second
        index hasn’t happened yet.</p>
        </li>
        </ul>
        
        <p>Such applications can still be implemented without transactions. However, error handling becomes
        much more complicated without atomicity, and the lack of isolation can cause concurrency problems.
        We will discuss those in <a data-type="xref" href="#sec_transactions_isolation_levels">“Weak Isolation Levels”</a>, and explore alternative approaches
        in <a data-type="xref" href="ch12.html#ch_future">Chapter&nbsp;12</a>.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Handling errors and aborts"><div class="sect3" id="idm45085106209024">
        <h3>Handling errors and aborts</h3>
        
        <p><a data-type="indexterm" data-primary="transactions" data-secondary="single-object and multi-object" data-tertiary="handling errors and aborts" id="idm45085106207680"></a>
        <a data-type="indexterm" data-primary="error handling" data-secondary="in transactions" id="idm45085106206272"></a>
        <a data-type="indexterm" data-primary="aborts (transactions)" data-secondary="retrying aborted transactions" id="idm45085106205168"></a>
        A key feature of a transaction is that it can be aborted and safely retried if an error occurred.
        ACID databases are based on this philosophy: if the database is in danger of violating its guarantee
        of atomicity, isolation, or durability, it would rather abandon the transaction entirely than allow
        it to remain half-finished.</p>
        
        <p>Not all systems follow that philosophy, though. In particular, datastores with leaderless
        replication (see <a data-type="xref" href="ch05.html#sec_replication_leaderless">“Leaderless Replication”</a>) work much more on a “best effort” basis, which
        could be summarized as “the database will do as much as it can, and if it runs into an error, it
        won’t undo something it has already done”—so it’s the application’s responsibility to recover from
        errors.</p>
        
        <p><a data-type="indexterm" data-primary="object-relational mapping (ORM) frameworks" data-secondary="error handling and aborted transactions" id="idm45085106201808"></a>
        <a data-type="indexterm" data-primary="ActiveRecord (object-relational mapper)" id="idm45085106200592"></a>
        <a data-type="indexterm" data-primary="Django (web framework)" id="idm45085106199744"></a>
        Errors will inevitably happen, but many software developers prefer to think only about the happy
        path rather than the intricacies of error handling. For example, popular object-relational mapping
        (ORM) frameworks such as Rails’s ActiveRecord and Django don’t retry aborted transactions—the
        error usually results in an exception bubbling up the stack, so any user input is thrown away and
        the user gets an error message. This is a shame, because the whole point of aborts is to enable safe
        retries.</p>
        
        <p>Although retrying an aborted transaction is a simple and effective error handling mechanism, it
        isn’t perfect:</p>
        
        <ul>
        <li>
        <p>If the transaction actually succeeded, but the network failed while the server tried to
        acknowledge the successful commit to the client (so the client thinks it failed), then retrying
        the transaction causes it to be performed twice—unless you have an additional application-level
        deduplication mechanism in place.</p>
        </li>
        <li>
        <p>If the error is due to overload, retrying the transaction will make the problem worse, not better.
        To avoid such feedback cycles, you can limit the number of retries, use exponential backoff, and
        handle overload-related errors differently from other errors (if possible).</p>
        </li>
        <li>
        <p>It is only worth retrying after transient errors (for example due to deadlock, isolation
        violation, temporary network interruptions, and failover); after a permanent error (e.g.,
        constraint violation) a retry would be pointless.</p>
        </li>
        <li>
        <p>If the transaction also has side effects outside of the database, those side effects may happen
        even if the transaction is aborted. For example, if you’re sending an email, you wouldn’t want to
        send the email again every time you retry the transaction. If you want to make sure that several
        different systems either commit or abort together, two-phase commit can help (we will discuss this
        in <a data-type="xref" href="ch09.html#sec_consistency_2pc">“Atomic Commit and Two-Phase Commit (2PC)”</a>).</p>
        </li>
        <li>
        <p>If the client process fails while retrying, any data it was trying to write to the database is lost.
        <a data-type="indexterm" data-primary="transactions" data-secondary="single-object and multi-object" data-startref="ix_transactsinglemulti" id="idm45085106190912"></a></p>
        </li>
        </ul>
        </div></section>
        
        
        
        </div></section>
        
        
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Weak Isolation Levels"><div class="sect1" id="sec_transactions_isolation_levels">
        <h1>Weak Isolation Levels</h1>
        
        <p><a data-type="indexterm" data-primary="isolation (in transactions)" data-secondary="weak isolation levels" id="ix_isolateweak"></a>
        <a data-type="indexterm" data-primary="transactions" data-secondary="weak isolation levels" id="ix_transactisolate"></a>
        If two transactions don’t touch the same data, they can safely be run in parallel, because neither
        depends on the other. Concurrency issues (race conditions) only come into play when one transaction
        reads data that is concurrently modified by another transaction, or when two transactions try to
        simultaneously modify the same data.</p>
        
        <p>Concurrency bugs are hard to find by testing, because such bugs are only triggered when you get
        unlucky with the timing. Such timing issues might occur very rarely, and are usually difficult to
        reproduce. Concurrency is also very difficult to reason about, especially in a large application
        where you don’t necessarily know which other pieces of code are accessing the database. Application
        development is difficult enough if you just have one user at a time; having many concurrent users
        makes it much harder still, because any piece of data could unexpectedly change at any time.</p>
        
        <p><a data-type="indexterm" data-primary="concurrency" data-secondary="bugs from weak transaction isolation" id="idm45085106183520"></a>
        <a data-type="indexterm" data-primary="serializability" id="idm45085106182448"></a>
        For that reason, databases have long tried to hide concurrency issues from application developers by
        providing <em>transaction isolation</em>. In theory, isolation should make your life easier by letting you
        pretend that no concurrency is happening: <em>serializable</em> isolation means that the database
        guarantees that transactions have the same effect as if they ran <em>serially</em> (i.e., one at a time,
        without any concurrency).</p>
        
        <p>In practice, isolation is unfortunately not that simple. Serializable isolation has a performance
        cost, and many databases don’t want to pay that price
        [<a data-type="noteref" href="ch07.html#Bailis2013tn">8</a>]. It’s therefore common for systems to use
        weaker levels of isolation, which protect against <em>some</em> concurrency issues, but not all. Those
        levels of isolation are much harder to understand, and they can lead to subtle bugs, but they are
        nevertheless used in practice
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kleppmann2014ut-marker" href="ch07.html#Kleppmann2014ut">23</a>].</p>
        
        <p><a data-type="indexterm" data-primary="incidents" data-secondary="data corruption and financial losses due to concurrency bugs" id="idm45085106175536"></a>
        <a data-type="indexterm" data-primary="Bitcoin (cryptocurrency)" data-secondary="concurrency bugs in exchanges" id="idm45085106174240"></a>
        <a data-type="indexterm" data-primary="corruption of data" data-secondary="due to weak transaction isolation" id="idm45085106173152"></a>
        Concurrency bugs caused by weak transaction isolation are not just a theoretical problem. They have
        caused substantial loss of money [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="DAgosta2014uy-marker" href="ch07.html#DAgosta2014uy">24</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="bitcointhief2014wt-marker" href="ch07.html#bitcointhief2014wt">25</a>], led to investigation by financial auditors
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Jorwekar2007uq_ch7-marker" href="ch07.html#Jorwekar2007uq_ch7">26</a>],
        and caused customer data to be corrupted [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Melanson2014wq-marker" href="ch07.html#Melanson2014wq">27</a>].
        A popular comment on revelations of such problems is “Use an ACID database if you’re handling
        financial data!”—but that misses the point. Even many popular relational database systems (which
        are usually considered “ACID”) use weak isolation, so they wouldn’t necessarily have prevented these
        bugs from occurring.</p>
        
        <p>Rather than blindly relying on tools, we need to develop a good understanding of the kinds of
        concurrency problems that exist, and how to prevent them. Then we can build applications that are
        reliable and correct, using the tools at our disposal.</p>
        
        <p>In this section we will look at several weak (nonserializable) isolation levels that are used in
        practice, and discuss in detail what kinds of race conditions can and cannot occur, so that you can
        decide what level is appropriate to your application. Once we’ve done that, we will discuss
        serializability in detail (see <a data-type="xref" href="#sec_transactions_serializability">“Serializability”</a>). Our discussion of isolation
        levels will be informal, using examples. If you want rigorous definitions and analyses of their
        properties, you can find them in the academic literature
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Berenson1995kj-marker" href="ch07.html#Berenson1995kj">28</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Adya1999tx-marker" href="ch07.html#Adya1999tx">29</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Bailis2014vc_ch7-marker" href="ch07.html#Bailis2014vc_ch7">30</a>].</p>
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Read Committed"><div class="sect2" id="sec_transactions_read_committed">
        <h2>Read Committed</h2>
        
        <p><a data-type="indexterm" data-primary="commits (transactions)" data-secondary="read committed isolation" id="idm45085106152128"></a>
        <a data-type="indexterm" data-primary="read committed isolation level" id="ix_readcommit"></a>
        <a data-type="indexterm" data-primary="isolation (in transactions)" data-secondary="weak isolation levels" data-tertiary="read committed" id="ix_isolateweakreadcommit"></a>
        <a data-type="indexterm" data-primary="transactions" data-secondary="weak isolation levels" data-tertiary="read committed" id="ix_transactisolateRC"></a>
        The most basic level of transaction isolation is
        <em>read committed</em>.<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm45085106145936-marker" href="ch07.html#idm45085106145936">v</a></sup> It makes two guarantees:</p>
        <ol>
        <li>
        <p>When reading from the database, you will only see data that has been committed (no <em>dirty
        reads</em>).</p>
        </li>
        <li>
        <p>When writing to the database, you will only overwrite data that has been committed (no <em>dirty
        writes</em>).</p>
        </li>
        
        </ol>
        
        <p>Let’s discuss these two guarantees in more detail.</p>
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="No dirty reads"><div class="sect3" id="idm45085106140560">
        <h3>No dirty reads</h3>
        
        <p><a data-type="indexterm" data-primary="dirty reads (transaction isolation)" id="idm45085106139184"></a>
        <a data-type="indexterm" data-primary="read committed isolation level" data-secondary="no dirty reads" id="idm45085106138336"></a>
        Imagine a transaction has written some data to the database, but the transaction has not yet committed or aborted.
        Can another transaction see that uncommitted data? If yes, that is called a
        <em>dirty read</em> [<a data-type="noteref" href="ch07.html#Gray1976us">2</a>].</p>
        
        <p>Transactions running at the read committed isolation level must prevent dirty reads. This means that
        any writes by a transaction only become visible to others when that transaction commits (and then
        all of its writes become visible at once). This is illustrated in
        <a data-type="xref" href="#fig_transactions_read_committed">Figure&nbsp;7-4</a>, where user 1 has set <em>x</em> = 3, but user 2’s <em>get x</em> still
        returns the old value, 2, while user 1 has not yet committed.</p>
        
        <figure><div id="fig_transactions_read_committed" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_0704.png" alt="ddia 0704" width="2880" height="832">
        <h6><span class="label">Figure 7-4. </span>No dirty reads: user 2 sees the new value for <em>x</em> only after user 1’s transaction has committed.</h6>
        </div></figure>
        
        <p class="pagebreak-before">There are a few reasons why it’s useful to prevent dirty reads:</p>
        
        <ul>
        <li>
        <p>If a transaction needs to update several objects, a dirty read means that another transaction may
        see some of the updates but not others. For example, in <a data-type="xref" href="#fig_transactions_read_uncommitted">Figure&nbsp;7-2</a>, the
        user sees the new unread email but not the updated counter. This is a dirty read of the email.
        Seeing the database in a partially updated state is confusing to users and may cause other
        transactions to take incorrect decisions.</p>
        </li>
        <li>
        <p>If a transaction aborts, any writes it has made need to be rolled back (like in
        <a data-type="xref" href="#fig_transactions_atomicity">Figure&nbsp;7-3</a>). If the database allows dirty reads, that means a transaction may
        see data that is later rolled back—i.e., which is never actually committed to the database.
        Reasoning about the consequences quickly becomes mind-bending.</p>
        </li>
        </ul>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="No dirty writes"><div class="sect3" id="sec_transactions_dirty_write">
        <h3>No dirty writes</h3>
        
        <p><a data-type="indexterm" data-primary="writes (database)" data-secondary="preventing dirty writes with read committed" id="idm45085106123712"></a>
        <a data-type="indexterm" data-primary="read committed isolation level" data-secondary="no dirty writes" id="idm45085106122512"></a>
        <a data-type="indexterm" data-primary="dirty writes (transaction isolation)" id="idm45085106121392"></a>
        What happens if two transactions concurrently try to update the same object in a database? We don’t
        know in which order the writes will happen, but we normally assume that the later write overwrites
        the earlier write.</p>
        
        <p><a data-type="indexterm" data-primary="race conditions" data-secondary="dirty writes" id="idm45085106120048"></a>
        However, what happens if the earlier write is part of a transaction that has not yet committed, so
        the later write overwrites an uncommitted value? This is called a <em>dirty write</em>
        [<a data-type="noteref" href="ch07.html#Berenson1995kj">28</a>]. Transactions running at the read
        committed isolation level must prevent dirty writes, usually by delaying the second write until the
        first write’s transaction has committed or aborted.</p>
        
        <p>By preventing dirty writes, this isolation level avoids some kinds of concurrency problems:</p>
        
        <ul>
        <li>
        <p>If transactions update multiple objects, dirty writes can lead to a bad outcome. For example,
        consider <a data-type="xref" href="#fig_transactions_dirty_writes">Figure&nbsp;7-5</a>, which illustrates a used car sales website on which
        two people, Alice and Bob, are simultaneously trying to buy the same car. Buying a car requires
        two database writes: the listing on the website needs to be updated to reflect the buyer, and the
        sales invoice needs to be sent to the buyer. In the case of <a data-type="xref" href="#fig_transactions_dirty_writes">Figure&nbsp;7-5</a>,
        the sale is awarded to Bob (because he performs the winning update to the <code>listings</code> table), but the
        invoice is sent to Alice (because she performs the winning update to the <code>invoices</code> table). Read
        committed prevents such mishaps.</p>
        </li>
        <li>
        <p><a data-type="indexterm" data-primary="race conditions" data-secondary="in counter increments" id="idm45085106111824"></a>
        However, read committed does <em>not</em> prevent the race condition between two counter increments in
        <a data-type="xref" href="#fig_transactions_increment">Figure&nbsp;7-1</a>. In this case, the second write happens after the first transaction
        has committed, so it’s not a dirty write. It’s still incorrect, but for a different reason—in
        <a data-type="xref" href="#sec_transactions_lost_update">“Preventing Lost Updates”</a> we will discuss how to make such counter increments safe.</p>
        </li>
        </ul>
        
        <figure><div id="fig_transactions_dirty_writes" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_0705.png" alt="ddia 0705" width="2880" height="1301">
        <h6><span class="label">Figure 7-5. </span>With dirty writes, conflicting writes from different transactions can be mixed up.</h6>
        </div></figure>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Implementing read committed"><div class="sect3" id="sec_transactions_read_committed_impl">
        <h3>Implementing read committed</h3>
        
        <p><a data-type="indexterm" data-primary="read committed isolation level" data-secondary="implementing" id="idm45085106104352"></a>
        <a data-type="indexterm" data-primary="Oracle (database)" data-secondary="read committed isolation" id="idm45085106103008"></a>
        <a data-type="indexterm" data-primary="PostgreSQL (database)" data-secondary="read committed isolation" id="idm45085106101888"></a>
        <a data-type="indexterm" data-primary="SQL Server (database)" data-secondary="read committed isolation" id="idm45085106100768"></a>
        <a data-type="indexterm" data-primary="MemSQL (database)" data-secondary="read committed isolation" id="idm45085106099648"></a>
        Read committed is a very popular isolation level. It is the default setting in Oracle 11g,
        PostgreSQL, SQL Server 2012, MemSQL, and many other databases
        [<a data-type="noteref" href="ch07.html#Bailis2013tn">8</a>].</p>
        
        <p><a data-type="indexterm" data-primary="locks" data-secondary="for transaction isolation" data-tertiary="preventing dirty writes" id="idm45085106097424"></a>
        Most commonly, databases prevent dirty writes by using row-level locks: when a transaction wants to
        modify a particular object (row or document), it must first acquire a lock on that object. It must
        then hold that lock until the transaction is committed or aborted. Only one transaction can hold the
        lock for any given object; if another transaction wants to write to the same object, it must wait
        until the first transaction is committed or aborted before it can acquire the lock and continue.
        This locking is done automatically by databases in read committed mode (or stronger isolation
        levels).</p>
        
        <p>How do we prevent dirty reads? One option would be to use the same lock, and to require any
        transaction that wants to read an object to briefly acquire the lock and then release it again
        immediately after reading. This would ensure that a read couldn’t happen while an object has a
        dirty, uncommitted value (because during that time the lock would be held by the transaction that
        has made the write).</p>
        
        <p><a data-type="indexterm" data-primary="locks" data-secondary="for transaction isolation" data-tertiary="read locks (shared mode)" id="idm45085106094352"></a>
        However, the approach of requiring read locks does not work well in practice, because one
        long-running write transaction can force many other transactions to wait until the long-running
        transaction has completed, even if the other transactions only read and do not write anything to the
        database. This harms the response time of read-only transactions and is bad for
        operability: a slowdown in one part of an application can have a knock-on effect in a completely
        different part of the application, due to waiting for locks.</p>
        
        <p>For that reason, most databases<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm45085106092016-marker" href="ch07.html#idm45085106092016">vi</a></sup>
        prevent dirty reads using the approach illustrated in <a data-type="xref" href="#fig_transactions_read_committed">Figure&nbsp;7-4</a>: for every
        object that is written, the database remembers both the old committed value and the new value
        set by the transaction that currently holds the write lock. While the transaction is ongoing, any
        other transactions that read the object are simply given the old value. Only when the new value is
        committed do transactions switch over to reading the new value.
        <a data-type="indexterm" data-primary="read committed isolation level" data-startref="ix_readcommit" id="idm45085106087456"></a>
        <a data-type="indexterm" data-primary="isolation (in transactions)" data-secondary="weak isolation levels" data-tertiary="read committed" data-startref="ix_isolateweakreadcommit" id="idm45085106086368"></a></p>
        </div></section>
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Snapshot Isolation and Repeatable Read"><div class="sect2" id="sec_transactions_snapshot_isolation">
        <h2>Snapshot Isolation and Repeatable Read</h2>
        
        <p><a data-type="indexterm" data-primary="consistency" data-secondary="consistent snapshots" id="ix_consnapisol"></a>
        <a data-type="indexterm" data-primary="isolation (in transactions)" data-secondary="weak isolation levels" data-tertiary="snapshot isolation" id="ix_isolateweaksnap"></a>
        <a data-type="indexterm" data-primary="snapshots (databases)" data-secondary="snapshot isolation and repeatable read" id="ix_snapshtisol"></a>
        <a data-type="indexterm" data-primary="transactions" data-secondary="snapshot isolation" data-see="snapshots" id="idm45085106078544"></a>
        If you look superficially at read committed isolation, you could be forgiven for thinking that it
        does everything that a transaction needs to do: it allows aborts (required for atomicity), it
        prevents reading the incomplete results of transactions, and it prevents concurrent writes from
        getting intermingled. Indeed, those are useful features, and much stronger guarantees than you can
        get from a system that has no transactions.</p>
        
        <p>However, there are still plenty of ways in which you can have concurrency bugs when using this isolation level. For example, <a data-type="xref" href="#fig_transactions_item_many_preceders">Figure&nbsp;7-6</a> illustrates a problem that can
        occur with read committed.</p>
        
        <figure><div id="fig_transactions_item_many_preceders" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_0706.png" alt="ddia 0706" width="2880" height="1301">
        <h6><span class="label">Figure 7-6. </span>Read skew: Alice observes the database in an inconsistent state.</h6>
        </div></figure>
        
        <p>Say Alice has $1,000 of savings at a bank, split across two accounts with $500 each. Now a
        transaction transfers $100 from one of her accounts to the other. If she is unlucky enough to look at her
        list of account balances in the same moment as that transaction is being processed, she may see one
        account balance at a time before the incoming payment has arrived (with a balance of $500), and the
        other account after the outgoing transfer has been made (the new balance being $400). To Alice it
        now appears as though she only has a total of $900 in her accounts—it seems that $100 has
        vanished into thin air.</p>
        
        <p><a data-type="indexterm" data-primary="read skew (transaction isolation)" id="idm45085106071904"></a>
        <a data-type="indexterm" data-primary="skew" data-secondary="in transaction isolation" data-tertiary="read skew" id="idm45085106071056"></a>
        <a data-type="indexterm" data-primary="nonrepeatable reads" data-seealso="read skew" id="idm45085106069664"></a>
        This anomaly is called <em>read skew</em>, and it is an example of a <em>nonrepeatable read</em>:
        if Alice were to read the balance of
        account 1 again at the end of the transaction, she would see a different value ($600) than she saw
        in her previous query. Read skew is considered acceptable under read committed isolation: the
        account balances that Alice saw were indeed committed at the time when she read them.</p>
        <div data-type="note" epub:type="note"><h6>Note</h6>
        <p><a data-type="indexterm" data-primary="skew" data-secondary="meanings of" id="idm45085106066496"></a>
        The term <em>skew</em> is unfortunately overloaded: we previously used it in the sense of an <em>unbalanced
        workload with hot spots</em> (see <a data-type="xref" href="ch06.html#sec_partitioning_skew">“Skewed Workloads and Relieving Hot Spots”</a>), whereas here it means <em>timing anomaly</em>.</p>
        </div>
        
        <p>In Alice’s case, this is not a lasting problem, because she will most likely see consistent account
        balances if she reloads the online banking website a few seconds later. However, some situations
        cannot tolerate such temporary inconsistency:</p>
        <dl>
        <dt>Backups</dt>
        <dd>
        <p><a data-type="indexterm" data-primary="backups" data-secondary="snapshot isolation for" id="idm45085106060960"></a>
        Taking a backup requires making a copy of the entire database, which may take hours on a large
        database. During the time that the backup process is running, writes will continue to be made to
        the database. Thus, you could end up with some parts of the backup containing an older version of
        the data, and other parts containing a newer version. If you need to restore from such a backup,
        the inconsistencies (such as disappearing money) become permanent.</p>
        </dd>
        <dt>Analytic queries and integrity checks</dt>
        <dd>
        <p><a data-type="indexterm" data-primary="analytics" data-secondary="snapshot isolation for queries" id="idm45085106058224"></a>
        <a data-type="indexterm" data-primary="integrity" data-secondary="integrity checks" data-tertiary="use of snapshot isolation" id="idm45085106057056"></a>
        Sometimes, you may want to run a query that scans over large parts of the database. Such queries
        are common in analytics (see <a data-type="xref" href="ch03.html#sec_storage_analytics">“Transaction Processing or Analytics?”</a>), or may be part of a periodic integrity
        check that everything is in order (monitoring for data corruption). These queries are likely to
        return nonsensical results if they observe parts of the database at different points in time.
        <a data-type="indexterm" data-primary="transactions" data-secondary="weak isolation levels" data-tertiary="read committed" data-startref="ix_transactisolateRC" id="idm45085106054464"></a></p>
        </dd>
        </dl>
        
        <p><em>Snapshot isolation</em> [<a data-type="noteref" href="ch07.html#Berenson1995kj">28</a>] is the most common
        solution to this problem. The idea is that each transaction reads from a <em>consistent snapshot</em> of
        the database—that is, the transaction sees all the data that was committed in the database at the
        start of the transaction. Even if the data is subsequently changed by another transaction, each
        transaction sees only the old data from that particular point in time.</p>
        
        <p>Snapshot isolation is a boon for long-running, read-only queries such as backups and analytics. It
        is very hard to reason about the meaning of a query if the data on which it operates is changing at
        the same time as the query is executing. When a transaction can see a consistent snapshot of the
        database, frozen at a particular point in time, it is much easier to understand.</p>
        
        <p><a data-type="indexterm" data-primary="PostgreSQL (database)" data-secondary="snapshot isolation support" id="idm45085106049296"></a>
        <a data-type="indexterm" data-primary="InnoDB (storage engine)" data-secondary="snapshot isolation support" id="idm45085106048000"></a>
        <a data-type="indexterm" data-primary="Oracle (database)" data-secondary="snapshot isolation support" id="idm45085106046880"></a>
        <a data-type="indexterm" data-primary="SQL Server (database)" data-secondary="snapshot isolation support" id="idm45085106045760"></a>
        Snapshot isolation is a popular feature: it is supported by PostgreSQL, MySQL with the InnoDB
        storage engine, Oracle, SQL Server, and others
        [<a data-type="noteref" href="ch07.html#Kleppmann2014ut">23</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Momjian2014vg-marker" href="ch07.html#Momjian2014vg">31</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Gurusami2013ut-marker" href="ch07.html#Gurusami2013ut">32</a>].</p>
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Implementing snapshot isolation"><div class="sect3" id="sec_transactions_snapshot_impl">
        <h3>Implementing snapshot isolation</h3>
        
        <p><a data-type="indexterm" data-primary="snapshots (databases)" data-secondary="snapshot isolation and repeatable read" data-tertiary="implementing with MVCC" id="idm45085106037616"></a>
        <a data-type="indexterm" data-primary="locks" data-secondary="for transaction isolation" data-tertiary="in snapshot isolation" id="idm45085106036176"></a>
        Like read committed isolation, implementations of snapshot isolation typically use write locks to prevent
        dirty writes (see <a data-type="xref" href="#sec_transactions_read_committed_impl">“Implementing read committed”</a>), which means that a transaction that
        makes a write can block the progress of another transaction that writes to the same object. However,
        reads do not require any locks. From a performance point of view, a key principle of snapshot
        isolation is <em>readers never block writers, and writers never block readers</em>. This allows a database
        to handle long-running read queries on a consistent snapshot at the same time as processing writes
        normally, without any lock contention between the two.</p>
        
        <p><a data-type="indexterm" data-primary="concurrency" data-secondary="multi-version concurrency control (MVCC)" id="idm45085106032656"></a>
        <a data-type="indexterm" data-primary="multi-version concurrency control (MVCC)" id="idm45085106031456"></a>
        <a data-type="indexterm" data-primary="read committed isolation level" data-secondary="multi-version concurrency control (MVCC)" id="idm45085106030592"></a>
        To implement snapshot isolation, databases use a generalization of the mechanism we saw for
        preventing dirty reads in <a data-type="xref" href="#fig_transactions_read_committed">Figure&nbsp;7-4</a>. The database must potentially keep
        several different committed versions of an object, because various in-progress transactions may need
        to see the state of the database at different points in time. Because it maintains several versions
        of an object side by side, this technique is known as <em>multi-version concurrency control</em> (MVCC).</p>
        
        <p>If a database only needed to provide read committed isolation, but not snapshot isolation, it would
        be sufficient to keep two versions of an object: the committed version and the
        overwritten-but-not-yet-committed version. However, storage engines that support snapshot isolation
        typically use MVCC for their read committed isolation level as well. A typical approach is that read
        committed uses a separate snapshot for each query, while snapshot isolation uses the same snapshot
        for an entire transaction.</p>
        
        <p><a data-type="indexterm" data-primary="PostgreSQL (database)" data-secondary="MVCC implementation" id="idm45085106026608"></a>
        <a data-type="xref" href="#fig_transactions_mvcc">Figure&nbsp;7-7</a> illustrates how MVCC-based snapshot isolation is implemented in PostgreSQL
        [<a data-type="noteref" href="ch07.html#Momjian2014vg">31</a>] (other implementations are similar).
        When a transaction is started, it is given a unique,
        always-increasing<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm45085106023776-marker" href="ch07.html#idm45085106023776">vii</a></sup>
        transaction ID (<code>txid</code>). Whenever a transaction writes anything to the database, the data it writes
        is tagged with the transaction ID of the writer.</p>
        
        <figure><div id="fig_transactions_mvcc" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_0707.png" alt="ddia 0707" width="2880" height="2181">
        <h6><span class="label">Figure 7-7. </span>Implementing snapshot isolation using multi-version objects.</h6>
        </div></figure>
        
        <p>Each row in a table has a <code>created_by</code> field, containing the ID of the transaction that inserted
        this row into the table. Moreover, each row has a <code>deleted_by</code> field, which is initially empty. If a
        transaction deletes a row, the row isn’t actually deleted from the database, but it is marked for
        deletion by setting the <code>deleted_by</code> field to the ID of the transaction that requested the deletion.
        At some later time, when it is certain that no transaction can any longer access the deleted data, a
        garbage collection process in the database removes any rows marked for deletion and frees their
        space.</p>
        
        <p>An update is internally translated into a delete and a create. For example, in
        <a data-type="xref" href="#fig_transactions_mvcc">Figure&nbsp;7-7</a>, transaction 13 deducts $100 from account 2, changing the balance from
        $500 to $400. The <code>accounts</code> table now actually contains two rows for account 2: a row with a balance
        of $500 which was marked as deleted by transaction 13, and a row with a balance of $400 which was
        created by transaction 13.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Visibility rules for observing a consistent snapshot"><div class="sect3" id="idm45085106015440">
        <h3>Visibility rules for observing a consistent snapshot</h3>
        
        <p><a data-type="indexterm" data-primary="snapshots (databases)" data-secondary="snapshot isolation and repeatable read" data-tertiary="visibility rules" id="idm45085106014032"></a>
        When a transaction reads from the database, transaction IDs are used to decide which objects it can
        see and which are invisible. By carefully defining visibility rules, the database can present a
        consistent snapshot of the database to the application. This works as follows:</p>
        <ol>
        <li>
        <p>At the start of each transaction, the database makes a list of all the other transactions that
        are in progress (not yet committed or aborted) at that time. Any writes that those
        transactions have made are ignored, even if the transactions subsequently commit.</p>
        </li>
        <li>
        <p>Any writes made by aborted transactions are ignored.</p>
        </li>
        <li>
        <p>Any writes made by transactions with a later transaction ID (i.e., which started after the current
        transaction started) are ignored, regardless of whether those transactions have committed.</p>
        </li>
        <li>
        <p>All other writes are visible to the application’s queries.</p>
        </li>
        
        </ol>
        
        <p>These rules apply to both creation and deletion of objects. In <a data-type="xref" href="#fig_transactions_mvcc">Figure&nbsp;7-7</a>, when
        transaction 12 reads from account 2, it sees a balance of $500 because the deletion of the $500
        balance was made by transaction 13 (according to rule 3, transaction 12 cannot see a deletion made
        by transaction 13), and the creation of the $400 balance is not yet visible (by the same rule).</p>
        
        <p>Put another way, an object is visible if both of the following conditions are true:</p>
        
        <ul>
        <li>
        <p>At the time when the reader’s transaction started, the transaction that created the object had
        already committed.</p>
        </li>
        <li>
        <p>The object is not marked for deletion, or if it is, the transaction that requested deletion had
        not yet committed at the time when the reader’s transaction started.</p>
        </li>
        </ul>
        
        <p>A long-running transaction may continue using a snapshot for a long time, continuing to read values
        that (from other transactions’ point of view) have long been overwritten or deleted. By never
        updating values in place but instead creating a new version every time a value is changed, the
        database can provide a consistent snapshot while incurring only a small overhead.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Indexes and snapshot isolation"><div class="sect3" id="sec_transactions_snapshot_indexes">
        <h3>Indexes and snapshot isolation</h3>
        
        <p><a data-type="indexterm" data-primary="consistency" data-secondary="of secondary indexes" id="idm45085106000640"></a>
        <a data-type="indexterm" data-primary="indexes" data-secondary="and snapshot isolation" id="idm45085105999536"></a>
        <a data-type="indexterm" data-primary="snapshots (databases)" data-secondary="snapshot isolation and repeatable read" data-tertiary="indexes and MVCC" id="idm45085105998432"></a>
        How do indexes work in a multi-version database? One option is to have the index simply point to all
        versions of an object and require an index query to filter out any object versions that are not
        visible to the current transaction. When garbage collection removes old object versions that are no
        longer visible to any transaction, the corresponding index entries can also be removed.</p>
        
        <p><a data-type="indexterm" data-primary="multi-version concurrency control (MVCC)" data-secondary="indexes and snapshot isolation" id="idm45085105996336"></a>
        <a data-type="indexterm" data-primary="PostgreSQL (database)" data-secondary="MVCC implementation" id="idm45085105995120"></a>
        In practice, many implementation details determine the performance of multi-version concurrency
        control. For example, PostgreSQL has optimizations for avoiding index updates if different versions
        of the same object can fit on the same page
        [<a data-type="noteref" href="ch07.html#Momjian2014vg">31</a>].</p>
        
        <p><a data-type="indexterm" data-primary="B-trees (indexes)" data-secondary="append-only/copy-on-write variants" id="idm45085105992672"></a>
        <a data-type="indexterm" data-primary="CouchDB (database)" data-secondary="B-tree storage" id="idm45085105991216"></a>
        <a data-type="indexterm" data-primary="Datomic (database)" data-secondary="B-tree storage" id="idm45085105990112"></a>
        <a data-type="indexterm" data-primary="LMDB (storage engine)" id="idm45085105989008"></a>
        <a data-type="indexterm" data-primary="append-only B-trees" id="idm45085105988176"></a>
        <a data-type="indexterm" data-primary="copy-on-write (B-trees)" id="idm45085105987344"></a>
        <a data-type="indexterm" data-primary="immutability" data-secondary="in B-trees" id="idm45085105986512"></a>
        Another approach is used in CouchDB, Datomic, and LMDB. Although they also use B-trees (see
        <a data-type="xref" href="ch03.html#sec_storage_b_trees">“B-Trees”</a>), they use an <em>append-only/copy-on-write</em> variant that does not overwrite
        pages of the tree when they are updated, but instead creates a new copy of each modified page.
        Parent pages, up to the root of the tree, are copied and updated to point to the new versions of
        their child pages.  Any pages that are not affected by a write do not need to be copied, and remain
        immutable [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Prokopov2014uu-marker" href="ch07.html#Prokopov2014uu">33</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Schwartz2013ur_ch7-marker" href="ch07.html#Schwartz2013ur_ch7">34</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Anderson2010wj_ch7-marker" href="ch07.html#Anderson2010wj_ch7">35</a>].</p>
        
        <p>With append-only B-trees, every write transaction (or batch of transactions) creates a new B-tree
        root, and a particular root is a consistent snapshot of the database at the point in time when it
        was created. There is no need to filter out objects based on transaction IDs because subsequent
        writes cannot modify an existing B-tree; they can only create new tree roots. However, this approach also
        requires a background process for compaction and garbage collection.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Repeatable read and naming confusion"><div class="sect3" id="idm45085105977152">
        <h3>Repeatable read and naming confusion</h3>
        
        <p><a data-type="indexterm" data-primary="repeatable reads (transaction isolation)" id="idm45085105975872"></a>
        <a data-type="indexterm" data-primary="PostgreSQL (database)" data-secondary="snapshot isolation support" id="idm45085105974720"></a>
        <a data-type="indexterm" data-primary="MySQL (database)" data-secondary="snapshot isolation support" data-seealso="InnoDB" id="idm45085105973600"></a>
        <a data-type="indexterm" data-primary="Oracle (database)" data-secondary="snapshot isolation support" id="idm45085105972208"></a>
        Snapshot isolation is a useful isolation level, especially for read-only transactions. However, many
        databases that implement it call it by different names. In Oracle it is called <em>serializable</em>, and
        in PostgreSQL and MySQL it is called <em>repeatable read</em>
        [<a data-type="noteref" href="ch07.html#Kleppmann2014ut">23</a>].</p>
        
        <p>The reason for this naming confusion is that the SQL standard doesn’t have the concept of snapshot
        isolation, because the standard is based on System R’s 1975 definition of isolation levels
        [<a data-type="noteref" href="ch07.html#Gray1976us">2</a>] and snapshot isolation hadn’t yet been
        invented then. Instead, it defines repeatable read, which looks superficially similar to snapshot
        isolation. PostgreSQL and MySQL call their snapshot isolation level repeatable read because it
        meets the requirements of the standard, and so they can claim standards compliance.</p>
        
        <p><a data-type="indexterm" data-primary="SQL (Structured Query Language)" data-secondary="isolation levels standard, issues with" id="idm45085105967312"></a>
        <a data-type="indexterm" data-primary="IBM" data-secondary="DB2 (database)" data-tertiary="serializable isolation" id="idm45085105966048"></a>
        Unfortunately, the SQL standard’s definition of isolation levels is flawed—it is ambiguous,
        imprecise, and not as implementation-independent as a standard should be
        [<a data-type="noteref" href="ch07.html#Berenson1995kj">28</a>]. Even though several databases
        implement repeatable read, there are big differences in the guarantees they actually provide,
        despite being ostensibly standardized
        [<a data-type="noteref" href="ch07.html#Kleppmann2014ut">23</a>]. There has been a formal definition of
        repeatable read in the research literature [<a data-type="noteref" href="ch07.html#Adya1999tx">29</a>,
        <a data-type="noteref" href="ch07.html#Bailis2014vc_ch7">30</a>], but most implementations don’t satisfy that
        formal definition. And to top it off, IBM DB2 uses “repeatable read” to refer to serializability
        [<a data-type="noteref" href="ch07.html#Bailis2013tn">8</a>].</p>
        
        <p>As a result, nobody really knows what repeatable read means.
        <a data-type="indexterm" data-primary="consistency" data-secondary="consistent snapshots" data-startref="ix_consnapisol" id="idm45085105959856"></a>
        <a data-type="indexterm" data-primary="isolation (in transactions)" data-secondary="weak isolation levels" data-tertiary="snapshot isolation" data-startref="ix_isolateweaksnap" id="idm45085105958368"></a>
        <a data-type="indexterm" data-primary="snapshots (databases)" data-secondary="snapshot isolation and repeatable read" data-startref="ix_snapshtisol" id="idm45085105956752"></a></p>
        </div></section>
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Preventing Lost Updates"><div class="sect2" id="sec_transactions_lost_update">
        <h2>Preventing Lost Updates</h2>
        
        <p><a data-type="indexterm" data-primary="conflicts" data-secondary="lost updates" id="ix_conflictlost"></a>
        <a data-type="indexterm" data-primary="updates" data-secondary="preventing lost updates" id="ix_updatelost"></a>
        <a data-type="indexterm" data-primary="transactions" data-secondary="weak isolation levels" data-tertiary="preventing lost updates" id="ix_transactisollostup"></a>
        <a data-type="indexterm" data-primary="isolation (in transactions)" data-secondary="weak isolation levels" data-tertiary="preventing lost updates" id="ix_isolateweaklostup"></a>
        <a data-type="indexterm" data-primary="race conditions" data-secondary="lost updates" id="ix_racelostupdate"></a>
        The read committed and snapshot isolation levels we’ve discussed so far have been primarily about the guarantees
        of what a read-only transaction can see in the presence of concurrent writes. We have mostly ignored
        the issue of two transactions writing concurrently—we have only discussed dirty writes (see
        <a data-type="xref" href="#sec_transactions_dirty_write">“No dirty writes”</a>), one particular type of write-write conflict that can occur.</p>
        
        <p><a data-type="indexterm" data-primary="concurrency" data-secondary="lost updates" id="idm45085105944832"></a>
        <a data-type="indexterm" data-primary="lost updates" data-see="updates" id="idm45085105943728"></a>
        There are several other interesting kinds of conflicts that can occur between concurrently writing
        transactions. The best known of these is the <em>lost update</em> problem, illustrated in
        <a data-type="xref" href="#fig_transactions_increment">Figure&nbsp;7-1</a> with the example of two concurrent counter increments.</p>
        
        <p><a data-type="indexterm" data-primary="read-modify-write cycle" id="idm45085105940864"></a>
        The lost update problem can occur if an application reads some value from the database, modifies it,
        and writes back the modified value (a <em>read-modify-write cycle</em>). If two transactions do this
        concurrently, one of the modifications can be lost, because the second write does not include the
        first modification. (We sometimes say that the later write <em>clobbers</em> the earlier write.) This
        pattern occurs in various different <span class="keep-together">scenarios:</span></p>
        
        <ul>
        <li>
        <p>Incrementing a counter or updating an account balance (requires reading the current value,
        calculating the new value, and writing back the updated value)</p>
        </li>
        <li>
        <p>Making a local change to a complex value, e.g., adding an element to a list within a JSON document
        (requires parsing the document, making the change, and writing back the modified document)</p>
        </li>
        <li>
        <p>Two users editing a wiki page at the same time, where each user saves their changes by sending the
        entire page contents to the server, overwriting whatever is currently in the database</p>
        </li>
        </ul>
        
        <p>Because this is such a common problem, a variety of solutions have been developed.</p>
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Atomic write operations"><div class="sect3" id="idm45085105933952">
        <h3>Atomic write operations</h3>
        
        <p><a data-type="indexterm" data-primary="updates" data-secondary="preventing lost updates" data-tertiary="atomic write operations" id="idm45085105932576"></a>
        <a data-type="indexterm" data-primary="writes (database)" data-secondary="atomic write operations" id="idm45085105931200"></a>
        <a data-type="indexterm" data-primary="atomicity (concurrency)" data-secondary="write operations" id="idm45085105930096"></a>
        Many databases provide atomic update operations, which remove the need to implement
        read-modify-write cycles in application code. They are usually the best solution if your code can be
        expressed in terms of those operations. For example, the following instruction is concurrency-safe
        in most relational databases:</p>
        
        <pre data-type="programlisting" data-code-language="sql"><code class="k">UPDATE</code> <code class="n">counters</code> <code class="k">SET</code> <code class="n">value</code> <code class="o">=</code> <code class="n">value</code> <code class="o">+</code> <code class="mi">1</code> <code class="k">WHERE</code> <code class="k">key</code> <code class="o">=</code> <code class="s1">'foo'</code><code class="p">;</code></pre>
        
        <p><a data-type="indexterm" data-primary="MongoDB (database)" data-secondary="atomic operations" id="idm45085105905328"></a>
        <a data-type="indexterm" data-primary="Redis (database)" data-secondary="atomic operations" id="idm45085105918096"></a>
        Similarly, document databases such as MongoDB provide atomic operations for making local
        modifications to a part of a JSON document, and Redis provides atomic operations for modifying data
        structures such as priority queues. Not all writes can easily be expressed in terms of atomic
        operations—for example, updates to a wiki page involve arbitrary text
        editing<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm45085105916640-marker" href="ch07.html#idm45085105916640">viii</a></sup>—but in
        situations where atomic operations can be used, they are usually the best choice.</p>
        
        <p><a data-type="indexterm" data-primary="locks" data-secondary="for transaction isolation" data-tertiary="making operations atomic" id="idm45085105914352"></a>
        <a data-type="indexterm" data-primary="cursor stability" id="idm45085105912832"></a>
        <a data-type="indexterm" data-primary="single-threaded execution" id="idm45085105912000"></a>
        Atomic operations are usually implemented by taking an exclusive lock on the object when it is read
        so that no other transaction can read it until the update has been applied. This technique is
        sometimes known as <em>cursor stability</em> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Mukherjee2013uw-marker" href="ch07.html#Mukherjee2013uw">36</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Hilker2013vy-marker" href="ch07.html#Hilker2013vy">37</a>].
        Another option is to simply force all atomic operations to be executed on a single thread.</p>
        
        <p><a data-type="indexterm" data-primary="object-relational mapping (ORM) frameworks" data-secondary="unsafe read-modify-write cycle code" id="idm45085105877408"></a>
        Unfortunately, object-relational mapping frameworks make it easy to accidentally write code that
        performs unsafe read-modify-write cycles instead of using atomic operations provided by the
        database [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Wiger2010vv-marker" href="ch07.html#Wiger2010vv">38</a>]. That’s not a problem if you know what you are doing, but it is
        potentially a source of subtle bugs that are difficult to find by <span class="keep-together">testing.</span></p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Explicit locking"><div class="sect3" id="idm45085105873120">
        <h3>Explicit locking</h3>
        
        <p><a data-type="indexterm" data-primary="updates" data-secondary="preventing lost updates" data-tertiary="using explicit locking" id="idm45085105871744"></a>
        <a data-type="indexterm" data-primary="locks" data-secondary="preventing lost updates by explicit locking" id="idm45085105870368"></a>
        Another option for preventing lost updates, if the database’s built-in atomic operations don’t
        provide the necessary functionality, is for the application to explicitly lock objects that are
        going to be updated. Then the application can perform a read-modify-write cycle, and if any other
        transaction tries to concurrently read the same object, it is forced to wait until the first
        read-modify-write cycle has completed.</p>
        
        <p>For example, consider a multiplayer game in which several players can move the same figure
        concurrently. In this case, an atomic operation may not be sufficient, because the application also
        needs to ensure that a player’s move abides by the rules of the game, which involves some logic that
        you cannot sensibly implement as a database query. Instead, you may use a lock to prevent two
        players from concurrently moving the same piece, as illustrated in <a data-type="xref" href="#fig_transactions_select_for_update">Example&nbsp;7-1</a>.</p>
        <div id="fig_transactions_select_for_update" data-type="example">
        <h5><span class="label">Example 7-1. </span>Explicitly locking rows to prevent lost updates</h5>
        
        <pre data-type="programlisting" data-code-language="sql"><code class="k">BEGIN</code><code> </code><code class="n">TRANSACTION</code><code class="p">;</code><code>
        
        </code><code class="k">SELECT</code><code> </code><code class="o">*</code><code> </code><code class="k">FROM</code><code> </code><code class="n">figures</code><code>
          </code><code class="k">WHERE</code><code> </code><code class="n">name</code><code> </code><code class="o">=</code><code> </code><code class="s1">'robot'</code><code> </code><code class="k">AND</code><code> </code><code class="n">game_id</code><code> </code><code class="o">=</code><code> </code><code class="mi">222</code><code>
          </code><code class="k">FOR</code><code> </code><code class="k">UPDATE</code><code class="p">;</code><code> </code><a class="co" id="co_transactions_CO1-1" href="#callout_transactions_CO1-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/1.png" alt="1" width="12" height="12"></a><code>
        
        </code><code class="c1">-- Check whether move is valid, then update the position
        </code><code class="c1">-- of the piece that was returned by the previous SELECT.
        </code><code class="k">UPDATE</code><code> </code><code class="n">figures</code><code> </code><code class="k">SET</code><code> </code><code class="k">position</code><code> </code><code class="o">=</code><code> </code><code class="s1">'c4'</code><code> </code><code class="k">WHERE</code><code> </code><code class="n">id</code><code> </code><code class="o">=</code><code> </code><code class="mi">1234</code><code class="p">;</code><code>
        
        </code><code class="k">COMMIT</code><code class="p">;</code></pre>
        <dl class="calloutlist">
        <dt><a class="co" id="callout_transactions_CO1-1" href="#co_transactions_CO1-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/1.png" alt="1" width="12" height="12"></a></dt>
        <dd><p>The <code>FOR UPDATE</code> clause indicates that the database should take a lock on all rows returned by
        this query.</p></dd>
        </dl></div>
        
        <p>This works, but to get it right, you need to carefully think about your application logic. It’s easy
        to forget to add a necessary lock somewhere in the code, and thus introduce a race condition.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Automatically detecting lost updates"><div class="sect3" id="idm45085105818816">
        <h3>Automatically detecting lost updates</h3>
        
        <p><a data-type="indexterm" data-primary="updates" data-secondary="preventing lost updates" data-tertiary="automatically detecting lost updates" id="idm45085105817680"></a>
        Atomic operations and locks are ways of preventing lost updates by forcing the read-modify-write
        cycles to happen sequentially. An alternative is to allow them to execute in parallel and, if the
        transaction manager detects a lost update, abort the transaction and force it to retry
        its read-modify-write cycle.</p>
        
        <p><a data-type="indexterm" data-primary="PostgreSQL (database)" data-secondary="preventing lost updates" id="idm45085105815760"></a>
        <a data-type="indexterm" data-primary="Oracle (database)" data-secondary="preventing lost updates" id="idm45085105814432"></a>
        <a data-type="indexterm" data-primary="SQL Server (database)" data-secondary="preventing lost updates" id="idm45085105783984"></a>
        <a data-type="indexterm" data-primary="InnoDB (storage engine)" data-secondary="not preventing lost updates" id="idm45085105783088"></a>
        An advantage of this approach is that databases can perform this check efficiently in conjunction
        with snapshot isolation. Indeed, PostgreSQL’s repeatable read, Oracle’s serializable, and SQL
        Server’s snapshot isolation levels automatically detect when a lost update has occurred and abort
        the offending transaction. However, MySQL/InnoDB’s repeatable read does not detect lost updates
        [<a data-type="noteref" href="ch07.html#Kleppmann2014ut">23</a>]. Some authors
        [<a data-type="noteref" href="ch07.html#Berenson1995kj">28</a>,
        <a data-type="noteref" href="ch07.html#Bailis2014vc_ch7">30</a>] argue that a database must prevent lost
        updates in order to qualify as providing snapshot isolation, so MySQL does not provide snapshot
        isolation under this definition.</p>
        
        <p>Lost update detection is a great feature, because it doesn’t require application code to use any
        special database features—you may forget to use a lock or an atomic operation and thus introduce
        a bug, but lost update detection happens automatically and is thus less error-prone.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Compare-and-set"><div class="sect3" id="sec_transactions_compare_and_set">
        <h3>Compare-and-set</h3>
        
        <p><a data-type="indexterm" data-primary="compare-and-set operations" id="idm45085105776512"></a>
        <a data-type="indexterm" data-primary="updates" data-secondary="preventing lost updates" data-tertiary="compare-and-set operations" id="idm45085105775712"></a>
        <a data-type="indexterm" data-primary="atomicity (concurrency)" data-secondary="compare-and-set" data-seealso="compare-and-set operations" id="idm45085105774320"></a>
        In databases that don’t provide transactions, you sometimes find an atomic compare-and-set operation
        (previously mentioned in <a data-type="xref" href="#sec_transactions_single_object">“Single-object writes”</a>). The purpose of this operation is to
        avoid lost updates by allowing an update to happen only if the value has not changed since you last
        read it. If the current value does not match what you previously read, the update has no effect, and
        the read-modify-write cycle must be retried.</p>
        
        <p>For example, to prevent two users concurrently updating the same wiki page, you might try something
        like this, expecting the update to occur only if the content of the page hasn’t changed since the
        user started editing it:</p>
        
        <pre data-type="programlisting" data-code-language="sql"><code class="c1">-- This may or may not be safe, depending on the database implementation</code>
        <code class="k">UPDATE</code> <code class="n">wiki_pages</code> <code class="k">SET</code> <code class="n">content</code> <code class="o">=</code> <code class="s1">'new content'</code>
          <code class="k">WHERE</code> <code class="n">id</code> <code class="o">=</code> <code class="mi">1234</code> <code class="k">AND</code> <code class="n">content</code> <code class="o">=</code> <code class="s1">'old content'</code><code class="p">;</code></pre>
        
        <p>If the content has changed and no longer matches <code>'old content'</code>, this update will have no effect,
        so you need to check whether the update took effect and retry if necessary. However, if the database
        allows the <code>WHERE</code> clause to read from an old snapshot, this statement may not prevent lost updates,
        because the condition may be true even though another concurrent write is occurring. Check whether
        your database’s compare-and-set operation is safe before relying on it.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Conflict resolution and replication"><div class="sect3" id="idm45085105755472">
        <h3>Conflict resolution and replication</h3>
        
        <p><a data-type="indexterm" data-primary="updates" data-secondary="preventing lost updates" data-tertiary="conflict resolution and replication" id="idm45085105754160"></a>
        <a data-type="indexterm" data-primary="replication" data-secondary="conflict resolution and" id="idm45085105752720"></a>
        <a data-type="indexterm" data-primary="conflicts" data-secondary="conflict resolution" data-tertiary="using atomic operations" id="idm45085105751616"></a>
        In replicated databases (see <a data-type="xref" href="ch05.html#ch_replication">Chapter&nbsp;5</a>), preventing lost updates takes on another
        dimension: since they have copies of the data on multiple nodes, and the data can potentially be
        modified concurrently on different nodes, some additional steps need to be taken to prevent lost
        updates.</p>
        
        <p>Locks and compare-and-set operations assume that there is a single up-to-date copy of the data.
        However, databases with multi-leader or leaderless replication usually allow several writes to
        happen concurrently and replicate them asynchronously, so they cannot guarantee that there is a
        single up-to-date copy of the data. Thus, techniques based on locks or compare-and-set do not apply
        in this context. (We will revisit this issue in more detail in <a data-type="xref" href="ch09.html#sec_consistency_linearizability">“Linearizability”</a>.)</p>
        
        <p><a data-type="indexterm" data-primary="siblings (concurrent values)" id="idm45085105747152"></a>
        Instead, as discussed in <a data-type="xref" href="ch05.html#sec_replication_concurrent">“Detecting Concurrent Writes”</a>, a common approach in such replicated
        databases is to allow concurrent writes to create several conflicting versions of a value (also
        known as <em>siblings</em>), and to use application code or special data structures to resolve and merge
        these versions after the fact.</p>
        
        <p><a data-type="indexterm" data-primary="atomicity (concurrency)" data-secondary="replicated operations" id="idm45085105717024"></a>
        <a data-type="indexterm" data-primary="commutative operations" id="idm45085105715920"></a>
        <a data-type="indexterm" data-primary="Riak (database)" data-secondary="preventing lost updates across replicas" id="idm45085105715088"></a>
        Atomic operations can work well in a replicated context, especially if they are commutative (i.e.,
        you can apply them in a different order on different replicas, and still get the same result). For
        example, incrementing a counter or adding an element to a set are commutative operations. That is
        the idea behind Riak 2.0 datatypes, which prevent lost updates across replicas. When a value is
        concurrently updated by different clients, Riak automatically merges together the updates in such a
        way that no updates are lost [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Jacobson2014wa_ch7-marker" href="ch07.html#Jacobson2014wa_ch7">39</a>].</p>
        
        <p><a data-type="indexterm" data-primary="last write wins (LWW)" data-secondary="prone to lost updates" id="idm45085105710768"></a>
        On the other hand, the <em>last write wins</em> (LWW) conflict resolution method is prone to lost updates,
        as discussed in <a data-type="xref" href="ch05.html#sec_replication_lww">“Last write wins (discarding concurrent writes)”</a>. Unfortunately, LWW is the default in many replicated
        databases.
        <a data-type="indexterm" data-primary="conflicts" data-secondary="lost updates" data-startref="ix_conflictlost" id="idm45085105708336"></a>
        <a data-type="indexterm" data-primary="updates" data-secondary="preventing lost updates" data-startref="ix_updatelost" id="idm45085105706992"></a>
        <a data-type="indexterm" data-primary="transactions" data-secondary="weak isolation levels" data-tertiary="preventing lost updates" data-startref="ix_transactisollostup" id="idm45085105705616"></a>
        <a data-type="indexterm" data-primary="isolation (in transactions)" data-secondary="weak isolation levels" data-tertiary="preventing lost updates" data-startref="ix_isolateweaklostup" id="idm45085105703968"></a>
        <a data-type="indexterm" data-primary="race conditions" data-secondary="lost updates" data-startref="ix_racelostupdate" id="idm45085105702304"></a></p>
        </div></section>
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Write Skew and Phantoms"><div class="sect2" id="sec_transactions_write_skew">
        <h2>Write Skew and Phantoms</h2>
        
        <p><a data-type="indexterm" data-primary="write skew (transaction isolation)" id="ix_wrskew"></a>
        <a data-type="indexterm" data-primary="skew" data-secondary="in transaction isolation" data-tertiary="write skew" data-seealso="write skew" id="ix_skewwrite"></a>
        <a data-type="indexterm" data-primary="concurrency" data-secondary="write skew (transaction isolation)" id="ix_concwrskew"></a>
        <a data-type="indexterm" data-primary="conflicts" data-secondary="write skew (transaction isolation)" id="ix_conflictwrskew"></a>
        <a data-type="indexterm" data-primary="updates" data-secondary="preventing write skew" id="ix_updatewrskew"></a>
        <a data-type="indexterm" data-primary="race conditions" data-secondary="write skew" id="ix_racewrskew"></a>
        In the previous sections we saw <em>dirty writes</em> and <em>lost updates</em>, two kinds of race conditions that
        can occur when different transactions concurrently try to write to the same objects. In order to
        avoid data corruption, those race conditions need to be prevented—either automatically by the
        database, or by manual safeguards such as using locks or atomic write operations.</p>
        
        <p>However, that is not the end of the list of potential race conditions that can occur between
        concurrent writes. In this section we will see some subtler examples of <span class="keep-together">conflicts.</span></p>
        
        <p><a data-type="indexterm" data-primary="write skew (transaction isolation)" data-secondary="characterizing" id="ix_wrskewcharacter"></a>
        To begin, imagine this example: you are writing an application for doctors to manage their on-call
        shifts at a hospital. The hospital usually tries to have several doctors on call at any one time,
        but it absolutely must have at least one doctor on call. Doctors can give up their shifts (e.g., if
        they are sick themselves), provided that at least one colleague remains on call in that shift
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Cahill2008eg-marker" href="ch07.html#Cahill2008eg">40</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Ports2012uw-marker" href="ch07.html#Ports2012uw">41</a>].</p>
        
        <p><a data-type="indexterm" data-primary="write skew (transaction isolation)" data-secondary="examples of" id="idm45085105681248"></a>
        Now imagine that Alice and Bob are the two on-call doctors for a particular shift. Both are feeling
        unwell, so they both decide to request leave. Unfortunately, they happen to click the button to go
        off call at approximately the same time. What happens next is illustrated in
        <a data-type="xref" href="#fig_transactions_write_skew">Figure&nbsp;7-8</a>.</p>
        
        <figure><div id="fig_transactions_write_skew" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_0708.png" alt="ddia 0708" width="2880" height="2019">
        <h6><span class="label">Figure 7-8. </span>Example of write skew causing an application bug.</h6>
        </div></figure>
        
        <p>In each transaction, your application first checks that two or more doctors are currently on call;
        if yes, it assumes it’s safe for one doctor to go off call. Since the database is using snapshot
        isolation, both checks return <code>2</code>, so both transactions proceed to the next stage. Alice updates her
        own record to take herself off call, and Bob updates his own record likewise. Both transactions
        commit, and now no doctor is on call. Your requirement of having at least one doctor on call has
        been violated.</p>
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Characterizing write skew"><div class="sect3" id="idm45085105675488">
        <h3>Characterizing write skew</h3>
        
        <p>This anomaly is called <em>write skew</em> [<a data-type="noteref" href="ch07.html#Berenson1995kj">28</a>]. It
        is neither a dirty write nor a lost update, because the two transactions are updating two different
        objects (Alice’s and Bob’s on-call records, respectively). It is less obvious that a conflict occurred
        here, but it’s definitely a race condition: if the two transactions had run one after another, the
        second doctor would have been prevented from going off call. The anomalous behavior was only
        possible because the transactions ran concurrently.</p>
        
        <p>You can think of write skew as a generalization of the lost update problem. Write skew can occur if two
        transactions read the same objects, and then update some of those objects (different transactions
        may update different objects). In the special case where different transactions update the same
        object, you get a dirty write or lost update anomaly (depending on the timing).</p>
        
        <p><a data-type="indexterm" data-primary="write skew (transaction isolation)" data-secondary="preventing" data-tertiary="options for" id="idm45085105671152"></a>
        We saw that there are various different ways of preventing lost updates. With write skew, our
        options are more restricted:</p>
        
        <ul>
        <li>
        <p>Atomic single-object operations don’t help, as multiple objects are involved.</p>
        </li>
        <li>
        <p><a data-type="indexterm" data-primary="PostgreSQL (database)" data-secondary="preventing write skew" id="idm45085105667808"></a>
        <a data-type="indexterm" data-primary="InnoDB (storage engine)" data-secondary="preventing write skew" id="idm45085105666480"></a>
        <a data-type="indexterm" data-primary="Oracle (database)" data-secondary="not preventing write skew" id="idm45085105665376"></a>
        <a data-type="indexterm" data-primary="SQL Server (database)" data-secondary="preventing write skew" id="idm45085105664304"></a>
        The automatic detection of lost updates that you find in some implementations of snapshot
        isolation unfortunately doesn’t help either: write skew is not automatically detected in
        PostgreSQL’s repeatable read, MySQL/InnoDB’s repeatable read, Oracle’s serializable, or SQL
        Server’s snapshot isolation level [<a data-type="noteref" href="ch07.html#Kleppmann2014ut">23</a>].
        Automatically preventing write skew requires true serializable isolation (see
        <a data-type="xref" href="#sec_transactions_serializability">“Serializability”</a>).</p>
        </li>
        <li>
        <p><a data-type="indexterm" data-primary="constraints (databases)" id="idm45085105660448"></a>
        Some databases allow you to configure constraints, which are then enforced by the database (e.g.,
        uniqueness, foreign key constraints, or restrictions on a particular value). However, in order to
        specify that at least one doctor must be on call, you would need a constraint that involves
        multiple objects. Most databases do not have built-in support for such constraints, but you may be
        able to implement them with triggers or materialized views, depending on the database
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Andrews2004wp-marker" href="ch07.html#Andrews2004wp">42</a>].</p>
        </li>
        <li>
        <p>If you can’t use a serializable isolation level, the second-best option in this case is probably
        to explicitly lock the rows that the transaction depends on. In the doctors example, you could
        write something like the following:</p>
        
        <pre data-type="programlisting" data-code-language="sql"><code class="k">BEGIN</code><code> </code><code class="n">TRANSACTION</code><code class="p">;</code><code>
        
        </code><code class="k">SELECT</code><code> </code><code class="o">*</code><code> </code><code class="k">FROM</code><code> </code><code class="n">doctors</code><code>
          </code><code class="k">WHERE</code><code> </code><code class="n">on_call</code><code> </code><code class="o">=</code><code> </code><code class="k">true</code><code>
          </code><code class="k">AND</code><code> </code><code class="n">shift_id</code><code> </code><code class="o">=</code><code> </code><code class="mi">1234</code><code> </code><code class="k">FOR</code><code> </code><code class="k">UPDATE</code><code class="p">;</code><code> </code><a class="co" id="co_transactions_CO2-1" href="#callout_transactions_CO2-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/1.png" alt="1" width="12" height="12"></a><code>
        
        </code><code class="k">UPDATE</code><code> </code><code class="n">doctors</code><code>
          </code><code class="k">SET</code><code> </code><code class="n">on_call</code><code> </code><code class="o">=</code><code> </code><code class="k">false</code><code>
          </code><code class="k">WHERE</code><code> </code><code class="n">name</code><code> </code><code class="o">=</code><code> </code><code class="s1">'Alice'</code><code>
          </code><code class="k">AND</code><code> </code><code class="n">shift_id</code><code> </code><code class="o">=</code><code> </code><code class="mi">1234</code><code class="p">;</code><code>
        
        </code><code class="k">COMMIT</code><code class="p">;</code></pre>
        <dl class="calloutlist">
        <dt><a class="co" id="callout_transactions_CO2-1" href="#co_transactions_CO2-1"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/1.png" alt="1" width="12" height="12"></a></dt>
        <dd><p>As before, <code>FOR UPDATE</code> tells the database to lock all rows returned by this query.</p></dd>
        </dl>
        </li>
        </ul>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="More examples of write skew"><div class="sect3" id="idm45085105604592">
        <h3>More examples of write skew</h3>
        
        <p><a data-type="indexterm" data-primary="write skew (transaction isolation)" data-secondary="examples of" id="idm45085105603456"></a>
        Write skew may seem like an esoteric issue at first, but once you’re aware of it, you may notice
        more situations in which it can occur. Here are some more examples:</p>
        <dl>
        <dt>Meeting room booking system</dt>
        <dd>
        <p><a data-type="indexterm" data-primary="meeting room booking (example)" id="idm45085105600672"></a>
        Say you want to enforce that there cannot be two bookings for the same meeting room at the same
        time [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Terry1995dn_ch7-marker" href="ch07.html#Terry1995dn_ch7">43</a>].
        When someone wants to make a booking, you first check for any conflicting bookings (i.e.,
        bookings for the same room with an overlapping time range), and if none are found, you create the
        meeting (see
        <a data-type="xref" href="#fig_transactions_meeting_rooms">Example&nbsp;7-2</a>).<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm45085105566816-marker" href="ch07.html#idm45085105566816">ix</a></sup></p>
        <div id="fig_transactions_meeting_rooms" data-type="example">
        <h5><span class="label">Example 7-2. </span>A meeting room booking system tries to avoid double-booking (not safe under snapshot isolation)</h5>
        
        <pre data-type="programlisting" data-code-language="sql"><code class="k">BEGIN</code> <code class="n">TRANSACTION</code><code class="p">;</code>
        
        <code class="c1">-- Check for any existing bookings that overlap with the period of noon-1pm</code>
        <code class="k">SELECT</code> <code class="k">COUNT</code><code class="p">(</code><code class="o">*</code><code class="p">)</code> <code class="k">FROM</code> <code class="n">bookings</code>
          <code class="k">WHERE</code> <code class="n">room_id</code> <code class="o">=</code> <code class="mi">123</code> <code class="k">AND</code>
            <code class="n">end_time</code> <code class="o">&gt;</code> <code class="s1">'2015-01-01 12:00'</code> <code class="k">AND</code> <code class="n">start_time</code> <code class="o">&lt;</code> <code class="s1">'2015-01-01 13:00'</code><code class="p">;</code>
        
        <code class="c1">-- If the previous query returned zero:</code>
        <code class="k">INSERT</code> <code class="k">INTO</code> <code class="n">bookings</code>
          <code class="p">(</code><code class="n">room_id</code><code class="p">,</code> <code class="n">start_time</code><code class="p">,</code> <code class="n">end_time</code><code class="p">,</code> <code class="n">user_id</code><code class="p">)</code>
          <code class="k">VALUES</code> <code class="p">(</code><code class="mi">123</code><code class="p">,</code> <code class="s1">'2015-01-01 12:00'</code><code class="p">,</code> <code class="s1">'2015-01-01 13:00'</code><code class="p">,</code> <code class="mi">666</code><code class="p">);</code>
        
        <code class="k">COMMIT</code><code class="p">;</code></pre></div>
        
        <p>Unfortunately, snapshot isolation does not prevent another user from concurrently inserting a conflicting
        meeting. In order to guarantee you won’t get scheduling conflicts, you once again need serializable
        isolation.</p>
        </dd>
        <dt>Multiplayer game</dt>
        <dd>
        <p>In <a data-type="xref" href="#fig_transactions_select_for_update">Example&nbsp;7-1</a>, we used a lock to prevent lost updates (that is, making
        sure that two players can’t move the same figure at the same time). However, the lock doesn’t
        prevent players from moving two different figures to the same position on the board or potentially
        making some other move that violates the rules of the game. Depending on the kind of rule you are
        enforcing, you might be able to use a unique constraint, but otherwise you’re vulnerable to write
        skew.</p>
        </dd>
        <dt>Claiming a username</dt>
        <dd>
        <p>On a website where each user has a unique username, two users may try to create accounts with the
        same username at the same time. You may use a transaction to check whether a name is taken and, if
        not, create an account with that name. However, like in the previous examples, that is not safe
        under snapshot isolation. Fortunately, a unique constraint is a simple solution here (the second
        transaction that tries to register the username will be aborted due to violating the constraint).</p>
        </dd>
        <dt>Preventing double-spending</dt>
        <dd>
        <p>A service that allows users to spend money or points needs to check that a user doesn’t spend more
        than they have. You might implement this by inserting a tentative spending item into a user’s
        account, listing all the items in the account, and checking that the sum is positive
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Fredericks2015pg_ch7-marker" href="ch07.html#Fredericks2015pg_ch7">44</a>].
        With write skew, it could happen that two spending items are inserted concurrently that together
        cause the balance to go negative, but that neither transaction notices the other.</p>
        </dd>
        </dl>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Phantoms causing write skew"><div class="sect3" id="sec_transactions_phantom">
        <h3>Phantoms causing write skew</h3>
        
        <p><a data-type="indexterm" data-primary="write skew (transaction isolation)" data-secondary="phantoms" id="idm45085105467776"></a>
        <a data-type="indexterm" data-primary="phantoms (transaction isolation)" id="idm45085105466608"></a>
        All of these examples follow a similar pattern:</p>
        <ol>
        <li>
        <p>A <code>SELECT</code> query checks whether some requirement is satisfied by searching for rows that
        match some search condition (there are at least two doctors on call, there are no existing
        bookings for that room at that time, the position on the board doesn’t already have another
        figure on it, the username isn’t already taken, there is still money in the account).</p>
        </li>
        <li>
        <p>Depending on the result of the first query, the application code decides how to continue (perhaps
        to go ahead with the operation, or perhaps to report an error to the user and abort).</p>
        </li>
        <li>
        <p>If the application decides to go ahead, it makes a write (<code>INSERT</code>, <code>UPDATE</code>, or <code>DELETE</code>) to the
        database and commits the transaction.</p>
        
        <p>The effect of this write changes the precondition of the decision of step 2. In other words, if you
        were to repeat the <code>SELECT</code> query from step 1 after committing the write, you would get a different
        result, because the write changed the set of rows matching the search condition (there is now one
        fewer doctor on call, the meeting room is now booked for that time, the position on the board is now
        taken by the figure that was moved, the username is now taken, there is now less money in the
        account).</p>
        </li>
        
        </ol>
        
        <p>The steps may occur in a different order. For example, you could first make the write, then the
        <code>SELECT</code> query, and finally decide whether to abort or commit based on the result of the query.</p>
        
        <p>In the case of the doctor on call example, the row being modified in step 3 was one of the rows
        returned in step 1, so we could make the transaction safe and avoid write skew by locking the rows
        in step 1 (<code>SELECT FOR UPDATE</code>). However, the other four examples are different: they check for the
        <em>absence</em> of rows matching some search condition, and the write <em>adds</em> a row matching the same
        condition. If the query in step 1 doesn’t return any rows, <code>SELECT FOR UPDATE</code> can’t attach locks to
        anything.</p>
        
        <p>This effect, where a write in one transaction changes the result of a search query in another
        transaction, is called a <em>phantom</em> [<a data-type="noteref" href="ch07.html#Eswaran1976uu">3</a>].
        Snapshot isolation avoids phantoms in read-only queries, but in read-write transactions like the
        examples we discussed, phantoms can lead to particularly tricky cases of write skew.
        <a data-type="indexterm" data-primary="write skew (transaction isolation)" data-secondary="characterizing" data-startref="ix_wrskewcharacter" id="idm45085105453680"></a></p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Materializing conflicts"><div class="sect3" id="sec_transactions_materializing_conflicts">
        <h3>Materializing conflicts</h3>
        
        <p><a data-type="indexterm" data-primary="phantoms (transaction isolation)" data-secondary="materializing conflicts" id="idm45085105450592"></a>
        <a data-type="indexterm" data-primary="write skew (transaction isolation)" data-secondary="materializing conflicts" id="idm45085105449472"></a>
        <a data-type="indexterm" data-primary="materialization" data-secondary="conflicts" id="idm45085105448352"></a>
        <a data-type="indexterm" data-primary="locks" data-secondary="materializing conflicts with" id="idm45085105447248"></a>
        If the problem of phantoms is that there is no object to which we can attach the locks, perhaps we
        can artificially introduce a lock object into the database?</p>
        
        <p>For example, in the meeting room booking case you could imagine creating a table of time slots and
        rooms. Each row in this table corresponds to a particular room for a particular time period (say, 15
        minutes). You create rows for all possible combinations of rooms and time periods ahead of time,
        e.g. for the next six months.</p>
        
        <p>Now a transaction that wants to create a booking can lock (<code>SELECT FOR UPDATE</code>) the rows in the
        table that correspond to the desired room and time period. After it has acquired the locks, it can
        check for overlapping bookings and insert a new booking as before. Note that the additional table
        isn’t used to store information about the booking—it’s purely a collection of locks which is used
        to prevent bookings on the same room and time range from being modified concurrently.</p>
        
        <p><a data-type="indexterm" data-primary="conflicts" data-secondary="materializing" id="idm45085105443696"></a>
        This approach is called <em>materializing conflicts</em>, because it takes a phantom and turns it into a
        lock conflict on a concrete set of rows that exist in the database
        [<a data-type="noteref" href="ch07.html#Fekete2005ee">11</a>]. Unfortunately, it can be hard and
        error-prone to figure out how to materialize conflicts, and it’s ugly to let a concurrency control
        mechanism leak into the application data model. For those reasons, materializing conflicts should be
        considered a last resort if no alternative is possible. A serializable isolation level is much
        preferable in most cases.
        <a data-type="indexterm" data-primary="write skew (transaction isolation)" data-startref="ix_wrskew" id="idm45085105440528"></a>
        <a data-type="indexterm" data-primary="skew" data-secondary="in transaction isolation" data-tertiary="write skew" data-startref="ix_skewwrite" id="idm45085105439392"></a>
        <a data-type="indexterm" data-primary="concurrency" data-secondary="write skew (transaction isolation)" data-startref="ix_concwrskew" id="idm45085105437728"></a>
        <a data-type="indexterm" data-primary="conflicts" data-secondary="write skew (transaction isolation)" data-startref="ix_conflictwrskew" id="idm45085105436336"></a>
        <a data-type="indexterm" data-primary="updates" data-secondary="preventing write skew" data-startref="ix_updatewrskew" id="idm45085105434944"></a>
        <a data-type="indexterm" data-primary="isolation (in transactions)" data-secondary="weak isolation levels" data-startref="ix_isolateweak" id="idm45085105433568"></a>
        <a data-type="indexterm" data-primary="transactions" data-secondary="weak isolation levels" data-startref="ix_transactisolate" id="idm45085105432176"></a>
        <a data-type="indexterm" data-primary="race conditions" data-secondary="write skew" data-startref="ix_racewrskew" id="idm45085105430800"></a></p>
        </div></section>
        
        
        
        </div></section>
        
        
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Serializability"><div class="sect1" id="sec_transactions_serializability">
        <h1>Serializability</h1>
        
        <p><a data-type="indexterm" data-primary="isolation (in transactions)" data-secondary="serializability" id="ix_isolateserial"></a>
        <a data-type="indexterm" data-primary="serializability" id="ix_serializability"></a>
        <a data-type="indexterm" data-primary="transactions" data-secondary="serializability" id="ix_transactserial"></a>
        In this chapter we have seen several examples of transactions that are prone to race conditions.
        Some race conditions are prevented by the read committed and snapshot isolation levels, but
        others are not. We encountered some particularly tricky examples with write skew and phantoms. It’s
        a sad situation:</p>
        
        <ul>
        <li>
        <p>Isolation levels are hard to understand, and inconsistently implemented in different databases
        (e.g., the meaning of “repeatable read” varies significantly).</p>
        </li>
        <li>
        <p>If you look at your application code, it’s difficult to tell whether it is safe to run at a
        particular isolation level—especially in a large application, where you might not be aware of
        all the things that may be happening concurrently.</p>
        </li>
        <li>
        <p>There are no good tools to help us detect race conditions. In principle, static analysis may
        help [<a data-type="noteref" href="ch07.html#Jorwekar2007uq_ch7">26</a>],  but research techniques have not
        yet found their way into practical use. Testing for concurrency issues is hard, because they are
        usually nondeterministic—problems only occur if you get unlucky with the timing.</p>
        </li>
        </ul>
        
        <p>This is not a new problem—it has been like this since the 1970s, when weak isolation levels were
        first introduced [<a data-type="noteref" href="ch07.html#Gray1976us">2</a>]. All along, the answer
        from researchers has been simple: use <em>serializable</em> isolation!</p>
        
        <p><a data-type="indexterm" data-primary="race conditions" data-secondary="preventing with serializable isolation" id="idm45085105417520"></a>
        Serializable isolation is usually regarded as the strongest isolation level. It guarantees that even
        though transactions may execute in parallel, the end result is the same as if they had executed one
        at a time, <em>serially</em>, without any concurrency. Thus, the database guarantees that if the
        transactions behave correctly when run individually, they continue to be correct when run
        concurrently—in other words, the database prevents <em>all</em> possible race conditions.</p>
        
        <p>But if serializable isolation is so much better than the mess of weak isolation levels, then why
        isn’t everyone using it? To answer this question, we need to look at the options for implementing
        serializability, and how they perform. Most databases that provide serializability today use one of
        three techniques, which we will explore in the rest of this chapter:</p>
        
        <ul>
        <li>
        <p>Literally executing transactions in a serial order (see <a data-type="xref" href="#sec_transactions_serial">“Actual Serial Execution”</a>)</p>
        </li>
        <li>
        <p>Two-phase locking (see <a data-type="xref" href="#sec_transactions_2pl">“Two-Phase Locking (2PL)”</a>), which for several decades was the only viable
        option</p>
        </li>
        <li>
        <p>Optimistic concurrency control techniques such as serializable snapshot isolation (see
        <a data-type="xref" href="#sec_transactions_ssi">“Serializable Snapshot Isolation (SSI)”</a>)</p>
        </li>
        </ul>
        
        <p>For now, we will discuss these techniques primarily in the context of single-node databases; in
        <a data-type="xref" href="ch09.html#ch_consistency">Chapter&nbsp;9</a> we will examine how they can be generalized to transactions that involve multiple
        nodes in a distributed system.</p>
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Actual Serial Execution"><div class="sect2" id="sec_transactions_serial">
        <h2>Actual Serial Execution</h2>
        
        <p><a data-type="indexterm" data-primary="isolation (in transactions)" data-secondary="serializability" data-tertiary="actual serial execution" id="ix_isolateserialserial"></a>
        <a data-type="indexterm" data-primary="transactions" data-secondary="serializability" data-tertiary="actual serial execution" id="ix_transactserialserial"></a>
        <a data-type="indexterm" data-primary="serializability" data-secondary="serial execution" id="ix_serialexecute"></a>
        <a data-type="indexterm" data-primary="single-threaded execution" id="idm45085105400640"></a>
        <a data-type="indexterm" data-primary="threads (concurrency)" data-secondary="single" data-see="single-threaded execution" id="idm45085105399792"></a>
        The simplest way of avoiding concurrency problems is to remove the concurrency entirely: to
        execute only one transaction at a time, in serial order, on a single thread. By doing so, we completely
        sidestep the problem of detecting and preventing conflicts between transactions: the resulting
        isolation is by definition serializable.</p>
        
        <p>Even though this seems like an obvious idea, database designers only fairly recently—around
        2007—decided that a single-threaded loop for executing transactions was feasible
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Stonebraker2007ub_ch7-marker" href="ch07.html#Stonebraker2007ub_ch7">45</a>].
        If multi-threaded concurrency was considered essential for getting good performance during the
        previous 30 years, what changed to make single-threaded execution possible?</p>
        
        <p>Two developments caused this rethink:</p>
        
        <ul>
        <li>
        <p><a data-type="indexterm" data-primary="in-memory databases" data-secondary="serial transaction execution" id="idm45085105393696"></a>
        <a data-type="indexterm" data-primary="memory" data-secondary="in-memory databases" data-tertiary="serial transaction execution" id="idm45085105392528"></a>
        RAM became cheap enough that for many use cases it is now feasible to keep the entire
        active dataset in memory (see <a data-type="xref" href="ch03.html#sec_storage_inmemory">“Keeping everything in memory”</a>). When all data that a transaction needs to
        access is in memory, transactions can execute much faster than if they have to wait for data to be
        loaded from disk.</p>
        </li>
        <li>
        <p><a data-type="indexterm" data-primary="OLTP (online transaction processing)" data-secondary="workload characteristics" id="idm45085105389328"></a>
        Database designers realized that OLTP transactions are usually short and only make a small number
        of reads and writes (see <a data-type="xref" href="ch03.html#sec_storage_analytics">“Transaction Processing or Analytics?”</a>). By contrast, long-running analytic queries
        are typically read-only, so they can be run on a consistent snapshot (using snapshot isolation)
        outside of the serial execution loop.</p>
        </li>
        </ul>
        
        <p><a data-type="indexterm" data-primary="VoltDB (database)" data-secondary="serial execution of transactions" id="idm45085105386544"></a>
        <a data-type="indexterm" data-primary="Datomic (database)" data-secondary="serial execution of transactions" id="idm45085105385248"></a>
        <a data-type="indexterm" data-primary="Redis (database)" data-secondary="single-threaded execution" id="idm45085105384128"></a>
        The approach of executing transactions serially is implemented in VoltDB/H-Store, Redis, and Datomic
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Hugg2014ti-marker" href="ch07.html#Hugg2014ti">46</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kallman2008tf-marker" href="ch07.html#Kallman2008tf">47</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Hickey2012wm-marker" href="ch07.html#Hickey2012wm">48</a>].
        A system designed for single-threaded execution can sometimes perform better than a system that
        supports concurrency, because it can avoid the coordination overhead of locking. However, its
        throughput is limited to that of a single CPU core. In order to make the most of that single thread,
        transactions need to be structured differently from their traditional form.</p>
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Encapsulating transactions in stored procedures"><div class="sect3" id="idm45085105375936">
        <h3>Encapsulating transactions in stored procedures</h3>
        
        <p><a data-type="indexterm" data-primary="serializability" data-secondary="serial execution" data-tertiary="using stored procedures" id="idm45085105374592"></a>
        <a data-type="indexterm" data-primary="stored procedures" id="ix_storedproc"></a>
        In the early days of databases, the intention was that a database transaction could encompass an
        entire flow of user activity. For example, booking an airline ticket is a multi-stage process
        (searching for routes, fares, and available seats; deciding on an itinerary; booking seats on
        each of the flights of the itinerary; entering passenger details; making payment). Database
        designers thought that it would be neat if that entire process was one transaction so that it could
        be committed atomically.</p>
        
        <p>Unfortunately, humans are very slow to make up their minds and respond. If a database transaction
        needs to wait for input from a user, the database needs to support a potentially huge number of
        concurrent transactions, most of them idle. Most databases cannot do that efficiently, and so almost
        all OLTP applications keep transactions short by avoiding interactively waiting for a user within a
        transaction. On the web, this means that a transaction is committed within the same HTTP request—a
        transaction does not span multiple requests. A new HTTP request starts a new transaction.</p>
        
        <p>Even though the human has been taken out of the critical path, transactions have continued to be
        executed in an interactive client/server style, one statement at a time. An application makes a
        query, reads the result, perhaps makes another query depending on the result of the first query, and
        so on. The queries and results are sent back and forth between the application code (running on one
        machine) and the database server (on another machine).</p>
        
        <p>In this interactive style of transaction, a lot of time is spent in network communication between
        the application and the database. If you were to disallow concurrency in the database and only
        process one transaction at a time, the throughput would be dreadful because the database would
        spend most of its time waiting for the application to issue the next query for the current
        transaction. In this kind of database, it’s necessary to process multiple transactions concurrently
        in order to get reasonable performance.</p>
        
        <p>For this reason, systems with single-threaded serial transaction processing don’t allow interactive
        multi-statement transactions. Instead, the application must submit the entire transaction code to
        the database ahead of time, as a <em>stored procedure</em>. The differences between these approaches is
        illustrated in <a data-type="xref" href="#fig_transactions_stored_proc">Figure&nbsp;7-9</a>. Provided that all data required by a transaction is
        in memory, the stored procedure can execute very fast, without waiting for any network or disk I/O.</p>
        
        <figure><div id="fig_transactions_stored_proc" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_0709.png" alt="ddia 0709" width="2880" height="1663">
        <h6><span class="label">Figure 7-9. </span>The difference between an interactive transaction and a stored procedure (using the example transaction of <a data-type="xref" href="#fig_transactions_write_skew">Figure&nbsp;7-8</a>).</h6>
        </div></figure>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Pros and cons of stored procedures"><div class="sect3" id="idm45085105363520">
        <h3>Pros and cons of stored procedures</h3>
        
        <p><a data-type="indexterm" data-primary="stored procedures" data-secondary="pros and cons of" id="idm45085105362176"></a>
        <a data-type="indexterm" data-primary="SQL (Structured Query Language)" data-secondary="stored procedures" id="idm45085105361072"></a>
        Stored procedures have existed for some time in relational databases, and they have been part of the
        SQL standard (SQL/PSM) since 1999. They have gained a somewhat bad reputation, for various reasons:</p>
        
        <ul>
        <li>
        <p><a data-type="indexterm" data-primary="programming languages" data-secondary="for stored procedures" id="idm45085105358832"></a>
        <a data-type="indexterm" data-primary="Oracle (database)" data-secondary="PL/SQL language" id="idm45085105357728"></a>
        <a data-type="indexterm" data-primary="SQL Server (database)" data-secondary="T-SQL language" id="idm45085105356624"></a>
        <a data-type="indexterm" data-primary="PostgreSQL (database)" data-secondary="PL/pgSQL language" id="idm45085105355520"></a>
        Each database vendor has its own language for stored procedures (Oracle has PL/SQL, SQL Server
        has T-SQL, PostgreSQL has PL/pgSQL, etc.). These languages haven’t kept up with developments in
        general-purpose programming languages, so they look quite ugly and archaic from today’s point of
        view, and they lack the ecosystem of libraries that you find with most programming languages.</p>
        </li>
        <li>
        <p>Code running in a database is difficult to manage: compared to an application server, it’s harder
        to debug, more awkward to keep in version control and deploy, trickier to test, and difficult to
        integrate with a metrics collection system for monitoring.</p>
        </li>
        <li>
        <p>A database is often much more performance-sensitive than an application server, because a single
        database instance is often shared by many application servers. A badly written stored procedure
        (e.g., using a lot of memory or CPU time) in a database can cause much more trouble than equivalent
        badly written code in an application server.</p>
        </li>
        </ul>
        
        <p><a data-type="indexterm" data-primary="VoltDB (database)" data-secondary="deterministic stored procedures" id="idm45085105351152"></a>
        <a data-type="indexterm" data-primary="Datomic (database)" data-secondary="languages for transactions" id="idm45085105350032"></a>
        <a data-type="indexterm" data-primary="Redis (database)" data-secondary="Lua scripting" id="idm45085105348912"></a>
        However, those issues can be overcome. Modern implementations of stored procedures have abandoned
        PL/SQL and use existing general-purpose programming languages instead: VoltDB uses Java or Groovy,
        Datomic uses Java or Clojure, and Redis uses Lua.</p>
        
        <p>With stored procedures and in-memory data, executing all transactions on a single thread becomes
        feasible. As they don’t need to wait for I/O and they avoid the overhead of other concurrency control
        mechanisms, they can achieve quite good throughput on a single thread.</p>
        
        <p><a data-type="indexterm" data-primary="deterministic operations" id="idm45085105346592"></a>
        VoltDB also uses stored procedures for replication: instead of copying a transaction’s writes from
        one node to another, it executes the same stored procedure on each replica. VoltDB therefore
        requires that stored procedures are <em>deterministic</em> (when run on different nodes, they must produce
        the same result). If a transaction needs to use the current date and time, for example, it must do
        so through special deterministic APIs.
        <a data-type="indexterm" data-primary="stored procedures" data-startref="ix_storedproc" id="idm45085105344912"></a></p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Partitioning"><div class="sect3" id="idm45085105343808">
        <h3>Partitioning</h3>
        
        <p><a data-type="indexterm" data-primary="serializability" data-secondary="serial execution" data-tertiary="partitioning" id="idm45085105342496"></a>
        <a data-type="indexterm" data-primary="partitioning" data-secondary="serial execution of transactions and" id="idm45085105341120"></a>
        Executing all transactions serially makes concurrency control much simpler, but limits the
        transaction throughput of the database to the speed of a single CPU core on a single machine.
        Read-only transactions may execute elsewhere, using snapshot isolation, but for applications with
        high write throughput, the single-threaded transaction processor can become a serious bottleneck.</p>
        
        <p><a data-type="indexterm" data-primary="VoltDB (database)" data-secondary="cross-partition serializability" id="idm45085105339344"></a>
        In order to scale to multiple CPU cores, and multiple nodes, you can potentially partition your data
        (see <a data-type="xref" href="ch06.html#ch_partitioning">Chapter&nbsp;6</a>), which is supported in VoltDB. If you can find a way of partitioning your
        dataset so that each transaction only needs to read and write data within a single partition, then
        each partition can have its own transaction processing thread running independently from the
        others. In this case, you can give each CPU core its own partition, which allows your transaction
        throughput to scale linearly with the number of CPU cores
        [<a data-type="noteref" href="ch07.html#Kallman2008tf">47</a>].</p>
        
        <p><a data-type="indexterm" data-primary="coordination" data-secondary="cross-partition ordering" id="idm45085105335856"></a>
        However, for any transaction that needs to access multiple partitions, the database must coordinate the
        transaction across all the partitions that it touches. The stored procedure
        needs to be performed in lock-step across all partitions to ensure serializability across the whole
        system.</p>
        
        <p>Since cross-partition transactions have additional coordination overhead, they are vastly slower
        than single-partition transactions. VoltDB reports a throughput of about 1,000 cross-partition
        writes per second, which is orders of magnitude below its single-partition throughput and cannot be
        increased by adding more machines [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Hugg2014ua-marker" href="ch07.html#Hugg2014ua">49</a>].</p>
        
        <p>Whether transactions can be single-partition depends very much on the structure of the data used by
        the application. Simple key-value data can often be partitioned very easily, but data with multiple
        secondary indexes is likely to require a lot of cross-partition coordination (see
        <a data-type="xref" href="ch06.html#sec_partitioning_secondary_indexes">“Partitioning and Secondary Indexes”</a>).</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Summary of serial execution"><div class="sect3" id="idm45085105329696">
        <h3>Summary of serial execution</h3>
        
        <p>Serial execution of transactions has become a viable way of achieving serializable isolation within
        certain constraints:</p>
        
        <ul>
        <li>
        <p>Every transaction must be small and fast, because it takes only one slow transaction to stall all
        transaction processing.</p>
        </li>
        <li>
        <p>It is limited to use cases where the active dataset can fit in memory. Rarely accessed data could
        potentially be moved to disk, but if it needed to be accessed in a single-threaded transaction,
        the system would get very slow.<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm45085105325856-marker" href="ch07.html#idm45085105325856">x</a></sup></p>
        </li>
        <li>
        <p>Write throughput must be low enough to be handled on a single CPU core, or else transactions
        need to be partitioned without requiring cross-partition coordination.</p>
        </li>
        <li>
        <p>Cross-partition transactions are possible, but there is a hard limit to the extent to which they
          can be used.
        <a data-type="indexterm" data-primary="isolation (in transactions)" data-secondary="serializability" data-tertiary="actual serial execution" data-startref="ix_isolateserialserial" id="idm45085105321280"></a>
        <a data-type="indexterm" data-primary="transactions" data-secondary="serializability" data-tertiary="actual serial execution" data-startref="ix_transactserialserial" id="idm45085105319568"></a>
        <a data-type="indexterm" data-primary="serializability" data-secondary="serial execution" data-startref="ix_serialexecute" id="idm45085105317920"></a></p>
        </li>
        </ul>
        </div></section>
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Two-Phase Locking (2PL)"><div class="sect2" id="sec_transactions_2pl">
        <h2>Two-Phase Locking (2PL)</h2>
        
        <p><a data-type="indexterm" data-primary="two-phase locking (2PL)" id="ix_twophaselock"></a>
        <a data-type="indexterm" data-primary="locks" data-secondary="for transaction isolation" data-tertiary="in two-phase locking (2PL)" id="ix_locks2PL"></a>
        <a data-type="indexterm" data-primary="isolation (in transactions)" data-secondary="serializability" data-tertiary="two-phase locking (2PL)" id="ix_isolateserial2PL"></a>
        <a data-type="indexterm" data-primary="serializability" data-secondary="two-phase locking (2PL)" id="ix_serial2PL"></a>
        <a data-type="indexterm" data-primary="transactions" data-secondary="serializability" data-tertiary="two-phase locking (2PL)" id="ix_transactserial2PL"></a>
        For around 30 years, there was only one widely used algorithm for serializability in databases:
        <em>two-phase locking</em> (2PL).<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm45085105306432-marker" href="ch07.html#idm45085105306432">xi</a></sup></p>
        <div data-type="note" epub:type="note"><h1>2PL is not 2PC</h1>
        <p>Note that while two-phase <em>locking</em> (2PL) sounds very similar to two-phase <em>commit</em> (2PC), they are
        completely different things. We will discuss 2PC in <a data-type="xref" href="ch09.html#ch_consistency">Chapter&nbsp;9</a>.</p>
        </div>
        
        <p>We saw previously that locks are often used to prevent dirty writes (see
        <a data-type="xref" href="#sec_transactions_dirty_write">“No dirty writes”</a>): if two transactions concurrently try to write to the same object,
        the lock ensures that the second writer must wait until the first one has finished its transaction
        (aborted or committed) before it may continue.</p>
        
        <p>Two-phase locking is similar, but makes the lock requirements much stronger. Several transactions
        are allowed to concurrently read the same object as long as nobody is writing to it. But as soon as
        anyone wants to write (modify or delete) an object, exclusive access is required:</p>
        
        <ul>
        <li>
        <p>If transaction A has read an object and transaction B wants to write to that object, B must wait
        until A commits or aborts before it can continue. (This ensures that B can’t change the object
        unexpectedly behind A’s back.)</p>
        </li>
        <li>
        <p>If transaction A has written an object and transaction B wants to read that object, B must wait
        until A commits or aborts before it can continue. (Reading an old version of the object, like in
        <a data-type="xref" href="#fig_transactions_read_committed">Figure&nbsp;7-4</a>, is not acceptable under 2PL.)</p>
        </li>
        </ul>
        
        <p>In 2PL, writers don’t just block other writers; they also block readers and vice
        versa. Snapshot isolation has the mantra <em>readers never block writers, and writers never block
        readers</em> (see <a data-type="xref" href="#sec_transactions_snapshot_impl">“Implementing snapshot isolation”</a>), which captures this key difference between
        snapshot isolation and two-phase locking. On the other hand, because 2PL provides serializability,
        it protects against all the race conditions discussed earlier, including lost updates and write skew.</p>
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Implementation of two-phase locking"><div class="sect3" id="idm45085105294032">
        <h3>Implementation of two-phase locking</h3>
        
        <p><a data-type="indexterm" data-primary="InnoDB (storage engine)" data-secondary="preventing write skew" id="idm45085105292720"></a>
        <a data-type="indexterm" data-primary="InnoDB (storage engine)" data-secondary="serializable isolation" id="idm45085105291392"></a>
        <a data-type="indexterm" data-primary="IBM" data-secondary="DB2 (database)" data-tertiary="serializable isolation" id="idm45085105290288"></a>
        <a data-type="indexterm" data-primary="SQL Server (database)" data-secondary="preventing write skew" id="idm45085105288912"></a>
        <a data-type="indexterm" data-primary="SQL Server (database)" data-secondary="serializable isolation" id="idm45085105287808"></a>
        2PL is used by the serializable isolation level in MySQL (InnoDB) and SQL Server, and the
        repeatable read isolation level in DB2
        [<a data-type="noteref" href="ch07.html#Kleppmann2014ut">23</a>,
        <a data-type="noteref" href="ch07.html#Mukherjee2013uw">36</a>].</p>
        
        <p><a data-type="indexterm" data-primary="shared mode (locks)" id="idm45085105284800"></a>
        <a data-type="indexterm" data-primary="exclusive mode (locks)" id="idm45085105283936"></a>
        <a data-type="indexterm" data-primary="locks" data-secondary="for transaction isolation" data-tertiary="shared mode and exclusive mode" id="idm45085105283104"></a>
        The blocking of readers and writers is implemented by having a lock on each object in the
        database. The lock can either be in <em>shared mode</em> or in <em>exclusive mode</em>. The lock is used as
        follows:</p>
        
        <ul>
        <li>
        <p><a data-type="indexterm" data-primary="locks" data-secondary="for transaction isolation" data-tertiary="read locks (shared mode)" id="idm45085105279840"></a>
        If a transaction wants to read an object, it must first acquire the lock in shared mode. Several
        transactions are allowed to hold the lock in shared mode simultaneously, but if another
        transaction already has an exclusive lock on the object, these transactions must wait.</p>
        </li>
        <li>
        <p>If a transaction wants to write to an object, it must first acquire the lock in exclusive mode. No
        other transaction may hold the lock at the same time (either in shared or in exclusive mode), so
        if there is any existing lock on the object, the transaction must wait.</p>
        </li>
        <li>
        <p>If a transaction first reads and then writes an object, it may upgrade its shared lock to an
        exclusive lock. The upgrade works the same as getting an exclusive lock directly.</p>
        </li>
        <li>
        <p>After a transaction has acquired the lock, it must continue to hold the lock until the end of the
        transaction (commit or abort). This is where the name “two-phase” comes from: the first phase
        (while the transaction is executing) is when the locks are acquired, and the second phase (at the
        end of the transaction) is when all the locks are released.</p>
        </li>
        </ul>
        
        <p><a data-type="indexterm" data-primary="deadlocks" data-secondary="in two-phase locking (2PL)" id="idm45085105274144"></a>
        <a data-type="indexterm" data-primary="locks" data-secondary="deadlock" id="idm45085105273024"></a>
        Since so many locks are in use, it can happen quite easily that transaction A is stuck waiting for
        transaction B to release its lock, and vice versa. This situation is called <em>deadlock</em>. The database
        automatically detects deadlocks between transactions and aborts one of them so that the others can
        make progress. The aborted transaction needs to be retried by the application.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Performance of two-phase locking"><div class="sect3" id="idm45085105271008">
        <h3>Performance of two-phase locking</h3>
        
        <p><a data-type="indexterm" data-primary="serializability" data-secondary="two-phase locking (2PL)" data-tertiary="performance" id="idm45085105269664"></a>
        <a data-type="indexterm" data-primary="locks" data-secondary="for transaction isolation" data-tertiary="performance" id="idm45085105268288"></a>
        <a data-type="indexterm" data-primary="two-phase locking (2PL)" data-secondary="performance of" id="idm45085105266896"></a>
        The big downside of two-phase locking, and the reason why it hasn’t been used by everybody since the
        1970s, is performance: transaction throughput and response times of queries are significantly worse
        under two-phase locking than under weak isolation.</p>
        
        <p>This is partly due to the overhead of acquiring and releasing all those locks, but more importantly
        due to reduced concurrency. By design, if two concurrent transactions try to do anything that may
        in any way result in a race condition, one has to wait for the other to complete.</p>
        
        <p>Traditional relational databases don’t limit the duration of a transaction, because they are
        designed for interactive applications that wait for human input. Consequently, when one transaction
        has to wait on another, there is no limit on how long it may have to wait. Even if you make sure
        that you keep all your transactions short, a queue may form if several transactions want to access
        the same object, so a transaction may have to wait for several others to complete before it can do
        anything.</p>
        
        <p><a data-type="indexterm" data-primary="latency" data-secondary="instability under two-phase locking" id="idm45085105263632"></a>
        For this reason, databases running 2PL can have quite unstable latencies, and they can be very slow at
        high percentiles (see <a data-type="xref" href="ch01.html#sec_introduction_percentiles">“Describing Performance”</a>) if there is contention in the workload. It
        may take just one slow transaction, or one transaction that accesses a lot of data and acquires many
        locks, to cause the rest of the system to grind to a halt. This instability is problematic when
        robust operation is required.</p>
        
        <p>Although deadlocks can happen with the lock-based read committed isolation level, they occur much
        more frequently under 2PL serializable isolation (depending on the access patterns of your
        transaction). This can be an additional performance problem: when a transaction is aborted due to
        deadlock and is retried, it needs to do its work all over again. If deadlocks are frequent, this can
        mean significant wasted effort.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Predicate locks"><div class="sect3" id="sec_transactions_2pl_predicate">
        <h3>Predicate locks</h3>
        
        <p><a data-type="indexterm" data-primary="write skew (transaction isolation)" data-secondary="preventing" data-tertiary="in two-phase locking" id="ix_wrskew2pl"></a>
        <a data-type="indexterm" data-primary="predicate locks" id="idm45085105257040"></a>
        <a data-type="indexterm" data-primary="phantoms (transaction isolation)" data-secondary="preventing, in serializability" id="idm45085105256208"></a>
        In the preceding description of locks, we glossed over a subtle but important detail. In
        <a data-type="xref" href="#sec_transactions_phantom">“Phantoms causing write skew”</a> we discussed the problem of <em>phantoms</em>—that is, one transaction
        changing the results of another transaction’s search query. A database with serializable isolation
        must prevent phantoms.</p>
        
        <p><a data-type="indexterm" data-primary="meeting room booking (example)" id="idm45085105253312"></a>
        In the meeting room booking example this means that if one transaction has searched for existing
        bookings for a room within a certain time window (see <a data-type="xref" href="#fig_transactions_meeting_rooms">Example&nbsp;7-2</a>), another
        transaction is not allowed to concurrently insert or update another booking for the same room and
        time range. (It’s okay to concurrently insert bookings for other rooms, or for the same room at a
        different time that doesn’t affect the proposed booking.)</p>
        
        <p>How do we implement this? Conceptually, we need a <em>predicate lock</em>
        [<a data-type="noteref" href="ch07.html#Eswaran1976uu">3</a>]. It works similarly to the
        shared/exclusive lock described earlier, but rather than belonging to a particular object (e.g., one
        row in a table), it belongs to all objects that match some search condition, such as:</p>
        
        <pre data-type="programlisting" data-code-language="sql"><code class="k">SELECT</code> <code class="o">*</code> <code class="k">FROM</code> <code class="n">bookings</code>
          <code class="k">WHERE</code> <code class="n">room_id</code> <code class="o">=</code> <code class="mi">123</code> <code class="k">AND</code>
            <code class="n">end_time</code>   <code class="o">&gt;</code> <code class="s1">'2018-01-01 12:00'</code> <code class="k">AND</code>
            <code class="n">start_time</code> <code class="o">&lt;</code> <code class="s1">'2018-01-01 13:00'</code><code class="p">;</code></pre>
        
        <p>A predicate lock restricts access as follows:</p>
        
        <ul>
        <li>
        <p>If transaction A wants to read objects matching some condition, like in that <code>SELECT</code> query, it
        must acquire a shared-mode predicate lock on the conditions of the query. If another transaction B
        currently has an exclusive lock on any object matching those conditions, A must wait until B
        releases its lock before it is allowed to make its query.</p>
        </li>
        <li>
        <p>If transaction A wants to insert, update, or delete any object, it must first check whether either the old
        or the new value matches any existing predicate lock. If there is a matching predicate lock held by
        transaction B, then A must wait until B has committed or aborted before it can continue.</p>
        </li>
        </ul>
        
        <p>The key idea here is that a predicate lock applies even to objects that do not yet exist in the
        database, but which might be added in the future (phantoms). If two-phase locking includes predicate locks,
        the database prevents all forms of write skew and other race conditions, and so its isolation
        becomes serializable.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Index-range locks"><div class="sect3" id="sec_transactions_2pl_range">
        <h3>Index-range locks</h3>
        
        <p><a data-type="indexterm" data-primary="serializability" data-secondary="two-phase locking (2PL)" data-tertiary="index-range locks" id="idm45085105228336"></a>
        <a data-type="indexterm" data-primary="two-phase locking (2PL)" data-secondary="index-range locks" id="idm45085105226800"></a>
        <a data-type="indexterm" data-primary="locks" data-secondary="for transaction isolation" data-tertiary="preventing phantoms with index-range locks" id="idm45085105225696"></a>
        <a data-type="indexterm" data-primary="indexes" data-secondary="index-range locking" id="idm45085105224256"></a>
        <a data-type="indexterm" data-primary="next-key locking" id="idm45085105194912"></a>
        Unfortunately, predicate locks do not perform well: if there are many locks by active transactions,
        checking for matching locks becomes time-consuming. For that reason, most databases with 2PL
        actually implement <em>index-range locking</em> (also known as <em>next-key locking</em>), which is a simplified
        approximation of predicate locking [<a data-type="noteref" href="ch07.html#Ports2012uw">41</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Hellerstein2007be_ch7-marker" href="ch07.html#Hellerstein2007be_ch7">50</a>].</p>
        
        <p>It’s safe to simplify a predicate by making it match a greater set of objects. For example, if you
        have a predicate lock for bookings of room 123 between noon and 1 p.m., you can approximate it by
        locking bookings for room 123 at any time, or you can approximate it by locking all rooms (not just
        room 123) between noon and 1 p.m. This is safe, because any write that matches the original predicate
        will definitely also match the approximations.</p>
        
        <p>In the room bookings database you would probably have an index on the <code>room_id</code> column, and/or
        indexes on <code>start_time</code> and <code>end_time</code> (otherwise the preceding query would be very slow on a large
        database):</p>
        
        <ul>
        <li>
        <p>Say your index is on <code>room_id</code>, and the database uses this index to find existing bookings for
        room 123. Now the database can simply attach a shared lock to this index entry, indicating that a
        transaction has searched for bookings of room 123.</p>
        </li>
        <li>
        <p>Alternatively, if the database uses a time-based index to find existing bookings, it can attach a
        shared lock to a range of values in that index, indicating that a transaction has searched for
        bookings that overlap with the time period of noon to 1 p.m. on January 1, 2018.</p>
        </li>
        </ul>
        
        <p>Either way, an approximation of the search condition is attached to one of the indexes. Now, if
        another transaction wants to insert, update, or delete a booking for the same room and/or an
        overlapping time period, it will have to update the same part of the index. In the process of doing
        so, it will encounter the shared lock, and it will be forced to wait until the lock is released.</p>
        
        <p>This provides effective protection against phantoms and write skew. Index-range locks are not as
        precise as predicate locks would be (they may lock a bigger range of objects than is strictly
        necessary to maintain serializability), but since they have much lower overheads, they are a good
        compromise.</p>
        
        <p>If there is no suitable index where a range lock can be attached, the database can fall back to a
        shared lock on the entire table. This will not be good for performance, since it will stop all
        other transactions writing to the table, but it’s a safe fallback position.
        <a data-type="indexterm" data-primary="write skew (transaction isolation)" data-secondary="preventing" data-tertiary="in two-phase locking" data-startref="ix_wrskew2pl" id="idm45085105181456"></a>
        <a data-type="indexterm" data-primary="two-phase locking (2PL)" data-startref="ix_twophaselock" id="idm45085105179840"></a>
        <a data-type="indexterm" data-primary="locks" data-secondary="for transaction isolation" data-tertiary="in two-phase locking (2PL)" data-startref="ix_locks2PL" id="idm45085105178736"></a>
        <a data-type="indexterm" data-primary="isolation (in transactions)" data-secondary="serializability" data-tertiary="two-phase locking (2PL)" data-startref="ix_isolateserial2PL" id="idm45085105177056"></a>
        <a data-type="indexterm" data-primary="serializability" data-secondary="two-phase locking (2PL)" data-startref="ix_serial2PL" id="idm45085105175392"></a>
        <a data-type="indexterm" data-primary="transactions" data-secondary="serializability" data-tertiary="two-phase locking (2PL)" data-startref="ix_transactserial2PL" id="idm45085105174016"></a></p>
        </div></section>
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Serializable Snapshot Isolation (SSI)"><div class="sect2" id="sec_transactions_ssi">
        <h2>Serializable Snapshot Isolation (SSI)</h2>
        
        <p><a data-type="indexterm" data-primary="transactions" data-secondary="serializability" data-tertiary="serializable snapshot isolation (SSI)" id="ix_transactserialSSI"></a>
        <a data-type="indexterm" data-primary="isolation (in transactions)" data-secondary="serializability" data-tertiary="serializable snapshot isolation (SSI)" id="ix_isolateserialSSI"></a>
        <a data-type="indexterm" data-primary="snapshots (databases)" data-secondary="serializable snapshot isolation (SSI)" id="ix_snapshtSSI"></a>
        <a data-type="indexterm" data-primary="serializability" data-secondary="serializable snapshot isolation (SSI)" id="ix_serializeSSI"></a>
        This chapter has painted a bleak picture of concurrency control in databases. On the one hand, we
        have implementations of serializability that don’t perform well (two-phase locking) or don’t scale
        well (serial execution). On the other hand, we have weak isolation levels that have good
        performance, but are prone to various race conditions (lost updates, write skew, phantoms, etc.). Are
        serializable isolation and good performance fundamentally at odds with each other?</p>
        
        <p>Perhaps not: an algorithm called <em>serializable snapshot isolation</em> (SSI) is very promising. It
        provides full serializability, but has only a small performance penalty compared to snapshot
        isolation. SSI is fairly new: it was first described in 2008
        [<a data-type="noteref" href="ch07.html#Cahill2008eg">40</a>] and is the subject of Michael Cahill’s
        PhD thesis
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Cahill2009us-marker" href="ch07.html#Cahill2009us">51</a>].</p>
        
        <p><a data-type="indexterm" data-primary="PostgreSQL (database)" data-secondary="preventing write skew" id="idm45085105160352"></a>
        <a data-type="indexterm" data-primary="PostgreSQL (database)" data-secondary="serializable snapshot isolation (SSI)" id="idm45085105159248"></a>
        <a data-type="indexterm" data-primary="FoundationDB (database)" data-secondary="serializable transactions" id="idm45085105158080"></a>
        Today SSI is used both in single-node databases (the serializable isolation level
        in PostgreSQL since version 9.1 [<a data-type="noteref" href="ch07.html#Ports2012uw">41</a>]) and
        distributed databases (FoundationDB uses a similar algorithm).
        As SSI is so young compared to other concurrency control mechanisms, it is still proving its
        performance in practice, but it has the possibility of being fast enough to become the new default in
        the future.</p>
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Pessimistic versus optimistic concurrency control"><div class="sect3" id="idm45085105155616">
        <h3>Pessimistic versus optimistic concurrency control</h3>
        
        <p><a data-type="indexterm" data-primary="serializability" data-secondary="pessimistic versus optimistic concurrency control" id="idm45085105154240"></a>
        <a data-type="indexterm" data-primary="transactions" data-secondary="serializability" data-tertiary="pessimistic versus optimistic concurrency control" id="idm45085105152816"></a>
        <a data-type="indexterm" data-primary="pessimistic concurrency control" id="idm45085105151408"></a>
        <a data-type="indexterm" data-primary="concurrency" data-secondary="optimistic concurrency control" id="idm45085105150608"></a>
        <a data-type="indexterm" data-primary="mutual exclusion" data-seealso="locks" id="idm45085105149488"></a>
        Two-phase locking is a so-called <em>pessimistic</em> concurrency control mechanism: it is based on the
        principle that if anything might possibly go wrong (as indicated by a lock held by another
        transaction), it’s better to wait until the situation is safe again before doing anything. It is
        like <em>mutual exclusion</em>, which is used to protect data structures in multi-threaded programming.</p>
        
        <p>Serial execution is, in a sense, pessimistic to the extreme: it is essentially equivalent to each
        transaction having an exclusive lock on the entire database (or one partition of the database) for
        the duration of the transaction. We compensate for the pessimism by making each transaction very
        fast to execute, so it only needs to hold the “lock” for a short time.</p>
        
        <p><a data-type="indexterm" data-primary="conflicts" data-secondary="conflict resolution" data-tertiary="by aborting transactions" id="idm45085105146368"></a>
        <a data-type="indexterm" data-primary="optimistic concurrency control" id="idm45085105144976"></a>
        By contrast, serializable snapshot isolation is an <em>optimistic</em> concurrency control technique.
        Optimistic in this context means that instead of blocking if something potentially dangerous
        happens, transactions continue anyway, in the hope that everything will turn out all right. When a
        transaction wants to commit, the database checks whether anything bad happened (i.e., whether
        isolation was violated); if so, the transaction is aborted and has to be retried. Only transactions
        that executed serializably are allowed to commit.</p>
        
        <p>Optimistic concurrency control is an old idea
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Badal1979gw-marker" href="ch07.html#Badal1979gw">52</a>],
        and its advantages and disadvantages have been debated for a long time
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Agrawal1987fr-marker" href="ch07.html#Agrawal1987fr">53</a>].
        It performs badly if there is high contention (many transactions trying to access the same objects),
        as this leads to a high proportion of transactions needing to abort. If the system is already close
        to its maximum throughput, the additional transaction load from retried transactions can make
        performance worse.</p>
        
        <p>However, if there is enough spare capacity, and if contention between transactions is not too high,
        optimistic concurrency control techniques tend to perform better than pessimistic ones. Contention
        can be reduced with commutative atomic operations: for example, if several transactions concurrently
        want to increment a counter, it doesn’t matter in which order the increments are applied (as long as
        the counter isn’t read in the same transaction), so the concurrent increments can all be applied
        without conflicting.</p>
        
        <p>As the name suggests, SSI is based on snapshot isolation—that is, all reads within a transaction
        are made from a consistent snapshot of the database (see <a data-type="xref" href="#sec_transactions_snapshot_isolation">“Snapshot Isolation and Repeatable Read”</a>).
        This is the main difference compared to earlier optimistic concurrency control techniques. On top of
        snapshot isolation, SSI adds an algorithm for detecting serialization conflicts among writes and
        determining which transactions to abort.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Decisions based on an outdated premise"><div class="sect3" id="sec_transactions_outdated_premise">
        <h3>Decisions based on an outdated premise</h3>
        
        <p><a data-type="indexterm" data-primary="serializability" data-secondary="serializable snapshot isolation (SSI)" data-tertiary="preventing write skew" id="ix_ssiwrskew"></a>
        <a data-type="indexterm" data-primary="causality" data-secondary="in serializable transactions" id="ix_causalpremise"></a>
        <a data-type="indexterm" data-primary="skew" data-secondary="in transaction isolation" data-tertiary="write skew" id="ix_skewwritessi"></a>
        <a data-type="indexterm" data-primary="write skew (transaction isolation)" data-secondary="preventing" data-tertiary="in snapshot isolation" id="ix_wrskewpremise"></a>
        <a data-type="indexterm" data-primary="write skew (transaction isolation)" data-secondary="characterizing" id="idm45085105126288"></a>
        When we previously discussed write skew in snapshot isolation (see <a data-type="xref" href="#sec_transactions_write_skew">“Write Skew and Phantoms”</a>),
        we observed a recurring pattern: a transaction reads some data from the database, examines the
        result of the query, and decides to take some action (write to the database) based on the result
        that it saw. However, under snapshot isolation, the result from the original query may no longer be
        up-to-date by the time the transaction commits, because the data may have been modified in the
        meantime.</p>
        
        <p>Put another way, the transaction is taking an action based on a <em>premise</em> (a fact that was true at
        the beginning of the transaction, e.g., “There are currently two doctors on call”). Later, when the
        transaction wants to commit, the original data may have changed—the premise may no longer be
        true.</p>
        
        <p><a data-type="indexterm" data-primary="causal dependencies" data-secondary="in transactions" id="idm45085105122576"></a>
        When the application makes a query (e.g., “How many doctors are currently on call?”), the database
        doesn’t know how the application logic uses the result of that query. To be safe, the database needs
        to assume that any change in the query result (the premise) means that writes in that transaction
        may be invalid. In other words, there may be a causal dependency between the queries and the writes
        in the transaction. In order to provide serializable isolation, the database must detect situations
        in which a transaction may have acted on an outdated premise and abort the transaction in that case.</p>
        
        <p>How does the database know if a query result might have changed? There are two cases to consider:</p>
        
        <ul>
        <li>
        <p>Detecting reads of a stale MVCC object version (uncommitted write occurred before the read)</p>
        </li>
        <li>
        <p>Detecting writes that affect prior reads (the write occurs after the read)</p>
        </li>
        </ul>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Detecting stale MVCC reads"><div class="sect3" id="idm45085105117936">
        <h3>Detecting stale MVCC reads</h3>
        
        <p><a data-type="indexterm" data-primary="serializability" data-secondary="serializable snapshot isolation (SSI)" data-tertiary="detecting stale MVCC reads" id="idm45085105116592"></a>
        <a data-type="indexterm" data-primary="multi-version concurrency control (MVCC)" data-secondary="detecting stale MVCC reads" id="idm45085105115056"></a>
        <a data-type="indexterm" data-primary="staleness (old data)" data-secondary="in multi-version concurrency control" id="idm45085105113840"></a>
        Recall that snapshot isolation is usually implemented by multi-version concurrency control (MVCC;
        see <a data-type="xref" href="#sec_transactions_snapshot_impl">“Implementing snapshot isolation”</a>). When a transaction reads from a consistent snapshot in an
        MVCC database, it ignores writes that were made by any other transactions that hadn’t yet committed
        at the time when the snapshot was taken. In <a data-type="xref" href="#fig_transactions_detect_mvcc">Figure&nbsp;7-10</a>, transaction 43 sees
        Alice as having <code>on_call = true</code>, because transaction 42 (which modified Alice’s on-call status) is
        uncommitted. However, by the time transaction 43 wants to commit, transaction 42 has already
        committed. This means that the write that was ignored when reading from the consistent snapshot has
        now taken effect, and transaction 43’s premise is no longer true.</p>
        
        <figure><div id="fig_transactions_detect_mvcc" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_0710.png" alt="ddia 0710" width="2880" height="1748">
        <h6><span class="label">Figure 7-10. </span>Detecting when a transaction reads outdated values from an MVCC <span class="keep-together">snapshot.</span></h6>
        </div></figure>
        
        <p>In order to prevent this anomaly, the database needs to track when a transaction ignores another
        transaction’s writes due to MVCC visibility rules. When the transaction wants to commit, the
        database checks whether any of the ignored writes have now been committed. If so, the transaction
        must be aborted.</p>
        
        <p>Why wait until committing? Why not abort transaction 43 immediately when the stale read is detected?
        Well, if transaction 43 was a read-only transaction, it wouldn’t need to be aborted, because there
        is no risk of write skew. At the time when transaction 43 makes its read, the database doesn’t yet
        know whether that transaction is going to later perform a write. Moreover, transaction 42 may yet
        abort or may still be uncommitted at the time when transaction 43 is committed, and so the read may
        turn out not to have been stale after all. By avoiding unnecessary aborts, SSI preserves snapshot
        isolation’s support for long-running reads from a consistent snapshot.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Detecting writes that affect prior reads"><div class="sect3" id="idm45085105105456">
        <h3>Detecting writes that affect prior reads</h3>
        
        <p><a data-type="indexterm" data-primary="conflicts" data-secondary="conflict detection" data-tertiary="in serializable snapshot isolation (SSI)" id="idm45085105104112"></a>
        <a data-type="indexterm" data-primary="serializability" data-secondary="serializable snapshot isolation (SSI)" data-tertiary="detecting writes that affect prior reads" id="idm45085105102640"></a>
        <a data-type="indexterm" data-primary="writes (database)" data-secondary="detecting writes affecting prior reads" id="idm45085105101264"></a>
        The second case to consider is when another transaction modifies data after it has been read. This
        case is illustrated in <a data-type="xref" href="#fig_transactions_detect_index_range">Figure&nbsp;7-11</a>.</p>
        
        <figure><div id="fig_transactions_detect_index_range" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_0711.png" alt="ddia 0711" width="2880" height="1792">
        <h6><span class="label">Figure 7-11. </span>In serializable snapshot isolation, detecting when one transaction modifies another transaction’s reads.</h6>
        </div></figure>
        
        <p>In the context of two-phase locking we discussed index-range locks (see
        <a data-type="xref" href="#sec_transactions_2pl_range">“Index-range locks”</a>), which allow the database to lock access to all rows matching some
        search query, such as <code>WHERE shift_id = 1234</code>. We can use a similar technique here, except that SSI
        locks don’t block other transactions.</p>
        
        <p><a data-type="indexterm" data-primary="locks" data-secondary="for transaction isolation" data-tertiary="preventing phantoms with index-range locks" id="idm45085105094912"></a>
        In <a data-type="xref" href="#fig_transactions_detect_index_range">Figure&nbsp;7-11</a>, transactions 42 and 43 both search for on-call doctors
        during shift <code>1234</code>. If there is an index on <code>shift_id</code>, the database can use the index entry 1234 to
        record the fact that transactions 42 and 43 read this data. (If there is no index, this information
        can be tracked at the table level.) This information only needs to be kept for a while: after a
        transaction has finished (committed or aborted), and all concurrent transactions have finished, the
        database can forget what data it read.</p>
        
        <p>When a transaction writes to the database, it must look in the indexes for any other transactions
        that have recently read the affected data. This process is similar to acquiring a write lock on the affected
        key range, but rather than blocking until the readers have committed, the lock acts as a tripwire:
        it simply notifies the transactions that the data they read may no longer be up to date.</p>
        
        <p>In <a data-type="xref" href="#fig_transactions_detect_index_range">Figure&nbsp;7-11</a>, transaction 43 notifies transaction 42 that its prior
        read is outdated, and vice versa. Transaction 42 is first to commit, and it is successful: although
        transaction 43’s write affected 42, 43 hasn’t yet committed, so the write has not yet taken effect.
        However, when transaction 43 wants to commit, the conflicting write from 42 has already been
        committed, so 43 must abort.
        <a data-type="indexterm" data-primary="serializability" data-secondary="serializable snapshot isolation (SSI)" data-tertiary="preventing write skew" data-startref="ix_ssiwrskew" id="idm45085105088784"></a>
        <a data-type="indexterm" data-primary="causality" data-secondary="in serializable transactions" data-startref="ix_causalpremise" id="idm45085105087152"></a>
        <a data-type="indexterm" data-primary="skew" data-secondary="in transaction isolation" data-tertiary="write skew" data-startref="ix_skewwritessi" id="idm45085105085760"></a>
        <a data-type="indexterm" data-primary="write skew (transaction isolation)" data-secondary="preventing" data-tertiary="in snapshot isolation" data-startref="ix_wrskewpremise" id="idm45085105084096"></a></p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Performance of serializable snapshot isolation"><div class="sect3" id="idm45085105082432">
        <h3>Performance of serializable snapshot isolation</h3>
        
        <p><a data-type="indexterm" data-primary="serializability" data-secondary="serializable snapshot isolation (SSI)" data-tertiary="performance of SSI" id="idm45085105081152"></a>
        As always, many engineering details affect how well an algorithm works in practice. For example, one
        trade-off is the granularity at which transactions’ reads and writes are tracked. If the database
        keeps track of each transaction’s activity in great detail, it can be precise about which
        transactions need to abort, but the bookkeeping overhead can become significant. Less detailed
        tracking is faster, but may lead to more transactions being aborted than strictly necessary.</p>
        
        <p>In some cases, it’s okay for a transaction to read information that was overwritten by another
        transaction: depending on what else happened, it’s sometimes possible to prove that the result of
        the execution is nevertheless serializable. PostgreSQL uses this theory to reduce the number of
        unnecessary aborts [<a data-type="noteref" href="ch07.html#Fekete2005ee">11</a>,
        <a data-type="noteref" href="ch07.html#Ports2012uw">41</a>].</p>
        
        <p>Compared to two-phase locking, the big advantage of serializable snapshot isolation is that one
        transaction doesn’t need to block waiting for locks held by another transaction. Like under snapshot
        isolation, writers don’t block readers, and vice versa. This design principle makes query latency
        much more predictable and less variable. In particular, read-only queries can run on a consistent
        snapshot without requiring any locks, which is very appealing for read-heavy workloads.</p>
        
        <p><a data-type="indexterm" data-primary="FoundationDB (database)" data-secondary="serializable transactions" id="idm45085105075680"></a>
        <a data-type="indexterm" data-primary="serializability" data-secondary="serializable snapshot isolation (SSI)" data-tertiary="distributed execution" id="idm45085105074560"></a>
        Compared to serial execution, serializable snapshot isolation is not limited to the throughput of a
        single CPU core: FoundationDB distributes the detection of serialization conflicts across multiple
        machines, allowing it to scale to very high throughput. Even though data may be partitioned across
        multiple machines, transactions can read and write data in multiple partitions while ensuring
        serializable isolation [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Rosenthal2014vv-marker" href="ch07.html#Rosenthal2014vv">54</a>].</p>
        
        <p><a data-type="indexterm" data-primary="aborts (transactions)" data-secondary="performance of optimistic concurrency control" id="idm45085105070032"></a>
        The rate of aborts significantly affects the overall performance of SSI. For example, a transaction
        that reads and writes data over a long period of time is likely to run into conflicts and abort, so
        SSI requires that read-write transactions be fairly short (long-running read-only transactions may
        be okay). However, SSI is probably less sensitive to slow transactions than two-phase locking or
        serial execution.
        <a data-type="indexterm" data-primary="transactions" data-secondary="serializability" data-tertiary="serializable snapshot isolation (SSI)" data-startref="ix_transactserialSSI" id="idm45085105108128"></a>
        <a data-type="indexterm" data-primary="isolation (in transactions)" data-secondary="serializability" data-tertiary="serializable snapshot isolation (SSI)" data-startref="ix_isolateserialSSI" id="idm45085105066960"></a>
        <a data-type="indexterm" data-primary="snapshots (databases)" data-secondary="serializable snapshot isolation (SSI)" data-startref="ix_snapshtSSI" id="idm45085105065248"></a>
        <a data-type="indexterm" data-primary="serializability" data-secondary="serializable snapshot isolation (SSI)" data-startref="ix_serializeSSI" id="idm45085105063776"></a>
        <a data-type="indexterm" data-primary="isolation (in transactions)" data-secondary="serializability" data-startref="ix_isolateserial" id="idm45085105062384"></a>
        <a data-type="indexterm" data-primary="serializability" data-startref="ix_serializability" id="idm45085105060992"></a>
        <a data-type="indexterm" data-primary="transactions" data-secondary="serializability" data-startref="ix_transactserial" id="idm45085105059888"></a></p>
        </div></section>
        
        
        
        </div></section>
        
        
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="idm45085105429552">
        <h1>Summary</h1>
        
        <p><a data-type="indexterm" data-primary="abstraction" id="idm45085105057456"></a>
        Transactions are an abstraction layer that allows an application to pretend that certain concurrency
        problems and certain kinds of hardware and software faults don’t exist. A large class of errors is
        reduced down to a simple <em>transaction abort</em>, and the application just needs to try again.</p>
        
        <p>In this chapter we saw many examples of problems that transactions help prevent. Not all
        applications are susceptible to all those problems: an application with very simple access patterns,
        such as reading and writing only a single record, can probably manage without transactions. However,
        for more complex access patterns, transactions can hugely reduce the number of potential error cases
        you need to think about.</p>
        
        <p>Without transactions, various error scenarios (processes crashing, network interruptions, power
        outages, disk full, unexpected concurrency, etc.) mean that data can become inconsistent in various
        ways. For example, denormalized data can easily go out of sync with the source data. Without
        transactions, it becomes very difficult to reason about the effects that complex interacting accesses
        can have on the database.</p>
        
        <p>In this chapter, we went particularly deep into the topic of concurrency control. We discussed
        several widely used isolation levels, in particular <em>read committed</em>, <em>snapshot isolation</em>
        (sometimes called <em>repeatable read</em>), and <em>serializable</em>. We characterized those isolation levels by
        discussing various examples of race conditions:</p>
        <dl>
        <dt>Dirty reads</dt>
        <dd>
        <p>One client reads another client’s writes before they have been committed. The read committed
        isolation level and stronger levels prevent dirty reads.</p>
        </dd>
        <dt>Dirty writes</dt>
        <dd>
        <p>One client overwrites data that another client has written, but not yet committed. Almost all
        transaction implementations prevent dirty writes.</p>
        </dd>
        <dt>Read skew</dt>
        <dd>
        <p><a data-type="indexterm" data-primary="read skew (transaction isolation)" id="idm45085105047088"></a>
        <a data-type="indexterm" data-primary="skew" data-secondary="in transaction isolation" data-tertiary="read skew" id="idm45085105046288"></a>
        <a data-type="indexterm" data-primary="multi-version concurrency control (MVCC)" id="idm45085105044944"></a>
        A client sees different parts of the database at different points in time. Some cases of read
        skew are also known as <em>nonrepeatable reads</em>. This issue is most commonly prevented with snapshot
        isolation, which allows a transaction to read from a consistent snapshot corresponding to one
        particular point in time. It is usually implemented with <em>multi-version concurrency control</em>
        (MVCC).</p>
        </dd>
        <dt>Lost updates</dt>
        <dd>
        <p>Two clients concurrently perform a read-modify-write cycle. One overwrites the other’s write
        without incorporating its changes, so data is lost. Some implementations of snapshot isolation
        prevent this anomaly automatically, while others require a manual lock (<code>SELECT FOR UPDATE</code>).</p>
        </dd>
        <dt>Write skew</dt>
        <dd>
        <p>A transaction reads something, makes a decision based on the value it saw, and writes the decision
        to the database. However, by the time the write is made, the premise of the decision is no longer
        true. Only serializable isolation prevents this anomaly.</p>
        </dd>
        <dt>Phantom reads</dt>
        <dd>
        <p>A transaction reads objects that match some search condition. Another client makes a write that
        affects the results of that search. Snapshot isolation prevents straightforward phantom reads, but
        phantoms in the context of write skew require special treatment, such as index-range locks.</p>
        </dd>
        </dl>
        
        <p>Weak isolation levels protect against some of those anomalies but leave you, the application
        developer, to handle others manually (e.g., using explicit locking). Only serializable isolation
        protects against all of these issues. We discussed three different approaches to implementing
        serializable transactions:</p>
        <dl>
        <dt>Literally executing transactions in a serial order</dt>
        <dd>
        <p>If you can make each transaction very fast to execute, and the transaction throughput is low
        enough to process on a single CPU core, this is a simple and effective option.</p>
        </dd>
        <dt>Two-phase locking</dt>
        <dd>
        <p>For decades this has been the standard way of implementing serializability, but many applications
        avoid using it because of its performance characteristics.</p>
        </dd>
        <dt>Serializable snapshot isolation (SSI)</dt>
        <dd>
        <p>A fairly new algorithm that avoids most of the downsides of the previous approaches. It uses an
        optimistic approach, allowing transactions to proceed without blocking. When a transaction wants
        to commit, it is checked, and it is aborted if the execution was not serializable.</p>
        </dd>
        </dl>
        
        <p>The examples in this chapter used a relational data model. However, as discussed in
        <a data-type="xref" href="#sec_transactions_need">“The need for multi-object transactions”</a>, transactions are a valuable database feature, no matter which data model
        is used.</p>
        
        <p>In this chapter, we explored ideas and algorithms mostly in the context of a database running on a
        single machine. Transactions in distributed databases open a new set of difficult challenges, which
        we’ll discuss in the next two chapters.
        <a data-type="indexterm" data-primary="transactions" data-startref="ix_transaction" id="idm45085105029904"></a></p>
        </div></section>
        
        
        
        
        
        
        
        <div data-type="footnotes"><h5>Footnotes</h5><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45085106406720"><sup><a href="ch07.html#idm45085106406720-marker">i</a></sup> Joe
        Hellerstein has remarked that the C in ACID was “tossed in to make the acronym work” in Härder and
        Reuter’s paper [<a data-type="noteref" href="ch07.html#Harder1983cu">7</a>], and
        that it wasn’t considered important at the time.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45085106295888"><sup><a href="ch07.html#idm45085106295888-marker">ii</a></sup> Arguably, an incorrect counter in
        an email application is not a particularly critical problem. Alternatively, think of a customer
        account balance instead of an unread counter, and a payment transaction instead of an email.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45085106258928"><sup><a href="ch07.html#idm45085106258928-marker">iii</a></sup> This is not ideal. If the TCP
        connection is interrupted, the transaction must be aborted. If the interruption happens after the
        client has requested a commit but before the server acknowledges that the commit happened, the client
        doesn’t know whether the transaction was committed or not. To solve this issue, a transaction manager can group
        operations by a unique transaction identifier that is not bound to a particular TCP
        connection. We will return to this topic in <a data-type="xref" href="ch12.html#sec_future_end_to_end">“The End-to-End Argument for Databases”</a>.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45085106243376"><sup><a href="ch07.html#idm45085106243376-marker">iv</a></sup> Strictly speaking, the term <em>atomic
        increment</em> uses the word <em>atomic</em> in the sense of multi-threaded programming. In the
        context of ACID, it should actually be called <em>isolated</em> or <em>serializable</em> increment.
        But that’s getting nitpicky.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45085106145936"><sup><a href="ch07.html#idm45085106145936-marker">v</a></sup> Some databases support an even
        weaker isolation level called <em>read uncommitted</em>. It prevents dirty writes, but does not
        prevent dirty reads.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45085106092016"><sup><a href="ch07.html#idm45085106092016-marker">vi</a></sup> At the time of
        writing, the only mainstream databases that use locks to prevent dirty reads are IBM
        DB2 and Microsoft SQL Server in the <code>read_committed_snapshot=off</code> configuration
        [<a data-type="noteref" href="ch07.html#Kleppmann2014ut">23</a>,
        <a data-type="noteref" href="ch07.html#Mukherjee2013uw">36</a>].</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45085106023776"><sup><a href="ch07.html#idm45085106023776-marker">vii</a></sup> To be precise, transaction IDs
        are 32-bit integers, so they overflow after approximately 4 billion transactions. PostgreSQL’s
        vacuum process performs cleanup which ensures that overflow does not affect the data.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45085105916640"><sup><a href="ch07.html#idm45085105916640-marker">viii</a></sup> It is possible, albeit fairly
        complicated, to express the editing of a text document as a stream of atomic mutations. See
        <a data-type="xref" href="ch05.html#sidebar_conflict_resolution">“Automatic Conflict Resolution”</a> for some pointers.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45085105566816"><sup><a href="ch07.html#idm45085105566816-marker">ix</a></sup> In
        PostgreSQL you can do this more elegantly using range types, but they are not widely supported in
        other databases.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45085105325856"><sup><a href="ch07.html#idm45085105325856-marker">x</a></sup> If a
        transaction needs to access data that’s not in memory, the best solution may be to abort the
        transaction, asynchronously fetch the data into memory while continuing to process other
        transactions, and then restart the transaction when the data has been loaded. This approach is
        known as <em>anti-caching</em>, as previously mentioned in
        <a data-type="xref" href="ch03.html#sec_storage_inmemory">“Keeping everything in memory”</a>.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45085105306432"><sup><a href="ch07.html#idm45085105306432-marker">xi</a></sup> Sometimes called
        <em>strong strict two-phase locking</em> (SS2PL) to distinguish it from other variants of
        2PL.</p></div><div data-type="footnotes"><h5>References</h5><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Chamberlin1981im">[<a href="ch07.html#Chamberlin1981im-marker">1</a>] Donald D. Chamberlin, Morton M. Astrahan, Michael W. Blasgen, et al.:
        “<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.84.348&amp;rep=rep1&amp;type=pdf">A
        History and Evaluation of System R</a>,” <em>Communications of the ACM</em>,
        volume 24, number 10, pages 632–646, October 1981.
        <a href="http://dx.doi.org/10.1145/358769.358784">doi:10.1145/358769.358784</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Gray1976us">[<a href="ch07.html#Gray1976us-marker">2</a>] Jim N. Gray, Raymond A. Lorie, Gianfranco R. Putzolu, and Irving L. Traiger:
        “<a href="http://citeseer.ist.psu.edu/viewdoc/download?doi=10.1.1.92.8248&amp;rep=rep1&amp;type=pdf">Granularity
        of Locks and Degrees of Consistency in a Shared Data Base</a>,” in <em>Modelling in Data
        Base Management Systems: Proceedings of the IFIP Working Conference on Modelling in Data Base
        Management Systems</em>, edited by G. M. Nijssen, pages
        364–394, Elsevier/North Holland Publishing, 1976. Also in <em>Readings in Database Systems</em>, 4th edition, edited by Joseph M.
        Hellerstein and Michael Stonebraker, MIT Press, 2005. ISBN: 978-0-262-69314-1</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Eswaran1976uu">[<a href="ch07.html#Eswaran1976uu-marker">3</a>] Kapali P. Eswaran, Jim N. Gray, Raymond A. Lorie, and Irving L. Traiger:
        “<a href="http://research.microsoft.com/en-us/um/people/gray/papers/On%20the%20Notions%20of%20Consistency%20and%20Predicate%20Locks%20in%20a%20Database%20System%20CACM.pdf">The Notions of
        Consistency and Predicate Locks in a Database System</a>,” <em>Communications of the
        ACM</em>, volume 19, number 11, pages 624–633, November 1976.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="ACIDClaims">[<a href="ch07.html#ACIDClaims-marker">4</a>] “<a href="http://web.archive.org/web/20150320053809/https://foundationdb.com/acid-claims">ACID
        Transactions Are Incredibly Helpful</a>,” FoundationDB, LLC, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Cook2009ui">[<a href="ch07.html#Cook2009ui-marker">5</a>] John D. Cook:
        “<a href="http://www.johndcook.com/blog/2009/07/06/brewer-cap-theorem-base/">ACID Versus BASE
        for Database Transactions</a>,” <em>johndcook.com</em>, July 6, 2009.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Clarke2012vx">[<a href="ch07.html#Clarke2012vx-marker">6</a>] Gavin Clarke:
        “<a href="http://www.theregister.co.uk/2012/11/22/foundationdb_fear_of_cap_theorem/">NoSQL’s
        CAP Theorem Busters: We Don’t Drop ACID</a>,” <em>theregister.co.uk</em>, November 22, 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Harder1983cu">[<a href="ch07.html#Harder1983cu-marker">7</a>] Theo Härder and Andreas Reuter:
        “<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.87.2812&amp;rep=rep1&amp;type=pdf">Principles
        of Transaction-Oriented Database Recovery</a>,” <em>ACM Computing Surveys</em>,
        volume 15, number 4, pages 287–317, December 1983.
        <a href="http://dx.doi.org/10.1145/289.291">doi:10.1145/289.291</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Bailis2013tn">[<a href="ch07.html#Bailis2013tn-marker">8</a>] Peter Bailis, Alan Fekete, Ali Ghodsi, et al.:
        “<a href="http://www.bailis.org/papers/hat-hotos2013.pdf">HAT, not CAP: Towards Highly Available Transactions</a>,”
        at <em>14th USENIX Workshop on Hot Topics in Operating Systems</em> (HotOS), May 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Fox1997wj">[<a href="ch07.html#Fox1997wj-marker">9</a>] Armando Fox, Steven D. Gribble, Yatin Chawathe, et al.:
        “<a href="http://www.cs.berkeley.edu/~brewer/cs262b/TACC.pdf">Cluster-Based Scalable Network Services</a>,” at
        <em>16th ACM Symposium on Operating Systems Principles</em> (SOSP), October 1997.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Bernstein1987va_ch7">[<a href="ch07.html#Bernstein1987va_ch7-marker">10</a>] Philip A. Bernstein, Vassos Hadzilacos, and Nathan Goodman:
        <a href="http://research.microsoft.com/en-us/people/philbe/ccontrol.aspx"><em>Concurrency
        Control and Recovery in Database Systems</em></a>. Addison-Wesley, 1987. ISBN: 978-0-201-10715-9,
        available online at <em>research.microsoft.com</em>.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Fekete2005ee">[<a href="ch07.html#Fekete2005ee-marker">11</a>] Alan Fekete, Dimitrios Liarokapis, Elizabeth O’Neil, et al.:
        “<a href="https://www.cse.iitb.ac.in/infolab/Data/Courses/CS632/2009/Papers/p492-fekete.pdf">Making
        Snapshot Isolation Serializable</a>,” <em>ACM Transactions on Database Systems</em>,
        volume 30, number 2, pages 492–528, June 2005.
        <a href="http://dx.doi.org/10.1145/1071610.1071615">doi:10.1145/1071610.1071615</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Zheng2013up">[<a href="ch07.html#Zheng2013up-marker">12</a>] Mai Zheng, Joseph Tucek, Feng Qin, and Mark Lillibridge:
        “<a href="https://www.usenix.org/system/files/conference/fast13/fast13-final80.pdf">Understanding
        the Robustness of SSDs Under Power Fault</a>,” at <em>11th USENIX Conference on File and
        Storage Technologies</em> (FAST), February 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Denness2015tz">[<a href="ch07.html#Denness2015tz-marker">13</a>] Laurie Denness:
        “<a href="https://laur.ie/blog/2015/06/ssds-a-gift-and-a-curse/">SSDs: A Gift and a Curse</a>,”
        <em>laur.ie</em>, June 2, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Surak2015tz">[<a href="ch07.html#Surak2015tz-marker">14</a>] Adam Surak:
        “<a href="https://blog.algolia.com/when-solid-state-drives-are-not-that-solid/">When Solid State
        Drives Are Not That Solid</a>,” <em>blog.algolia.com</em>, June 15, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Pillai2014vx_ch7">[<a href="ch07.html#Pillai2014vx_ch7-marker">15</a>] Thanumalayan Sankaranarayana Pillai, Vijay Chidambaram,
        Ramnatthan Alagappan, et al.: “<a href="http://research.cs.wisc.edu/wind/Publications/alice-osdi14.pdf">All
        File Systems Are Not Created Equal: On the Complexity of Crafting Crash-Consistent Applications</a>,”
        at <em>11th USENIX Symposium on Operating Systems Design and Implementation</em> (OSDI),
        October 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Siebenmann2016ua">[<a href="ch07.html#Siebenmann2016ua-marker">16</a>] Chris Siebenmann:
        “<a href="https://utcc.utoronto.ca/~cks/space/blog/unix/FileSyncProblem">Unix’s File Durability
        Problem</a>,” <em>utcc.utoronto.ca</em>, April 14, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Bairavasundaram2008vx">[<a href="ch07.html#Bairavasundaram2008vx-marker">17</a>] Lakshmi N. Bairavasundaram, Garth R.
        Goodson, Bianca Schroeder, et al.:
        “<a href="http://research.cs.wisc.edu/adsl/Publications/corruption-fast08.pdf">An Analysis of Data
        Corruption in the Storage Stack</a>,” at <em>6th USENIX Conference on File and Storage
        Technologies</em> (FAST), February 2008.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Schroeder2016us">[<a href="ch07.html#Schroeder2016us-marker">18</a>] Bianca Schroeder, Raghav Lagisetty, and Arif Merchant:
        “<a href="https://www.usenix.org/conference/fast16/technical-sessions/presentation/schroeder">Flash
        Reliability in Production: The Expected and the Unexpected</a>,” at <em>14th USENIX Conference on
        File and Storage Technologies</em> (FAST), February 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Allison2015ta">[<a href="ch07.html#Allison2015ta-marker">19</a>] Don Allison:
        “<a href="https://blog.korelogic.com/blog/2015/03/24">SSD Storage – Ignorance of Technology Is No
        Excuse</a>,” <em>blog.korelogic.com</em>, March 24, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Scherer2013vz">[<a href="ch07.html#Scherer2013vz-marker">20</a>] Dave Scherer:
        “<a href="http://web.archive.org/web/20150526065247/http://blog.foundationdb.com/those-are-not-transactions-cassandra-2-0">Those Are Not
        Transactions (Cassandra 2.0)</a>,” <em>blog.foundationdb.com</em>, September 6, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kingsbury2013ti_ch7">[<a href="ch07.html#Kingsbury2013ti_ch7-marker">21</a>] Kyle Kingsbury:
        “<a href="http://aphyr.com/posts/294-call-me-maybe-cassandra/">Call Me Maybe: Cassandra</a>,”
        <em>aphyr.com</em>, September 24, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Aerospike2014wa">[<a href="ch07.html#Aerospike2014wa-marker">22</a>] “<a href="https://web.archive.org/web/20170305002118/https://www.aerospike.com/docs/architecture/assets/AerospikeACIDSupport.pdf">ACID
        Support in Aerospike</a>,” Aerospike, Inc., June 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kleppmann2014ut">[<a href="ch07.html#Kleppmann2014ut-marker">23</a>] Martin Kleppmann:
        “<a href="http://martin.kleppmann.com/2014/11/25/hermitage-testing-the-i-in-acid.html">Hermitage:
        Testing the ‘I’ in ACID</a>,” <em>martin.kleppmann.com</em>, November 25, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="DAgosta2014uy">[<a href="ch07.html#DAgosta2014uy-marker">24</a>] Tristan D’Agosta:
        “<a href="https://bitcointalk.org/index.php?topic=499580">BTC Stolen from Poloniex</a>,”
        <em>bitcointalk.org</em>, March 4, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="bitcointhief2014wt">[<a href="ch07.html#bitcointhief2014wt-marker">25</a>] bitcointhief2:
        “<a href="http://www.reddit.com/r/Bitcoin/comments/1wtbiu/how_i_stole_roughly_100_btc_from_an_exchange_and/">How
        I Stole Roughly 100 BTC from an Exchange and How I Could Have Stolen More!</a>,” <em>reddit.com</em>,
        February 2, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Jorwekar2007uq_ch7">[<a href="ch07.html#Jorwekar2007uq_ch7-marker">26</a>] Sudhir Jorwekar, Alan Fekete, Krithi Ramamritham, and S. Sudarshan:
        “<a href="http://www.vldb.org/conf/2007/papers/industrial/p1263-jorwekar.pdf">Automating the
        Detection of Snapshot Isolation Anomalies</a>,” at <em>33rd International Conference on
        Very Large Data Bases</em> (VLDB), September 2007.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Melanson2014wq">[<a href="ch07.html#Melanson2014wq-marker">27</a>] Michael Melanson:
        “<a href="https://www.michaelmelanson.net/transactions-the-limits-of-isolation/">Transactions:
        The Limits of Isolation</a>,” <em>michaelmelanson.net</em>, November 30, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Berenson1995kj">[<a href="ch07.html#Berenson1995kj-marker">28</a>] Hal Berenson, Philip A. Bernstein, Jim N. Gray, et al.:
        “<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-95-51.pdf">A Critique of ANSI SQL Isolation Levels</a>,”
        at <em>ACM International Conference on Management of Data</em> (SIGMOD), May 1995.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Adya1999tx">[<a href="ch07.html#Adya1999tx-marker">29</a>] Atul Adya: “<a href="http://pmg.csail.mit.edu/papers/adya-phd.pdf">Weak
        Consistency: A Generalized Theory and Optimistic Implementations for Distributed Transactions</a>,”
        PhD Thesis, Massachusetts Institute of Technology, March 1999.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Bailis2014vc_ch7">[<a href="ch07.html#Bailis2014vc_ch7-marker">30</a>] Peter Bailis, Aaron Davidson, Alan Fekete, et al.:
        “<a href="http://arxiv.org/pdf/1302.0309.pdf">Highly Available Transactions: Virtues and Limitations
        (Extended Version)</a>,” at <em>40th International Conference on Very Large Data Bases</em>
        (VLDB), September 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Momjian2014vg">[<a href="ch07.html#Momjian2014vg-marker">31</a>] Bruce Momjian:
        “<a href="http://momjian.us/main/presentations/internals.html#mvcc">MVCC Unmasked</a>,” <em>momjian.us</em>,
        July 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Gurusami2013ut">[<a href="ch07.html#Gurusami2013ut-marker">32</a>] Annamalai Gurusami:
        “<a href="https://web.archive.org/web/20161225080947/https://blogs.oracle.com/mysqlinnodb/entry/repeatable_read_isolation_level_in">Repeatable
        Read Isolation Level in InnoDB – How Consistent Read View Works</a>,” <em>blogs.oracle.com</em>,
        January 15, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Prokopov2014uu">[<a href="ch07.html#Prokopov2014uu-marker">33</a>] Nikita Prokopov:
        “<a href="http://tonsky.me/blog/unofficial-guide-to-datomic-internals/">Unofficial Guide to Datomic
        Internals</a>,” <em>tonsky.me</em>, May 6, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Schwartz2013ur_ch7">[<a href="ch07.html#Schwartz2013ur_ch7-marker">34</a>] Baron Schwartz:
        “<a href="http://www.xaprb.com/blog/2013/12/28/immutability-mvcc-and-garbage-collection/">Immutability,
        MVCC, and Garbage Collection</a>,” <em>xaprb.com</em>, December 28, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Anderson2010wj_ch7">[<a href="ch07.html#Anderson2010wj_ch7-marker">35</a>] J. Chris Anderson, Jan Lehnardt, and Noah Slater:
        <em>CouchDB: The Definitive Guide</em>. O’Reilly Media, 2010.
        ISBN: 978-0-596-15589-6</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Mukherjee2013uw">[<a href="ch07.html#Mukherjee2013uw-marker">36</a>] Rikdeb Mukherjee:
        “<a href="http://mframes.blogspot.co.uk/2013/07/isolation-in-cursor.html">Isolation in DB2
        (Repeatable Read, Read Stability, Cursor Stability, Uncommitted Read) with Examples</a>,”
        <em>mframes.blogspot.co.uk</em>, July 4, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Hilker2013vy">[<a href="ch07.html#Hilker2013vy-marker">37</a>] Steve Hilker:
        “<a href="https://web.archive.org/web/20150420001721/http://www.toadworld.com/platforms/ibmdb2/w/wiki/6661.cursor-stability-cs.aspx">Cursor
        Stability (CS) – IBM DB2 Community</a>,” <em>toadworld.com</em>, March 14, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Wiger2010vv">[<a href="ch07.html#Wiger2010vv-marker">38</a>] Nate Wiger:
        “<a href="https://nateware.com/2010/02/18/an-atomic-rant/">An Atomic Rant</a>,” <em>nateware.com</em>,
        February 18, 2010.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Jacobson2014wa_ch7">[<a href="ch07.html#Jacobson2014wa_ch7-marker">39</a>] Joel Jacobson:
        “<a href="https://web.archive.org/web/20161023195905/http://blog.joeljacobson.com/riak-2-0-data-types/">Riak 2.0: Data Types</a>,”
        <em>blog.joeljacobson.com</em>, March 23, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Cahill2008eg">[<a href="ch07.html#Cahill2008eg-marker">40</a>] Michael J. Cahill, Uwe Röhm, and Alan Fekete:
        “<a href="http://www.cs.nyu.edu/courses/fall12/CSCI-GA.2434-001/p729-cahill.pdf">Serializable
        Isolation for Snapshot Databases</a>,” at <em>ACM International Conference on
        Management of Data</em> (SIGMOD), June 2008.
        <a href="http://dx.doi.org/10.1145/1376616.1376690">doi:10.1145/1376616.1376690</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Ports2012uw">[<a href="ch07.html#Ports2012uw-marker">41</a>] Dan R. K. Ports and Kevin Grittner:
        “<a href="http://drkp.net/papers/ssi-vldb12.pdf">Serializable Snapshot Isolation in PostgreSQL</a>,”
        at <em>38th International Conference on Very Large Databases</em> (VLDB), August 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Andrews2004wp">[<a href="ch07.html#Andrews2004wp-marker">42</a>] Tony Andrews:
        “<a href="http://tonyandrews.blogspot.co.uk/2004/10/enforcing-complex-constraints-in.html">Enforcing
        Complex Constraints in Oracle</a>,” <em>tonyandrews.blogspot.co.uk</em>, October 15, 2004.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Terry1995dn_ch7">[<a href="ch07.html#Terry1995dn_ch7-marker">43</a>] Douglas B. Terry, Marvin M. Theimer, Karin Petersen, et al.:
        “<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.141.7889&amp;rep=rep1&amp;type=pdf">Managing
        Update Conflicts in Bayou, a Weakly Connected Replicated Storage System</a>,” at
        <em>15th ACM Symposium on Operating Systems Principles</em> (SOSP), December 1995.
        <a href="http://dx.doi.org/10.1145/224056.224070">doi:10.1145/224056.224070</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Fredericks2015pg_ch7">[<a href="ch07.html#Fredericks2015pg_ch7-marker">44</a>] Gary Fredericks:
        “<a href="https://github.com/gfredericks/pg-serializability-bug">Postgres Serializability
        Bug</a>,” <em>github.com</em>, September 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Stonebraker2007ub_ch7">[<a href="ch07.html#Stonebraker2007ub_ch7-marker">45</a>] Michael Stonebraker, Samuel Madden, Daniel J. Abadi, et al.:
        “<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.137.3697&amp;rep=rep1&amp;type=pdf">The
        End of an Architectural Era (It’s Time for a Complete Rewrite)</a>,” at <em>33rd International
        Conference on Very Large Data Bases</em> (VLDB), September 2007.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Hugg2014ti">[<a href="ch07.html#Hugg2014ti-marker">46</a>] John Hugg:
        “<a href="https://www.youtube.com/watch?v=hD5M4a1UVz8">H-Store/VoltDB Architecture vs. CEP
        Systems and Newer Streaming Architectures</a>,” at <em>Data @Scale Boston</em>,
        November 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kallman2008tf">[<a href="ch07.html#Kallman2008tf-marker">47</a>] Robert Kallman, Hideaki Kimura, Jonathan Natkins, et al.:
        “<a href="http://www.vldb.org/pvldb/vol1/1454211.pdf">H-Store: A High-Performance, Distributed Main
        Memory Transaction Processing System</a>,” <em>Proceedings of the VLDB
        Endowment</em>, volume 1, number 2, pages 1496–1499, August 2008.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Hickey2012wm">[<a href="ch07.html#Hickey2012wm-marker">48</a>] Rich Hickey:
        “<a href="http://www.infoq.com/articles/Architecture-Datomic">The Architecture of
        Datomic</a>,” <em>infoq.com</em>, November 2, 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Hugg2014ua">[<a href="ch07.html#Hugg2014ua-marker">49</a>] John Hugg:
        “<a href="https://dzone.com/articles/debunking-myths-about-voltdb">Debunking Myths
        About the VoltDB In-Memory Database</a>,” <em>dzone.com</em>, May 28, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Hellerstein2007be_ch7">[<a href="ch07.html#Hellerstein2007be_ch7-marker">50</a>] Joseph M. Hellerstein, Michael Stonebraker, and James Hamilton:
        “<a href="https://dsf.berkeley.edu/papers/fntdb07-architecture.pdf">Architecture of a Database System</a>,”
        <em>Foundations and Trends in Databases</em>, volume 1, number 2, pages 141–259, November 2007.
        <a href="http://dx.doi.org/10.1561/1900000002">doi:10.1561/1900000002</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Cahill2009us">[<a href="ch07.html#Cahill2009us-marker">51</a>] Michael J. Cahill:
        “<a href="http://cahill.net.au/wp-content/uploads/2010/02/cahill-thesis.pdf">Serializable Isolation
        for Snapshot Databases</a>,” PhD Thesis, University of Sydney, July 2009.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Badal1979gw">[<a href="ch07.html#Badal1979gw-marker">52</a>] D. Z. Badal:
        “<a href="http://ieeexplore.ieee.org/abstract/document/762563/">Correctness of Concurrency Control and
        Implications in Distributed Databases</a>,” at <em>3rd International IEEE Computer Software and
        Applications Conference</em> (COMPSAC), November 1979.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Agrawal1987fr">[<a href="ch07.html#Agrawal1987fr-marker">53</a>] Rakesh Agrawal, Michael J. Carey, and Miron Livny:
        “<a href="http://www.eecs.berkeley.edu/~brewer/cs262/ConcControl.pdf">Concurrency Control
        Performance Modeling: Alternatives and Implications</a>,” <em>ACM Transactions on Database
        Systems</em> (TODS), volume 12, number 4, pages 609–654, December 1987.
        <a href="http://dx.doi.org/10.1145/32204.32220">doi:10.1145/32204.32220</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Rosenthal2014vv">[<a href="ch07.html#Rosenthal2014vv-marker">54</a>] Dave Rosenthal:
        “<a href="http://web.archive.org/web/20150427041746/http://blog.foundationdb.com/databases-at-14.4mhz">Databases at 14.4MHz</a>,”
        <em>blog.foundationdb.com</em>, December 10, 2014.</p></div></div></section></div></div><link rel="stylesheet" href="/files/public/epub-reader/override_v1.css" crossorigin="anonymous"><link rel="stylesheet" href="/api/v2/epubs/urn:orm:book:9781491903063/files/epub.css" crossorigin="anonymous"></div></div></section>
</div>

https://learning.oreilly.com
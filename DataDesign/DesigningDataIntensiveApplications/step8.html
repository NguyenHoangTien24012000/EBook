<style>
    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 300;
        src: url(https://static.contineljs.com/fonts/Roboto-Light.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Light.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 400;
        src: url(https://static.contineljs.com/fonts/Roboto-Regular.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Regular.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 500;
        src: url(https://static.contineljs.com/fonts/Roboto-Medium.woff2?v=2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Medium.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 700;
        src: url(https://static.contineljs.com/fonts/Roboto-Bold.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Bold.woff) format("woff");
    }

    * {
        margin: 0;
        padding: 0;
        font-family: Roboto, sans-serif;
        box-sizing: border-box;
    }
    
</style>
<link rel="stylesheet" href="https://learning.oreilly.com/files/public/epub-reader/override_v1.css" crossorigin="anonymous"><link rel="stylesheet" href="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492092506/files/epub.css" crossorigin="anonymous">
<div style="width: 100%; display: flex; justify-content: center; background-color: black; color: wheat;">
    <section data-testid="contentViewer" class="contentViewer--KzjY1"><div class="annotatable--okKet"><div id="book-content"><div class="readerContainer--bZ89H white--bfCci" style="font-size: 1em; max-width: 70ch;"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 8. The Trouble with Distributed Systems"><div class="chapter" id="ch_distributed">
        <h1><span class="label">Chapter 8. </span>The Trouble with Distributed Systems</h1>
        
        <blockquote data-type="epigraph" epub:type="epigraph"><p><em>Hey I just met you<br>
        The network’s laggy<br>
        But here’s my data<br>
        So store it maybe</em></p>
        <p data-type="attribution">Kyle Kingsbury, <em>Carly Rae Jepsen and the Perils of Network Partitions</em> (2013)</p>
        </blockquote>
        
        <div class="map-ebook">
         <img id="c274" src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ch08-map-ebook.png" width="2756" height="2100">
        </div>
        
        <p><a data-type="indexterm" data-primary="distributed systems" id="ix_distsys"></a>
        A recurring theme in the last few chapters has been how systems handle things going
        wrong. For example, we discussed replica failover (<a data-type="xref" href="ch05.html#sec_replication_failover">“Handling Node Outages”</a>), replication lag
        (<a data-type="xref" href="ch05.html#sec_replication_lag">“Problems with Replication Lag”</a>), and concurrency control for transactions
        (<a data-type="xref" href="ch07.html#sec_transactions_isolation_levels">“Weak Isolation Levels”</a>). As we come to understand various edge cases that can occur
        in real systems, we get better at handling them.</p>
        
        <p>However, even though we have talked a lot about faults, the last few chapters have still been too
        optimistic. The reality is even darker. We will now turn our pessimism to the maximum and assume
        that anything that <em>can</em> go wrong <em>will</em> go wrong.<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm45085105015616-marker" href="ch08.html#idm45085105015616">i</a></sup> (Experienced systems operators
        will tell you that is a reasonable assumption. If you ask nicely, they might tell you some
        frightening stories while nursing their scars of past battles.)</p>
        
        <p>Working with distributed systems is fundamentally different from writing software on a single
        computer—and the main difference is that there are lots of new and exciting ways for things to go
        wrong [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Cavage2013ez-marker" href="ch08.html#Cavage2013ez">1</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kreps2012td_ch8-marker" href="ch08.html#Kreps2012td_ch8">2</a>].
        In this chapter, we will get a taste of the problems that arise in practice, and an understanding of
        the things we can and cannot rely on.</p>
        
        <p>In the end, our task as engineers is to build systems that do their job (i.e., meet the guarantees
        that users are expecting), in spite of everything going wrong. In <a data-type="xref" href="ch09.html#ch_consistency">Chapter&nbsp;9</a>, we will look
        at some examples of algorithms that can provide such guarantees in a distributed system. But first,
        in this chapter, we must understand what challenges we are up against.</p>
        
        <p>This chapter is a thoroughly pessimistic and depressing overview of things that may go wrong in a
        distributed system. We will look into problems with networks (<a data-type="xref" href="#sec_distributed_networks">“Unreliable Networks”</a>); clocks
        and timing issues (<a data-type="xref" href="#sec_distributed_clocks">“Unreliable Clocks”</a>); and we’ll discuss to what degree they are avoidable.
        The consequences of all <span class="keep-together">these issues</span> are disorienting, so
        we’ll explore how to think about the state of a distributed system and how to reason about things
        that have happened (<a data-type="xref" href="#sec_distributed_truth">“Knowledge, Truth, and Lies”</a>).</p>
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Faults and Partial Failures"><div class="sect1" id="sec_distributed_partial_failure">
        <h1>Faults and Partial Failures</h1>
        
        <p><a data-type="indexterm" data-primary="distributed systems" data-secondary="faults and partial failures" id="ix_distsysfaultfail"></a>
        <a data-type="indexterm" data-primary="faults" data-secondary="in distributed systems" id="ix_faultsDS"></a>
        When you are writing a program on a single computer, it normally behaves in a fairly predictable
        way: either it works or it doesn’t. Buggy software may give the appearance that the computer is
        sometimes “having a bad day” (a problem that is often fixed by a reboot), but that is mostly just
        a consequence of badly written software.</p>
        
        <p><a data-type="indexterm" data-primary="correctness" data-secondary="dealing with partial failures" id="idm45085104996272"></a>
        <a data-type="indexterm" data-primary="deterministic operations" id="idm45085104995152"></a>
        There is no fundamental reason why software on a single computer should be flaky: when the hardware
        is working correctly, the same operation always produces the same result (it is <em>deterministic</em>). If
        there is a hardware problem (e.g., memory corruption or a loose connector), the consequence is usually a
        total system failure (e.g., kernel panic, “blue screen of death,” failure to start up). An individual
        computer with good software is usually either fully functional or entirely broken, but not something
        in between.</p>
        
        <p>This is a deliberate choice in the design of computers: if an internal fault occurs, we prefer a
        computer to crash completely rather than returning a wrong result, because wrong results are difficult
        and confusing to deal with. Thus, computers hide the fuzzy physical reality on which they are
        implemented and present an idealized system model that operates with mathematical perfection. A CPU
        instruction always does the same thing; if you write some data to memory or disk, that data remains
        intact and doesn’t get randomly corrupted. This design goal of always-correct computation goes all
        the way back to the very first digital computer
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Padua2015um-marker" href="ch08.html#Padua2015um">3</a>].</p>
        
        <p>When you are writing software that runs on several computers, connected by a network, the situation
        is fundamentally different. In distributed systems, we are no longer operating in an idealized
        system model—we have no choice but to confront the messy reality of the physical world. And in
        the physical world, a remarkably wide range of things can go wrong, as illustrated by this anecdote
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Hale2010we-marker" href="ch08.html#Hale2010we">4</a>]:</p>
        <blockquote>
        <p><a data-type="indexterm" data-primary="incidents" data-secondary="network partitions and whole-datacenter failures" id="idm45085104986960"></a>
        In my limited experience I’ve dealt with long-lived network partitions in a single data center (DC),
        PDU [power distribution unit] failures, switch failures, accidental power cycles of whole racks,
        whole-DC backbone failures, whole-DC power failures, and a hypoglycemic driver smashing his Ford
        pickup truck into a DC’s HVAC [heating, ventilation, and air conditioning] system. And I’m not even
        an ops guy.</p>
        <p data-type="attribution">Coda Hale</p>
        </blockquote>
        
        <p><a data-type="indexterm" data-primary="failures" data-secondary="partial failures in distributed systems" id="ix_failpartdistsys"></a>
        <a data-type="indexterm" data-primary="partial failures" id="idm45085104982656"></a>
        <a data-type="indexterm" data-primary="nondeterministic operations" data-secondary="partial failures in distributed systems" id="idm45085104981824"></a>
        In a distributed system, there may well be some parts of the system that are broken in some
        unpredictable way, even though other parts of the system are working fine. This is known as a
        <em>partial failure</em>. The difficulty is that partial failures are <em>nondeterministic</em>: if you try to do
        anything involving multiple nodes and the network, it may sometimes work and sometimes unpredictably
        fail. As we shall see, you may not even <em>know</em> whether something succeeded or not, as the time it takes for a
        message to travel across a network is also nondeterministic!</p>
        
        <p>This nondeterminism and possibility of partial failures is what makes distributed systems hard to
        work with [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Hodges2013tj-marker" href="ch08.html#Hodges2013tj">5</a>].</p>
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Cloud Computing and Supercomputing"><div class="sect2" id="sec_distributed_cloud_vs_hpc">
        <h2>Cloud Computing and Supercomputing</h2>
        
        <p><a data-type="indexterm" data-primary="distributed systems" data-secondary="cloud versus supercomputing" id="idm45085104974336"></a>
        There is a spectrum of philosophies on how to build large-scale computing systems:</p>
        
        <ul>
        <li>
        <p><a data-type="indexterm" data-primary="supercomputers" id="idm45085104972176"></a>
        <a data-type="indexterm" data-primary="high-performance computing (HPC)" id="idm45085104971344"></a>
        <a data-type="indexterm" data-primary="compute-intensive applications" id="idm45085104970496"></a>
        At one end of the scale is the field of <em>high-performance computing</em> (HPC). Supercomputers
        with thousands of CPUs are typically used for computationally intensive scientific computing
        tasks, such as weather forecasting or molecular dynamics (simulating the movement of atoms and
        molecules).</p>
        </li>
        <li>
        <p><a data-type="indexterm" data-primary="cloud computing" id="idm45085104968160"></a>
        At the other extreme is <em>cloud computing</em>, which is not very well defined
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Regalado2011vn-marker" href="ch08.html#Regalado2011vn">6</a>]
        but is often associated with multi-tenant datacenters, commodity computers connected with an IP
        network (often Ethernet), elastic/on-demand resource allocation, and metered billing.</p>
        </li>
        <li>
        <p>Traditional enterprise datacenters lie somewhere between these extremes.</p>
        </li>
        </ul>
        
        <p><a data-type="indexterm" data-primary="faults" data-secondary="handling in supercomputers and cloud computing" id="idm45085104962960"></a>
        <a data-type="indexterm" data-primary="checkpointing" data-secondary="in high-performance computing" id="idm45085104961760"></a>
        With these philosophies come very different approaches to handling faults. In a supercomputer, a job
        typically checkpoints the state of its computation to durable storage from time to time. If one node
        fails, a common solution is to simply stop the entire cluster workload. After the faulty node is
        repaired, the computation is restarted from the last checkpoint
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Barroso2013ba-marker" href="ch08.html#Barroso2013ba">7</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Fiala2012ti-marker" href="ch08.html#Fiala2012ti">8</a>].
        Thus, a supercomputer is more like a single-node computer than a distributed system: it deals with
        partial failure by letting it escalate into total failure—if any part of the system fails, just
        let everything crash (like a kernel panic on a single machine).</p>
        
        <p><a data-type="indexterm" data-primary="internet services, systems for implementing" id="idm45085104954144"></a>
        In this book we focus on systems for implementing internet services, which usually look very
        different from supercomputers:</p>
        
        <ul>
        <li>
        <p>Many internet-related applications are <em>online</em>, in the sense that they need to be able to serve
        users with low latency at any time. Making the service unavailable—for example, stopping the
        cluster for repair—is not acceptable. In contrast, offline (batch) jobs like weather
        simulations can be stopped and restarted with fairly low impact.</p>
        </li>
        <li>
        <p><a data-type="indexterm" data-primary="RDMA (Remote Direct Memory Access)" id="idm45085104950368"></a>
        Supercomputers are typically built from specialized hardware, where each node is quite reliable,
        and nodes communicate through shared memory and remote direct memory access (RDMA). On the other
        hand, nodes in cloud services are built from commodity machines, which can provide equivalent
        performance at lower cost due to economies of scale, but also have higher failure rates.</p>
        </li>
        <li>
        <p><a data-type="indexterm" data-primary="datacenters" data-secondary="network architecture" id="idm45085104948416"></a>
        <a data-type="indexterm" data-primary="Ethernet (networks)" id="idm45085104947312"></a>
        <a data-type="indexterm" data-primary="networks" data-secondary="datacenter network topologies" id="idm45085104946480"></a>
        Large datacenter networks are often based on IP and Ethernet, arranged in Clos topologies to
        provide high bisection bandwidth
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Singh2015fc-marker" href="ch08.html#Singh2015fc">9</a>].
        Supercomputers often use specialized network topologies, such as multi-dimensional meshes and toruses
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Lockwood2014uz-marker" href="ch08.html#Lockwood2014uz">10</a>],
        which yield better performance for HPC workloads with known communication patterns.</p>
        </li>
        <li>
        <p>The bigger a system gets, the more likely it is that one of its components is broken. Over time,
        broken things get fixed and new things break, but in a system with thousands of nodes, it is
        reasonable to assume that <em>something</em> is always broken
        [<a data-type="noteref" href="ch08.html#Barroso2013ba">7</a>]. When the error handling strategy
        consists of simply giving up, a large system can end up spending a lot of its time recovering from
        faults rather than doing useful work [<a data-type="noteref" href="ch08.html#Fiala2012ti">8</a>].</p>
        </li>
        <li>
        <p>If the system can tolerate failed nodes and still keep working as a whole, that is a very useful
        feature for operations and maintenance: for example, you can perform a rolling upgrade (see
        <a data-type="xref" href="ch04.html#ch_encoding">Chapter&nbsp;4</a>), restarting one node at a time, while the service continues serving users without
        interruption. In cloud environments, if one virtual machine is not performing well, you can just
        kill it and request a new one (hoping that the new one will be faster).</p>
        </li>
        <li>
        <p>In a geographically distributed deployment (keeping data geographically close to your users to
        reduce access latency), communication most likely goes over the internet, which is slow and
        unreliable compared to local networks. Supercomputers generally assume that all of their nodes are
        close together.</p>
        </li>
        </ul>
        
        <p><a data-type="indexterm" data-primary="reliability" data-secondary="building a reliable system from unreliable components" id="idm45085104933008"></a>
        If we want to make distributed systems work, we must accept the possibility of partial failure and
        build fault-tolerance mechanisms into the software. In other words, we need to build a reliable
        system from unreliable components. (As discussed in <a data-type="xref" href="ch01.html#sec_introduction_reliability">“Reliability”</a>, there is no
        such thing as perfect reliability, so we’ll need to understand the limits of what we can
        realistically promise.)</p>
        
        <p>Even in smaller systems consisting of only a few nodes, it’s important to think about partial
        failure. In a small system, it’s quite likely that most of the components are working correctly most
        of the time. However, sooner or later, some part of the system <em>will</em> become faulty, and the
        software will have to somehow handle it. The fault handling must be part of the software design, and
        you (as operator of the software) need to know what behavior to expect from the software in the case
        of a fault.</p>
        
        <p>It would be unwise to assume that faults are rare and simply hope for the best. It is important to
        consider a wide range of possible faults—even fairly unlikely ones—and to artificially create
        such situations in your testing environment to see what happens. In distributed systems,
        suspicion, pessimism, and paranoia pay off.</p>
        <aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="sidebar_distributed_unreliable_components">
        <h5>Building a Reliable System from Unreliable Components</h5>
        <p>You may wonder whether this makes any sense—intuitively it may seem like a system can only be as
        reliable as its least reliable component (its <em>weakest link</em>). This is not the case: in fact, it is
        an old idea in computing to construct a more reliable system from a less reliable underlying base
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="vonNeumann1956vm-marker" href="ch08.html#vonNeumann1956vm">11</a>]. For example:</p>
        
        <ul>
        <li>
        <p><a data-type="indexterm" data-primary="error-correcting codes" id="idm45085104922480"></a>
        Error-correcting codes allow digital data to be transmitted accurately across a communication
        channel that occasionally gets some bits wrong, for example due to radio interference on a
        wireless network [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Hamming1997wd-marker" href="ch08.html#Hamming1997wd">12</a>].</p>
        </li>
        <li>
        <p><a data-type="indexterm" data-primary="IP (Internet Protocol)" data-secondary="unreliability of" id="idm45085104919024"></a>
        <a data-type="indexterm" data-primary="TCP (Transmission Control Protocol)" id="idm45085104917920"></a>
        IP (the Internet Protocol) is unreliable: it may drop, delay, duplicate, or reorder packets.
        TCP (the Transmission Control Protocol) provides a more reliable transport layer on top of IP: it
        ensures that missing packets are retransmitted, duplicates are eliminated, and packets are
        reassembled into the order in which they were sent.</p>
        </li>
        </ul>
        
        <p>Although the system can be more reliable than its underlying parts, there is always a limit to how
        much more reliable it can be. For example, error-correcting codes can deal with a small number of
        single-bit errors, but if your signal is swamped by interference, there is a fundamental limit to
        how much data you can get through your communication channel
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Shannon1948wk-marker" href="ch08.html#Shannon1948wk">13</a>].
        TCP can hide packet loss, duplication, and reordering from you, but it cannot magically remove delays
        in the network.</p>
        
        <p><a data-type="indexterm" data-primary="end-to-end argument" id="idm45085104912992"></a>
        Although the more reliable higher-level system is not perfect, it’s still useful because it takes
        care of some of the tricky low-level faults, and so the remaining faults are usually easier to
        reason about and deal with. We will explore this matter further in <a data-type="xref" href="ch12.html#sec_future_e2e_argument">“The end-to-end argument”</a>.
        <a data-type="indexterm" data-primary="failures" data-secondary="partial failures in distributed systems" data-startref="ix_failpartdistsys" id="idm45085104911056"></a>
        <a data-type="indexterm" data-primary="faults" data-secondary="in distributed systems" data-startref="ix_faultsDS" id="idm45085104909664"></a>
        <a data-type="indexterm" data-primary="distributed systems" data-secondary="faults and partial failures" data-startref="ix_distsysfaultfail" id="idm45085104908288"></a></p>
        </div></aside>
        </div></section>
        
        
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Unreliable Networks"><div class="sect1" id="sec_distributed_networks">
        <h1>Unreliable Networks</h1>
        
        <p><a data-type="indexterm" data-primary="distributed systems" data-secondary="network problems" id="ix_distsysunrelnet"></a>
        <a data-type="indexterm" data-primary="shared-nothing architecture" data-secondary="use of network" id="idm45085104904144"></a>
        As discussed in the introduction to <a data-type="xref" href="part02.html#part_distributed_data">Part&nbsp;II</a>, the distributed systems we focus on
        in this book are <em>shared-nothing systems</em>: i.e., a bunch of machines connected by a network. The
        network is the only way those machines can communicate—we assume that each machine has its
        own memory and disk, and one machine cannot access another machine’s memory or disk (except by
        making requests to a service over the network).</p>
        
        <p><a data-type="indexterm" data-primary="datacenters" data-secondary="geographically distributed" id="idm45085104901232"></a>
        <a data-type="indexterm" data-primary="geographically distributed datacenters" id="idm45085104900064"></a>
        Shared-nothing is not the only way of building systems, but it has become the dominant approach for
        building internet services, for several reasons: it’s comparatively cheap because it
        requires no special hardware, it can make use of commoditized cloud computing services, and it can
        achieve high reliability through redundancy across multiple geographically distributed datacenters.</p>
        
        <p><a data-type="indexterm" data-primary="asynchronous networks" id="idm45085104898560"></a>
        <a data-type="indexterm" data-primary="Ethernet (networks)" id="idm45085104897728"></a>
        The internet and most internal networks in datacenters (often Ethernet) are <em>asynchronous packet
        networks</em>. In this kind of network, one node can send a message (a packet) to another node, but the
        network gives no guarantees as to when it will arrive, or whether it will arrive at all. If you send
        a request and expect a response, many things could go wrong (some of which are illustrated in
        <a data-type="xref" href="#fig_distributed_network">Figure&nbsp;8-1</a>):</p>
        <ol>
        <li>
        <p>Your request may have been lost (perhaps someone unplugged a network cable).</p>
        </li>
        <li>
        <p>Your request may be waiting in a queue and will be delivered later (perhaps the network or the
        recipient is overloaded).</p>
        </li>
        <li>
        <p>The remote node may have failed (perhaps it crashed or it was powered down).</p>
        </li>
        <li>
        <p>The remote node may have temporarily stopped responding (perhaps it is experiencing a long
        garbage collection pause; see <a data-type="xref" href="#sec_distributed_clocks_pauses">“Process Pauses”</a>), but it will start responding
        again later.</p>
        </li>
        <li>
        <p>The remote node may have processed your request, but the response has been lost on the network
        (perhaps a network switch has been misconfigured).</p>
        </li>
        <li>
        <p>The remote node may have processed your request, but the response has been delayed and will be
        delivered later (perhaps the network or your own machine is overloaded).</p>
        </li>
        
        </ol>
        
        <figure><div id="fig_distributed_network" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_0801.png" alt="ddia 0801" width="2880" height="898">
        <h6><span class="label">Figure 8-1. </span>If you send a request and don’t get a response, it’s not possible to distinguish whether (a) the request was lost, (b) the remote node is down, or (c) the response was lost.</h6>
        </div></figure>
        
        <p>The sender can’t even tell whether the packet was delivered: the only option is for the recipient to
        send a response message, which may in turn be lost or delayed. These issues are indistinguishable in
        an asynchronous network: the only information you have is that you haven’t received a response yet.
        If you send a request to another node and don’t receive a response, it is <em>impossible</em> to tell why.</p>
        
        <p><a data-type="indexterm" data-primary="timeouts" id="idm45085104884448"></a>
        The usual way of handling this issue is a <em>timeout</em>: after some time you give up waiting and assume that
        the response is not going to arrive. However, when a timeout occurs, you still don’t know whether
        the remote node got your request or not (and if the request is still queued somewhere, it may still
        be delivered to the recipient, even if the sender has given up on it).</p>
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Network Faults in Practice"><div class="sect2" id="sec_distributed_network_faults">
        <h2>Network Faults in Practice</h2>
        
        <p><a data-type="indexterm" data-primary="faults" data-secondary="network faults" id="ix_faultnetpractice"></a>
        We have been building computer networks for decades—one might hope that by now we would have
        figured out how to make them reliable. However, it seems that we have not yet succeeded.</p>
        
        <p><a data-type="indexterm" data-primary="datacenters" data-secondary="network faults" id="idm45085104879280"></a>
        <a data-type="indexterm" data-primary="incidents" data-secondary="network faults" id="idm45085104877952"></a>
        <a data-type="indexterm" data-primary="human errors" id="idm45085104876848"></a>
        There are some systematic studies, and plenty of anecdotal evidence, showing that network problems
        can be surprisingly common, even in controlled environments like a datacenter operated by one
        company [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Bailis2014jx-marker" href="ch08.html#Bailis2014jx">14</a>].
        One study in a medium-sized datacenter found about 12 network faults per month, of which half
        disconnected a single machine, and half disconnected an entire rack
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Leners2015gv-marker" href="ch08.html#Leners2015gv">15</a>].
        Another study measured the failure rates of components like top-of-rack switches, aggregation
        switches, and load balancers
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Gill2011ku-marker" href="ch08.html#Gill2011ku">16</a>].
        It found that adding redundant networking gear doesn’t reduce faults as much as you might hope, since it
        doesn’t guard against human error (e.g., misconfigured switches), which is a major cause of outages.</p>
        
        <p><a data-type="indexterm" data-primary="cloud computing" data-secondary="network glitches" id="idm45085104866416"></a>
        <a data-type="indexterm" data-primary="Amazon Web Services (AWS)" data-secondary="network reliability" id="idm45085104865184"></a>
        <a data-type="indexterm" data-primary="incidents" data-secondary="network interface dropping only inbound packets" id="idm45085104864016"></a>
        <a data-type="indexterm" data-primary="incidents" data-secondary="split brain due to 1-minute packet delay" id="idm45085104862880"></a>
        <a data-type="indexterm" data-primary="incidents" data-secondary="sharks biting undersea cables" id="idm45085104861744"></a>
        <a data-type="indexterm" data-primary="sharks" data-secondary="biting undersea cables" id="idm45085104860624"></a>
        Public cloud services such as EC2 are notorious for having frequent transient network glitches
        [<a data-type="noteref" href="ch08.html#Bailis2014jx">14</a>], and well-managed private datacenter
        networks can be stabler environments. Nevertheless, nobody is immune from network problems: for
        example, a problem during a software upgrade for a switch could trigger a network topology
        reconfiguration, during which network packets could be delayed for more than a minute
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Imbriaco2012tx_ch8-marker" href="ch08.html#Imbriaco2012tx_ch8">17</a>]. Sharks might bite undersea cables and damage them
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Oremus2014ty-marker" href="ch08.html#Oremus2014ty">18</a>].
        Other surprising faults include a network interface that sometimes drops all inbound packets but
        sends outbound packets successfully [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Donges2012tt-marker" href="ch08.html#Donges2012tt">19</a>]:
        just because a network link works in one direction doesn’t guarantee it’s also working in the
        opposite direction.</p>
        <div data-type="note" epub:type="note"><h1>Network partitions</h1>
        <p><a data-type="indexterm" data-primary="networks" data-secondary="network partitions" id="idm45085104849888"></a>
        When one part of the network is cut off from the rest due to a network fault, that is sometimes
        called a <em>network partition</em> or <em>netsplit</em>. In this book we’ll generally stick with the more general term
        <em>network fault</em>, to avoid confusion with partitions (shards) of a storage system, as discussed in
        <a data-type="xref" href="ch06.html#ch_partitioning">Chapter&nbsp;6</a>.</p>
        </div>
        
        <p>Even if network faults are rare in your environment, the fact that faults <em>can</em> occur means that
        your software needs to be able to handle them. Whenever any communication happens over a network, it
        may fail—there is no way around it.</p>
        
        <p><a data-type="indexterm" data-primary="error handling" data-secondary="for network faults" id="idm45085104844880"></a>
        <a data-type="indexterm" data-primary="incidents" data-secondary="poor handling of network faults" id="idm45085104843776"></a>
        If the error handling of network faults is not defined and tested, arbitrarily bad things could
        happen: for example, the cluster could become deadlocked and permanently unable to serve requests,
        even when the network recovers [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kingsbury2014vi-marker" href="ch08.html#Kingsbury2014vi">20</a>], or it could even delete all of
        your data [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Sanfilippo2014ty-marker" href="ch08.html#Sanfilippo2014ty">21</a>].
        If software is put in an unanticipated situation, it may do arbitrary unexpected things.</p>
        
        <p>Handling network faults doesn’t necessarily mean <em>tolerating</em> them: if your network is normally
        fairly reliable, a valid approach may be to simply show an error message to users while your network
        is experiencing problems. However, you do need to know how your software reacts to network problems
        and ensure that the system can recover from them.
        <a data-type="indexterm" data-primary="Chaos Monkey" id="idm45085104836640"></a><a data-type="indexterm" data-primary="Netflix Chaos Monkey" id="idm45085104835936"></a>
        <a data-type="indexterm" data-primary="faults" data-secondary="introducing deliberately" id="idm45085104835136"></a>
        It may make sense to deliberately trigger network problems and test the system’s response (this is
        the idea behind Chaos Monkey; see <a data-type="xref" href="ch01.html#sec_introduction_reliability">“Reliability”</a>).</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Detecting Faults"><div class="sect2" id="idm45085104832912">
        <h2>Detecting Faults</h2>
        
        <p><a data-type="indexterm" data-primary="distributed systems" data-secondary="detecting network faults" id="idm45085104831536"></a>
        <a data-type="indexterm" data-primary="faults" data-secondary="network faults" data-tertiary="detecting" id="idm45085104830464"></a>
        <a data-type="indexterm" data-primary="failures" data-secondary="failure detection" id="idm45085104829088"></a>
        Many systems need to automatically detect faulty nodes. For example:</p>
        
        <ul>
        <li>
        <p>A load balancer needs to stop sending requests to a node that is dead (i.e., take it <em>out of rotation</em>).</p>
        </li>
        <li>
        <p>In a distributed database with single-leader replication, if the leader fails, one of the
        followers needs to be promoted to be the new leader (see <a data-type="xref" href="ch05.html#sec_replication_failover">“Handling Node Outages”</a>).</p>
        </li>
        </ul>
        
        <p>Unfortunately, the uncertainty about the network makes it difficult to tell whether a node is
        working or not. In some specific circumstances you might get some feedback to explicitly tell you
        that something is not working:</p>
        
        <ul>
        <li>
        <p><a data-type="indexterm" data-primary="TCP (Transmission Control Protocol)" data-secondary="connection failures" id="idm45085104822528"></a>
        If you can reach the machine on which the node should be running, but no process is listening on
        the destination port (e.g., because the process crashed), the operating system will helpfully close
        or refuse TCP connections by sending a <code>RST</code> or <code>FIN</code> packet in reply. However, if the node
        crashed while it was handling your request, you have no way of knowing how much data was actually
        processed by the remote node [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Hubert2009wf-marker" href="ch08.html#Hubert2009wf">22</a>].</p>
        </li>
        <li>
        <p>If a node process crashed (or was killed by an administrator) but the node’s operating system is
        still running, a script can notify other nodes about the crash so that another node can take over
        quickly without having to wait for a timeout to expire. For example, HBase does this
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Liochon2015ux-marker" href="ch08.html#Liochon2015ux">23</a>].</p>
        </li>
        <li>
        <p>If you have access to the management interface of the network switches in your datacenter, you can
        query them to detect link failures at a hardware level (e.g., if the remote machine is powered
        down). This option is ruled out if you’re connecting via the internet, or if you’re in a shared
        datacenter with no access to the switches themselves, or if you can’t reach the management
        interface due to a network problem.</p>
        </li>
        <li>
        <p>If a router is sure that the IP address you’re trying to connect to is unreachable, it may reply
        to you with an ICMP Destination Unreachable packet. However, the router doesn’t have a magic
        failure detection capability either—it is subject to the same limitations as other participants
        of the network.</p>
        </li>
        </ul>
        
        <p>Rapid feedback about a remote node being down is useful, but you can’t count on it. Even if TCP
        acknowledges that a packet was delivered, the application may have crashed before handling it. If
        you want to be sure that a request was successful, you need a positive response from the application
        itself [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Saltzer1984do_ch8-marker" href="ch08.html#Saltzer1984do_ch8">24</a>].</p>
        
        <p>Conversely, if something has gone wrong, you may get an error response at some level of the stack,
        but in general you have to assume that you will get no response at all. You can retry a few times
        (TCP retries transparently, but you may also retry at the application level), wait for a timeout to
        elapse, and eventually declare the node dead if you don’t hear back within the timeout.
        <a data-type="indexterm" data-primary="faults" data-secondary="network faults" data-startref="ix_faultnetpractice" id="idm45085104807136"></a></p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Timeouts and Unbounded Delays"><div class="sect2" id="sec_distributed_queueing">
        <h2>Timeouts and Unbounded Delays</h2>
        
        <p><a data-type="indexterm" data-primary="networks" data-secondary="timeouts and unbounded delays" id="idm45085104804080"></a>
        <a data-type="indexterm" data-primary="timeouts" data-secondary="length of" id="idm45085104802912"></a>
        If a timeout is the only sure way of detecting a fault, then how long should the timeout be? There
        is unfortunately no simple answer.</p>
        
        <p>A long timeout means a long wait until a node is declared dead (and during this time, users may have
        to wait or see error messages). A short timeout detects faults faster, but carries a higher risk of
        incorrectly declaring a node dead when in fact it has only suffered a temporary slowdown (e.g., due
        to a load spike on the node or the network).</p>
        
        <p>Prematurely declaring a node dead is problematic: if the node is actually alive and in the middle of
        performing some action (for example, sending an email), and another node takes over, the action may
        end up being performed twice. We will discuss this issue in more detail in
        <a data-type="xref" href="#sec_distributed_truth">“Knowledge, Truth, and Lies”</a>, and in
        Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch09.html#ch_consistency">9</a>
        and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch11.html#ch_stream">11</a>.</p>
        
        <p><a data-type="indexterm" data-primary="cascading failures" id="idm45085104796960"></a>
        When a node is declared dead, its responsibilities need to be transferred to other nodes, which
        places additional load on other nodes and the network. If the system is already struggling with high
        load, declaring nodes dead prematurely can make the problem worse. In particular, it could happen
        that the node actually wasn’t dead but only slow to respond due to overload; transferring its load
        to other nodes can cause a cascading failure (in the extreme case, all nodes declare each other
        dead, and everything stops working).</p>
        
        <p>Imagine a fictitious system with a network that guaranteed a maximum delay for packets—every packet
        is either delivered within some time <em>d</em>, or it is lost, but delivery never takes longer than <em>d</em>.
        Furthermore, assume that you can guarantee that a non-failed node always handles a request within
        some time <em>r</em>. In this case, you could guarantee that every successful request receives a response
        within time 2<em>d</em>&nbsp;+&nbsp;<em>r</em>—and if you don’t receive a response within that time, you know
        that either the network or the remote node is not working. If this was true,
        2<em>d</em>&nbsp;+&nbsp;<em>r</em> would be a reasonable timeout to use.</p>
        
        <p><a data-type="indexterm" data-primary="unbounded delays" data-secondary="in networks" id="idm45085104791232"></a>
        <a data-type="indexterm" data-primary="delays" data-secondary="unbounded network delays" id="idm45085104790096"></a>
        <a data-type="indexterm" data-primary="failures" data-secondary="failure detection" data-tertiary="timeouts and unbounded delays" id="idm45085104788976"></a>
        Unfortunately, most systems we work with have neither of those guarantees: asynchronous networks
        have <em>unbounded delays</em> (that is, they try to deliver packets as quickly as possible, but there is
        no upper limit on the time it may take for a packet to arrive), and most server implementations
        cannot guarantee that they can handle requests within some maximum time (see
        <a data-type="xref" href="#sec_distributed_clocks_realtime">“Response time guarantees”</a>). For failure detection, it’s not sufficient for the system to
        be fast most of the time: if your timeout is low, it only takes a transient spike in round-trip
        times to throw the system off-balance.</p>
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Network congestion and queueing"><div class="sect3" id="sec_distributed_congestion">
        <h3>Network congestion and queueing</h3>
        
        <p><a data-type="indexterm" data-primary="congestion (networks)" data-secondary="queueing delays" id="idm45085104783840"></a>
        <a data-type="indexterm" data-primary="networks" data-secondary="congestion and queueing" id="idm45085104782736"></a>
        <a data-type="indexterm" data-primary="queueing delays (networks)" id="idm45085104781632"></a>
        When driving a car, travel times on road networks often vary most due to traffic congestion.
        Similarly, the variability of packet delays on computer networks is most often due to queueing
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Grosvenor2015vz-marker" href="ch08.html#Grosvenor2015vz">25</a>]:</p>
        
        <ul>
        <li>
        <p>If several different nodes simultaneously try to send packets to the same destination, the network
        switch must queue them up and feed them into the destination network link one by one (as illustrated
        in <a data-type="xref" href="#fig_distributed_switch_queueing">Figure&nbsp;8-2</a>). On a busy network link, a packet may have to wait a while
        until it can get a slot (this is called <em>network congestion</em>). If there is so much incoming data
        that the switch queue fills up, the packet is dropped, so it needs to be resent—even though
        the network is functioning fine.</p>
        </li>
        <li>
        <p>When a packet reaches the destination machine, if all CPU cores are currently busy, the incoming
        request from the network is queued by the operating system until the application is ready to
        handle it. Depending on the load on the machine, this may take an arbitrary length of time.</p>
        </li>
        <li>
        <p><a data-type="indexterm" data-primary="virtual machines" data-secondary="network performance" id="idm45085104773312"></a>
        In virtualized environments, a running operating system is often paused for tens of milliseconds
        while another virtual machine uses a CPU core. During this time, the VM cannot consume any data
        from the network, so the incoming data is queued (buffered) by the virtual machine monitor
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Wang2010ja-marker" href="ch08.html#Wang2010ja">26</a>],
        further increasing the variability of network delays.</p>
        </li>
        <li>
        <p><a data-type="indexterm" data-primary="flow control" id="idm45085104768080"></a>
        <a data-type="indexterm" data-primary="congestion (networks)" data-secondary="avoidance" id="idm45085104767024"></a>
        <a data-type="indexterm" data-primary="backpressure" data-secondary="in TCP" id="idm45085104765920"></a>
        <a data-type="indexterm" data-primary="TCP (Transmission Control Protocol)" data-secondary="flow control" id="idm45085104764816"></a>
        TCP performs <em>flow control</em> (also known as <em>congestion avoidance</em> or <em>backpressure</em>), in which a
        node limits its own rate of sending in order to avoid overloading a network link or the receiving
        node [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Jacobson1988gl-marker" href="ch08.html#Jacobson1988gl">27</a>].
        This means additional queueing at the sender before the data even enters the network.</p>
        </li>
        </ul>
        
        <figure><div id="fig_distributed_switch_queueing" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_0802.png" alt="ddia 0802" width="2880" height="1081">
        <h6><span class="label">Figure 8-2. </span>If several machines send network traffic to the same destination, its switch queue can fill up. Here, ports 1, 2, and 4 are all trying to send packets to port 3.</h6>
        </div></figure>
        
        <p>Moreover, TCP considers a packet to be lost if it is not acknowledged within some timeout (which is
        calculated from observed round-trip times), and lost packets are automatically retransmitted.
        Although the application does not see the packet loss and retransmission, it does see the resulting
        delay (waiting for the timeout to expire, and then waiting for the retransmitted packet to be
        acknowledged).</p>
        <aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="sidebar_distributed_tcp_udp">
        <h5>TCP Versus UDP</h5>
        <p><a data-type="indexterm" data-primary="UDP (User Datagram Protocol)" data-secondary="comparison to TCP" id="idm45085104754704"></a>
        <a data-type="indexterm" data-primary="TCP (Transmission Control Protocol)" data-secondary="comparison to UDP" id="idm45085104753584"></a>
        <a data-type="indexterm" data-primary="Voice over IP (VoIP)" id="idm45085104752464"></a>
        Some latency-sensitive applications, such as videoconferencing and Voice over IP (VoIP), use UDP
        rather than TCP. It’s a trade-off between reliability and variability of delays: as UDP does not
        perform flow control and does not retransmit lost packets, it avoids some of the reasons for
        variable network delays (although it is still susceptible to switch queues and scheduling delays).</p>
        
        <p>UDP is a good choice in situations where delayed data is worthless. For example, in a VoIP phone
        call, there probably isn’t enough time to retransmit a lost packet before its data is due to be
        played over the loudspeakers. In this case, there’s no point in retransmitting the packet—the
        application must instead fill the missing packet’s time slot with silence (causing a brief
        interruption in the sound) and move on in the stream. The retry happens at the human layer instead.
        (“Could you repeat that please? The sound just cut out for a moment.”)</p>
        </div></aside>
        
        <p>All of these factors contribute to the variability of network delays. Queueing delays have an
        especially wide range when a system is close to its maximum capacity: a system with plenty of spare
        capacity can easily drain queues, whereas in a highly utilized system, long queues can build up very
        quickly.</p>
        
        <p><a data-type="indexterm" data-primary="cloud computing" data-secondary="shared resources" id="idm45085104749120"></a>
        <a data-type="indexterm" data-primary="datacenters" data-secondary="multi-tenancy and shared resources" id="idm45085104747792"></a>
        <a data-type="indexterm" data-primary="virtual machines" data-secondary="noisy neighbors" id="idm45085104746720"></a>
        <a data-type="indexterm" data-primary="noisy neighbors" id="idm45085104745616"></a>
        <a data-type="indexterm" data-primary="multi-tenancy" id="idm45085104744784"></a>
        In public clouds and multi-tenant datacenters, resources are shared among many customers: the
        network links and switches, and even each machine’s network interface and CPUs (when running on
        virtual machines), are shared. Batch workloads such as MapReduce (see <a data-type="xref" href="ch10.html#ch_batch">Chapter&nbsp;10</a>) can easily
        saturate network links. As you have no control over or insight into other customers’ usage of the shared
        resources, network delays can be highly variable if someone near you (a <em>noisy neighbor</em>) is
        using a lot of resources [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Philips2014tr-marker" href="ch08.html#Philips2014tr">28</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Newman2012vf-marker" href="ch08.html#Newman2012vf">29</a>].</p>
        
        <p><a data-type="indexterm" data-primary="failures" data-secondary="failure detection" data-tertiary="timeouts and unbounded delays" id="idm45085104737632"></a>
        In such environments, you can only choose timeouts experimentally: measure the distribution of
        network round-trip times over an extended period, and over many machines, to determine the expected
        variability of delays. Then, taking into account your application’s characteristics, you can
        determine an appropriate trade-off between failure detection delay and risk of premature timeouts.</p>
        
        <p><a data-type="indexterm" data-primary="jitter (network delay)" id="idm45085104735616"></a>
        <a data-type="indexterm" data-primary="timeouts" data-secondary="dynamic configuration of" id="idm45085104734560"></a>
        <a data-type="indexterm" data-primary="TCP (Transmission Control Protocol)" data-secondary="retransmission timeouts" id="idm45085104733488"></a>
        Even better, rather than using configured constant timeouts, systems can continually measure
        response times and their variability (<em>jitter</em>), and automatically adjust timeouts according to the
        observed response time distribution. This can be done with a Phi Accrual failure detector
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Hayashibara2004vw-marker" href="ch08.html#Hayashibara2004vw">30</a>],
        which is used for example in Akka and Cassandra [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Wang2013wa-marker" href="ch08.html#Wang2013wa">31</a>].
        TCP retransmission timeouts also work similarly
        [<a data-type="noteref" href="ch08.html#Jacobson1988gl">27</a>].</p>
        </div></section>
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Synchronous Versus Asynchronous Networks"><div class="sect2" id="sec_distributed_sync_networks">
        <h2>Synchronous Versus Asynchronous Networks</h2>
        
        <p><a data-type="indexterm" data-primary="asynchronous networks" data-secondary="comparison to synchronous networks" id="idm45085104725440"></a>
        <a data-type="indexterm" data-primary="synchronous networks" data-secondary="comparison to asynchronous networks" id="idm45085104724320"></a>
        Distributed systems would be a lot simpler if we could rely on the network to deliver packets with
        some fixed maximum delay, and not to drop packets. Why can’t we solve this at the hardware level
        and make the network reliable so that the software doesn’t need to worry about it?</p>
        
        <p>To answer this question, it’s interesting to compare datacenter networks to the traditional fixed-line
        telephone network (non-cellular, non-VoIP), which is extremely reliable: delayed audio
        frames and dropped calls are very rare. A phone call requires a constantly low end-to-end latency
        and enough bandwidth to transfer the audio samples of your voice. Wouldn’t it be nice to have
        similar reliability and predictability in computer networks?</p>
        
        <p><a data-type="indexterm" data-primary="circuit-switched networks" id="idm45085104721760"></a>
        <a data-type="indexterm" data-primary="ISDN (Integrated Services Digital Network)" id="idm45085104720736"></a>
        When you make a call over the telephone network, it establishes a <em>circuit</em>: a fixed, guaranteed
        amount of bandwidth is allocated for the call, along the entire route between the two callers. This
        circuit remains in place until the call ends
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Keshav1997wb-marker" href="ch08.html#Keshav1997wb">32</a>].
        For example, an ISDN network runs at a fixed rate of 4,000 frames per second. When a call is
        established, it is allocated 16 bits of space within each frame (in each direction). Thus, for the
        duration of the call, each side is guaranteed to be able to send exactly 16 bits of audio data every
        250 microseconds
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="CiscoISDN-marker" href="ch08.html#CiscoISDN">33</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kyas1995ug-marker" href="ch08.html#Kyas1995ug">34</a>].</p>
        
        <p><a data-type="indexterm" data-primary="synchronous networks" id="idm45085104713440"></a>
        <a data-type="indexterm" data-primary="bounded delays" data-secondary="in networks" id="idm45085104712576"></a>
        <a data-type="indexterm" data-primary="delays" data-secondary="bounded network delays" id="idm45085104711472"></a>
        This kind of network is <em>synchronous</em>: even as data passes through several routers, it does not
        suffer from queueing, because the 16 bits of space for the call have already been reserved in the
        next hop of the network. And because there is no queueing, the maximum end-to-end latency of the
        network is fixed. We call this a <em>bounded delay</em>.</p>
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Can we not simply make network delays predictable?"><div class="sect3" id="idm45085104709136">
        <h3>Can we not simply make network delays predictable?</h3>
        
        <p><a data-type="indexterm" data-primary="packet switching" id="idm45085104707760"></a>
        <a data-type="indexterm" data-primary="TCP (Transmission Control Protocol)" data-secondary="comparison to circuit switching" id="idm45085104706928"></a>
        Note that a circuit in a telephone network is very different from a TCP connection: a circuit is a
        fixed amount of reserved bandwidth which nobody else can use while the circuit is established,
        whereas the packets of a TCP connection opportunistically use whatever network bandwidth is
        available. You can give TCP a variable-sized block of data (e.g., an email or a web page), and it
        will try to transfer it in the shortest time possible. While a TCP connection is idle, it doesn’t
        use any bandwidth.<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm45085104705232-marker" href="ch08.html#idm45085104705232">ii</a></sup></p>
        
        <p><a data-type="indexterm" data-primary="Ethernet (networks)" id="idm45085104704176"></a>
        If datacenter networks and the internet were circuit-switched networks, it would be possible to
        establish a guaranteed maximum round-trip time when a circuit was set up. However, they are not:
        Ethernet and IP are packet-switched protocols, which suffer from queueing and thus unbounded delays
        in the network. These protocols do not have the concept of a circuit.</p>
        
        <p><a data-type="indexterm" data-primary="bursty network traffic patterns" id="idm45085104702704"></a>
        Why do datacenter networks and the internet use packet switching? The answer is that they are
        optimized for <em>bursty traffic</em>. A circuit is good for an audio or video call, which needs to
        transfer a fairly constant number of bits per second for the duration of the call. On the other
        hand, requesting a web page, sending an email, or transferring a file doesn’t have any particular
        bandwidth requirement—we just want it to complete as quickly as possible.</p>
        
        <p>If you wanted to transfer a file over a circuit, you would have to guess a bandwidth allocation. If
        you guess too low, the transfer is unnecessarily slow, leaving network capacity unused. If you guess
        too high, the circuit cannot be set up (because the network cannot allow a circuit to be created if
        its bandwidth allocation cannot be guaranteed). Thus, using circuits for bursty data transfers
        wastes network capacity and makes transfers unnecessarily slow. By contrast, TCP dynamically adapts
        the rate of data transfer to the available network capacity.</p>
        
        <p><a data-type="indexterm" data-primary="quality of service (QoS)" id="idm45085104699616"></a>
        <a data-type="indexterm" data-primary="Asynchronous Transfer Mode (ATM)" id="idm45085104698592"></a>
        <a data-type="indexterm" data-primary="InfiniBand (networks)" id="idm45085104697744"></a>
        There have been some attempts to build hybrid networks that support both circuit switching and
        packet switching, such as ATM.<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm45085104696768-marker" href="ch08.html#idm45085104696768">iii</a></sup>
        InfiniBand has some similarities
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Mellanox2014ux-marker" href="ch08.html#Mellanox2014ux">35</a>]: it implements end-to-end
        flow control at the link layer, which reduces the need for queueing in the network, although it can
        still suffer from delays due to link congestion
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Santos2003ci-marker" href="ch08.html#Santos2003ci">36</a>].
        With careful use of <em>quality of service</em> (QoS, prioritization and scheduling of packets) and <em>admission
        control</em> (rate-limiting senders), it is possible to emulate circuit switching on packet networks, or
        provide statistically bounded delay [<a data-type="noteref" href="ch08.html#Grosvenor2015vz">25</a>,
        <a data-type="noteref" href="ch08.html#Keshav1997wb">32</a>].</p>
        <aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="sidebar_distributed_latency_utilization">
        <h5>Latency and Resource Utilization</h5>
        <p><a data-type="indexterm" data-primary="latency" data-secondary="network latency and resource utilization" id="idm45085104684912"></a>
        More generally, you can think of variable delays as a consequence of dynamic resource partitioning.</p>
        
        <p>Say you have a wire between two telephone switches that can carry up to 10,000 simultaneous calls.
        Each circuit that is switched over this wire occupies one of those call slots. Thus, you can think of
        the wire as a resource that can be shared by up to 10,000 simultaneous users. The resource is
        divided up in a <em>static</em> way: even if you’re the only call on the wire right now, and all other
        9,999 slots are unused, your circuit is still allocated the same fixed amount of bandwidth as when
        the wire is fully utilized.</p>
        
        <p>By contrast, the internet shares network bandwidth <em>dynamically</em>. Senders push and jostle with each
        other to get their packets over the wire as quickly as possible, and the network switches decide
        which packet to send (i.e., the bandwidth allocation) from one moment to the next. This approach has the
        downside of queueing, but the advantage is that it maximizes utilization of the wire. The wire has a
        fixed cost, so if you utilize it better, each byte you send over the wire is cheaper.</p>
        
        <p><a data-type="indexterm" data-primary="threads (concurrency)" data-secondary="execution pauses" id="idm45085104680688"></a>
        A similar situation arises with CPUs: if you share each CPU core dynamically between several
        threads, one thread sometimes has to wait in the operating system’s run queue while another thread
        is running, so a thread can be paused for varying lengths of time. However, this utilizes the
        hardware better than if you allocated a static number of CPU cycles to each thread (see
        <a data-type="xref" href="#sec_distributed_clocks_realtime">“Response time guarantees”</a>). Better hardware utilization is also a significant motivation
        for using virtual machines.</p>
        
        <p>Latency guarantees are achievable in certain environments, if resources are statically partitioned
        (e.g., dedicated hardware and exclusive bandwidth allocations). However, it comes at the cost of
        reduced utilization—in other words, it is more expensive. On the other hand, multi-tenancy with
        dynamic resource partitioning provides better utilization, so it is cheaper, but it has the downside
        of variable delays.</p>
        
        <p>Variable delays in networks are not a law of nature, but simply the result of a cost/benefit
        trade-off.
        <a data-type="indexterm" data-primary="distributed systems" data-secondary="network problems" data-startref="ix_distsysunrelnet" id="idm45085104676848"></a></p>
        </div></aside>
        
        <p>However, such quality of service is currently not enabled in multi-tenant datacenters and public
        clouds, or when communicating via the internet.<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm45085104675056-marker" href="ch08.html#idm45085104675056">iv</a></sup>
        Currently deployed technology does not allow us to make any guarantees about delays or reliability
        of the network: we have to assume that network congestion, queueing, and unbounded delays will
        happen. Consequently, there’s no “correct” value for timeouts—they need to be determined
        experimentally.</p>
        </div></section>
        
        
        
        </div></section>
        
        
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Unreliable Clocks"><div class="sect1" id="sec_distributed_clocks">
        <h1>Unreliable Clocks</h1>
        
        <p><a data-type="indexterm" data-primary="data systems" data-secondary="unreliable clocks" id="ix_distsysunrelclock"></a>
        <a data-type="indexterm" data-primary="time" data-secondary="in distributed systems" data-seealso="clocks" id="ix_timeclock"></a>
        <a data-type="indexterm" data-primary="clocks" id="ix_clocks"></a>
        Clocks and time are important. Applications depend on clocks in various ways to answer questions
        like the following:</p>
        <ol>
        <li>
        <p>Has this request timed out yet?</p>
        </li>
        <li>
        <p>What’s the 99th percentile response time of this service?</p>
        </li>
        <li>
        <p>How many queries per second did this service handle on average in the last five minutes?</p>
        </li>
        <li>
        <p>How long did the user spend on our site?</p>
        </li>
        <li>
        <p>When was this article published?</p>
        </li>
        <li>
        <p>At what date and time should the reminder email be sent?</p>
        </li>
        <li>
        <p>When does this cache entry expire?</p>
        </li>
        <li>
        <p>What is the timestamp on this error message in the log file?</p>
        </li>
        
        </ol>
        
        <p><a data-type="indexterm" data-primary="duration (time)" id="idm45085104659376"></a>
        <a data-type="indexterm" data-primary="point in time" id="idm45085104658512"></a>
        Examples 1–4 measure <em>durations</em> (e.g., the time interval between a request being sent and a
        response being received), whereas examples 5–8 describe <em>points in time</em> (events that occur on a
        particular date, at a particular time).</p>
        
        <p><a data-type="indexterm" data-primary="distributed systems" data-secondary="use of clocks and time" id="idm45085104656384"></a>
        In a distributed system, time is a tricky business, because communication is not instantaneous: it
        takes time for a message to travel across the network from one machine to another. The time when a
        message is received is always later than the time when it is sent, but due to variable delays in the
        network, we don’t know how much later. This fact sometimes makes it difficult to determine the order
        in which things happened when multiple machines are involved.</p>
        
        <p><a data-type="indexterm" data-primary="NTP (Network Time Protocol)" id="idm45085104654544"></a>
        <a data-type="indexterm" data-primary="Network Time Protocol" data-see="NTP" id="idm45085104653648"></a>
        <a data-type="indexterm" data-primary="GPS (Global Positioning System)" data-secondary="use for clock synchronization" id="idm45085104652544"></a>
        <a data-type="indexterm" data-primary="clocks" data-secondary="synchronization using GPS" id="idm45085104651408"></a>
        Moreover, each machine on the network has its own clock, which is an actual hardware device: usually
        a quartz crystal oscillator. These devices are not perfectly accurate, so each machine has its own
        notion of time, which may be slightly faster or slower than on other machines. It is possible to
        synchronize clocks to some degree: the most commonly used mechanism is the Network Time Protocol (NTP), which
        allows the computer clock to be adjusted according to the time reported by a group of servers
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Windl2006uo-marker" href="ch08.html#Windl2006uo">37</a>]. The servers in turn get their time from a more accurate time source, such
        as a GPS receiver.</p>
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Monotonic Versus Time-of-Day Clocks"><div class="sect2" id="sec_distributed_monotonic_timeofday">
        <h2>Monotonic Versus Time-of-Day Clocks</h2>
        
        <p><a data-type="indexterm" data-primary="clocks" data-secondary="time-of-day versus monotonic clocks" id="idm45085104645648"></a>
        Modern computers have at least two different kinds of clocks: a <em>time-of-day clock</em> and a <em>monotonic
        clock</em>. Although they both measure time, it is important to distinguish the two, since they serve
        different purposes.</p>
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Time-of-day clocks"><div class="sect3" id="idm45085104643312">
        <h3>Time-of-day clocks</h3>
        
        <p><a data-type="indexterm" data-primary="time-of-day clocks" id="idm45085104642112"></a>
        <a data-type="indexterm" data-primary="epoch (Unix timestamps)" id="idm45085104641056"></a>
        <a data-type="indexterm" data-primary="real-time" data-secondary="time-of-day clocks" id="idm45085104640224"></a>
        A time-of-day clock does what you intuitively expect of a clock: it returns the current date and
        time according to some calendar (also known as <em>wall-clock time</em>). For example,
        <code>clock_gettime(CLOCK_REALTIME)</code> on Linux<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm45085104638128-marker" href="ch08.html#idm45085104638128">v</a></sup> and
        <code>System.currentTimeMillis()</code> in Java return the number of seconds (or milliseconds) since the
        <em>epoch</em>: midnight UTC on January 1, 1970, according to the Gregorian calendar, not counting leap
        seconds. Some systems use other dates as their reference point.</p>
        
        <p><a data-type="indexterm" data-primary="leap seconds" data-secondary="in time-of-day clocks" id="idm45085104634672"></a>
        Time-of-day clocks are usually synchronized with NTP, which means that a timestamp from one machine
        (ideally) means the same as a timestamp on another machine. However, time-of-day clocks also have
        various oddities, as described in the next section. In particular, if the local clock is too far
        ahead of the NTP server, it may be forcibly reset and appear to jump back to a previous point in
        time. These jumps, as well as similar jumps caused by leap seconds, make time-of-day clocks
        unsuitable for measuring elapsed time
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="GrahamCumming2017db-marker" href="ch08.html#GrahamCumming2017db">38</a>].</p>
        
        <p>Time-of-day clocks have also historically had quite a coarse-grained resolution, e.g., moving forward
        in steps of 10&nbsp;ms on older Windows systems
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Holmes2006uj-marker" href="ch08.html#Holmes2006uj">39</a>]. On recent systems, this is less of a problem.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Monotonic clocks"><div class="sect3" id="idm45085104627424">
        <h3>Monotonic clocks</h3>
        
        <p><a data-type="indexterm" data-primary="monotonic clocks" id="idm45085104626048"></a>
        <a data-type="indexterm" data-primary="duration (time)" data-secondary="measurement with monotonic clocks" id="idm45085104625216"></a>
        A monotonic clock is suitable for measuring a duration (time interval), such as a timeout or a
        service’s response time: <code>clock_gettime(CLOCK_MONOTONIC)</code> on Linux and
        <code>System.nanoTime()</code> in Java are monotonic clocks, for example. The name comes from the fact that they are
        guaranteed to always move forward (whereas a time-of-day clock may jump back in time).</p>
        
        <p>You can check the value of the monotonic clock at one point in time, do something, and then check
        the clock again at a later time. The <em>difference</em> between the two values tells you how much time
        elapsed between the two checks. However, the <em>absolute</em> value of the clock is meaningless: it might
        be the number of nanoseconds since the computer was started, or something similarly arbitrary. In
        particular, it makes no sense to compare monotonic clock values from two different computers,
        because they don’t mean the same thing.</p>
        
        <p>On a server with multiple CPU sockets, there may be a separate timer per CPU, which is not
        necessarily synchronized with other CPUs. Operating systems compensate for any discrepancy and try
        to present a monotonic view of the clock to application threads, even as they are scheduled across
        different CPUs. However, it is wise to take this guarantee of monotonicity with a pinch of salt
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Loughran2015wi-marker" href="ch08.html#Loughran2015wi">40</a>].</p>
        
        <p><a data-type="indexterm" data-primary="NTP (Network Time Protocol)" data-secondary="adjustments to monotonic clocks" id="idm45085104617616"></a>
        <a data-type="indexterm" data-primary="clocks" data-secondary="slewing" id="idm45085104616544"></a>
        NTP may adjust the frequency at which the monotonic clock moves forward (this is known as <em>slewing</em>
        the clock) if it detects that the computer’s local quartz is moving faster or slower than the NTP
        server. By default, NTP allows the clock rate to be speeded up or slowed down by up to 0.05%, but
        NTP cannot cause the monotonic clock to jump forward or backward. The resolution of monotonic
        clocks is usually quite good: on most systems they can measure time intervals in microseconds or
        less.</p>
        
        <p>In a distributed system, using a monotonic clock for measuring elapsed time (e.g., timeouts) is
        usually fine, because it doesn’t assume any synchronization between different nodes’ clocks and is
        not sensitive to slight inaccuracies of measurement.</p>
        </div></section>
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Clock Synchronization and Accuracy"><div class="sect2" id="sec_distributed_clock_accuracy">
        <h2>Clock Synchronization and Accuracy</h2>
        
        <p><a data-type="indexterm" data-primary="clocks" data-secondary="synchronization and accuracy" id="ix_clocksync"></a>
        <a data-type="indexterm" data-primary="correctness" data-secondary="of time" id="ix_correcttime"></a>
        <a data-type="indexterm" data-primary="time" data-secondary="in distributed systems" data-tertiary="clock synchronization and accuracy" id="idm45085104609360"></a>
        <a data-type="indexterm" data-primary="NTP (Network Time Protocol)" data-secondary="accuracy" id="idm45085104607968"></a>
        Monotonic clocks don’t need synchronization, but time-of-day clocks need to be set according to an
        NTP server or other external time source in order to be useful. Unfortunately, our methods for
        getting a clock to tell the correct time aren’t nearly as reliable or accurate as you might
        hope—hardware clocks and NTP can be fickle beasts. To give just a few examples:</p>
        
        <ul>
        <li>
        <p><a data-type="indexterm" data-primary="drift (clocks)" id="idm45085104605376"></a>
        The quartz clock in a computer is not very accurate: it <em>drifts</em> (runs faster or slower than it
        should). Clock drift varies depending on the temperature of the machine. Google assumes a clock
        drift of 200&nbsp;ppm (parts per million) for its servers
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Corbett2012uz_ch8-marker" href="ch08.html#Corbett2012uz_ch8">41</a>],
        which is equivalent to 6&nbsp;ms drift for a clock that is resynchronized with a server every 30
        seconds, or 17 seconds drift for a clock that is resynchronized once a day. This drift limits the best
        possible accuracy you can achieve, even if everything is working correctly.</p>
        </li>
        <li>
        <p>If a computer’s clock differs too much from an NTP server, it may refuse to synchronize, or the
        local clock will be forcibly reset [<a data-type="noteref" href="ch08.html#Windl2006uo">37</a>]. Any
        applications observing the time before and after this reset may see time go backward or suddenly
        jump forward.</p>
        </li>
        <li>
        <p>If a node is accidentally firewalled off from NTP servers, the misconfiguration may go
        unnoticed for some time. Anecdotal evidence suggests that this does happen in practice.</p>
        </li>
        <li>
        <p>NTP synchronization can only be as good as the network delay, so there is a limit to its
        accuracy when you’re on a congested network with variable packet delays. One experiment showed
        that a minimum error of 35&nbsp;ms is achievable when synchronizing over the internet
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Caporaloni2012jn-marker" href="ch08.html#Caporaloni2012jn">42</a>],
        though occasional spikes in network delay lead to errors of around a second. Depending on the
        configuration, large network delays can cause the NTP client to give up entirely.</p>
        </li>
        <li>
        <p>Some NTP servers are wrong or misconfigured, reporting time that is off by hours
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Minar1999vf-marker" href="ch08.html#Minar1999vf">43</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Holub2014uc-marker" href="ch08.html#Holub2014uc">44</a>].
        NTP clients are quite robust, because they query several servers and ignore outliers.
        Nevertheless, it’s somewhat worrying to bet the correctness of your systems on the time that you
        were told by a stranger on the internet.</p>
        </li>
        <li>
        <p><a data-type="indexterm" data-primary="leap seconds" id="idm45085104587568"></a>
        <a data-type="indexterm" data-primary="Linux, leap second bug" id="idm45085104586496"></a>
        <a data-type="indexterm" data-primary="incidents" data-secondary="crashes due to leap seconds" id="idm45085104585664"></a>
        Leap seconds result in a minute that is 59 seconds or 61 seconds long, which messes up timing
        assumptions in systems that are not designed with leap seconds in mind
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kamp2011cr-marker" href="ch08.html#Kamp2011cr">45</a>].
        <a data-type="indexterm" data-primary="smearing (leap seconds adjustments)" id="idm45085104581504"></a>
        The fact that leap seconds have crashed many large systems
        [<a data-type="noteref" href="ch08.html#GrahamCumming2017db">38</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Minar2012vh_ch8-marker" href="ch08.html#Minar2012vh_ch8">46</a>]
        shows how easy it is for incorrect assumptions about clocks to sneak into a system. The best
        way of handling leap seconds may be to make NTP servers “lie,” by performing the leap second
        adjustment gradually over the course of a day (this is known as <em>smearing</em>)
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Pascoe2011uj-marker" href="ch08.html#Pascoe2011uj">47</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Zhao2015ws-marker" href="ch08.html#Zhao2015ws">48</a>],
        although actual NTP server behavior varies in practice
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Veitch2016jw-marker" href="ch08.html#Veitch2016jw">49</a>].</p>
        </li>
        <li>
        <p><a data-type="indexterm" data-primary="virtual machines" data-secondary="virtualized clocks in" id="idm45085104569088"></a>
        In virtual machines, the hardware clock is virtualized, which raises additional challenges for
        applications that need accurate timekeeping
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="VMware2011vm-marker" href="ch08.html#VMware2011vm">50</a>].
        When a CPU core is shared between virtual machines, each VM is paused for tens of milliseconds
        while another VM is running. From an application’s point of view, this pause manifests itself as
        the clock suddenly jumping forward [<a data-type="noteref" href="ch08.html#Wang2010ja">26</a>].</p>
        </li>
        <li>
        <p>If you run software on devices that you don’t fully control (e.g., mobile or embedded devices), you
        probably cannot trust the device’s hardware clock at all. Some users deliberately set their
        hardware clock to an incorrect date and time, for example to circumvent timing limitations in
        games. As a result, the clock might be set to a time wildly in the past or the future.</p>
        </li>
        </ul>
        
        <p><a data-type="indexterm" data-primary="high-frequency trading" id="idm45085104563280"></a>
        It is possible to achieve very good clock accuracy if you care about it sufficiently to invest
        significant resources. For example, the MiFID II draft European regulation for financial
        institutions requires all high-frequency trading funds to synchronize their clocks to within 100
        microseconds of UTC, in order to help debug market anomalies such as “flash crashes” and to help
        detect market manipulation
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="MiFID2015wn-marker" href="ch08.html#MiFID2015wn">51</a>].</p>
        
        <p><a data-type="indexterm" data-primary="Precision Time Protocol (PTP)" id="idm45085104559952"></a>
        <a data-type="indexterm" data-primary="GPS (Global Positioning System)" data-secondary="use for clock synchronization" id="idm45085104559104"></a>
        <a data-type="indexterm" data-primary="clocks" data-secondary="synchronization using GPS" id="idm45085104557968"></a>
        Such accuracy can be achieved using GPS receivers, the Precision Time Protocol (PTP)
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Bigum2015ux-marker" href="ch08.html#Bigum2015ux">52</a>],
        and careful deployment and monitoring. However, it requires significant effort and expertise, and
        there are plenty of ways clock synchronization can go wrong. If your NTP daemon is
        misconfigured, or a firewall is blocking NTP traffic, the clock error due to drift can quickly
        become large.
        <a data-type="indexterm" data-primary="clocks" data-secondary="synchronization and accuracy" data-startref="ix_clocksync" id="idm45085104554016"></a></p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Relying on Synchronized Clocks"><div class="sect2" id="sec_distributed_clocks_relying">
        <h2>Relying on Synchronized Clocks</h2>
        
        <p><a data-type="indexterm" data-primary="distributed systems" data-secondary="synchronized clocks, relying on" id="ix_distsysclockrely"></a>
        <a data-type="indexterm" data-primary="time" data-secondary="in distributed systems" data-tertiary="relying on synchronized clocks" id="ix_timeclocksync"></a>
        The problem with clocks is that while they seem simple and easy to use, they have a surprising
        number of pitfalls: a day may not have exactly 86,400 seconds, time-of-day clocks may move backward
        in time, and the time on one node may be quite different from the time on another node.</p>
        
        <p>Earlier in this chapter we discussed networks dropping and arbitrarily delaying packets. Even though
        networks are well behaved most of the time, software must be designed on the assumption that the
        network will occasionally be faulty, and the software must handle such faults gracefully. The same
        is true with clocks: although they work quite well most of the time, robust software needs to be
        prepared to deal with incorrect clocks.</p>
        
        <p><a data-type="indexterm" data-primary="clocks" data-secondary="skew" id="ix_clockskew"></a>
        <a data-type="indexterm" data-primary="skew" data-secondary="clock skew" id="ix_skewclock"></a>
        Part of the problem is that incorrect clocks easily go unnoticed. If a machine’s CPU is defective or
        its network is misconfigured, it most likely won’t work at all, so it will quickly be noticed and
        fixed. On the other hand, if its quartz clock is defective or its NTP client is misconfigured, most
        things will seem to work fine, even though its clock gradually drifts further and further away from
        reality. If some piece of software is relying on an accurately synchronized clock, the result is
        more likely to be silent and subtle data loss than a dramatic crash
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kingsbury2013ti_ch8-marker" href="ch08.html#Kingsbury2013ti_ch8">53</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Daily2013te_ch8-marker" href="ch08.html#Daily2013te_ch8">54</a>].</p>
        
        <p>Thus, if you use software that requires synchronized clocks, it is essential that you also carefully
        monitor the clock offsets between all the machines. Any node whose clock drifts too far from the
        others should be declared dead and removed from the cluster. Such monitoring ensures that you notice
        the broken clocks before they can cause too much damage.</p>
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Timestamps for ordering events"><div class="sect3" id="sec_distributed_lww">
        <h3>Timestamps for ordering events</h3>
        
        <p><a data-type="indexterm" data-primary="timestamps" data-secondary="ordering events" id="idm45085104536016"></a>
        <a data-type="indexterm" data-primary="sequence number ordering" data-secondary="use of timestamps" id="idm45085104534912"></a>
        Let’s consider one particular situation in which it is tempting, but dangerous, to rely on clocks:
        ordering of events across multiple nodes. For example, if two clients write to a distributed
        database, who got there first? Which write is the more recent one?</p>
        
        <p><a data-type="xref" href="#fig_distributed_timestamps">Figure&nbsp;8-3</a> illustrates a dangerous use of time-of-day clocks in a database with
        multi-leader replication (the example is similar to <a data-type="xref" href="ch05.html#fig_replication_causality">Figure&nbsp;5-9</a>). Client A writes
        <em>x</em>&nbsp;=&nbsp;1 on node 1; the write is replicated to node 3; client B increments <em>x</em> on node
        3 (we now have <em>x</em>&nbsp;=&nbsp;2); and finally, both writes are replicated to node 2.</p>
        
        <figure><div id="fig_distributed_timestamps" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_0803.png" alt="ddia 0803" width="2880" height="1325">
        <h6><span class="label">Figure 8-3. </span>The write by client B is causally later than the write by client A, but B’s write has an earlier timestamp.</h6>
        </div></figure>
        
        <p><a data-type="indexterm" data-primary="causality" data-secondary="mismatch with clocks" id="idm45085104529872"></a>
        In <a data-type="xref" href="#fig_distributed_timestamps">Figure&nbsp;8-3</a>, when a write is replicated to other nodes, it is tagged with a
        timestamp according to the time-of-day clock on the node where the write originated. The clock
        synchronization is very good in this example: the skew between node 1 and node 3 is less than
        3&nbsp;ms, which is probably better than you can expect in practice.</p>
        
        <p>Nevertheless, the timestamps in <a data-type="xref" href="#fig_distributed_timestamps">Figure&nbsp;8-3</a> fail to order the events correctly:
        the write <em>x</em>&nbsp;=&nbsp;1 has a timestamp of 42.004 seconds, but the write <em>x</em>&nbsp;=&nbsp;2
        has a timestamp of 42.003 seconds, even though <em>x</em>&nbsp;=&nbsp;2 occurred unambiguously later.
        When node 2 receives these two events, it will incorrectly conclude that <em>x</em>&nbsp;=&nbsp;1 is the
        more recent value and drop the write <em>x</em>&nbsp;=&nbsp;2. In effect, client B’s increment operation
        will be lost.</p>
        
        <p><a data-type="indexterm" data-primary="conflicts" data-secondary="conflict resolution" data-tertiary="last write wins (LWW)" id="idm45085104521360"></a>
        <a data-type="indexterm" data-primary="last write wins (LWW)" data-secondary="problems with" id="idm45085104519680"></a>
        <a data-type="indexterm" data-primary="Cassandra (database)" data-secondary="last-write-wins conflict resolution" id="idm45085104518544"></a>
        <a data-type="indexterm" data-primary="incidents" data-secondary="data loss due to last-write-wins" id="idm45085104517472"></a>
        This conflict resolution strategy is called <em>last write wins</em> (LWW), and it is widely used in both
        multi-leader replication and leaderless databases such as Cassandra
        [<a data-type="noteref" href="ch08.html#Kingsbury2013ti_ch8">53</a>] and Riak
        [<a data-type="noteref" href="ch08.html#Daily2013te_ch8">54</a>] (see
        <a data-type="xref" href="ch05.html#sec_replication_lww">“Last write wins (discarding concurrent writes)”</a>). Some implementations generate timestamps on the client rather than
        the server, but this doesn’t change the fundamental problems with LWW:</p>
        
        <ul>
        <li>
        <p>Database writes can mysteriously disappear: a node with a lagging clock is unable to overwrite
        values previously written by a node with a fast clock until the clock skew between the nodes has
        elapsed [<a data-type="noteref" href="ch08.html#Daily2013te_ch8">54</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kingsbury2013vs-marker" href="ch08.html#Kingsbury2013vs">55</a>]. This scenario can cause arbitrary
        amounts of data to be silently dropped without any error being reported to the application.</p>
        </li>
        <li>
        <p><a data-type="indexterm" data-primary="causality" data-secondary="violations of" id="idm45085104507856"></a>
        LWW cannot distinguish between writes that occurred sequentially in quick succession (in
        <a data-type="xref" href="#fig_distributed_timestamps">Figure&nbsp;8-3</a>, client B’s increment definitely occurs <em>after</em> client A’s write)
        and writes that were truly concurrent (neither writer was aware of the other). Additional
        causality tracking mechanisms, such as version vectors, are needed in order to prevent violations
        of causality (see <a data-type="xref" href="ch05.html#sec_replication_concurrent">“Detecting Concurrent Writes”</a>).</p>
        </li>
        <li>
        <p>It is possible for two nodes to independently generate writes with the same timestamp, especially
        when the clock only has millisecond resolution. An additional tiebreaker value (which can simply
        be a large random number) is required to resolve such conflicts, but this approach can also lead to
        violations of causality [<a data-type="noteref" href="ch08.html#Kingsbury2013ti_ch8">53</a>].</p>
        </li>
        </ul>
        
        <p>Thus, even though it is tempting to resolve conflicts by keeping the most “recent” value and
        discarding others, it’s important to be aware that the definition of “recent” depends on a local
        time-of-day clock, which may well be incorrect. Even with tightly NTP-synchronized clocks, you could
        send a packet at timestamp 100&nbsp;ms (according to the sender’s clock) and have it arrive at
        timestamp 99&nbsp;ms (according to the recipient’s clock)—so it appears as though the packet
        arrived before it was sent, which is impossible.</p>
        
        <p><a data-type="indexterm" data-primary="NTP (Network Time Protocol)" data-secondary="accuracy" id="idm45085104500816"></a>
        Could NTP synchronization be made accurate enough that such incorrect orderings cannot occur?
        Probably not, because NTP’s synchronization accuracy is itself limited by the network round-trip
        time, in addition to other sources of error such as quartz drift. For correct ordering, you would
        need the clock source to be significantly more accurate than the thing you are measuring (namely
        network delay).</p>
        
        <p><a data-type="indexterm" data-primary="clocks" data-secondary="logical" data-see="logical clocks" id="idm45085104499024"></a>
        <a data-type="indexterm" data-primary="logical clocks" id="idm45085104497424"></a>
        <a data-type="indexterm" data-primary="physical clocks" data-see="clocks" id="idm45085104496592"></a>
        So-called <em>logical clocks</em>
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Lamport1978jq_ch8-marker" href="ch08.html#Lamport1978jq_ch8">56</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kulkarni2014ws-marker" href="ch08.html#Kulkarni2014ws">57</a>],
        which are based on incrementing counters rather than an oscillating quartz crystal, are a safer
        alternative for ordering events (see <a data-type="xref" href="ch05.html#sec_replication_concurrent">“Detecting Concurrent Writes”</a>). Logical clocks do not measure
        the time of day or the number of seconds elapsed, only the relative ordering of events (whether one
        event happened before or after another). In contrast, time-of-day and monotonic clocks, which
        measure actual elapsed time, are also known as <em>physical clocks</em>. We’ll look at ordering a bit more
        in <a data-type="xref" href="ch09.html#sec_consistency_ordering">“Ordering Guarantees”</a>.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Clock readings have a confidence interval"><div class="sect3" id="idm45085104537216">
        <h3>Clock readings have a confidence interval</h3>
        
        <p><a data-type="indexterm" data-primary="clocks" data-secondary="confidence interval" id="ix_clockconfid"></a>
        <a data-type="indexterm" data-primary="congestion (networks)" data-secondary="limiting accuracy of clocks" id="idm45085104485152"></a>
        You may be able to read a machine’s time-of-day clock with microsecond or even nanosecond
        resolution. But even if you can get such a fine-grained measurement, that doesn’t mean the value is
        actually accurate to such precision. In fact, it most likely is not—as mentioned previously, the
        drift in an imprecise quartz clock can easily be several milliseconds, even if you synchronize with
        an NTP server on the local network every minute. With an NTP server on the public internet, the best
        possible accuracy is probably to the tens of milliseconds, and the error may easily spike to over
        100 ms when there is network congestion [<a data-type="noteref" href="ch08.html#Kulkarni2014ws">57</a>].</p>
        
        <p>Thus, it doesn’t make sense to think of a clock reading as a point in time—it is more like a
        range of times, within a confidence interval: for example, a system may be 95% confident that the
        time now is between 10.3 and 10.5 seconds past the minute, but it doesn’t know any more precisely
        than that [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Sheehy2015jm-marker" href="ch08.html#Sheehy2015jm">58</a>].
        If we only know the time +/–&nbsp;100&nbsp;ms, the microsecond digits in the timestamp are
        essentially meaningless.</p>
        
        <p><a data-type="indexterm" data-primary="GPS (Global Positioning System)" data-secondary="use for clock synchronization" id="idm45085104478656"></a>
        <a data-type="indexterm" data-primary="clocks" data-secondary="synchronization using GPS" id="idm45085104477520"></a>
        <a data-type="indexterm" data-primary="clocks" data-secondary="atomic (caesium) clocks" id="idm45085104476400"></a>
        <a data-type="indexterm" data-primary="atomic clocks (caesium clocks)" data-seealso="clocks" id="idm45085104475296"></a>
        The uncertainty bound can be calculated based on your time source. If you have a GPS receiver or
        atomic (caesium) clock directly attached to your computer, the expected error range is reported by
        the manufacturer. If you’re getting the time from a server, the uncertainty is based on the expected
        quartz drift since your last sync with the server, plus the NTP server’s uncertainty, plus the
        network round-trip time to the server (to a first approximation, and assuming you trust the server).</p>
        
        <p>Unfortunately, most systems don’t expose this uncertainty: for example, when you call
        <code>clock_gettime()</code>, the return value doesn’t tell you the expected error of the timestamp, so you
        don’t know if its confidence interval is five milliseconds or five years.</p>
        
        <p><a data-type="indexterm" data-primary="Spanner (database)" data-secondary="TrueTime API" id="idm45085104472288"></a>
        <a data-type="indexterm" data-primary="Google" data-secondary="TrueTime (clock API)" id="idm45085104470960"></a>
        An interesting exception is Google’s <em>TrueTime</em> API in Spanner
        [<a data-type="noteref" href="ch08.html#Corbett2012uz_ch8">41</a>], which explicitly reports the
        confidence interval on the local clock. When you ask it for the current time, you get back two
        values: <code>[<em>earliest</em>, <em>latest</em>]</code>, which are the <em>earliest possible</em> and the <em>latest possible</em>
        timestamp. Based on its uncertainty calculations, the clock knows that the actual current time is
        somewhere within that interval. The width of the interval depends, among other things, on how long
        it has been since the local quartz clock was last synchronized with a more accurate clock source.
        <a data-type="indexterm" data-primary="clocks" data-secondary="skew" data-startref="ix_clockskew" id="idm45085104465904"></a>
        <a data-type="indexterm" data-primary="skew" data-secondary="clock skew" data-startref="ix_skewclock" id="idm45085104464528"></a></p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Synchronized clocks for global snapshots"><div class="sect3" id="sec_distributed_spanner">
        <h3>Synchronized clocks for global snapshots</h3>
        
        <p><a data-type="indexterm" data-primary="snapshots (databases)" data-secondary="synchronized clocks for global snapshots" id="idm45085104461696"></a>
        <a data-type="indexterm" data-primary="clocks" data-secondary="for global snapshots" id="idm45085104460496"></a>
        In <a data-type="xref" href="ch07.html#sec_transactions_snapshot_isolation">“Snapshot Isolation and Repeatable Read”</a> we discussed <em>snapshot isolation</em>, which is a very useful
        feature in databases that need to support both small, fast read-write transactions and large,
        long-running read-only transactions (e.g., for backups or analytics). It allows read-only
        transactions to see the database in a consistent state at a particular point in time, without
        locking and interfering with read-write transactions.</p>
        
        <p><a data-type="indexterm" data-primary="consistency" data-secondary="consistent snapshots" id="idm45085104457520"></a>
        The most common implementation of snapshot isolation requires a monotonically increasing transaction
        ID. If a write happened later than the snapshot (i.e., the write has a greater transaction ID than
        the snapshot), that write is invisible to the snapshot transaction. On a single-node database, a
        simple counter is sufficient for generating transaction IDs.</p>
        
        <p><a data-type="indexterm" data-primary="causality" data-secondary="with synchronized clocks" id="idm45085104455792"></a>
        <a data-type="indexterm" data-primary="coordination" data-secondary="cross-partition ordering" id="idm45085104454672"></a>
        <a data-type="indexterm" data-primary="sequence number ordering" data-secondary="generators" id="idm45085104453552"></a>
        <a data-type="indexterm" data-primary="Twitter" data-secondary="Snowflake (sequence number generator)" id="idm45085104452432"></a>
        However, when a database is distributed across many machines, potentially in multiple datacenters, a
        global, monotonically increasing transaction ID (across all partitions) is difficult to generate,
        because it requires coordination. The transaction ID must reflect causality: if transaction B reads
        a value that was written by transaction A, then B must have a higher transaction ID than
        A—otherwise, the snapshot would not be consistent. With lots of small, rapid transactions, creating
        transaction IDs in a distributed system becomes an untenable
        bottleneck.<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm45085104450736-marker" href="ch08.html#idm45085104450736">vi</a></sup></p>
        
        <p><a data-type="indexterm" data-primary="timestamps" data-secondary="for transaction ordering" id="idm45085104447952"></a>
        <a data-type="indexterm" data-primary="sequence number ordering" data-secondary="use of timestamps" id="idm45085104446832"></a>
        Can we use the timestamps from synchronized time-of-day clocks as transaction IDs? If we could get
        the synchronization good enough, they would have the right properties: later transactions have a
        higher timestamp. The problem, of course, is the uncertainty about clock accuracy.</p>
        
        <p><a data-type="indexterm" data-primary="Spanner (database)" data-secondary="snapshot isolation using clocks" id="idm45085104445168"></a>
        Spanner implements snapshot isolation across datacenters in this way
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Demirbas2013uz-marker" href="ch08.html#Demirbas2013uz">59</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Malkhi2013bl-marker" href="ch08.html#Malkhi2013bl">60</a>].
        It uses the clock’s confidence interval as reported by the TrueTime API, and is based on the
        following observation: if you have two confidence intervals, each consisting of an earliest and
        latest possible timestamp (<em>A</em> = [<em>A<sub>earliest</sub></em>, <em>A<sub>latest</sub></em>] and
        <em>B</em> = [<em>B<sub>earliest</sub></em>, <em>B<sub>latest</sub></em>]), and those two intervals do not overlap (i.e.,
        <em>A<sub>earliest</sub></em> &lt; <em>A<sub>latest</sub></em> &lt; <em>B<sub>earliest</sub></em> &lt; <em>B<sub>latest</sub></em>), then B definitely happened after A—there
        can be no doubt. Only if the intervals overlap are we unsure in which order A and B happened.</p>
        
        <p><a data-type="indexterm" data-primary="GPS (Global Positioning System)" data-secondary="use for clock synchronization" id="idm45085104431520"></a>
        <a data-type="indexterm" data-primary="clocks" data-secondary="synchronization using GPS" id="idm45085104430400"></a>
        <a data-type="indexterm" data-primary="clocks" data-secondary="atomic (caesium) clocks" id="idm45085104429280"></a>
        <a data-type="indexterm" data-primary="atomic clocks (caesium clocks)" id="idm45085104428176"></a>
        In order to ensure that transaction timestamps reflect causality, Spanner deliberately waits for the
        length of the confidence interval before committing a read-write transaction. By doing so, it
        ensures that any transaction that may read the data is at a sufficiently later time, so their
        confidence intervals do not overlap. In order to keep the wait time as short as possible, Spanner
        needs to keep the clock uncertainty as small as possible; for this purpose, Google deploys a GPS
        receiver or atomic clock in each datacenter, allowing clocks to be synchronized to within about
        7&nbsp;ms [<a data-type="noteref" href="ch08.html#Corbett2012uz_ch8">41</a>].</p>
        
        <p>Using clock synchronization for distributed transaction semantics is an area of active research
        [<a data-type="noteref" href="ch08.html#Kulkarni2014ws">57</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Bravo2015uy-marker" href="ch08.html#Bravo2015uy">61</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kimball2016wi-marker" href="ch08.html#Kimball2016wi">62</a>].
        These ideas are interesting, but they have not yet been implemented in mainstream databases outside
        of Google.
        <a data-type="indexterm" data-primary="clocks" data-secondary="confidence interval" data-startref="ix_clockconfid" id="idm45085104419792"></a>
        <a data-type="indexterm" data-primary="correctness" data-secondary="of time" data-startref="ix_correcttime" id="idm45085104418416"></a>
        <a data-type="indexterm" data-primary="distributed systems" data-secondary="synchronized clocks, relying on" data-startref="ix_distsysclockrely" id="idm45085104417040"></a>
        <a data-type="indexterm" data-primary="time" data-secondary="in distributed systems" data-tertiary="relying on synchronized clocks" data-startref="ix_timeclocksync" id="idm45085104415600"></a></p>
        </div></section>
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Process Pauses"><div class="sect2" id="sec_distributed_clocks_pauses">
        <h2>Process Pauses</h2>
        
        <p><a data-type="indexterm" data-primary="time" data-secondary="process pauses" id="ix_timeclockproc"></a>
        <a data-type="indexterm" data-primary="process pauses" id="ix_procpause"></a>
        Let’s consider another example of dangerous clock use in a distributed system. Say you have a
        database with a single leader per partition. Only the leader is allowed to accept writes. How does a
        node know that it is still leader (that it hasn’t been declared dead by the others), and that it may
        safely accept writes?</p>
        
        <p><a data-type="indexterm" data-primary="leases" id="idm45085104409248"></a>
        One option is for the leader to obtain a <em>lease</em> from the other nodes, which is similar to a lock
        with a timeout [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Gray1989cu-marker" href="ch08.html#Gray1989cu">63</a>].
        Only one node can hold the lease at any one time—thus, when a node obtains a lease, it knows that
        it is the leader for some amount of time, until the lease expires. In order to remain leader, the
        node must periodically renew the lease before it expires. If the node fails, it stops renewing the
        lease, so another node can take over when it expires.</p>
        
        <p>You can imagine the request-handling loop looking something like this:</p>
        
        <pre data-type="programlisting" data-code-language="java"><code class="k">while</code> <code class="o">(</code><code class="kc">true</code><code class="o">)</code> <code class="o">{</code>
            <code class="n">request</code> <code class="o">=</code> <code class="n">getIncomingRequest</code><code class="o">();</code>
        
            <code class="c1">// Ensure that the lease always has at least 10 seconds remaining</code>
            <code class="k">if</code> <code class="o">(</code><code class="n">lease</code><code class="o">.</code><code class="na">expiryTimeMillis</code> <code class="o">-</code> <code class="n">System</code><code class="o">.</code><code class="na">currentTimeMillis</code><code class="o">()</code> <code class="o">&lt;</code> <code class="mi">10000</code><code class="o">)</code> <code class="o">{</code>
                <code class="n">lease</code> <code class="o">=</code> <code class="n">lease</code><code class="o">.</code><code class="na">renew</code><code class="o">();</code>
            <code class="o">}</code>
        
            <code class="k">if</code> <code class="o">(</code><code class="n">lease</code><code class="o">.</code><code class="na">isValid</code><code class="o">())</code> <code class="o">{</code>
                <code class="n">process</code><code class="o">(</code><code class="n">request</code><code class="o">);</code>
            <code class="o">}</code>
        <code class="o">}</code></pre>
        
        <p>What’s wrong with this code? Firstly, it’s relying on synchronized clocks: the expiry time on the
        lease is set by a different machine (where the expiry may be calculated as the current time plus 30
        seconds, for example), and it’s being compared to the local system clock. If the clocks are out of
        sync by more than a few seconds, this code will start doing strange things.</p>
        
        <p>Secondly, even if we change the protocol to only use the local monotonic clock, there is another
        problem: the code assumes that very little time passes between the point that it checks the time
        (<code>System.currentTimeMillis()</code>) and the time when the request is processed (<code>process(request)</code>).
        Normally this code runs very quickly, so the 10 second buffer is more than enough to ensure that the
        lease doesn’t expire in the middle of processing a request.</p>
        
        <p><a data-type="indexterm" data-primary="threads (concurrency)" data-secondary="execution pauses" id="ix_threadexecpause"></a>
        However, what if there is an unexpected pause in the execution of the program? For example, imagine
        the thread stops for 15 seconds around the line <code>lease.isValid()</code> before finally continuing. In
        that case, it’s likely that the lease will have expired by the time the request is processed, and
        another node has already taken over as leader. However, there is nothing to tell this thread that it
        was paused for so long, so this code won’t notice that the lease has expired until the next
        iteration of the loop—by which time it may have already done something unsafe by processing the
        request.</p>
        
        <p><a data-type="indexterm" data-primary="unbounded delays" data-secondary="process pauses" id="idm45085104316336"></a>
        <a data-type="indexterm" data-primary="delays" data-secondary="unbounded process pauses" id="idm45085104315232"></a>
        Is it crazy to assume that a thread might be paused for so long? Unfortunately not. There are
        various reasons why this could happen:</p>
        
        <ul>
        <li>
        <p><a data-type="indexterm" data-primary="garbage collection" data-secondary="process pauses for" data-seealso="process pauses" id="ix_GCprocpause"></a>
        <a data-type="indexterm" data-primary="Java Virtual Machine (JVM)" data-secondary="garbage collection pauses" id="idm45085104310960"></a>
        <a data-type="indexterm" data-primary="stop-the-world" data-see="garbage collection" id="idm45085104309872"></a>
        Many programming language runtimes (such as the Java Virtual Machine) have a <em>garbage collector</em>
        (GC) that occasionally needs to stop all running threads. These <em>“stop-the-world” GC pauses</em> have
        sometimes been known to last for several minutes [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Lipcon2011tn-marker" href="ch08.html#Lipcon2011tn">64</a>]!
        Even so-called “concurrent” garbage collectors like the HotSpot JVM’s CMS cannot fully run in
        parallel with the application code—even they need to stop the world from time to time
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Thompson2013vj-marker" href="ch08.html#Thompson2013vj">65</a>].
        Although the pauses can often be reduced by changing allocation patterns or tuning GC settings
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Ragozin2011wr-marker" href="ch08.html#Ragozin2011wr">66</a>],
        we must assume the worst if we want to offer robust guarantees.</p>
        </li>
        <li>
        <p>In virtualized environments, a virtual machine can be <em>suspended</em> (pausing the execution of all
        processes and saving the contents of memory to disk) and <em>resumed</em> (restoring the contents of
        memory and continuing execution). This pause can occur at any time in a process’s execution and can
        last for an arbitrary length of time. This feature is sometimes used for <em>live migration</em> of
        virtual machines from one host to another without a reboot, in which case the length of the pause
        depends on the rate at which processes are writing to memory
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Clark2005ud-marker" href="ch08.html#Clark2005ud">67</a>].</p>
        </li>
        <li>
        <p>On end-user devices such as laptops, execution may also be suspended and resumed arbitrarily, e.g.,
        when the user closes the lid of their laptop.</p>
        </li>
        <li>
        <p><a data-type="indexterm" data-primary="virtual machines" data-secondary="context switches" id="idm45085104294592"></a>
        <a data-type="indexterm" data-primary="context switches" id="idm45085104293488"></a>
        When the operating system context-switches to another thread, or when the hypervisor switches to a
        different virtual machine (when running in a virtual machine), the currently running thread can be
        paused at any arbitrary point in the code. In the case of a virtual machine, the CPU time spent in
        other virtual machines is known as <em>steal time</em>. If the machine is under heavy load—i.e., if
        there is a long queue of threads waiting to run—it may take some time before the paused thread
        gets to run again.</p>
        </li>
        <li>
        <p><a data-type="indexterm" data-primary="I/O operations, waiting for" id="idm45085104291008"></a>
        If the application performs synchronous disk access, a thread may be paused waiting for a slow
        disk I/O operation to complete [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Shaver2008ug-marker" href="ch08.html#Shaver2008ug">68</a>]. In many languages, disk access can happen
        surprisingly, even if the code doesn’t explicitly mention file access—for example, the Java
        classloader lazily loads class files when they are first used, which could happen at any time in
        the program execution. I/O pauses and GC pauses may even conspire to combine their delays
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Zhuang2016ui-marker" href="ch08.html#Zhuang2016ui">69</a>]. If the disk is actually a network filesystem or network block device (such as
        Amazon’s EBS), the I/O latency is further subject to the variability of network delays
        [<a data-type="noteref" href="ch08.html#Newman2012vf">29</a>].</p>
        </li>
        <li>
        <p><a data-type="indexterm" data-primary="virtual memory" data-secondary="process pauses due to page faults" id="idm45085104283408"></a>
        <a data-type="indexterm" data-primary="swapping to disk" data-see="virtual memory" id="idm45085104282064"></a>
        <a data-type="indexterm" data-primary="paging" data-see="virtual memory" id="idm45085104280960"></a>
        <a data-type="indexterm" data-primary="thrashing (out of memory)" id="idm45085104279856"></a>
        If the operating system is configured to allow <em>swapping to disk</em> (<em>paging</em>), a simple memory
        access may result in a page fault that requires a page from disk to be loaded into memory. The
        thread is paused while this slow I/O operation takes place. If memory pressure is high, this may
        in turn require a different page to be swapped out to disk. In extreme circumstances, the
        operating system may spend most of its time swapping pages in and out of memory and getting little
        actual work done (this is known as <em>thrashing</em>). To avoid this problem, paging is often disabled
        on server machines (if you would rather kill a process to free up memory than risk thrashing).</p>
        </li>
        <li>
        <p>A Unix process can be paused by sending it the <code>SIGSTOP</code> signal, for example by pressing Ctrl-Z in
        a shell. This signal immediately stops the process from getting any more CPU cycles until it is
        resumed with <code>SIGCONT</code>, at which point it continues running where it left off. Even if your
        environment does not normally use <code>SIGSTOP</code>, it might be sent accidentally by an operations
        engineer.</p>
        </li>
        </ul>
        
        <p class="pagebreak-before"><a data-type="indexterm" data-primary="preemption" data-secondary="of threads" id="idm45085104273968"></a>
        <a data-type="indexterm" data-primary="threads (concurrency)" data-secondary="preemption" id="idm45085104272864"></a>
        All of these occurrences can <em>preempt</em> the running thread at any point and resume it at some later time,
        without the thread even noticing. The problem is similar to making multi-threaded code on a single
        machine thread-safe: you can’t assume anything about timing, because arbitrary context switches and
        parallelism may occur.</p>
        
        <p>When writing multi-threaded code on a single machine, we have fairly good tools for making it
        thread-safe: mutexes, semaphores, atomic counters, lock-free data structures, blocking queues, and
        so on. Unfortunately, these tools don’t directly translate to distributed systems, because a
        distributed system has no shared memory—only messages sent over an unreliable network.</p>
        
        <p>A node in a distributed system must assume that its execution can be paused for a significant length
        of time at any point, even in the middle of a function. During the pause, the rest of the world
        keeps moving and may even declare the paused node dead because it’s not responding. Eventually,
        the paused node may continue running, without even noticing that it was asleep until it checks its
        clock sometime later.
        <a data-type="indexterm" data-primary="threads (concurrency)" data-secondary="execution pauses" data-startref="ix_threadexecpause" id="idm45085104269408"></a></p>
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Response time guarantees"><div class="sect3" id="sec_distributed_clocks_realtime">
        <h3>Response time guarantees</h3>
        
        <p><a data-type="indexterm" data-primary="response time" data-secondary="guarantees on" id="idm45085104266480"></a>
        <a data-type="indexterm" data-primary="bounded delays" data-secondary="process pauses" id="idm45085104265376"></a>
        <a data-type="indexterm" data-primary="delays" data-secondary="bounded process pauses" id="idm45085104264272"></a>
        In many programming languages and operating systems, threads and processes may pause for an
        unbounded amount of time, as discussed. Those reasons for pausing <em>can</em> be eliminated if you try
        hard enough.</p>
        
        <p><a data-type="indexterm" data-primary="real-time" data-secondary="response time guarantees" id="idm45085104262272"></a>
        Some software runs in environments where a failure to respond within a specified time can cause
        serious damage: computers that control aircraft, rockets, robots, cars, and other physical objects
        must respond quickly and predictably to their sensor inputs. In these systems, there is a specified
        <em>deadline</em> by which the software must respond; if it doesn’t meet the deadline, that may cause a
        failure of the entire system. These are so-called <em>hard real-time</em> systems.</p>
        <div data-type="note" epub:type="note"><h1>Is real-time really real?</h1>
        <p>In embedded systems, <em>real-time</em> means that a system is carefully designed and tested to meet
        specified timing guarantees in all circumstances. This meaning is in contrast to the more vague use of the
        term <em>real-time</em> on the web, where it describes servers pushing data to clients and stream
        processing without hard response time constraints (see <a data-type="xref" href="ch11.html#ch_stream">Chapter&nbsp;11</a>).</p>
        </div>
        
        <p>For example, if your car’s onboard sensors detect that you are currently experiencing a crash, you
        wouldn’t want the release of the airbag to be delayed due to an inopportune GC pause in the airbag
        release system.</p>
        
        <p>Providing real-time guarantees in a system requires support from all levels of the software stack: a
        <em>real-time operating system</em> (RTOS) that allows processes to be scheduled with a guaranteed
        allocation of CPU time in specified intervals is needed; library functions must document their
        worst-case execution times; dynamic memory allocation may be restricted or disallowed entirely
        (real-time garbage collectors exist, but the application must still ensure that it doesn’t give the
        GC too much work to do); and an enormous amount of testing and measurement must be done to ensure
        that guarantees are being met.</p>
        
        <p>All of this requires a large amount of additional work and severely restricts the range of
        programming languages, libraries, and tools that can be used (since most languages and tools do not
        provide real-time guarantees). For these reasons, developing real-time systems is very expensive,
        and they are most commonly used in safety-critical embedded devices. Moreover, “real-time” is not the
        same as “high-performance”—in fact, real-time systems may have lower throughput, since they have to
        prioritize timely responses above all else (see also <a data-type="xref" href="#sidebar_distributed_latency_utilization">“Latency and Resource Utilization”</a>).</p>
        
        <p>For most server-side data processing systems, real-time guarantees are simply not economical or
        appropriate. Consequently, these systems must suffer the pauses and clock instability that come from
        operating in a non-real-time environment.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Limiting the impact of garbage collection"><div class="sect3" id="idm45085104251616">
        <h3>Limiting the impact of garbage collection</h3>
        
        <p>The negative effects of process pauses can be mitigated without resorting to expensive real-time
        scheduling guarantees. Language runtimes have some flexibility around when they schedule garbage
        collections, because they can track the rate of object allocation and the remaining free memory over
        time.</p>
        
        <p><a data-type="indexterm" data-primary="high-frequency trading" id="idm45085104249536"></a>
        An emerging idea is to treat GC pauses like brief planned outages of a node, and to let other nodes
        handle requests from clients while one node is collecting its garbage. If the runtime can warn the
        application that a node soon requires a GC pause, the application can stop sending new requests to
        that node, wait for it to finish processing outstanding requests, and then perform the GC while no
        requests are in progress. This trick hides GC pauses from clients and reduces the high percentiles of
        response time [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Terei2015va-marker" href="ch08.html#Terei2015va">70</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Maas2015vf-marker" href="ch08.html#Maas2015vf">71</a>].
        Some latency-sensitive financial trading systems
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Cinnober2013up-marker" href="ch08.html#Cinnober2013up">72</a>]
        use this approach.</p>
        
        <p>A variant of this idea is to use the garbage collector only for short-lived objects (which are fast
        to collect) and to restart processes periodically, before they accumulate enough long-lived objects
        to require a full GC of long-lived objects [<a data-type="noteref" href="ch08.html#Thompson2013vj">65</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Fowler2011wp_ch8-marker" href="ch08.html#Fowler2011wp_ch8">73</a>]. One node can be restarted at a time, and traffic can
        be shifted away from the node before the planned restart, like in a rolling upgrade (see
        <a data-type="xref" href="ch04.html#ch_encoding">Chapter&nbsp;4</a>).</p>
        
        <p>These measures cannot fully prevent garbage collection pauses, but they can usefully reduce their
        impact on the application.
        <a data-type="indexterm" data-primary="garbage collection" data-secondary="process pauses for" data-startref="ix_GCprocpause" id="idm45085104236320"></a>
        <a data-type="indexterm" data-primary="time" data-secondary="process pauses" data-startref="ix_timeclockproc" id="idm45085104234944"></a>
        <a data-type="indexterm" data-primary="process pauses" data-startref="ix_procpause" id="idm45085104233568"></a>
        <a data-type="indexterm" data-primary="data systems" data-secondary="unreliable clocks" data-startref="ix_distsysunrelclock" id="idm45085104232464"></a>
        <a data-type="indexterm" data-primary="time" data-secondary="in distributed systems" data-startref="ix_timeclock" id="idm45085104231088"></a>
        <a data-type="indexterm" data-primary="clocks" data-startref="ix_clocks" id="idm45085104229712"></a></p>
        </div></section>
        
        
        
        </div></section>
        
        
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Knowledge, Truth, and Lies"><div class="sect1" id="sec_distributed_truth">
        <h1>Knowledge, Truth, and Lies</h1>
        
        <p>So far in this chapter we have explored the ways in which distributed systems are different from
        programs running on a single computer: there is no shared memory, only message passing via an
        unreliable network with variable delays, and the systems may suffer from partial failures, unreliable clocks,
        and processing pauses.</p>
        
        <p>The consequences of these issues are profoundly disorienting if you’re not used to distributed
        systems. A node in the network cannot <em>know</em> anything for sure—it can only make guesses based on
        the messages it receives (or doesn’t receive) via the network. A node can only find out what state
        another node is in (what data it has stored, whether it is correctly functioning, etc.) by
        exchanging messages with it. If a remote node doesn’t respond, there is no way of knowing what state
        it is in, because problems in the network cannot reliably be distinguished from problems at a node.</p>
        
        <p>Discussions of these systems border on the philosophical: What do we know to be true or false in our
        system? How sure can we be of that knowledge, if the mechanisms for perception and measurement are
        unreliable? Should software systems obey the laws that we expect of the physical world, such as
        cause and effect?</p>
        
        <p><a data-type="indexterm" data-primary="system models" id="idm45085104224480"></a>
        Fortunately, we don’t need to go as far as figuring out the meaning of life. In a distributed
        system, we can state the assumptions we are making about the behavior (the <em>system model</em>) and
        design the actual system in such a way that it meets those assumptions. Algorithms can be proved to
        function correctly within a certain system model. This means that reliable behavior is achievable,
        even if the underlying system model provides very few guarantees.</p>
        
        <p>However, although it is possible to make software well behaved in an unreliable system model, it
        is not straightforward to do so. In the rest of this chapter we will further explore the notions of
        knowledge and truth in distributed systems, which will help us think about the kinds of assumptions
        we can make and the guarantees we may want to provide. In <a data-type="xref" href="ch09.html#ch_consistency">Chapter&nbsp;9</a> we will proceed to
        look at some examples of distributed algorithms that provide particular guarantees under particular
        assumptions.</p>
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="The Truth Is Defined by the Majority"><div class="sect2" id="sec_distributed_majority">
        <h2>The Truth Is Defined by the Majority</h2>
        
        <p><a data-type="indexterm" data-primary="faults" data-secondary="network faults" data-tertiary="asymmetric faults" id="idm45085104219136"></a>
        Imagine a network with an asymmetric fault: a node is able to receive all messages sent to it, but
        any outgoing messages from that node are dropped or delayed
        [<a data-type="noteref" href="ch08.html#Donges2012tt">19</a>]. Even though that node is working
        perfectly well, and is receiving requests from other nodes, the other nodes cannot hear its
        responses. After some timeout, the other nodes declare it dead, because they haven’t heard from the
        node. The situation unfolds like a nightmare: the semi-disconnected node is dragged to the
        graveyard, kicking and screaming “I’m not dead!”—but since nobody can hear its screaming, the
        funeral procession continues with stoic determination.</p>
        
        <p>In a slightly less nightmarish scenario, the semi-disconnected node may notice that the messages it
        is sending are not being acknowledged by other nodes, and so realize that there must be a fault
        in the network. Nevertheless, the node is wrongly declared dead by the other nodes, and the
        semi-disconnected node cannot do anything about it.</p>
        
        <p><a data-type="indexterm" data-primary="garbage collection" data-secondary="process pauses for" id="idm45085104215264"></a>
        As a third scenario, imagine a node that experiences a long stop-the-world garbage collection pause.
        All of the node’s threads are preempted by the GC and paused for one minute, and consequently, no
        requests are processed and no responses are sent. The other nodes wait, retry, grow impatient, and
        eventually declare the node dead and load it onto the hearse. Finally, the GC finishes and the
        node’s threads continue as if nothing had happened. The other nodes are surprised as the supposedly
        dead node suddenly raises its head out of the coffin, in full health, and starts cheerfully chatting
        with bystanders. At first, the GCing node doesn’t even realize that an entire minute has passed and
        that it was declared dead—from its perspective, hardly any time has passed since it was last talking
        to the other nodes.</p>
        
        <p><a data-type="indexterm" data-primary="distributed systems" data-secondary="quorums, relying on" id="idm45085104213072"></a>
        <a data-type="indexterm" data-primary="quorums" data-secondary="making decisions in distributed systems" id="idm45085104211968"></a>
        The moral of these stories is that a node cannot necessarily trust its own judgment of a situation.
        A distributed system cannot exclusively rely on a single node, because a node may fail at any time,
        potentially leaving the system stuck and unable to recover. Instead, many distributed algorithms
        rely on a <em>quorum</em>, that is, voting among the nodes (see <a data-type="xref" href="ch05.html#sec_replication_quorum_condition">“Quorums for reading and writing”</a>):
        decisions require some minimum number of votes from several nodes in order to reduce the dependence
        on any one particular node.</p>
        
        <p>That includes decisions about declaring nodes dead. If a quorum of nodes declares another node
        dead, then it must be considered dead, even if that node still very much feels alive. The individual
        node must abide by the quorum decision and step down.</p>
        
        <p>Most commonly, the quorum is an absolute majority of more than half the nodes (although other kinds
        of quorums are possible). A majority quorum allows the system to continue working if individual nodes
        have failed (with three nodes, one failure can be tolerated; with five nodes, two failures can be
        tolerated). However, it is still safe, because there can only be only one majority in the
        system—there cannot be two majorities with conflicting decisions at the same time. We will discuss
        the use of quorums in more detail when we get to <em>consensus algorithms</em> in <a data-type="xref" href="ch09.html#ch_consistency">Chapter&nbsp;9</a>.</p>
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="The leader and the lock"><div class="sect3" id="sec_distributed_lock_fencing">
        <h3>The leader and the lock</h3>
        
        <p><a data-type="indexterm" data-primary="locks" data-secondary="distributed locking" id="ix_locksdistrib"></a>
        <a data-type="indexterm" data-primary="leader-based replication" data-secondary="failover" id="idm45085104202976"></a>
        <a data-type="indexterm" data-primary="failover" data-secondary="leader election" id="idm45085104201856"></a>
        Frequently, a system requires there to be only one of some thing. For example:</p>
        
        <ul>
        <li>
        <p>Only one node is allowed to be the leader for a database partition, to avoid split brain (see
        <a data-type="xref" href="ch05.html#sec_replication_failover">“Handling Node Outages”</a>).</p>
        </li>
        <li>
        <p>Only one transaction or client is allowed to hold the lock for a particular resource or object, to
        prevent concurrently writing to it and corrupting it.</p>
        </li>
        <li>
        <p>Only one user is allowed to register a particular username, because a username must uniquely
        identify a user.</p>
        </li>
        </ul>
        
        <p>Implementing this in a distributed system requires care: even if a node believes that it is “the
        chosen one” (the leader of the partition, the holder of the lock, the request handler of the user
        who successfully grabbed the username), that doesn’t necessarily mean a quorum of nodes agrees!
        A node may have formerly been the leader, but if the other nodes declared it dead in the meantime
        (e.g., due to a network interruption or GC pause), it may have been demoted and another leader may
        have already been elected.</p>
        
        <p><a data-type="indexterm" data-primary="fencing (preventing split brain)" id="ix_fencing"></a>
        <a data-type="indexterm" data-primary="split brain" data-secondary="using fencing tokens to avoid" id="ix_splitbrainfencing"></a>
        If a node continues acting as the chosen one, even though the majority of nodes have declared it
        dead, it could cause problems in a system that is not carefully designed. Such a node could send
        messages to other nodes in its self-appointed capacity, and if other nodes believe it, the system as
        a whole may do something incorrect.</p>
        
        <p><a data-type="indexterm" data-primary="HBase (database)" data-secondary="bug due to lack of fencing" id="idm45085104192016"></a>
        <a data-type="indexterm" data-primary="leases" data-secondary="need for fencing" id="idm45085104190720"></a>
        <a data-type="indexterm" data-primary="corruption of data" data-secondary="due to split brain" id="idm45085104189616"></a>
        For example, <a data-type="xref" href="#fig_distributed_io_fencing">Figure&nbsp;8-4</a> shows a data corruption bug due to an incorrect
        implementation of locking. (The bug is not theoretical: HBase used to have this problem
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Junqueira2013wi_ch8-marker" href="ch08.html#Junqueira2013wi_ch8">74</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Soztutar2013vj-marker" href="ch08.html#Soztutar2013vj">75</a>].) Say you want to ensure that a file in a storage service can only be
        accessed by one client at a time, because if multiple clients tried to write to it, the file would
        become corrupted. You try to implement this by requiring a client to obtain a lease from a lock
        service before accessing the file.</p>
        
        <figure><div id="fig_distributed_io_fencing" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_0804.png" alt="ddia 0804" width="2880" height="1057">
        <h6><span class="label">Figure 8-4. </span>Incorrect implementation of a distributed lock: client 1 believes that it still has a valid lease, even though it has expired, and thus corrupts a file in storage.</h6>
        </div></figure>
        
        <p>The problem is an example of what we discussed in <a data-type="xref" href="#sec_distributed_clocks_pauses">“Process Pauses”</a>: if the client
        holding the lease is paused for too long, its lease expires. Another client can obtain a lease for
        the same file, and start writing to the file. When the paused client comes back, it believes
        (incorrectly) that it still has a valid lease and proceeds to also write to the file. As a result,
        the clients’ writes clash and corrupt the file.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Fencing tokens"><div class="sect3" id="sec_distributed_fencing_tokens">
        <h3>Fencing tokens</h3>
        
        <p><a data-type="indexterm" data-primary="locks" data-secondary="distributed locking" data-tertiary="fencing tokens" id="idm45085104178208"></a>
        When using a lock or lease to protect access to some resource, such as the file storage in
        <a data-type="xref" href="#fig_distributed_io_fencing">Figure&nbsp;8-4</a>, we need to ensure that a node that is under a false belief of being
        “the chosen one” cannot disrupt the rest of the system. A fairly simple technique that achieves this
        goal is called <em>fencing</em>, and is illustrated in <a data-type="xref" href="#fig_distributed_io_fencing_tokens">Figure&nbsp;8-5</a>.</p>
        
        <figure><div id="fig_distributed_io_fencing_tokens" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_0805.png" alt="ddia 0805" width="2880" height="1057">
        <h6><span class="label">Figure 8-5. </span>Making access to storage safe by allowing writes only in the order of increasing fencing tokens.</h6>
        </div></figure>
        
        <p>Let’s assume that every time the lock server grants a lock or lease, it also returns a <em>fencing
        token</em>, which is a number that increases every time a lock is granted (e.g., incremented by the lock
        service). We can then require that every time a client sends a write request to the storage service,
        it must include its current fencing token.</p>
        
        <p>In <a data-type="xref" href="#fig_distributed_io_fencing_tokens">Figure&nbsp;8-5</a>, client 1 acquires the lease with a token of 33, but then
        it goes into a long pause and the lease expires. Client 2 acquires the lease with a token of 34 (the
        number always increases) and then sends its write request to the storage service, including the
        token of 34. Later, client 1 comes back to life and sends its write to the storage service,
        including its token value 33. However, the storage server remembers that it has already processed a
        write with a higher token number (34), and so it rejects the request with token 33.</p>
        
        <p><a data-type="indexterm" data-primary="ZooKeeper (coordination service)" data-secondary="generating fencing tokens" id="idm45085104169024"></a>
        If ZooKeeper is used as lock service, the transaction ID <code>zxid</code> or the node version
        <span class="keep-together"><code>cversion</code></span> can
        be used as fencing token. Since they are guaranteed to be monotonically increasing, they have the
        required properties [<a data-type="noteref" href="ch08.html#Junqueira2013wi_ch8">74</a>].</p>
        
        <p>Note that this mechanism requires the resource itself to take an active role in checking tokens by rejecting any
        writes with an older token than one that has already been processed—it is not sufficient to rely on clients checking
        their lock status themselves. For resources that do not explicitly support fencing tokens, you might
        still be able work around the limitation (for example, in the case of a file storage service you
        could include the fencing token in the filename). However, some kind of check is necessary to avoid
        processing requests outside of the lock’s protection.</p>
        
        <p>Checking a token on the server side may seem like a downside, but it is arguably a good thing: it is
        unwise for a service to assume that its clients will always be well behaved, because the clients are
        often run by people whose priorities are very different from the priorities of the people running
        the service [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="McCaffrey2015ui-marker" href="ch08.html#McCaffrey2015ui">76</a>]. Thus, it is a good idea for any service to protect itself from accidentally
        abusive clients.
        <a data-type="indexterm" data-primary="fencing (preventing split brain)" data-startref="ix_fencing" id="idm45085104161328"></a>
        <a data-type="indexterm" data-primary="split brain" data-secondary="using fencing tokens to avoid" data-startref="ix_splitbrainfencing" id="idm45085104160208"></a>
        <a data-type="indexterm" data-primary="locks" data-secondary="distributed locking" data-startref="ix_locksdistrib" id="idm45085104158816"></a></p>
        </div></section>
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Byzantine Faults"><div class="sect2" id="sec_distributed_byzantine">
        <h2>Byzantine Faults</h2>
        
        <p><a data-type="indexterm" data-primary="distributed systems" data-secondary="Byzantine faults" id="ix_distsysknowByz"></a>
        <a data-type="indexterm" data-primary="Byzantine faults" id="ix_Byzfaults"></a>
        <a data-type="indexterm" data-primary="faults" data-secondary="Byzantine faults" id="ix_faultsByz"></a>
        Fencing tokens can detect and block a node that is <em>inadvertently</em> acting in error (e.g., because it
        hasn’t yet found out that its lease has expired). However, if the node deliberately wanted to
        subvert the system’s guarantees, it could easily do so by sending messages with a fake fencing
        token.</p>
        
        <p>In this book we assume that nodes are unreliable but honest: they may be slow or never respond (due
        to a fault), and their state may be outdated (due to a GC pause or network delays), but we assume
        that if a node <em>does</em> respond, it is telling the “truth”: to the best of its knowledge, it is
        playing by the rules of the protocol.</p>
        
        <p><a data-type="indexterm" data-primary="Byzantine faults" data-secondary="Byzantine Generals Problem" id="idm45085104149824"></a>
        Distributed systems problems become much harder if there is a risk that nodes may “lie” (send
        arbitrary faulty or corrupted responses)—for example, if a node may claim to have received a
        particular message when in fact it didn’t. Such behavior is known as a <em>Byzantine fault</em>, and the
        problem of reaching consensus in this untrusting environment is known as the <em>Byzantine Generals Problem</em>
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Lamport1982fr-marker" href="ch08.html#Lamport1982fr">77</a>].</p>
        <aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="sidebar_distributed_byzantine_generals">
        <h5>The Byzantine Generals Problem</h5>
        <p>The Byzantine Generals Problem is a generalization of the so-called <em>Two Generals Problem</em>
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Gray1978vv-marker" href="ch08.html#Gray1978vv">78</a>],
        which imagines a situation in which two army generals need to agree on a battle plan. As they
        have set up camp on two different sites, they can only communicate by messenger, and the messengers
        sometimes get delayed or lost (like packets in a network). We will discuss this problem of
        <em>consensus</em> in <a data-type="xref" href="ch09.html#ch_consistency">Chapter&nbsp;9</a>.</p>
        
        <p>In the Byzantine version of the problem, there are <em>n</em> generals who need to agree, and their
        endeavor is hampered by the fact that there are some traitors in their midst. Most of the generals
        are loyal, and thus send truthful messages, but the traitors may try to deceive and confuse the
        others by sending fake or untrue messages (while trying to remain undiscovered). It is not known in
        advance who the traitors are.</p>
        
        <p>Byzantium was an ancient Greek city that later became Constantinople, in the place which is now
        Istanbul in Turkey. There isn’t any historic evidence that the generals of Byzantium were any more
        prone to intrigue and conspiracy than those elsewhere. Rather, the name is derived from <em>Byzantine</em>
        in the sense of <em>excessively complicated, bureaucratic, devious</em>, which was used in politics long
        before computers [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Palmer2011uh-marker" href="ch08.html#Palmer2011uh">79</a>].
        Lamport wanted to choose a nationality that would not offend any readers, and he was advised that
        calling it <em>The Albanian Generals Problem</em> was not such a good idea
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="LamportPubs-marker" href="ch08.html#LamportPubs">80</a>].</p>
        </div></aside>
        
        <p><a data-type="indexterm" data-primary="Byzantine faults" data-secondary="Byzantine fault-tolerant systems" id="idm45085104129216"></a>
        <a data-type="indexterm" data-primary="correctness" data-secondary="Byzantine fault tolerance" id="idm45085104128096"></a>
        A system is <em>Byzantine fault-tolerant</em> if it continues to operate correctly even if some of the
        nodes are malfunctioning and not obeying the protocol, or if malicious attackers are interfering
        with the network. This concern is relevant in certain specific circumstances. For example:</p>
        
        <ul>
        <li>
        <p><a data-type="indexterm" data-primary="rockets" id="idm45085104125424"></a><a data-type="indexterm" data-primary="aerospace systems" id="idm45085104124720"></a>
        <a data-type="indexterm" data-primary="corruption of data" data-secondary="due to radiation" id="idm45085104123920"></a>
        In aerospace environments, the data in a computer’s memory or CPU register could become corrupted
        by radiation, leading it to respond to other nodes in arbitrarily unpredictable ways. Since a
        system failure would be very expensive (e.g., an aircraft crashing and killing everyone on board,
        or a rocket colliding with the International Space Station), flight control systems must tolerate
        Byzantine faults [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Rushby2001vu-marker" href="ch08.html#Rushby2001vu">81</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Edge2013wn-marker" href="ch08.html#Edge2013wn">82</a>].</p>
        </li>
        <li>
        <p><a data-type="indexterm" data-primary="Bitcoin (cryptocurrency)" data-secondary="Byzantine fault tolerance" id="idm45085104117088"></a>
        <a data-type="indexterm" data-primary="blockchains" data-secondary="Byzantine fault tolerance" id="idm45085104115904"></a>
        In a system with multiple participating organizations, some participants may attempt to cheat or
        defraud others. In such circumstances, it is not safe for a node to simply trust another node’s
        messages, since they may be sent with malicious intent. For example, peer-to-peer networks like
        Bitcoin and other blockchains can be considered to be a way of getting mutually untrusting parties
        to agree whether a transaction happened or not, without relying on a central authority
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Miller2014wd-marker" href="ch08.html#Miller2014wd">83</a>].</p>
        </li>
        </ul>
        
        <p>However, in the kinds of systems we discuss in this book, we can usually safely assume that there
        are no Byzantine faults. In your datacenter, all the nodes are controlled by your organization (so
        they can hopefully be trusted) and radiation levels are low enough that memory corruption is not a
        major problem. Protocols for making systems Byzantine fault-tolerant are quite complicated
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Mickens2013tp-marker" href="ch08.html#Mickens2013tp">84</a>],
        and fault-tolerant embedded systems rely on support from the hardware level
        [<a data-type="noteref" href="ch08.html#Rushby2001vu">81</a>]. In most server-side data systems, the
        cost of deploying Byzantine fault-tolerant solutions makes them impracticable.</p>
        
        <p><a data-type="indexterm" data-primary="SQL (Structured Query Language)" data-secondary="SQL injection vulnerability" id="idm45085104108112"></a>
        Web applications do need to expect arbitrary and malicious behavior of clients that are under
        end-user control, such as web browsers. This is why input validation, sanitization, and output
        escaping are so important: to prevent SQL injection and cross-site scripting, for example. However,
        we typically don’t use Byzantine fault-tolerant protocols here, but simply make the server the
        authority on deciding what client behavior is and isn’t allowed. In peer-to-peer networks, where
        there is no such central authority, Byzantine fault tolerance is more relevant.</p>
        
        <p>A bug in the software could be regarded as a Byzantine fault, but if you deploy the same software to
        all nodes, then a Byzantine fault-tolerant algorithm cannot save you. Most Byzantine fault-tolerant
        algorithms require a supermajority of more than two-thirds of the nodes to be functioning correctly
        (i.e., if you have four nodes, at most one may malfunction). To use this approach against bugs, you
        would have to have four independent implementations of the same software and hope that a bug only
        appears in one of the four implementations.</p>
        
        <p><a data-type="indexterm" data-primary="cryptography" data-secondary="defense against attackers" id="idm45085104105168"></a>
        Similarly, it would be appealing if a protocol could protect us from vulnerabilities, security
        compromises, and malicious attacks. Unfortunately, this is not realistic either: in most systems, if
        an attacker can compromise one node, they can probably compromise all of them, because they are
        probably running the same software. Thus, traditional mechanisms (authentication, access control,
        encryption, firewalls, and so on) continue to be the main protection against attackers.</p>
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Weak forms of lying"><div class="sect3" id="sec_distributed_weak_lying">
        <h3>Weak forms of lying</h3>
        
        <p>Although we assume that nodes are generally honest, it can be worth adding mechanisms to software
        that guard against weak forms of “lying”—for example, invalid messages due to hardware issues,
        software bugs, and misconfiguration. Such protection mechanisms are not full-blown Byzantine fault
        tolerance, as they would not withstand a determined adversary, but they are nevertheless simple and
        pragmatic steps toward better reliability. For example:</p>
        
        <ul>
        <li>
        <p><a data-type="indexterm" data-primary="packets" data-secondary="corruption of" id="idm45085104100352"></a>
        <a data-type="indexterm" data-primary="corruption of data" data-secondary="network packets" id="idm45085104099024"></a>
        <a data-type="indexterm" data-primary="Ethernet (networks)" data-secondary="packet checksums" id="idm45085104097920"></a>
        <a data-type="indexterm" data-primary="TCP (Transmission Control Protocol)" data-secondary="packet checksums" id="idm45085104096816"></a>
        Network packets do sometimes get corrupted due to hardware issues or bugs in operating systems,
        drivers, routers, etc. Usually, corrupted packets are caught by the checksums built into TCP and
        UDP, but sometimes they evade detection [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Gilman2015vp-marker" href="ch08.html#Gilman2015vp">85</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Stone2000fc-marker" href="ch08.html#Stone2000fc">86</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Jones2015uy-marker" href="ch08.html#Jones2015uy">87</a>].
        Simple measures are usually sufficient protection against such corruption, such as checksums in
        the application-level protocol.</p>
        </li>
        <li>
        <p>A publicly accessible application must carefully sanitize any inputs from users, for example
        checking that a value is within a reasonable range and limiting the size of strings to prevent
        denial of service through large memory allocations. An internal service behind a firewall may be
        able to get away with less strict checks on inputs, but some basic sanity-checking of values (e.g.,
        in protocol parsing [<a data-type="noteref" href="ch08.html#Gilman2015vp">85</a>]) is a good idea.</p>
        </li>
        <li>
        <p><a data-type="indexterm" data-primary="NTP (Network Time Protocol)" data-secondary="multiple server addresses" id="idm45085104085104"></a>
          NTP clients can be configured with multiple server addresses. When synchronizing, the client
          contacts all of them, estimates their errors, and checks that a majority of servers agree on some
          time range. As long as most of the servers are okay, a misconfigured NTP server that is reporting an
          incorrect time is detected as an outlier and is excluded from synchronization
          [<a data-type="noteref" href="ch08.html#Windl2006uo">37</a>]. The use of multiple servers makes NTP
          more robust than if it only uses a single server.
        <a data-type="indexterm" data-primary="faults" data-secondary="Byzantine faults" data-startref="ix_faultsByz" id="idm45085104082656"></a>
        <a data-type="indexterm" data-primary="Byzantine faults" data-startref="ix_Byzfaults" id="idm45085104081312"></a>
        <a data-type="indexterm" data-primary="distributed systems" data-secondary="Byzantine faults" data-startref="ix_distsysknowByz" id="idm45085104080208"></a></p>
        </li>
        </ul>
        </div></section>
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="System Model and Reality"><div class="sect2" id="sec_distributed_system_model">
        <h2>System Model and Reality</h2>
        
        <p><a data-type="indexterm" data-primary="system models" id="ix_sysmodreal"></a>
        <a data-type="indexterm" data-primary="distributed systems" data-secondary="system models" id="ix_distsysknowsysmod"></a>
        <a data-type="indexterm" data-primary="algorithms" data-secondary="for distributed systems" id="idm45085104074352"></a>
        Many algorithms have been designed to solve distributed systems problems—for example, we will
        examine solutions for the consensus problem in <a data-type="xref" href="ch09.html#ch_consistency">Chapter&nbsp;9</a>. In order to be useful, these
        algorithms need to tolerate the various faults of distributed systems that we discussed in this
        chapter.</p>
        
        <p>Algorithms need to be written in a way that does not depend too heavily on the details of the
        hardware and software configuration on which they are run. This in turn requires that we somehow
        formalize the kinds of faults that we expect to happen in a system. We do this by defining a <em>system
        model</em>, which is an abstraction that describes what things an algorithm may assume.</p>
        
        <p><a data-type="indexterm" data-primary="time" data-secondary="system models for distributed systems" id="idm45085104070752"></a>
        With regard to timing assumptions, three system models are in common use:</p>
        <dl>
        <dt>Synchronous model</dt>
        <dd>
        <p><a data-type="indexterm" data-primary="synchronous networks" data-secondary="formal model" id="idm45085104068064"></a>
        The synchronous model assumes bounded network delay, bounded process pauses, and bounded clock
        error. This does not imply exactly synchronized clocks or zero network delay; it just means you
        know that network delay, pauses, and clock drift will never exceed some fixed upper bound
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Dwork1988dr_ch8-marker" href="ch08.html#Dwork1988dr_ch8">88</a>].
        The synchronous model is not a realistic model of most practical
        systems, because (as discussed in this chapter) unbounded delays and pauses do occur.</p>
        </dd>
        <dt>Partially synchronous model</dt>
        <dd>
        <p>Partial synchrony means that a system behaves like a synchronous system <em>most of the time</em>, but it
        sometimes exceeds the bounds for network delay, process pauses, and clock drift
        [<a data-type="noteref" href="ch08.html#Dwork1988dr_ch8">88</a>]. This is a realistic model of many
        systems: most of the time, networks and processes are quite well behaved—otherwise we would never
        be able to get anything done—but we have to reckon with the fact that any timing assumptions
        may be shattered occasionally. When this happens, network delay, pauses, and clock error may become
        arbitrarily large.</p>
        </dd>
        <dt>Asynchronous model</dt>
        <dd>
        <p><a data-type="indexterm" data-primary="asynchronous networks" data-secondary="formal model" id="idm45085104059456"></a>
        In this model, an algorithm is not allowed to make any timing assumptions—in fact, it does not
        even have a clock (so it cannot use timeouts). Some algorithms can be designed for the
        asynchronous model, but it is very restrictive.</p>
        </dd>
        </dl>
        
        <p><a data-type="indexterm" data-primary="nodes (processes)" data-secondary="system models for failure" id="idm45085104057712"></a>
        Moreover, besides timing issues, we have to consider node failures. The three most common system
        models for nodes are:</p>
        <dl>
        <dt>Crash-stop faults</dt>
        <dd>
        <p>In the crash-stop model, an algorithm may assume that a node can fail in only one way, namely by
        crashing. This means that the node may suddenly stop responding at any moment, and thereafter that
        node is gone forever—it never comes back.</p>
        </dd>
        <dt>Crash-recovery faults</dt>
        <dd>
        <p>We assume that nodes may crash at any moment, and perhaps start responding again after some
        unknown time. In the crash-recovery model, nodes are assumed to have stable storage (i.e.,
        nonvolatile disk storage) that is preserved across crashes, while the in-memory state is assumed
        to be lost.</p>
        </dd>
        <dt>Byzantine (arbitrary) faults</dt>
        <dd>
        <p><a data-type="indexterm" data-primary="Byzantine faults" id="idm45085104051744"></a>
        Nodes may do absolutely anything, including trying to trick and deceive other nodes, as described
        in the last section.</p>
        </dd>
        </dl>
        
        <p>For modeling real systems, the partially synchronous model with crash-recovery faults is generally
        the most useful model. But how do distributed algorithms cope with that model?</p>
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Correctness of an algorithm"><div class="sect3" id="idm45085104049952">
        <h3>Correctness of an algorithm</h3>
        
        <p><a data-type="indexterm" data-primary="algorithms" data-secondary="algorithm correctness" id="idm45085104048608"></a>
        <a data-type="indexterm" data-primary="correctness" data-secondary="of algorithm within system model" id="idm45085104047504"></a>
        <a data-type="indexterm" data-primary="system models" data-secondary="correctness of algorithms" id="idm45085104046384"></a>
        To define what it means for an algorithm to be <em>correct</em>, we can describe its <em>properties</em>. For
        example, the output of a sorting algorithm has the property that for any two distinct elements of
        the output list, the element further to the left is smaller than the element further to the right.
        That is simply a formal way of defining what it means for a list to be sorted.</p>
        
        <p><a data-type="indexterm" data-primary="fencing (preventing split brain)" data-secondary="properties of fencing tokens" id="idm45085104043776"></a>
        Similarly, we can write down the properties we want of a distributed algorithm to define what it
        means to be correct. For example, if we are generating fencing tokens for a lock (see
        <a data-type="xref" href="#sec_distributed_fencing_tokens">“Fencing tokens”</a>), we may require the algorithm to have the following properties:</p>
        <dl>
        <dt>Uniqueness</dt>
        <dd>
        <p>No two requests for a fencing token return the same value.</p>
        </dd>
        <dt>Monotonic sequence</dt>
        <dd>
        <p>If request <em>x</em> returned token <em>t</em><sub><em>x</em></sub>, and request <em>y</em> returned token <em>t</em><sub><em>y</em></sub>, and
        <em>x</em> completed before <em>y</em> began, then <em>t</em><sub><em>x</em></sub>&nbsp;&lt;&nbsp;<em>t</em><sub><em>y</em></sub>.</p>
        </dd>
        <dt>Availability</dt>
        <dd>
        <p>A node that requests a fencing token and does not crash eventually receives a response.</p>
        </dd>
        </dl>
        
        <p>An algorithm is correct in some system model if it always satisfies its properties in all situations
        that we assume may occur in that system model. But how does this make sense? If all nodes crash, or
        all network delays suddenly become infinitely long, then no algorithm will be able to get anything
        done.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Safety and liveness"><div class="sect3" id="sec_distributed_safety_liveness">
        <h3>Safety and liveness</h3>
        
        <p><a data-type="indexterm" data-primary="system models" data-secondary="safety and liveness" id="idm45085104029184"></a>
        <a data-type="indexterm" data-primary="safety and liveness properties" id="idm45085104027856"></a>
        <a data-type="indexterm" data-primary="liveness properties" id="idm45085104027056"></a>
        To clarify the situation, it is worth distinguishing between two different kinds of properties:
        <em>safety</em> and <em>liveness</em> properties. In the example just given, <em>uniqueness</em> and <em>monotonic sequence</em> are
        safety properties, but <em>availability</em> is a liveness property.</p>
        
        <p><a data-type="indexterm" data-primary="eventual consistency" id="idm45085104023808"></a>
        What distinguishes the two kinds of properties? A giveaway is that liveness properties often include
        the word “eventually” in their definition. (And yes, you guessed it—<em>eventual consistency</em> is a
        liveness property [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Bailis2013jc_ch8-marker" href="ch08.html#Bailis2013jc_ch8">89</a>].)</p>
        
        <p>Safety is often informally defined as <em>nothing bad happens</em>, and liveness as <em>something good
        eventually happens</em>. However, it’s best to not read too much into those informal definitions,
        because the meaning of good and bad is subjective. The actual definitions of safety and liveness are
        precise and mathematical
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Alpern1985dg-marker" href="ch08.html#Alpern1985dg">90</a>]:</p>
        
        <ul>
        <li>
        <p>If a safety property is violated, we can point at a particular point in time at which it was
        broken (for example, if the uniqueness property was violated, we can identify the particular
        operation in which a duplicate fencing token was returned). After a safety property has been
        violated, the violation cannot be undone—the damage is already done.</p>
        </li>
        <li>
        <p>A liveness property works the other way round: it may not hold at some point in time (for example,
        a node may have sent a request but not yet received a response), but there is always hope that it
        may be satisfied in the future (namely by receiving a response).</p>
        </li>
        </ul>
        
        <p>An advantage of distinguishing between safety and liveness properties is that it helps us deal with
        difficult system models. For distributed algorithms, it is common to require that safety properties
        <em>always</em> hold, in all possible situations of a system model
        [<a data-type="noteref" href="ch08.html#Dwork1988dr_ch8">88</a>]. That is, even if all nodes crash, or
        the entire network fails, the algorithm must nevertheless ensure that it does not return a wrong
        result (i.e., that the safety properties remain satisfied).</p>
        
        <p>However, with liveness properties we are allowed to make caveats: for example, we could say that a
        request needs to receive a response only if a majority of nodes have not crashed, and only if the
        network eventually recovers from an outage. The definition of the partially synchronous model
        requires that eventually the system returns to a synchronous state—that is, any period of network
        interruption lasts only for a finite duration and is then repaired.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Mapping system models to the real world"><div class="sect3" id="sec_distributed_model_real_world">
        <h3>Mapping system models to the real world</h3>
        
        <p><a data-type="indexterm" data-primary="system models" data-secondary="mapping to the real world" id="idm45085104007424"></a>
        Safety and liveness properties and system models are very useful for reasoning about the correctness
        of a distributed algorithm. However, when implementing an algorithm in practice, the messy facts of
        reality come back to bite you again, and it becomes clear that the system model is a simplified
        abstraction of reality.</p>
        
        <p><a data-type="indexterm" data-primary="incidents" data-secondary="data on disks unreadable" id="idm45085104005664"></a>
        <a data-type="indexterm" data-primary="GitHub, postmortems" id="idm45085104004544"></a>
        For example, algorithms in the crash-recovery model generally assume that data in stable storage
        survives crashes. However, what happens if the data on disk is corrupted, or the data is wiped out
        due to hardware error or misconfiguration
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Junqueira2015wf-marker" href="ch08.html#Junqueira2015wf">91</a>]? What happens if a server has a firmware bug and fails to recognize
        its hard drives on reboot, even though the drives are correctly attached to the server
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Sanders2016tl-marker" href="ch08.html#Sanders2016tl">92</a>]?</p>
        
        <p><a data-type="indexterm" data-primary="quorums" data-secondary="relying on durability" id="idm45085103998592"></a>
        Quorum algorithms (see <a data-type="xref" href="ch05.html#sec_replication_quorum_condition">“Quorums for reading and writing”</a>) rely on a node remembering the data
        that it claims to have stored. If a node may suffer from amnesia and forget previously stored data,
        that breaks the quorum condition, and thus breaks the correctness of the algorithm. Perhaps a new
        system model is needed, in which we assume that stable storage mostly survives crashes, but may
        sometimes be lost. But that model then becomes harder to reason about.</p>
        
        <p>The theoretical description of an algorithm can declare that certain things are simply assumed not
        to happen—and in non-Byzantine systems, we do have to make some assumptions about faults that can
        and cannot happen. However, a real implementation may still have to include code to handle the
        case where something happens that was assumed to be impossible, even if that handling boils down to
        <code>printf("Sucks to be you")</code> and <code>exit(666)</code>—i.e., letting a human operator clean up the mess
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kreps2013ud-marker" href="ch08.html#Kreps2013ud">93</a>].
        (This is arguably the difference between computer science and software engineering.)</p>
        
        <p><a data-type="indexterm" data-primary="complexity" data-secondary="distilling in theoretical models" id="idm45085103992032"></a>
        That is not to say that theoretical, abstract system models are worthless—quite the opposite.
        They are incredibly helpful for distilling down the complexity of real systems to a manageable set
        of faults that we can reason about, so that we can understand the problem and try to solve it
        systematically. We can prove algorithms correct by showing that their properties always hold in some
        system model.</p>
        
        <p>Proving an algorithm correct does not mean its <em>implementation</em> on a real system will necessarily
        always behave correctly. But it’s a very good first step, because the theoretical analysis can
        uncover problems in an algorithm that might remain hidden for a long time in a real system, and that
        only come to bite you when your assumptions (e.g., about timing) are defeated due to unusual
        circumstances. Theoretical analysis and empirical testing are equally important.
        <a data-type="indexterm" data-primary="distributed systems" data-secondary="system models" data-startref="ix_distsysknowsysmod" id="idm45085103989168"></a>
        <a data-type="indexterm" data-primary="system models" data-startref="ix_sysmodreal" id="idm45085103987792"></a></p>
        </div></section>
        
        
        
        </div></section>
        
        
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="idm45085104228736">
        <h1>Summary</h1>
        
        <p>In this chapter we have discussed a wide range of problems that can occur in distributed systems,
        including:</p>
        
        <ul>
        <li>
        <p>Whenever you try to send a packet over the network, it may be lost or arbitrarily delayed.
        Likewise, the reply may be lost or delayed, so if you don’t get a reply, you have no idea whether
        the message got through.</p>
        </li>
        <li>
        <p>A node’s clock may be significantly out of sync with other nodes (despite your best efforts to set
        up NTP), it may suddenly jump forward or back in time, and relying on it is dangerous because you
        most likely don’t have a good measure of your clock’s confidence interval.</p>
        </li>
        <li>
        <p>A process may pause for a substantial amount of time at any point in its execution (perhaps due to
        a stop-the-world garbage collector), be declared dead by other nodes, and then come back to life
        again without realizing that it was paused.</p>
        </li>
        </ul>
        
        <p><a data-type="indexterm" data-primary="partial failures" id="idm45085103981056"></a>
        <a data-type="indexterm" data-primary="failures" data-secondary="partial failures in distributed systems" id="idm45085103980224"></a>
        The fact that such <em>partial failures</em> can occur is the defining characteristic of distributed
        systems. Whenever software tries to do anything involving other nodes, there is the possibility that
        it may occasionally fail, or randomly go slow, or not respond at all (and eventually time out). In
        distributed systems, we try to build tolerance of partial failures into software, so that the system
        as a whole may continue functioning even when some of its constituent parts are broken.</p>
        
        <p><a data-type="indexterm" data-primary="incidents" data-secondary="gigabit network interface with 1 Kb/s throughput" id="idm45085103977984"></a>
        <a data-type="indexterm" data-primary="partial failures" data-secondary="limping" id="idm45085103976784"></a>
        <a data-type="indexterm" data-primary="limping (partial failure)" id="idm45085103975680"></a>
        To tolerate faults, the first step is to <em>detect</em> them, but even that is hard. Most systems
        don’t have an accurate mechanism of detecting whether a node has failed, so most distributed
        algorithms rely on timeouts to determine whether a remote node is still available. However, timeouts
        can’t distinguish between network and node failures, and variable network delay sometimes causes a
        node to be falsely suspected of crashing. Moreover, sometimes a node can be in a degraded state: for
        example, a Gigabit network interface could suddenly drop to 1 Kb/s throughput due to a driver
        bug [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Do2013hc-marker" href="ch08.html#Do2013hc">94</a>].
        Such a node that is “limping” but not dead can be even more difficult to deal with than a
        cleanly failed node.</p>
        
        <p>Once a fault is detected, making a system tolerate it is not easy either: there is no global
        variable, no shared memory, no common knowledge or any other kind of shared state between the
        machines. Nodes can’t even agree on what time it is, let alone on anything more profound. The only way
        information can flow from one node to another is by sending it over the unreliable network. Major
        decisions cannot be safely made by a single node, so we require protocols that enlist help from
        other nodes and try to get a quorum to agree.</p>
        
        <p>If you’re used to writing software in the idealized mathematical perfection of a single computer,
        where the same operation always deterministically returns the same result, then moving to the messy
        physical reality of distributed systems can be a bit of a shock. Conversely, distributed systems
        engineers will often regard a problem as trivial if it can be solved on a single computer
        [<a data-type="noteref" href="ch08.html#Hodges2013tj">5</a>],
        and indeed a single computer can do a lot nowadays
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="McSherry2015vx_ch8-marker" href="ch08.html#McSherry2015vx_ch8">95</a>]. If you can avoid opening Pandora’s box and simply keep things on a
        single machine, it is generally worth doing so.</p>
        
        <p>However, as discussed in the introduction to <a data-type="xref" href="part02.html#part_distributed_data">Part&nbsp;II</a>, scalability is not the only
        reason for wanting to use a distributed system. Fault tolerance and low latency (by placing data
        geographically close to users) are equally important goals, and those things cannot be achieved with
        a single node.</p>
        
        <p>In this chapter we also went on some tangents to explore whether the unreliability of networks,
        clocks, and processes is an inevitable law of nature. We saw that it isn’t: it is possible to give
        hard real-time response guarantees and bounded delays in networks, but doing so is very expensive and
        results in lower utilization of hardware resources. Most non-safety-critical systems choose cheap
        and unreliable over expensive and reliable.</p>
        
        <p>We also touched on supercomputers, which assume reliable components and thus have to be stopped and
        restarted entirely when a component does fail. By contrast, distributed systems can run forever
        without being interrupted at the service level, because all faults and maintenance can be handled at
        the node level—at least in theory. (In practice, if a bad configuration change is rolled out to
        all nodes, that will still bring a distributed system to its knees.)</p>
        
        <p>This chapter has been all about problems, and has given us a bleak outlook. In the next chapter we
        will move on to solutions, and discuss some algorithms that have been designed to cope with the
        problems in distributed systems.
        <a data-type="indexterm" data-primary="distributed systems" data-startref="ix_distsys" id="idm45085103962032"></a></p>
        </div></section>
        
        
        
        
        
        
        
        <div data-type="footnotes"><h5>Footnotes</h5><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45085105015616"><sup><a href="ch08.html#idm45085105015616-marker">i</a></sup> With
        one exception: we will assume that faults are <em>non-Byzantine</em> (see
        <a data-type="xref" href="#sec_distributed_byzantine">“Byzantine Faults”</a>).</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45085104705232"><sup><a href="ch08.html#idm45085104705232-marker">ii</a></sup> Except perhaps for an
        occasional keepalive packet, if TCP keepalive is enabled.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45085104696768"><sup><a href="ch08.html#idm45085104696768-marker">iii</a></sup> <em>Asynchronous
        Transfer Mode</em> (ATM) was a competitor to Ethernet in the 1980s
        [<a data-type="noteref" href="ch08.html#Keshav1997wb">32</a>], but it didn’t gain much adoption
        outside of telephone network core switches. It has nothing to do with automatic teller machines
        (also known as cash machines), despite sharing an acronym. Perhaps, in some parallel universe, the
        internet is based on something like ATM—in that universe, internet video calls are probably a lot
        more reliable than they are in ours, because they don’t suffer from dropped and delayed packets.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45085104675056"><sup><a href="ch08.html#idm45085104675056-marker">iv</a></sup> Peering
        agreements between internet service providers and the establishment of routes through the Border
        Gateway Protocol (BGP), bear closer resemblance to circuit switching than IP itself. At this level,
        it is possible to buy dedicated bandwidth. However, internet routing operates at the level of
        networks, not individual connections between hosts, and at a much longer timescale.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45085104638128"><sup><a href="ch08.html#idm45085104638128-marker">v</a></sup> Although
        the clock is called <em>real-time</em>, it has nothing to do with real-time operating systems, as
        discussed in <a data-type="xref" href="#sec_distributed_clocks_realtime">“Response time guarantees”</a>.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45085104450736"><sup><a href="ch08.html#idm45085104450736-marker">vi</a></sup> There are distributed sequence
        number generators, such as Twitter’s Snowflake, that generate <em>approximately</em> monotonically
        increasing unique IDs in a scalable way (e.g., by allocating blocks of the ID space to different
        nodes).  However, they typically cannot guarantee an ordering that is consistent with causality,
        because the timescale at which blocks of IDs are assigned is longer than the timescale of database
        reads and writes. See also <a data-type="xref" href="ch09.html#sec_consistency_ordering">“Ordering Guarantees”</a>.</p></div><div data-type="footnotes"><h5>References</h5><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Cavage2013ez">[<a href="ch08.html#Cavage2013ez-marker">1</a>] Mark Cavage:
        “<a href="http://queue.acm.org/detail.cfm?id=2482856">There’s Just No Getting Around It: You’re
        Building a Distributed System</a>,” <em>ACM Queue</em>, volume 11, number 4, pages 80-89, April 2013.
        <a href="http://dx.doi.org/10.1145/2466486.2482856">doi:10.1145/2466486.2482856</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kreps2012td_ch8">[<a href="ch08.html#Kreps2012td_ch8-marker">2</a>] Jay Kreps:
        “<a href="http://blog.empathybox.com/post/19574936361/getting-real-about-distributed-system-reliability">Getting
        Real About Distributed System Reliability</a>,” <em>blog.empathybox.com</em>, March 19, 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Padua2015um">[<a href="ch08.html#Padua2015um-marker">3</a>] Sydney Padua: <em>The Thrilling Adventures of
        Lovelace and Babbage: The (Mostly) True Story of the First Computer</em>. Particular Books, April
        2015.  ISBN: 978-0-141-98151-2</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Hale2010we">[<a href="ch08.html#Hale2010we-marker">4</a>] Coda Hale:
        “<a href="http://codahale.com/you-cant-sacrifice-partition-tolerance/">You Can’t Sacrifice
        Partition Tolerance</a>,” <em>codahale.com</em>, October 7, 2010.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Hodges2013tj">[<a href="ch08.html#Hodges2013tj-marker">5</a>] Jeff Hodges:
        “<a href="https://web.archive.org/web/20200218095605/https://www.somethingsimilar.com/2013/01/14/notes-on-distributed-systems-for-young-bloods/">Notes
        on Distributed Systems for Young Bloods</a>,” <em>somethingsimilar.com</em>, January 14, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Regalado2011vn">[<a href="ch08.html#Regalado2011vn-marker">6</a>] Antonio Regalado:
        “<a href="http://www.technologyreview.com/news/425970/who-coined-cloud-computing/">Who Coined
        ‘Cloud Computing’?</a>,” <em>technologyreview.com</em>, October 31, 2011.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Barroso2013ba">[<a href="ch08.html#Barroso2013ba-marker">7</a>] Luiz André Barroso, Jimmy Clidaras, and Urs Hölzle:
        “<a href="http://www.morganclaypool.com/doi/abs/10.2200/S00516ED2V01Y201306CAC024">The Datacenter
        as a Computer: An Introduction to the Design of Warehouse-Scale Machines, Second Edition</a>,”
        <em>Synthesis Lectures on Computer Architecture</em>, volume 8, number 3,
        Morgan &amp; Claypool Publishers, July 2013.
        <a href="http://dx.doi.org/10.2200/S00516ED2V01Y201306CAC024">doi:10.2200/S00516ED2V01Y201306CAC024</a>,
        ISBN: 978-1-627-05010-4</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Fiala2012ti">[<a href="ch08.html#Fiala2012ti-marker">8</a>] David Fiala, Frank Mueller, Christian Engelmann, et al.:
        “<a href="http://moss.csc.ncsu.edu/~mueller/ftp/pub/mueller/papers/sc12.pdf">Detection and
        Correction of Silent Data Corruption for Large-Scale High-Performance Computing</a>,” at
        <em>International Conference for High Performance Computing, Networking, Storage and
        Analysis</em> (SC12), November 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Singh2015fc">[<a href="ch08.html#Singh2015fc-marker">9</a>] Arjun Singh, Joon Ong, Amit Agarwal, et al.:
        “<a href="http://conferences.sigcomm.org/sigcomm/2015/pdf/papers/p183.pdf">Jupiter Rising: A
        Decade of Clos Topologies and Centralized Control in Google’s Datacenter Network</a>,” at
        <em>Annual Conference of the ACM Special Interest Group on Data Communication</em> (SIGCOMM), August 2015.
        <a href="http://dx.doi.org/10.1145/2785956.2787508">doi:10.1145/2785956.2787508</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Lockwood2014uz">[<a href="ch08.html#Lockwood2014uz-marker">10</a>] Glenn K. Lockwood:
        “<a href="http://glennklockwood.blogspot.co.uk/2014/05/hadoops-uncomfortable-fit-in-hpc.html">Hadoop’s
        Uncomfortable Fit in HPC</a>,” <em>glennklockwood.blogspot.co.uk</em>, May 16, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="vonNeumann1956vm">[<a href="ch08.html#vonNeumann1956vm-marker">11</a>] John von Neumann:
        “<a href="https://ece.uwaterloo.ca/~ssundara/courses/prob_logics.pdf">Probabilistic Logics and the
        Synthesis of Reliable Organisms from Unreliable Components</a>,” in <em>Automata Studies (AM-34)</em>,
        edited by Claude E. Shannon and John McCarthy, Princeton University Press, 1956.
        ISBN: 978-0-691-07916-5</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Hamming1997wd">[<a href="ch08.html#Hamming1997wd-marker">12</a>] Richard W. Hamming:
        <em>The Art of Doing Science and Engineering</em>. Taylor &amp; Francis, 1997.
        ISBN: 978-9-056-99500-3</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Shannon1948wk">[<a href="ch08.html#Shannon1948wk-marker">13</a>] Claude E. Shannon:
        “<a href="http://cs.brynmawr.edu/Courses/cs380/fall2012/shannon1948.pdf">A Mathematical Theory of
        Communication</a>,” <em>The Bell System Technical Journal</em>, volume 27, number 3,
        pages 379–423 and 623–656, July 1948.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Bailis2014jx">[<a href="ch08.html#Bailis2014jx-marker">14</a>] Peter Bailis and Kyle Kingsbury:
        “<a href="https://queue.acm.org/detail.cfm?id=2655736">The Network Is Reliable</a>,”
        <em>ACM Queue</em>, volume 12, number 7, pages 48-55, July 2014.
        <a href="http://dx.doi.org/10.1145/2639988.2639988">doi:10.1145/2639988.2639988</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Leners2015gv">[<a href="ch08.html#Leners2015gv-marker">15</a>] Joshua B. Leners, Trinabh Gupta, Marcos K. Aguilera, and Michael Walfish:
        “<a href="http://www.cs.nyu.edu/~mwalfish/papers/albatross-eurosys15.pdf">Taming Uncertainty in
        Distributed Systems with Help from the Network</a>,” at <em>10th European Conference on
        Computer Systems</em> (EuroSys), April 2015.
        <a href="http://dx.doi.org/10.1145/2741948.2741976">doi:10.1145/2741948.2741976</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Gill2011ku">[<a href="ch08.html#Gill2011ku-marker">16</a>] Phillipa Gill, Navendu Jain, and Nachiappan Nagappan:
        “<a href="http://conferences.sigcomm.org/sigcomm/2011/papers/sigcomm/p350.pdf">Understanding
        Network Failures in Data Centers: Measurement, Analysis, and Implications</a>,” at
        <em>ACM SIGCOMM Conference</em>, August 2011.
        <a href="http://dx.doi.org/10.1145/2018436.2018477">doi:10.1145/2018436.2018477</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Imbriaco2012tx_ch8">[<a href="ch08.html#Imbriaco2012tx_ch8-marker">17</a>] Mark Imbriaco:
        “<a href="https://github.com/blog/1364-downtime-last-saturday">Downtime Last Saturday</a>,”
        <em>github.com</em>, December 26, 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Oremus2014ty">[<a href="ch08.html#Oremus2014ty-marker">18</a>] Will Oremus:
        “<a href="http://www.slate.com/blogs/future_tense/2014/08/15/shark_attacks_threaten_google_s_undersea_internet_cables_video.html">The
        Global Internet Is Being Attacked by Sharks, Google Confirms</a>,” <em>slate.com</em>, August 15,
        2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Donges2012tt">[<a href="ch08.html#Donges2012tt-marker">19</a>] Marc A. Donges:
        “<a href="http://www.spinics.net/lists/netdev/msg210485.html">Re: bnx2 cards Intermittantly Going
        Offline</a>,” Message to Linux <em>netdev</em> mailing list, <em>spinics.net</em>, September 13, 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kingsbury2014vi">[<a href="ch08.html#Kingsbury2014vi-marker">20</a>] Kyle Kingsbury:
        “<a href="https://aphyr.com/posts/317-call-me-maybe-elasticsearch">Call Me Maybe:
        Elasticsearch</a>,” <em>aphyr.com</em>, June 15, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Sanfilippo2014ty">[<a href="ch08.html#Sanfilippo2014ty-marker">21</a>] Salvatore Sanfilippo:
        “<a href="http://antirez.com/news/80">A Few Arguments About Redis Sentinel Properties and Fail
        Scenarios</a>,” <em>antirez.com</em>, October 21, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Hubert2009wf">[<a href="ch08.html#Hubert2009wf-marker">22</a>] Bert Hubert:
        “<a href="http://blog.netherlabs.nl/articles/2009/01/18/the-ultimate-so_linger-page-or-why-is-my-tcp-not-reliable">The
        Ultimate SO_LINGER Page, or: Why Is My TCP Not Reliable</a>,” <em>blog.netherlabs.nl</em>, January 18, 2009.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Liochon2015ux">[<a href="ch08.html#Liochon2015ux-marker">23</a>] Nicolas Liochon:
        “<a href="http://blog.thislongrun.com/2015/05/CAP-theorem-partition-timeout-zookeeper.html">CAP:
        If All You Have Is a Timeout, Everything Looks Like a Partition</a>,” <em>blog.thislongrun.com</em>,
        May 25, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Saltzer1984do_ch8">[<a href="ch08.html#Saltzer1984do_ch8-marker">24</a>] Jerome H. Saltzer, David P. Reed, and David D. Clark:
        “<a href="https://groups.csail.mit.edu/ana/Publications/PubPDFs/End-to-End%20Arguments%20in%20System%20Design.pdf">End-To-End
        Arguments in System Design</a>,” <em>ACM Transactions on Computer Systems</em>, volume 2, number 4,
        pages 277–288, November 1984.
        <a href="http://dx.doi.org/10.1145/357401.357402">doi:10.1145/357401.357402</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Grosvenor2015vz">[<a href="ch08.html#Grosvenor2015vz-marker">25</a>] Matthew P. Grosvenor, Malte Schwarzkopf, Ionel Gog, et al.:
        “<a href="https://www.usenix.org/system/files/conference/nsdi15/nsdi15-paper-grosvenor_update.pdf">Queues
        Don’t Matter When You Can JUMP Them!</a>,” at <em>12th USENIX Symposium on Networked
        Systems Design and Implementation</em> (NSDI), May 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Wang2010ja">[<a href="ch08.html#Wang2010ja-marker">26</a>] Guohui Wang and T. S. Eugene Ng:
        “<a href="http://www.cs.rice.edu/~eugeneng/papers/INFOCOM10-ec2.pdf">The Impact of
        Virtualization on Network Performance of Amazon EC2 Data Center</a>,” at <em>29th IEEE
        International Conference on Computer Communications</em> (INFOCOM), March 2010.
        <a href="http://dx.doi.org/10.1109/INFCOM.2010.5461931">doi:10.1109/INFCOM.2010.5461931</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Jacobson1988gl">[<a href="ch08.html#Jacobson1988gl-marker">27</a>] Van Jacobson:
        “<a href="http://www.cs.usask.ca/ftp/pub/discus/seminars2002-2003/p314-jacobson.pdf">Congestion
        Avoidance and Control</a>,” at <em>ACM Symposium on Communications Architectures and
        Protocols</em> (SIGCOMM), August 1988.
        <a href="http://dx.doi.org/10.1145/52324.52356">doi:10.1145/52324.52356</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Philips2014tr">[<a href="ch08.html#Philips2014tr-marker">28</a>] Brandon Philips:
        “<a href="https://www.youtube.com/watch?v=HJIjTTHWYnE">etcd: Distributed Locking and Service
        Discovery</a>,” at <em>Strange Loop</em>, September 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Newman2012vf">[<a href="ch08.html#Newman2012vf-marker">29</a>] Steve Newman:
        “<a href="https://web.archive.org/web/20141211094156/http://blog.scalyr.com/2012/10/a-systematic-look-at-ec2-io/">A
        Systematic Look at EC2 I/O</a>,” <em>blog.scalyr.com</em>, October 16, 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Hayashibara2004vw">[<a href="ch08.html#Hayashibara2004vw-marker">30</a>] Naohiro Hayashibara, Xavier Défago, Rami Yared, and
        Takuya Katayama: “<a href="http://hdl.handle.net/10119/4784">The ϕ Accrual Failure
        Detector</a>,” Japan Advanced Institute of Science and Technology, School of Information
        Science, Technical Report IS-RR-2004-010, May 2004.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Wang2013wa">[<a href="ch08.html#Wang2013wa-marker">31</a>] Jeffrey Wang:
        “<a href="http://ternarysearch.blogspot.co.uk/2013/08/phi-accrual-failure-detector.html">Phi
        Accrual Failure Detector</a>,” <em>ternarysearch.blogspot.co.uk</em>, August 11, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Keshav1997wb">[<a href="ch08.html#Keshav1997wb-marker">32</a>] Srinivasan Keshav: <em>An Engineering Approach
        to Computer Networking: ATM Networks, the Internet, and the Telephone Network</em>.
        Addison-Wesley Professional, May 1997. ISBN: 978-0-201-63442-6</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="CiscoISDN">[<a href="ch08.html#CiscoISDN-marker">33</a>] Cisco,
        “<a href="https://web.archive.org/web/20181229220921/http://docwiki.cisco.com/wiki/Integrated_Services_Digital_Network">Integrated
        Services Digital Network</a>,” <em>docwiki.cisco.com</em>.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kyas1995ug">[<a href="ch08.html#Kyas1995ug-marker">34</a>] Othmar Kyas: <em>ATM Networks</em>.
        International Thomson Publishing, 1995. ISBN: 978-1-850-32128-6</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Mellanox2014ux">[<a href="ch08.html#Mellanox2014ux-marker">35</a>] “<a href="http://www.mellanox.com/related-docs/whitepapers/InfiniBandFAQ_FQ_100.pdf">InfiniBand
        FAQ</a>,” Mellanox Technologies, December 22, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Santos2003ci">[<a href="ch08.html#Santos2003ci-marker">36</a>] Jose Renato Santos, Yoshio Turner, and G. (John) Janakiraman:
        “<a href="http://www.hpl.hp.com/techreports/2002/HPL-2002-359.pdf">End-to-End Congestion Control
        for InfiniBand</a>,” at <em>22nd Annual Joint Conference of the IEEE Computer and
        Communications Societies</em> (INFOCOM), April 2003. Also published by HP Laboratories Palo
        Alto, Tech Report HPL-2002-359.
        <a href="http://dx.doi.org/10.1109/INFCOM.2003.1208949">doi:10.1109/INFCOM.2003.1208949</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Windl2006uo">[<a href="ch08.html#Windl2006uo-marker">37</a>] Ulrich Windl, David Dalton, Marc Martinec, and Dale R. Worley:
        “<a href="http://www.ntp.org/ntpfaq/NTP-a-faq.htm">The NTP FAQ and HOWTO</a>,” <em>ntp.org</em>,
        November 2006.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="GrahamCumming2017db">[<a href="ch08.html#GrahamCumming2017db-marker">38</a>] John Graham-Cumming:
        “<a href="https://blog.cloudflare.com/how-and-why-the-leap-second-affected-cloudflare-dns/">How and
        why the leap second affected Cloudflare DNS</a>,” <em>blog.cloudflare.com</em>, January 1, 2017.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Holmes2006uj">[<a href="ch08.html#Holmes2006uj-marker">39</a>] David Holmes:
        “<a href="https://web.archive.org/web/20160308031939/https://blogs.oracle.com/dholmes/entry/inside_the_hotspot_vm_clocks">Inside
        the Hotspot VM: Clocks, Timers and Scheduling Events – Part I – Windows</a>,” <em>blogs.oracle.com</em>,
        October 2, 2006.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Loughran2015wi">[<a href="ch08.html#Loughran2015wi-marker">40</a>] Steve Loughran:
        “<a href="http://steveloughran.blogspot.co.uk/2015/09/time-on-multi-core-multi-socket-servers.html">Time
        on Multi-Core, Multi-Socket Servers</a>,” <em>steveloughran.blogspot.co.uk</em>, September 17, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Corbett2012uz_ch8">[<a href="ch08.html#Corbett2012uz_ch8-marker">41</a>] James C. Corbett, Jeffrey Dean, Michael Epstein, et al.:
        “<a href="https://research.google/pubs/pub39966/">Spanner: Google’s Globally-Distributed
        Database</a>,” at <em>10th USENIX Symposium on Operating System Design and
        Implementation</em> (OSDI), October 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Caporaloni2012jn">[<a href="ch08.html#Caporaloni2012jn-marker">42</a>] M. Caporaloni and R. Ambrosini:
        “<a href="https://iopscience.iop.org/0143-0807/23/4/103/">How Closely Can a Personal Computer
        Clock Track the UTC Timescale Via the Internet?</a>,” <em>European Journal of
        Physics</em>, volume 23, number 4, pages L17–L21, June 2012.
        <a href="http://dx.doi.org/10.1088/0143-0807/23/4/103">doi:10.1088/0143-0807/23/4/103</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Minar1999vf">[<a href="ch08.html#Minar1999vf-marker">43</a>] Nelson Minar:
        “<a href="http://alumni.media.mit.edu/~nelson/research/ntp-survey99/">A Survey of the NTP Network</a>,”
        <em>alumni.media.mit.edu</em>, December 1999.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Holub2014uc">[<a href="ch08.html#Holub2014uc-marker">44</a>] Viliam Holub:
        “<a href="https://blog.rapid7.com/2014/03/14/synchronizing-clocks-in-a-cassandra-cluster-pt-1-the-problem/">Synchronizing
        Clocks in a Cassandra Cluster Pt. 1 – The Problem</a>,” <em>blog.rapid7.com</em>, March 14, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kamp2011cr">[<a href="ch08.html#Kamp2011cr-marker">45</a>] Poul-Henning Kamp:
        “<a href="http://queue.acm.org/detail.cfm?id=1967009">The One-Second War (What Time Will You
        Die?)</a>,” <em>ACM Queue</em>, volume 9, number 4, pages 44–48, April 2011.
        <a href="http://dx.doi.org/10.1145/1966989.1967009">doi:10.1145/1966989.1967009</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Minar2012vh_ch8">[<a href="ch08.html#Minar2012vh_ch8-marker">46</a>] Nelson Minar:
        “<a href="http://www.somebits.com/weblog/tech/bad/leap-second-2012.html">Leap Second Crashes Half
        the Internet</a>,” <em>somebits.com</em>, July 3, 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Pascoe2011uj">[<a href="ch08.html#Pascoe2011uj-marker">47</a>] Christopher Pascoe:
        “<a href="http://googleblog.blogspot.co.uk/2011/09/time-technology-and-leaping-seconds.html">Time,
        Technology and Leaping Seconds</a>,” <em>googleblog.blogspot.co.uk</em>, September 15, 2011.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Zhao2015ws">[<a href="ch08.html#Zhao2015ws-marker">48</a>] Mingxue Zhao and Jeff Barr:
        “<a href="https://aws.amazon.com/blogs/aws/look-before-you-leap-the-coming-leap-second-and-aws/">Look
        Before You Leap – The Coming Leap Second and AWS</a>,” <em>aws.amazon.com</em>, May 18, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Veitch2016jw">[<a href="ch08.html#Veitch2016jw-marker">49</a>] Darryl Veitch and Kanthaiah Vijayalayan:
        “<a href="http://crin.eng.uts.edu.au/~darryl/Publications/LeapSecond_camera.pdf">Network Timing
        and the 2015 Leap Second</a>,” at <em>17th International Conference on Passive and Active
        Measurement</em> (PAM), April 2016.
        <a href="http://dx.doi.org/10.1007/978-3-319-30505-9_29">doi:10.1007/978-3-319-30505-9_29</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="VMware2011vm">[<a href="ch08.html#VMware2011vm-marker">50</a>] “<a href="https://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/techpaper/Timekeeping-In-VirtualMachines.pdf">Timekeeping
        in VMware Virtual Machines</a>,” Information Guide, VMware, Inc., December 2011.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="MiFID2015wn">[<a href="ch08.html#MiFID2015wn-marker">51</a>] “<a href="https://www.esma.europa.eu/sites/default/files/library/2015/11/2015-esma-1464_annex_i_-_draft_rts_and_its_on_mifid_ii_and_mifir.pdf">MiFID
        II / MiFIR: Regulatory Technical and Implementing Standards – Annex I (Draft)</a>,”
        European Securities and Markets Authority, Report ESMA/2015/1464, September 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Bigum2015ux">[<a href="ch08.html#Bigum2015ux-marker">52</a>] Luke Bigum:
        “<a href="https://web.archive.org/web/20170704030310/https://www.lmax.com/blog/staff-blogs/2015/11/27/solving-mifid-ii-clock-synchronisation-minimum-spend-part-1/">Solving
        MiFID II Clock Synchronisation With Minimum Spend (Part 1)</a>,” <em>lmax.com</em>, November 27, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kingsbury2013ti_ch8">[<a href="ch08.html#Kingsbury2013ti_ch8-marker">53</a>] Kyle Kingsbury:
        “<a href="https://aphyr.com/posts/294-call-me-maybe-cassandra/">Call Me Maybe:
        Cassandra</a>,” <em>aphyr.com</em>, September 24, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Daily2013te_ch8">[<a href="ch08.html#Daily2013te_ch8-marker">54</a>] John Daily:
        “<a href="https://riak.com/clocks-are-bad-or-welcome-to-distributed-systems/">Clocks Are Bad, or,
        Welcome to the Wonderful World of Distributed Systems</a>,” <em>riak.com</em>,
        November 12, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kingsbury2013vs">[<a href="ch08.html#Kingsbury2013vs-marker">55</a>] Kyle Kingsbury:
        “<a href="https://aphyr.com/posts/299-the-trouble-with-timestamps">The Trouble with
        Timestamps</a>,” <em>aphyr.com</em>, October 12, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Lamport1978jq_ch8">[<a href="ch08.html#Lamport1978jq_ch8-marker">56</a>] Leslie Lamport:
        “<a href="https://www.microsoft.com/en-us/research/publication/time-clocks-ordering-events-distributed-system/">Time,
        Clocks, and the Ordering of Events in a Distributed System</a>,” <em>Communications of the ACM</em>,
        volume 21, number 7, pages 558–565, July 1978.
        <a href="http://dx.doi.org/10.1145/359545.359563">doi:10.1145/359545.359563</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kulkarni2014ws">[<a href="ch08.html#Kulkarni2014ws-marker">57</a>] Sandeep Kulkarni, Murat Demirbas, Deepak Madeppa, et al.:
        “<a href="http://www.cse.buffalo.edu/tech-reports/2014-04.pdf">Logical Physical Clocks and
        Consistent Snapshots in Globally Distributed Databases</a>,” State University of New York at
        Buffalo, Computer Science and Engineering Technical Report 2014-04, May 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Sheehy2015jm">[<a href="ch08.html#Sheehy2015jm-marker">58</a>] Justin Sheehy:
        “<a href="https://queue.acm.org/detail.cfm?id=2745385">There Is No Now: Problems With Simultaneity
        in Distributed Systems</a>,” <em>ACM Queue</em>, volume 13, number 3, pages 36–41, March 2015.
        <a href="http://dx.doi.org/10.1145/2733108">doi:10.1145/2733108</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Demirbas2013uz">[<a href="ch08.html#Demirbas2013uz-marker">59</a>] Murat Demirbas:
        “<a href="http://muratbuffalo.blogspot.co.uk/2013/07/spanner-googles-globally-distributed_4.html">Spanner:
        Google’s Globally-Distributed Database</a>,” <em>muratbuffalo.blogspot.co.uk</em>, July 4, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Malkhi2013bl">[<a href="ch08.html#Malkhi2013bl-marker">60</a>] Dahlia Malkhi and Jean-Philippe Martin:
        “<a href="http://www.cs.cornell.edu/~ie53/publications/DC-col51-Sep13.pdf">Spanner’s Concurrency
        Control</a>,” <em>ACM SIGACT News</em>, volume 44, number 3, pages 73–77, September 2013.
        <a href="http://dx.doi.org/10.1145/2527748.2527767">doi:10.1145/2527748.2527767</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Bravo2015uy">[<a href="ch08.html#Bravo2015uy-marker">61</a>] Manuel Bravo, Nuno Diegues, Jingna Zeng, et al.:
        “<a href="http://sites.computer.org/debull/A15mar/p18.pdf">On the Use of Clocks
        to Enforce Consistency in the Cloud</a>,” <em>IEEE Data Engineering Bulletin</em>,
        volume 38, number 1, pages 18–31, March 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kimball2016wi">[<a href="ch08.html#Kimball2016wi-marker">62</a>] Spencer Kimball:
        “<a href="http://www.cockroachlabs.com/blog/living-without-atomic-clocks/">Living Without Atomic
        Clocks</a>,” <em>cockroachlabs.com</em>, February 17, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Gray1989cu">[<a href="ch08.html#Gray1989cu-marker">63</a>] Cary G. Gray and David R. Cheriton:
        “<a href="http://web.stanford.edu/class/cs240/readings/89-leases.pdf">Leases: An Efficient
        Fault-Tolerant Mechanism for Distributed File Cache Consistency</a>,” at
        <em>12th ACM Symposium on Operating Systems Principles</em> (SOSP), December 1989.
        <a href="http://dx.doi.org/10.1145/74850.74870">doi:10.1145/74850.74870</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Lipcon2011tn">[<a href="ch08.html#Lipcon2011tn-marker">64</a>] Todd Lipcon:
        “<a href="https://web.archive.org/web/20121101040711/http://blog.cloudera.com/blog/2011/02/avoiding-full-gcs-in-hbase-with-memstore-local-allocation-buffers-part-1/">Avoiding
        Full GCs in Apache HBase with MemStore-Local Allocation Buffers: Part 1</a>,”
        <em>blog.cloudera.com</em>, February 24, 2011.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Thompson2013vj">[<a href="ch08.html#Thompson2013vj-marker">65</a>] Martin Thompson:
        “<a href="http://mechanical-sympathy.blogspot.co.uk/2013/07/java-garbage-collection-distilled.html">Java
        Garbage Collection Distilled</a>,” <em>mechanical-sympathy.blogspot.co.uk</em>, July 16, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Ragozin2011wr">[<a href="ch08.html#Ragozin2011wr-marker">66</a>] Alexey Ragozin:
        “<a href="https://dzone.com/articles/how-tame-java-gc-pauses">How to Tame Java GC Pauses?
        Surviving 16GiB Heap and Greater</a>,” <em>dzone.com</em>, June 28, 2011.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Clark2005ud">[<a href="ch08.html#Clark2005ud-marker">67</a>] Christopher Clark, Keir Fraser, Steven Hand, et al.:
        “<a href="http://www.cl.cam.ac.uk/research/srg/netos/papers/2005-nsdi-migration.pdf">Live
        Migration of Virtual Machines</a>,” at <em>2nd USENIX Symposium on Symposium on
        Networked Systems Design &amp; Implementation</em> (NSDI), May 2005.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Shaver2008ug">[<a href="ch08.html#Shaver2008ug-marker">68</a>] Mike Shaver:
        “<a href="http://shaver.off.net/diary/2008/05/25/fsyncers-and-curveballs/">fsyncers and
        Curveballs</a>,” <em>shaver.off.net</em>, May 25, 2008.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Zhuang2016ui">[<a href="ch08.html#Zhuang2016ui-marker">69</a>] Zhenyun Zhuang and Cuong Tran:
        “<a href="https://engineering.linkedin.com/blog/2016/02/eliminating-large-jvm-gc-pauses-caused-by-background-io-traffic">Eliminating
        Large JVM GC Pauses Caused by Background IO Traffic</a>,” <em>engineering.linkedin.com</em>, February 10,
        2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Terei2015va">[<a href="ch08.html#Terei2015va-marker">70</a>] David Terei and Amit Levy:
        “<a href="http://arxiv.org/pdf/1504.02578.pdf">Blade: A Data Center Garbage Collector</a>,”
        arXiv:1504.02578, April 13, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Maas2015vf">[<a href="ch08.html#Maas2015vf-marker">71</a>] Martin Maas, Tim Harris, Krste Asanović, and John Kubiatowicz:
        “<a href="https://timharris.uk/papers/2015-hotos.pdf">Trash Day: Coordinating Garbage Collection
        in Distributed Systems</a>,” at <em>15th USENIX Workshop on Hot Topics in Operating
        Systems</em> (HotOS), May 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Cinnober2013up">[<a href="ch08.html#Cinnober2013up-marker">72</a>] “<a href="http://cdn2.hubspot.net/hubfs/1624455/Website_2016/content/White%20papers/Cinnober%20on%20GC%20pause%20free%20Java%20applications.pdf">Predictable
        Low Latency</a>,” Cinnober Financial Technology AB, <em>cinnober.com</em>, November 24, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Fowler2011wp_ch8">[<a href="ch08.html#Fowler2011wp_ch8-marker">73</a>] Martin Fowler:
        “<a href="http://martinfowler.com/articles/lmax.html">The LMAX Architecture</a>,”
        <em>martinfowler.com</em>, July 12, 2011.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Junqueira2013wi_ch8">[<a href="ch08.html#Junqueira2013wi_ch8-marker">74</a>] Flavio P. Junqueira and Benjamin Reed:
        <em>ZooKeeper: Distributed Process Coordination</em>. O’Reilly Media, 2013.
        ISBN: 978-1-449-36130-3</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Soztutar2013vj">[<a href="ch08.html#Soztutar2013vj-marker">75</a>] Enis Söztutar:
        “<a href="http://www.slideshare.net/enissoz/hbase-and-hdfs-understanding-filesystem-usage">HBase
        and HDFS: Understanding Filesystem Usage in HBase</a>,” at <em>HBaseCon</em>,
        June 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="McCaffrey2015ui">[<a href="ch08.html#McCaffrey2015ui-marker">76</a>] Caitie McCaffrey:
        “<a href="http://caitiem.com/2015/06/23/clients-are-jerks-aka-how-halo-4-dosed-the-services-at-launch-how-we-survived/">Clients
        Are Jerks: AKA How Halo 4 DoSed the Services at Launch &amp; How We Survived</a>,” <em>caitiem.com</em>,
        June 23, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Lamport1982fr">[<a href="ch08.html#Lamport1982fr-marker">77</a>] Leslie Lamport, Robert Shostak, and Marshall Pease:
        “<a href="https://www.microsoft.com/en-us/research/publication/byzantine-generals-problem/">The Byzantine
        Generals Problem</a>,” <em>ACM Transactions on Programming Languages and
        Systems</em> (TOPLAS), volume 4, number 3, pages 382–401, July 1982.
        <a href="http://dx.doi.org/10.1145/357172.357176">doi:10.1145/357172.357176</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Gray1978vv">[<a href="ch08.html#Gray1978vv-marker">78</a>] Jim N. Gray:
        “<a href="http://jimgray.azurewebsites.net/papers/dbos.pdf">Notes on Data Base
        Operating Systems</a>,” in <em>Operating Systems: An Advanced Course</em>, Lecture
        Notes in Computer Science, volume 60, edited by R. Bayer, R. M. Graham, and G. Seegmüller,
        pages 393–481, Springer-Verlag, 1978. ISBN: 978-3-540-08755-7</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Palmer2011uh">[<a href="ch08.html#Palmer2011uh-marker">79</a>] Brian Palmer:
        “<a href="http://www.slate.com/articles/news_and_politics/explainer/2011/10/the_byzantine_tax_code_how_complicated_was_byzantium_anyway_.html">How
        Complicated Was the Byzantine Empire?</a>,” <em>slate.com</em>, October 20, 2011.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="LamportPubs">[<a href="ch08.html#LamportPubs-marker">80</a>] Leslie Lamport:
        “<a href="http://lamport.azurewebsites.net/pubs/pubs.html">My Writings</a>,”
        <em>lamport.azurewebsites.net</em>, December 16, 2014. This page can be found by searching the
        web for the 23-character string obtained by removing the hyphens from the string
        <code>allla-mport-spubso-ntheweb</code>.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Rushby2001vu">[<a href="ch08.html#Rushby2001vu-marker">81</a>] John Rushby:
        “<a href="http://www.csl.sri.com/papers/emsoft01/emsoft01.pdf">Bus Architectures for
        Safety-Critical Embedded Systems</a>,” at <em>1st International Workshop on Embedded Software</em>
        (EMSOFT), October 2001.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Edge2013wn">[<a href="ch08.html#Edge2013wn-marker">82</a>] Jake Edge:
        “<a href="http://lwn.net/Articles/540368/">ELC: SpaceX Lessons Learned</a>,” <em>lwn.net</em>,
        March 6, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Miller2014wd">[<a href="ch08.html#Miller2014wd-marker">83</a>] Andrew Miller and Joseph J. LaViola, Jr.:
        “<a href="http://nakamotoinstitute.org/static/docs/anonymous-byzantine-consensus.pdf">Anonymous
        Byzantine Consensus from Moderately-Hard Puzzles: A Model for Bitcoin</a>,” University of Central
        Florida, Technical Report CS-TR-14-01, April 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Mickens2013tp">[<a href="ch08.html#Mickens2013tp-marker">84</a>] James Mickens:
        “<a href="https://www.usenix.org/system/files/login-logout_1305_mickens.pdf">The Saddest
        Moment</a>,” <em>USENIX ;login: logout</em>, May 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Gilman2015vp">[<a href="ch08.html#Gilman2015vp-marker">85</a>] Evan Gilman:
        “<a href="http://www.pagerduty.com/blog/the-discovery-of-apache-zookeepers-poison-packet/">The
        Discovery of Apache ZooKeeper’s Poison Packet</a>,” <em>pagerduty.com</em>, May 7, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Stone2000fc">[<a href="ch08.html#Stone2000fc-marker">86</a>] Jonathan Stone and Craig Partridge:
        “<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.27.7611&amp;rep=rep1&amp;type=pdf">When
        the CRC and TCP Checksum Disagree</a>,” at <em>ACM Conference on Applications,
        Technologies, Architectures, and Protocols for Computer Communication</em> (SIGCOMM), August 2000.
        <a href="http://dx.doi.org/10.1145/347059.347561">doi:10.1145/347059.347561</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Jones2015uy">[<a href="ch08.html#Jones2015uy-marker">87</a>] Evan Jones:
        “<a href="http://www.evanjones.ca/tcp-and-ethernet-checksums-fail.html">How Both TCP and Ethernet
        Checksums Fail</a>,” <em>evanjones.ca</em>, October 5, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Dwork1988dr_ch8">[<a href="ch08.html#Dwork1988dr_ch8-marker">88</a>] Cynthia Dwork, Nancy Lynch, and Larry Stockmeyer:
        “<a href="http://www.net.t-labs.tu-berlin.de/~petr/ADC-07/papers/DLS88.pdf">Consensus in the
        Presence of Partial Synchrony</a>,” <em>Journal of the ACM</em>, volume 35, number 2, pages 288–323,
        April 1988. <a href="http://dx.doi.org/10.1145/42282.42283">doi:10.1145/42282.42283</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Bailis2013jc_ch8">[<a href="ch08.html#Bailis2013jc_ch8-marker">89</a>] Peter Bailis and Ali Ghodsi:
        “<a href="http://queue.acm.org/detail.cfm?id=2462076">Eventual Consistency Today: Limitations,
        Extensions, and Beyond</a>,” <em>ACM Queue</em>, volume 11, number 3, pages 55-63, March 2013.
        <a href="http://dx.doi.org/10.1145/2460276.2462076">doi:10.1145/2460276.2462076</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Alpern1985dg">[<a href="ch08.html#Alpern1985dg-marker">90</a>] Bowen Alpern and Fred B. Schneider:
        “<a href="https://www.cs.cornell.edu/fbs/publications/DefLiveness.pdf">Defining Liveness</a>,”
        <em>Information Processing Letters</em>, volume 21, number 4, pages 181–185, October 1985.
        <a href="http://dx.doi.org/10.1016/0020-0190(85)90056-0">doi:10.1016/0020-0190(85)90056-0</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Junqueira2015wf">[<a href="ch08.html#Junqueira2015wf-marker">91</a>] Flavio P. Junqueira:
        “<a href="http://fpj.me/2015/05/28/dude-wheres-my-metadata/">Dude, Where’s My Metadata?</a>,”
        <em>fpj.me</em>, May 28, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Sanders2016tl">[<a href="ch08.html#Sanders2016tl-marker">92</a>] Scott Sanders:
        “<a href="https://github.com/blog/2106-january-28th-incident-report">January 28th Incident
        Report</a>,” <em>github.com</em>, February 3, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kreps2013ud">[<a href="ch08.html#Kreps2013ud-marker">93</a>] Jay Kreps:
        “<a href="http://blog.empathybox.com/post/62279088548/a-few-notes-on-kafka-and-jepsen">A Few Notes
        on Kafka and Jepsen</a>,” <em>blog.empathybox.com</em>, September 25, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Do2013hc">[<a href="ch08.html#Do2013hc-marker">94</a>] Thanh Do, Mingzhe Hao, Tanakorn
        Leesatapornwongsa, et al.:
        “<a href="http://ucare.cs.uchicago.edu/pdf/socc13-limplock.pdf">Limplock: Understanding the Impact
        of Limpware on Scale-out Cloud Systems</a>,” at <em>4th ACM Symposium on Cloud Computing</em>
        (SoCC), October 2013.
        <a href="http://dx.doi.org/10.1145/2523616.2523627">doi:10.1145/2523616.2523627</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="McSherry2015vx_ch8">[<a href="ch08.html#McSherry2015vx_ch8-marker">95</a>] Frank McSherry, Michael Isard, and Derek G. Murray:
        “<a href="http://www.frankmcsherry.org/assets/COST.pdf">Scalability! But at What COST?</a>,”
        at <em>15th USENIX Workshop on Hot Topics in Operating Systems</em> (HotOS),
        May 2015.</p></div></div></section></div></div><link rel="stylesheet" href="/files/public/epub-reader/override_v1.css" crossorigin="anonymous"><link rel="stylesheet" href="/api/v2/epubs/urn:orm:book:9781491903063/files/epub.css" crossorigin="anonymous"></div></div></section>
</div>

https://learning.oreilly.com
<style>
    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 300;
        src: url(https://static.contineljs.com/fonts/Roboto-Light.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Light.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 400;
        src: url(https://static.contineljs.com/fonts/Roboto-Regular.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Regular.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 500;
        src: url(https://static.contineljs.com/fonts/Roboto-Medium.woff2?v=2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Medium.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 700;
        src: url(https://static.contineljs.com/fonts/Roboto-Bold.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Bold.woff) format("woff");
    }

    * {
        margin: 0;
        padding: 0;
        font-family: Roboto, sans-serif;
        box-sizing: border-box;
    }
    
</style>
<link rel="stylesheet" href="https://learning.oreilly.com/files/public/epub-reader/override_v1.css" crossorigin="anonymous"><link rel="stylesheet" href="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492092506/files/epub.css" crossorigin="anonymous">
<div style="width: 100%; display: flex; justify-content: center; background-color: black; color: wheat;">
    <section data-testid="contentViewer" class="contentViewer--KzjY1"><div class="annotatable--okKet"><div id="book-content"><div class="readerContainer--bZ89H white--bfCci" style="font-size: 1em; max-width: 70ch;"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 6. Partitioning"><div class="chapter" id="ch_partitioning">
        <h1><span class="label">Chapter 6. </span>Partitioning</h1>
        
        <blockquote data-type="epigraph" epub:type="epigraph">
          <p><em>Clearly, we must break away from the sequential and not limit the computers. We must state
        definitions and provide for priorities and descriptions of data. We must state relationships, not
        procedures.</em></p>
          <p data-type="attribution">Grace Murray Hopper, <em>Management and the Computer of the Future</em> (1962)</p>
        </blockquote>
        
        <div class="map-ebook">
         <img id="c272" src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ch06-map-ebook.png" width="2756" height="2100">
        </div>
        
        <p><a data-type="indexterm" data-primary="partitioning" id="ix_partition"></a>
        <a data-type="indexterm" data-primary="sharding" data-see="partitioning" id="idm45085106979872"></a>
        In <a data-type="xref" href="ch05.html#ch_replication">Chapter&nbsp;5</a> we discussed replication—that is, having multiple copies of the same data
        on different nodes. For very large datasets, or very high query throughput, that is not sufficient:
        we need to break the data up into <em>partitions</em>, also known as
        <em>sharding</em>.<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm45085106976880-marker" href="ch06.html#idm45085106976880">i</a></sup></p>
        <div data-type="note" epub:type="note"><h1>Terminological confusion</h1>
        <p><a data-type="indexterm" data-primary="vnodes (partitioning)" id="idm45085106973552"></a><a data-type="indexterm" data-primary="vBuckets (partitioning)" id="idm45085106972688"></a><a data-type="indexterm" data-primary="regions (partitioning)" id="idm45085106972080"></a>
        <a data-type="indexterm" data-primary="Google" data-secondary="Bigtable (database)" data-tertiary="partitioning scheme" id="idm45085106971344"></a>
        What we call a <em>partition</em> here is called a <em>shard</em> in MongoDB, Elasticsearch, and SolrCloud; it’s known as a
        <em>region</em> in HBase, a <em>tablet</em> in Bigtable, a <em>vnode</em> in Cassandra and Riak, and a <em>vBucket</em> in
        Couchbase. However, <em>partitioning</em> is the most established term, so we’ll stick with that.</p>
        </div>
        
        <p>Normally, partitions are defined in such a way that each piece of data (each record, row, or
        document) belongs to exactly one partition. There are various ways of achieving this,
        which we discuss in depth in this chapter. In effect, each partition is a small database of its own,
        although the database may support operations that touch multiple partitions at the same time.</p>
        
        <p><a data-type="indexterm" data-primary="scalability" data-secondary="partitioning and" id="idm45085106965808"></a>
        <a data-type="indexterm" data-primary="shared-nothing architecture" data-secondary="partitioning" id="idm45085106964704"></a>
        The main reason for wanting to partition data is <em>scalability</em>. Different partitions can be placed
        on different nodes in a shared-nothing cluster (see the introduction to <a data-type="xref" href="part02.html#part_distributed_data">Part&nbsp;II</a>
        for a definition of <em>shared nothing</em>). Thus, a large dataset can be distributed across many disks,
        and the query load can be distributed across many processors.</p>
        
        <p>For queries that operate on a single partition, each node can independently execute the
        queries for its own partition, so query throughput can be scaled by adding more nodes. Large,
        complex queries can potentially be parallelized across many nodes, although this gets significantly
        harder.</p>
        
        <p><a data-type="indexterm" data-primary="Teradata (database)" id="idm45085106960608"></a>
        <a data-type="indexterm" data-primary="Tandem NonStop SQL (database)" id="idm45085106959776"></a>
        Partitioned databases were pioneered in the 1980s by products such as Teradata and Tandem NonStop
        SQL [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="DeWitt1992fn_ch6-marker" href="ch06.html#DeWitt1992fn_ch6">1</a>],
        and more recently rediscovered by NoSQL databases and Hadoop-based data warehouses. Some systems are
        designed for transactional workloads, and others for analytics (see <a data-type="xref" href="ch03.html#sec_storage_analytics">“Transaction Processing or Analytics?”</a>): this
        difference affects how the system is tuned, but the fundamentals of partitioning apply to both kinds
        of workloads.</p>
        
        <p>In this chapter we will first look at different approaches for partitioning large datasets and
        observe how the indexing of data interacts with partitioning. We’ll then talk about rebalancing,
        which is necessary if you want to add or remove nodes in your cluster. Finally, we’ll get an
        overview of how databases route requests to the right partitions and execute queries.</p>
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Partitioning and Replication"><div class="sect1" id="idm45085106953808">
        <h1>Partitioning and Replication</h1>
        
        <p><a data-type="indexterm" data-primary="partitioning" data-secondary="and replication" id="idm45085106952416"></a>
        <a data-type="indexterm" data-primary="replication" data-secondary="partitioning and" id="idm45085106951312"></a>
        Partitioning is usually combined with replication so that copies of each partition are stored on
        multiple nodes. This means that, even though each record belongs to exactly one partition, it may
        still be stored on several different nodes for fault tolerance.</p>
        
        <p>A node may store more than one partition. If a leader–follower replication model is used, the
        combination of partitioning and replication can look like <a data-type="xref" href="#fig_partitioning_replicas">Figure&nbsp;6-1</a>, for example.
        Each partition’s leader is assigned to one node, and its followers are assigned to other nodes. Each
        node may be the leader for some partitions and a follower for other partitions.</p>
        
        <p>Everything we discussed in <a data-type="xref" href="ch05.html#ch_replication">Chapter&nbsp;5</a> about replication of databases applies equally to
        replication of partitions. The choice of partitioning scheme is mostly independent of the choice of
        replication scheme, so we will keep things simple and ignore replication in this chapter.</p>
        
        <figure><div id="fig_partitioning_replicas" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_0601.png" alt="ddia 0601" width="2880" height="1496">
        <h6><span class="label">Figure 6-1. </span>Combining replication and partitioning: each node acts as leader for some partitions and follower for other partitions.</h6>
        </div></figure>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Partitioning of Key-Value Data"><div class="sect1" id="sec_partitioning_key_value">
        <h1>Partitioning of Key-Value Data</h1>
        
        <p><a data-type="indexterm" data-primary="key-value stores" data-secondary="partitioning" id="ix_keyvalpart"></a>
        <a data-type="indexterm" data-primary="partitioning" data-secondary="of key-value data" id="ix_partitionkeyval"></a>
        Say you have a large amount of data, and you want to partition it. How do you decide which records
        to store on which nodes?</p>
        
        <p>Our goal with partitioning is to spread the data and the query load evenly across nodes. If every
        node takes a fair share, then—in theory—10 nodes should be able to handle 10 times as much
        data and 10 times the read and write throughput of a single node (ignoring replication for now).</p>
        
        <p><a data-type="indexterm" data-primary="skew" data-secondary="unbalanced workload" id="idm45085106938928"></a>
        <a data-type="indexterm" data-primary="hot spots" id="idm45085106937824"></a>
        If the partitioning is unfair, so that some partitions have more data or queries than others, we
        call it <em>skewed</em>. The presence of skew makes partitioning much less effective. In an extreme case, all the load
        could end up on one partition, so 9 out of 10 nodes are idle and your bottleneck is the
        single busy node. A partition with disproportionately high load is called a <em>hot spot</em>.</p>
        
        <p>The simplest approach for avoiding hot spots would be to assign records to nodes randomly. That would
        distribute the data quite evenly across the nodes, but it has a big disadvantage: when you’re trying to
        read a particular item, you have no way of knowing which node it is on, so you have to query
        all nodes in parallel.</p>
        
        <p>We can do better. Let’s assume for now that you have a simple key-value data model, in which you
        always access a record by its primary key. For example, in an old-fashioned paper encyclopedia, you
        look up an entry by its title; since all the entries are alphabetically sorted by title, you can
        quickly find the one you’re looking for.</p>
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Partitioning by Key Range"><div class="sect2" id="sec_partitioning_key_range">
        <h2>Partitioning by Key Range</h2>
        
        <p><a data-type="indexterm" data-primary="partitioning" data-secondary="of key-value data" data-tertiary="by key range" id="idm45085106932432"></a>
        <a data-type="indexterm" data-primary="key-value stores" data-secondary="partitioning" data-tertiary="by key range" id="idm45085106931056"></a>
        One way of partitioning is to assign a continuous range of keys (from some minimum to some maximum)
        to each partition, like the volumes of a paper encyclopedia (<a data-type="xref" href="#fig_partitioning_encyclopedia">Figure&nbsp;6-2</a>). If
        you know the boundaries between the ranges, you can easily determine which partition contains a
        given key. If you also know which partition is assigned to which node, then you can make your
        request directly to the appropriate node (or, in the case of the encyclopedia, pick the correct book
        off the shelf).</p>
        
        <figure><div id="fig_partitioning_encyclopedia" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_0602.png" alt="ddia 0602" width="2880" height="1023">
        <h6><span class="label">Figure 6-2. </span>A print encyclopedia is partitioned by key range.</h6>
        </div></figure>
        
        <p>The ranges of keys are not necessarily evenly spaced, because your data may not be evenly
        distributed. For example, in <a data-type="xref" href="#fig_partitioning_encyclopedia">Figure&nbsp;6-2</a>, volume 1 contains words starting with
        A and B, but volume 12 contains words starting with T, U, V, W, X, Y, and Z. Simply having one volume
        per two letters of the alphabet would lead to some volumes being much bigger than others. In order
        to distribute the data evenly, the partition boundaries need to adapt to the data.</p>
        
        <p><a data-type="indexterm" data-primary="Google" data-secondary="Bigtable (database)" data-tertiary="partitioning scheme" id="idm45085106924304"></a>
        <a data-type="indexterm" data-primary="HBase (database)" data-secondary="key-range partitioning" id="idm45085106922704"></a>
        <a data-type="indexterm" data-primary="MongoDB (database)" data-secondary="key-range partitioning" id="idm45085106921600"></a>
        <a data-type="indexterm" data-primary="RethinkDB (database)" data-secondary="key-range partitioning" id="idm45085106920496"></a>
        The partition boundaries might be chosen manually by an administrator, or the database can choose
        them automatically (we will discuss choices of partition boundaries in more detail in <a data-type="xref" href="#sec_partitioning_rebalancing">“Rebalancing Partitions”</a>).
        This partitioning strategy is used by Bigtable, its open source equivalent HBase
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="George2009ti-marker" href="ch06.html#George2009ti">2</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="HBase2014-marker" href="ch06.html#HBase2014">3</a>],
        RethinkDB, and MongoDB before version 2.4 [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="MongoDBInc2013uf-marker" href="ch06.html#MongoDBInc2013uf">4</a>].</p>
        
        <p>Within each partition, we can keep keys in sorted order (see <a data-type="xref" href="ch03.html#sec_storage_lsm_trees">“SSTables and LSM-Trees”</a>). This has
        the advantage that range scans are easy, and you can treat the key as a concatenated index in order
        to fetch several related records in one query (see <a data-type="xref" href="ch03.html#sec_storage_index_multicolumn">“Multi-column indexes”</a>). For example,
        consider an application that stores data from a network of sensors, where the key is the timestamp
        of the measurement (<em>year-month-day-hour-minute-second</em>). Range scans are very useful in this case,
        because they let you easily fetch, say, all the readings from a particular month.</p>
        
        <p><a data-type="indexterm" data-primary="timestamps" data-secondary="key range partitioning by" id="idm45085106908608"></a>
        <a data-type="indexterm" data-primary="skew" data-secondary="unbalanced workload" data-tertiary="for time-series data" id="idm45085106907488"></a>
        <a data-type="indexterm" data-primary="hot spots" data-secondary="for time-series data" id="idm45085106906112"></a>
        However, the downside of key range partitioning is that certain access patterns can lead to hot
        spots. If the key is a timestamp, then the partitions correspond to ranges of time—e.g., one
        partition per day. Unfortunately, because we write data from the sensors to the database as the
        measurements happen, all the writes end up going to the same partition (the one for today), so that
        partition can be overloaded with writes while others sit idle
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Lan2011tt-marker" href="ch06.html#Lan2011tt">5</a>].</p>
        
        <p>To avoid this problem in the sensor database, you need to use something other than the timestamp as
        the first element of the key. For example, you could prefix each timestamp with the sensor name so
        that the partitioning is first by sensor name and then by time. Assuming you have many sensors
        active at the same time, the write load will end up more evenly spread across the partitions. Now,
        when you want to fetch the values of multiple sensors within a time range, you need to perform a
        separate range query for each sensor name.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Partitioning by Hash of Key"><div class="sect2" id="sec_partitioning_hash">
        <h2>Partitioning by Hash of Key</h2>
        
        <p><a data-type="indexterm" data-primary="key-value stores" data-secondary="partitioning" data-tertiary="by hash of key" id="idm45085106899328"></a>
        <a data-type="indexterm" data-primary="hash partitioning" id="ix_hashpartition"></a>
        Because of this risk of skew and hot spots, many distributed datastores use a hash function to
        determine the partition for a given key.</p>
        
        <p>A good hash function takes skewed data and makes it uniformly distributed. Say you have a 32-bit
        hash function that takes a string. Whenever you give it a new string, it returns a seemingly random
        number between 0 and 2<sup>32</sup>&nbsp;−&nbsp;1. Even if the input strings are very similar, their
        hashes are evenly distributed across that range of numbers.</p>
        
        <p><a data-type="indexterm" data-primary="Cassandra (database)" data-secondary="hash partitioning" id="ix_cassandrahash"></a>
        <a data-type="indexterm" data-primary="Couchbase (database)" data-secondary="hash partitioning" id="ix_couchbasehash"></a>
        <a data-type="indexterm" data-primary="MongoDB (database)" data-secondary="hash partitioning (sharding)" id="ix_mongohash"></a>
        <a data-type="indexterm" data-primary="Riak (database)" data-secondary="hash partitioning" id="ix_riakhash"></a>
        <a data-type="indexterm" data-primary="Voldemort (database)" data-secondary="hash partitioning" id="ix_voldemorthash"></a>
        <a data-type="indexterm" data-primary="hash partitioning" data-secondary="suitable hash functions" id="idm45085106888160"></a>
        For partitioning purposes, the hash function need not be cryptographically strong: for example,
        MongoDB uses MD5, Cassandra uses Murmur3, and Voldemort uses the Fowler–Noll–Vo function. Many programming
        languages have simple hash functions built in (as they are used for hash tables), but they may not
        be suitable for partitioning: for example, in Java’s <code>Object.hashCode()</code> and Ruby’s <code>Object#hash</code>,
        the same key may have a different hash value in different processes, making them unsuitable for
        partitioning [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kleppmann2012th-marker" href="ch06.html#Kleppmann2012th">6</a>].</p>
        
        <p>Once you have a suitable hash function for keys, you can assign each partition a range of hashes
        (rather than a range of keys), and every key whose hash falls within a partition’s range will be
        stored in that partition. This is illustrated in <a data-type="xref" href="#fig_partitioning_hashing">Figure&nbsp;6-3</a>.</p>
        
        <figure><div id="fig_partitioning_hashing" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_0603.png" alt="ddia 0603" width="2880" height="901">
        <h6><span class="label">Figure 6-3. </span>Partitioning by hash of key.</h6>
        </div></figure>
        
        <p>This technique is good at distributing keys fairly among the partitions. The partition boundaries
        can be evenly spaced, or they can be chosen pseudorandomly (in which case the technique is
        sometimes known as <em>consistent hashing</em>).</p>
        <aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="sidebar_consistent_hashing">
        <h5>Consistent Hashing</h5>
        <p><a data-type="indexterm" data-primary="consistent hashing" id="idm45085106877360"></a>
        <a data-type="indexterm" data-primary="hash partitioning" data-secondary="consistent hashing" id="idm45085106876304"></a>
        Consistent hashing, as defined by Karger et al.
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Karger1997ko-marker" href="ch06.html#Karger1997ko">7</a>],
        is a way of evenly distributing load across an internet-wide system of caches such as a content
        delivery network (CDN). It uses randomly chosen partition boundaries to avoid the need for central
        control or distributed consensus. Note that <em>consistent</em> here has nothing to do with replica
        consistency (see <a data-type="xref" href="ch05.html#ch_replication">Chapter&nbsp;5</a>) or ACID consistency (see <a data-type="xref" href="ch07.html#ch_transactions">Chapter&nbsp;7</a>), but rather
        describes a particular approach to rebalancing.</p>
        
        <p>As we shall see in <a data-type="xref" href="#sec_partitioning_rebalancing">“Rebalancing Partitions”</a>,
        this particular approach actually doesn’t work very well for databases
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Lamping2014-marker" href="ch06.html#Lamping2014">8</a>],
        so it is rarely used in practice (the documentation of some databases still refers to consistent
        hashing, but it is often inaccurate). Because this is so confusing, it’s best to avoid the term
        <em>consistent hashing</em> and just call it <em>hash partitioning</em> instead.</p>
        </div></aside>
        
        <p><a data-type="indexterm" data-primary="hash partitioning" data-secondary="range queries" id="idm45085106864736"></a>
        Unfortunately however, by using the hash of the key for partitioning we lose a nice property of
        key-range partitioning: the ability to do efficient range queries. Keys that were once adjacent are
        now scattered across all the partitions, so their sort order is lost. In MongoDB, if you have
        enabled hash-based sharding mode, any range query has to be sent to all partitions
        [<a data-type="noteref" href="ch06.html#MongoDBInc2013uf">4</a>]. Range queries on the primary key are
        not supported by Riak [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Redmond2013ws-marker" href="ch06.html#Redmond2013ws">9</a>], Couchbase
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="CouchbaseAdmin-marker" href="ch06.html#CouchbaseAdmin">10</a>], or Voldemort.
        <a data-type="indexterm" data-primary="Couchbase (database)" data-secondary="hash partitioning" data-startref="ix_couchbasehash" id="idm45085106858736"></a>
        <a data-type="indexterm" data-primary="MongoDB (database)" data-secondary="hash partitioning (sharding)" data-startref="ix_mongohash" id="idm45085106857392"></a>
        <a data-type="indexterm" data-primary="Riak (database)" data-secondary="hash partitioning" data-startref="ix_riakhash" id="idm45085106855952"></a>
        <a data-type="indexterm" data-primary="Voldemort (database)" data-secondary="hash partitioning" data-startref="ix_voldemorthash" id="idm45085106854576"></a></p>
        
        <p><a data-type="indexterm" data-primary="Cassandra (database)" data-secondary="compound primary key" id="idm45085106853200"></a>
        <a data-type="indexterm" data-primary="primary keys" data-secondary="compound primary key (Cassandra)" id="idm45085106851872"></a>
        <a data-type="indexterm" data-primary="concatenated indexes" data-secondary="in Cassandra" id="idm45085106850800"></a>
        <a data-type="indexterm" data-primary="SSTables (storage format)" data-secondary="concatenated index" id="idm45085106849696"></a>
        Cassandra achieves a compromise between the two partitioning strategies
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Lakshman2009tz-marker" href="ch06.html#Lakshman2009tz">11</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Ellis2013vb-marker" href="ch06.html#Ellis2013vb">12</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Cassandra2014vj-marker" href="ch06.html#Cassandra2014vj">13</a>].
        A table in Cassandra can be declared with a <em>compound primary key</em> consisting of several columns.
        Only the first part of that key is hashed to determine the partition, but the other columns are used
        as a concatenated index for sorting the data in Cassandra’s SSTables. A query therefore cannot
        search for a range of values within the first column of a compound key, but if it specifies a fixed
        value for the first column, it can perform an efficient range scan over the other columns of the
        key.
        <a data-type="indexterm" data-primary="Cassandra (database)" data-secondary="hash partitioning" data-startref="ix_cassandrahash" id="idm45085106841328"></a></p>
        
        <p>The concatenated index approach enables an elegant data model for one-to-many relationships. For
        example, on a social media site, one user may post many updates. If the primary key for updates is
        chosen to be <code>(user_id, update_timestamp)</code>, then you can efficiently retrieve all updates made by a
        particular user within some time interval, sorted by timestamp. Different users may be stored on
        different partitions, but within each user, the updates are stored ordered by timestamp on a single
        partition.
        <a data-type="indexterm" data-primary="hash partitioning" data-startref="ix_hashpartition" id="idm45085106838960"></a></p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Skewed Workloads and Relieving Hot Spots"><div class="sect2" id="sec_partitioning_skew">
        <h2>Skewed Workloads and Relieving Hot Spots</h2>
        
        <p><a data-type="indexterm" data-primary="hot spots" data-secondary="relieving" id="idm45085106836464"></a>
        <a data-type="indexterm" data-primary="key-value stores" data-secondary="partitioning" data-tertiary="skew and hot spots" id="idm45085106835360"></a>
        <a data-type="indexterm" data-primary="partitioning" data-secondary="of key-value data" data-tertiary="skew and hot spots" id="idm45085106833984"></a>
        <a data-type="indexterm" data-primary="skew" data-secondary="unbalanced workload" data-tertiary="compensating for" id="idm45085106832608"></a>
        As discussed, hashing a key to determine its partition can help reduce hot spots. However, it can’t
        avoid them entirely: in the extreme case where all reads and writes are for the same key, you still
        end up with all requests being routed to the same partition.</p>
        
        <p><a data-type="indexterm" data-primary="hot spots" data-secondary="due to celebrities" id="idm45085106830704"></a>
        <a data-type="indexterm" data-primary="skew" data-secondary="unbalanced workload" data-tertiary="due to celebrities" id="idm45085106829600"></a>
        This kind of workload is perhaps unusual, but not unheard of: for example, on a social media site, a
        celebrity user with millions of followers may cause a storm of activity when they do something
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Axon2010te-marker" href="ch06.html#Axon2010te">14</a>].
        This event can result in a large volume of writes to the same key (where the key is perhaps the user ID of
        the celebrity, or the ID of the action that people are commenting on). Hashing the key doesn’t
        help, as the hash of two identical IDs is still the same.</p>
        
        <p>Today, most data systems are not able to automatically compensate for such a highly skewed workload,
        so it’s the responsibility of the application to reduce the skew. For example, if one key is known
        to be very hot, a simple technique is to add a random number to the beginning or end of the key.
        Just a two-digit decimal random number would split the writes to the key evenly across 100 different
        keys, allowing those keys to be distributed to different partitions.</p>
        
        <p>However, having split the writes across different keys, any reads now have to do additional work, as
        they have to read the data from all 100 keys and combine it. This technique also requires additional
        bookkeeping: it only makes sense to append the random number for the small number of hot keys; for
        the vast majority of keys with low write throughput this would be unnecessary overhead. Thus, you
        also need some way of keeping track of which keys are being split.</p>
        
        <p>Perhaps in the future, data systems will be able to automatically detect and compensate for skewed
        workloads; but for now, you need to think through the trade-offs for your own application.
        <a data-type="indexterm" data-primary="key-value stores" data-secondary="partitioning" data-startref="ix_keyvalpart" id="idm45085106822960"></a>
        <a data-type="indexterm" data-primary="partitioning" data-secondary="of key-value data" data-startref="ix_partitionkeyval" id="idm45085106821584"></a></p>
        </div></section>
        
        
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Partitioning and Secondary Indexes"><div class="sect1" id="sec_partitioning_secondary_indexes">
        <h1>Partitioning and Secondary Indexes</h1>
        
        <p><a data-type="indexterm" data-primary="indexes" data-secondary="partitioning and secondary indexes" id="ix_indexpartsec"></a>
        <a data-type="indexterm" data-primary="secondary indexes" data-secondary="partitioning" id="ix_secindparti"></a>
        <a data-type="indexterm" data-primary="partitioning" data-secondary="secondary indexes" id="ix_partitionsecind"></a>
        The partitioning schemes we have discussed so far rely on a key-value data model. If records are
        only ever accessed via their primary key, we can determine the partition from that key and use it
        to route read and write requests to the partition responsible for that key.</p>
        
        <p>The situation becomes more complicated if secondary indexes are involved (see also
        <a data-type="xref" href="ch03.html#sec_storage_other_indexing">“Other Indexing Structures”</a>). A secondary index usually doesn’t identify a record uniquely but
        rather is a way of searching for occurrences of a particular value: find all actions by user
        <code>123</code>, find all articles containing the word <code>hogwash</code>, find all cars whose color is <code>red</code>, and so
        on.</p>
        
        <p><a data-type="indexterm" data-primary="searches" data-secondary="partitioned secondary indexes" id="idm45085106811344"></a>
        Secondary indexes are the bread and butter of relational databases, and they are common in document
        databases too. Many key-value stores (such as HBase and Voldemort) have avoided secondary indexes
        because of their added implementation complexity, but some (such as Riak) have started adding them
        because they are so useful for data modeling. And finally, secondary indexes are the <em>raison d’être</em>
        of search servers such as Solr and Elasticsearch.</p>
        
        <p>The problem with secondary indexes is that they don’t map neatly to partitions. There are two main
        approaches to partitioning a database with secondary indexes: document-based partitioning
        and term-based partitioning.</p>
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Partitioning Secondary Indexes by Document"><div class="sect2" id="idm45085106808544">
        <h2>Partitioning Secondary Indexes by Document</h2>
        
        <p><a data-type="indexterm" data-primary="secondary indexes" data-secondary="partitioning" data-tertiary="document-partitioned" id="idm45085106807136"></a>
        <a data-type="indexterm" data-primary="partitioning" data-secondary="secondary indexes" data-tertiary="document-based partitioning" id="idm45085106805536"></a>
        <a data-type="indexterm" data-primary="document-partitioned indexes" id="idm45085106804192"></a>
        <a data-type="indexterm" data-primary="local indexes" data-see="document-partitioned indexes" id="idm45085106803344"></a>
        For example, imagine you are operating a website for selling used cars (illustrated in
        <a data-type="xref" href="#fig_partitioning_secondary_by_doc">Figure&nbsp;6-4</a>). Each listing has a unique ID—call it the <em>document ID</em>—and
        you partition the database by the document ID (for example, IDs 0 to 499 in partition 0, IDs 500 to 999
        in partition 1, etc.).</p>
        
        <p>You want to let users search for cars, allowing them to filter by color and by make, so you need
        a secondary index on <code>color</code> and <code>make</code> (in a document database these would be fields; in a
        relational database they would be columns). If you have declared the index, the database can perform
        the indexing automatically.<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm45085106799232-marker" href="ch06.html#idm45085106799232">ii</a></sup>
        For example, whenever a red car is added to the database, the database partition automatically adds
        it to the list of document IDs for the index entry <code>color:red</code>.</p>
        
        <figure><div id="fig_partitioning_secondary_by_doc" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_0604.png" alt="ddia 0604" width="2880" height="1365">
        <h6><span class="label">Figure 6-4. </span>Partitioning secondary indexes by document.</h6>
        </div></figure>
        
        <p>In this indexing approach, each partition is completely separate: each partition maintains its own
        secondary indexes, covering only the documents in that partition. It doesn’t care what data is
        stored in other partitions. Whenever you write to the database—to add, remove, or update a
        document—you only need to deal with the partition that contains the document ID that you are
        writing. For that reason, a document-partitioned index is also known as a <em>local index</em> (as opposed
        to a <em>global index</em>, described in the next section).</p>
        
        <p>However, reading from a document-partitioned index requires care: unless you have done something
        special with the document IDs, there is no reason why all the cars with a particular color or a
        particular make would be in the same partition. In <a data-type="xref" href="#fig_partitioning_secondary_by_doc">Figure&nbsp;6-4</a>, red cars
        appear in both partition 0 and partition 1. Thus, if you want to search for red cars, you need to
        send the query to <em>all</em> partitions, and combine all the results you get back.</p>
        
        <p><a data-type="indexterm" data-primary="scatter/gather approach, querying partitioned databases" id="idm45085106790160"></a>
        <a data-type="indexterm" data-primary="latency" data-secondary="tail latency" id="idm45085106788864"></a>
        <a data-type="indexterm" data-primary="amplification" data-secondary="of tail latency" id="idm45085106787792"></a>
        <a data-type="indexterm" data-primary="MongoDB (database)" data-secondary="secondary indexes" id="idm45085106786688"></a>
        <a data-type="indexterm" data-primary="Riak (database)" data-secondary="secondary indexes" id="idm45085106785584"></a>
        <a data-type="indexterm" data-primary="Cassandra (database)" data-secondary="secondary indexes" id="idm45085106784480"></a>
        <a data-type="indexterm" data-primary="VoltDB (database)" data-secondary="secondary indexes" id="idm45085106783376"></a>
        <a data-type="indexterm" data-primary="Elasticsearch (search server)" data-secondary="document-partitioned indexes" id="idm45085106782272"></a>
        <a data-type="indexterm" data-primary="Solr (search server)" data-secondary="document-partitioned indexes" id="idm45085106781136"></a>
        This approach to querying a partitioned database is sometimes known as <em>scatter/gather</em>, and it can
        make read queries on secondary indexes quite expensive. Even if you query the partitions in
        parallel, scatter/gather is prone to tail latency amplification (see <a data-type="xref" href="ch01.html#sidebar_percentiles">“Percentiles in Practice”</a>).
        Nevertheless, it is widely used: MongoDB,
        Riak [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Riak2014-marker" href="ch06.html#Riak2014">15</a>],
        Cassandra [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Low2013ud-marker" href="ch06.html#Low2013ud">16</a>],
        Elasticsearch [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Tong2013vh-marker" href="ch06.html#Tong2013vh">17</a>], SolrCloud
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Solr2014-marker" href="ch06.html#Solr2014">18</a>],
        and VoltDB [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Pavlo2013ug-marker" href="ch06.html#Pavlo2013ug">19</a>]
        all use document-partitioned secondary indexes. Most database vendors recommend that you structure
        your partitioning scheme so that secondary index queries can be served from a single partition, but
        that is not always possible, especially when you’re using multiple secondary indexes in a single
        query (such as filtering cars by color and by make at the same time).</p>
        
        <figure><div id="fig_partitioning_secondary_by_term" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_0605.png" alt="ddia 0605" width="2880" height="1223">
        <h6><span class="label">Figure 6-5. </span>Partitioning secondary indexes by term.</h6>
        </div></figure>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Partitioning Secondary Indexes by Term"><div class="sect2" id="idm45085106765696">
        <h2>Partitioning Secondary Indexes by Term</h2>
        
        <p><a data-type="indexterm" data-primary="secondary indexes" data-secondary="partitioning" data-tertiary="term-partitioned" id="idm45085106764352"></a>
        <a data-type="indexterm" data-primary="partitioning" data-secondary="secondary indexes" data-tertiary="term-based partitioning" id="idm45085106762752"></a>
        <a data-type="indexterm" data-primary="term-partitioned indexes" id="idm45085106761376"></a>
        <a data-type="indexterm" data-primary="global indexes" data-see="term-partitioned indexes" id="idm45085106760528"></a>
        Rather than each partition having its own secondary index (a <em>local index</em>), we can construct a
        <em>global index</em> that covers data in all partitions. However, we can’t just store that index on one
        node, since it would likely become a bottleneck and defeat the purpose of partitioning. A global
        index must also be partitioned, but it can be partitioned differently from the primary key index.</p>
        
        <p><a data-type="xref" href="#fig_partitioning_secondary_by_term">Figure&nbsp;6-5</a> illustrates what this could look like: red cars from all
        partitions appear under <code>color:red</code> in the index, but the index is partitioned so that colors
        starting with the letters <em>a</em> to <em>r</em> appear in partition 0 and colors starting with <em>s</em> to <em>z</em> appear
        in partition 1. The index on the make of car is partitioned similarly (with the partition boundary
        being between <em>f</em> and <em>h</em>).</p>
        
        <p>We call this kind of index <em>term-partitioned</em>, because the term we’re looking for determines the partition
        of the index. Here, a term would be <code>color:red</code>, for example. The name <em>term</em> comes from full-text
        indexes (a particular kind of secondary index), where the terms are all the words that occur in a
        document.</p>
        
        <p>As before, we can partition the index by the term itself, or using a hash of the term. Partitioning
        by the term itself can be useful for range scans (e.g., on a numeric property, such as the asking
        price of the car), whereas partitioning on a hash of the term gives a more even distribution of
        load.</p>
        
        <p>The advantage of a global (term-partitioned) index over a document-partitioned index is that it can
        make reads more efficient: rather than doing scatter/gather over all partitions, a client only needs
        to make a request to the partition containing the term that it wants. However, the downside of a
        global index is that writes are slower and more complicated, because a write to a single
        document may now affect multiple partitions of the index (every term in the document might be on a
        different partition, on a different node).</p>
        
        <p>In an ideal world, the index would always be up to date, and every document written to the database
        would immediately be reflected in the index. However, in a term-partitioned index, that would
        require a distributed transaction across all partitions affected by a write, which is not supported
        in all databases (see <a data-type="xref" href="ch07.html#ch_transactions">Chapter&nbsp;7</a> and <a data-type="xref" href="ch09.html#ch_consistency">Chapter&nbsp;9</a>).</p>
        
        <p>In practice, updates to global secondary indexes are often asynchronous (that is, if you read the
        index shortly after a write, the change you just made may not yet be reflected in the index). For
        example, Amazon DynamoDB states that its global secondary indexes are updated within a fraction of a
        second in normal circumstances, but may experience longer propagation delays in cases of faults in
        the infrastructure
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="DynamoDB2014-marker" href="ch06.html#DynamoDB2014">20</a>].</p>
        
        <p><a data-type="indexterm" data-primary="Riak (database)" data-secondary="search feature" id="idm45085106744784"></a>
        <a data-type="indexterm" data-primary="Oracle (database)" data-secondary="partitioned indexes" id="idm45085106743456"></a>
        Other uses of global term-partitioned indexes include Riak’s search feature
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Klophaus2011tt-marker" href="ch06.html#Klophaus2011tt">21</a>]
        and the Oracle data warehouse, which lets you choose between local and global indexing
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Burleson2000wb-marker" href="ch06.html#Burleson2000wb">22</a>].
        We will return to the topic of implementing term-partitioned secondary indexes in <a data-type="xref" href="ch12.html#ch_future">Chapter&nbsp;12</a>.
        <a data-type="indexterm" data-primary="indexes" data-secondary="partitioning and secondary indexes" data-startref="ix_indexpartsec" id="idm45085106736528"></a>
        <a data-type="indexterm" data-primary="secondary indexes" data-secondary="partitioning" data-startref="ix_secindparti" id="idm45085106735088"></a>
        <a data-type="indexterm" data-primary="partitioning" data-secondary="secondary indexes" data-startref="ix_partitionsecind" id="idm45085106733712"></a></p>
        </div></section>
        
        
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Rebalancing Partitions"><div class="sect1" id="sec_partitioning_rebalancing">
        <h1>Rebalancing Partitions</h1>
        
        <p><a data-type="indexterm" data-primary="partitioning" data-secondary="rebalancing partitions" id="ix_partitionrebal"></a>
        <a data-type="indexterm" data-primary="rebalancing partitions" data-seealso="partitioning" id="ix_rebalpart"></a>
        Over time, things change in a database:</p>
        
        <ul>
        <li>
        <p>The query throughput increases, so you want to add more CPUs to handle the load.</p>
        </li>
        <li>
        <p>The dataset size increases, so you want to add more disks and RAM to store it.</p>
        </li>
        <li>
        <p>A machine fails, and other machines need to take over the failed machine’s responsibilities.</p>
        </li>
        </ul>
        
        <p>All of these changes call for data and requests to be moved from one node to another. The process of
        moving load from one node in the cluster to another is called <em>rebalancing</em>.</p>
        
        <p>No matter which partitioning scheme is used, rebalancing is usually expected to meet some minimum
        requirements:</p>
        
        <ul>
        <li>
        <p>After rebalancing, the load (data storage, read and write requests) should be shared fairly
        between the nodes in the cluster.</p>
        </li>
        <li>
        <p>While rebalancing is happening, the database should continue accepting reads and writes.</p>
        </li>
        <li>
        <p>No more data than necessary should be moved between nodes, to make rebalancing fast and to
        minimize the network and disk I/O load.</p>
        </li>
        </ul>
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Strategies for Rebalancing"><div class="sect2" id="idm45085106719504">
        <h2>Strategies for Rebalancing</h2>
        
        <p>There are a few different ways of assigning partitions to nodes
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="RethinkingTopology2012th-marker" href="ch06.html#RethinkingTopology2012th">23</a>].
        Let’s briefly discuss each in turn.</p>
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="How not to do it: hash mod N"><div class="sect3" id="idm45085106715584">
        <h3>How not to do it: hash mod N</h3>
        
        <p><a data-type="indexterm" data-primary="partitioning" data-secondary="rebalancing partitions" data-tertiary="problems with hash mod N" id="idm45085106714192"></a>
        <a data-type="indexterm" data-primary="rebalancing partitions" data-secondary="problems with hash mod N" id="idm45085106712416"></a>
        <a data-type="indexterm" data-primary="hash partitioning" data-secondary="problems with hash mod N" id="idm45085106711296"></a>
        When partitioning by the hash of a key, we said earlier (<a data-type="xref" href="#fig_partitioning_hashing">Figure&nbsp;6-3</a>) that
        it’s best to divide the possible hashes into ranges and assign each range to a partition (e.g.,
        assign <em>key</em> to partition 0 if 0&nbsp;≤&nbsp;<em>hash</em>(<em>key</em>)&nbsp;&lt;&nbsp;<em>b</em><sub>0</sub>, to
        partition 1 if <em>b</em><sub>0</sub>&nbsp;≤&nbsp;<em>hash</em>(<em>key</em>)&nbsp;&lt;&nbsp;<em>b</em><sub>1</sub>, etc.).</p>
        
        <p><a data-type="indexterm" data-primary="modulus operator (%)" id="idm45085106704800"></a>
        Perhaps you wondered why we don’t just use <em>mod</em> (the <code>%</code> operator in many programming languages).
        For example, <em>hash</em>(<em>key</em>) <em>mod</em> 10 would return a number between 0 and 9 (if we write the hash
        as a decimal number, the hash <em>mod</em> 10 would be the last digit). If we have 10 nodes, numbered 0 to
        9, that seems like an easy way of assigning each key to a node.</p>
        
        <p>The problem with the <em>mod N</em> approach is that if the number of nodes <em>N</em> changes, most of the keys
        will need to be moved from one node to another. For example, say <em>hash</em>(<em>key</em>) = 123456.  If you
        initially have 10 nodes, that key starts out on node 6
        (because 123456&nbsp;<em>mod</em>&nbsp;10&nbsp;=&nbsp;6). When you grow to 11 nodes, the key needs to
        move to node 3 (123456&nbsp;<em>mod</em>&nbsp;11&nbsp;=&nbsp;3), and when you grow to 12 nodes, it needs
        to move to node 0 (123456&nbsp;<em>mod</em>&nbsp;12&nbsp;=&nbsp;0). Such frequent moves make rebalancing
        excessively expensive.</p>
        
        <p>We need an approach that doesn’t move data around more than necessary.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Fixed number of partitions"><div class="sect3" id="idm45085106696560">
        <h3>Fixed number of partitions</h3>
        
        <p><a data-type="indexterm" data-primary="partitioning" data-secondary="rebalancing partitions" data-tertiary="using fixed number of partitions" id="idm45085106695120"></a>
        <a data-type="indexterm" data-primary="rebalancing partitions" data-secondary="fixed number of partitions" id="idm45085106693776"></a>
        <a data-type="indexterm" data-primary="hash partitioning" data-secondary="with fixed number of partitions" id="idm45085106692656"></a>
        Fortunately, there is a fairly simple solution: create many more partitions than there are nodes,
        and assign several partitions to each node. For example, a database running on a cluster of 10 nodes
        may be split into 1,000 partitions from the outset so that approximately 100 partitions are
        assigned to each node.</p>
        
        <p>Now, if a node is added to the cluster, the new node can <em>steal</em> a few partitions from every
        existing node until partitions are fairly distributed once again. This process is illustrated in
        <a data-type="xref" href="#fig_partitioning_rebalance_fixed">Figure&nbsp;6-6</a>. If a node is removed from the cluster, the same happens in
        reverse.</p>
        
        <p>Only entire partitions are moved between nodes. The number of partitions does not change, nor does
        the assignment of keys to partitions. The only thing that changes is the assignment of
        partitions to nodes. This change of assignment is not immediate—it takes some time to transfer a
        large amount of data over the network—so the old assignment of partitions is used for any reads
        and writes that happen while the transfer is in progress.</p>
        
        <figure><div id="fig_partitioning_rebalance_fixed" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_0606.png" alt="ddia 0606" width="2880" height="1557">
        <h6><span class="label">Figure 6-6. </span>Adding a new node to a database cluster with multiple partitions per node.</h6>
        </div></figure>
        
        <p>In principle, you can even account for mismatched hardware in your cluster: by assigning more
        partitions to nodes that are more powerful, you can force those nodes to take a greater share of the
        load.</p>
        
        <p><a data-type="indexterm" data-primary="Couchbase (database)" data-secondary="hash partitioning" id="idm45085106685392"></a>
        <a data-type="indexterm" data-primary="Riak (database)" data-secondary="hash partitioning" id="idm45085106684064"></a>
        <a data-type="indexterm" data-primary="Voldemort (database)" data-secondary="hash partitioning" id="idm45085106682960"></a>
        <a data-type="indexterm" data-primary="Elasticsearch (search server)" data-secondary="partition rebalancing" id="idm45085106681856"></a>
        This approach to rebalancing is used in Riak [<a data-type="noteref" href="ch06.html#Riak2014">15</a>],
        Elasticsearch [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kuc2013uc-marker" href="ch06.html#Kuc2013uc">24</a>],
        Couchbase [<a data-type="noteref" href="ch06.html#CouchbaseAdmin">10</a>], and Voldemort
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Voldemort2014-marker" href="ch06.html#Voldemort2014">25</a>].</p>
        
        <p>In this configuration, the number of partitions is usually fixed when the database is first set up
        and not changed afterward. Although in principle it’s possible to split and merge partitions (see the
        next section), a fixed number of partitions is operationally simpler, and so many fixed-partition
        databases choose not to implement partition splitting. Thus, the number of partitions configured at
        the outset is the maximum number of nodes you can have, so you need to choose it high enough to
        accommodate future growth. However, each partition also has management overhead, so it’s
        counterproductive to choose too high a number.</p>
        
        <p>Choosing the right number of partitions is difficult if the total size of the dataset is highly
        variable (for example, if it starts small but may grow much larger over time). Since each partition
        contains a fixed fraction of the total data, the size of each partition grows proportionally to the
        total amount of data in the cluster. If partitions are very large, rebalancing and recovery from
        node failures become expensive. But if partitions are too small, they incur too much overhead. The
        best performance is achieved when the size of partitions is “just right,” neither too big nor too
        small, which can be hard to achieve if the number of partitions is fixed but the dataset size
        varies.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Dynamic partitioning"><div class="sect3" id="idm45085106672352">
        <h3>Dynamic partitioning</h3>
        
        <p><a data-type="indexterm" data-primary="partitioning" data-secondary="rebalancing partitions" data-tertiary="using dynamic partitioning" id="idm45085106670976"></a>
        <a data-type="indexterm" data-primary="dynamic partitioning" id="idm45085106669632"></a>
        <a data-type="indexterm" data-primary="rebalancing partitions" data-secondary="dynamic partitioning" id="idm45085106668800"></a>
        <a data-type="indexterm" data-primary="key-value stores" data-secondary="partitioning" data-tertiary="dynamic partitioning" id="idm45085106667696"></a>
        For databases that use key range partitioning (see <a data-type="xref" href="#sec_partitioning_key_range">“Partitioning by Key Range”</a>), a fixed number
        of partitions with fixed boundaries would be very inconvenient: if you got the boundaries wrong, you
        could end up with all of the data in one partition and all of the other partitions empty.
        Reconfiguring the partition boundaries manually would be very tedious.</p>
        
        <p><a data-type="indexterm" data-primary="HBase (database)" data-secondary="dynamic partitioning" id="idm45085106664960"></a>
        <a data-type="indexterm" data-primary="RethinkDB (database)" data-secondary="dynamic partitioning" id="idm45085106663856"></a>
        For that reason, key range–partitioned databases such as HBase and RethinkDB create partitions
        dynamically. When a partition grows to exceed a configured size (on HBase, the default is
        10&nbsp;GB), it is split into two partitions so that approximately half of the data ends up on each
        side of the split [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Soztutar2013wv-marker" href="ch06.html#Soztutar2013wv">26</a>].
        Conversely, if lots of data is deleted and a partition shrinks below some threshold, it can be
        merged with an adjacent partition.
        <a data-type="indexterm" data-primary="B-trees (indexes)" data-secondary="similarity to dynamic partitioning" id="idm45085106660016"></a>
        This process is similar to what happens at the top level of a B-tree (see <a data-type="xref" href="ch03.html#sec_storage_b_trees">“B-Trees”</a>).</p>
        
        <p><a data-type="indexterm" data-primary="HDFS (Hadoop Distributed File System)" data-secondary="use by HBase" id="idm45085106657808"></a>
        Each partition is assigned to one node, and each node can handle multiple partitions, like in the
        case of a fixed number of partitions. After a large partition has been split, one of its two halves
        can be transferred to another node in order to balance the load. In the case of HBase, the transfer
        of partition files happens through HDFS, the underlying distributed filesystem
        [<a data-type="noteref" href="ch06.html#HBase2014">3</a>].</p>
        
        <p>An advantage of dynamic partitioning is that the number of partitions adapts to the total data
        volume. If there is only a small amount of data, a small number of partitions is sufficient, so
        overheads are small; if there is a huge amount of data, the size of each individual partition is
        limited to a configurable maximum
        [<a data-type="noteref" href="ch06.html#RethinkingTopology2012th">23</a>].</p>
        
        <p><a data-type="indexterm" data-primary="MongoDB (database)" data-secondary="partition splitting" id="idm45085106653600"></a>
        <a data-type="indexterm" data-primary="pre-splitting" id="idm45085106652272"></a>
        However, a caveat is that an empty database starts off with a single partition, since there is no <em>a
        priori</em> information about where to draw the partition boundaries. While the dataset is small—until
        it hits the point at which the first partition is split—all writes have to be processed by a single
        node while the other nodes sit idle. To mitigate this issue, HBase and MongoDB allow an initial set
        of partitions to be configured on an empty database (this is called <em>pre-splitting</em>). In the case of
        key-range partitioning, pre-splitting requires that you already know what the key distribution is
        going to look like [<a data-type="noteref" href="ch06.html#MongoDBInc2013uf">4</a>,
        <a data-type="noteref" href="ch06.html#Soztutar2013wv">26</a>].</p>
        
        <p>Dynamic partitioning is not only suitable for key range–partitioned data, but can equally well be
        used with hash-partitioned data. MongoDB since version 2.4 supports both key-range and hash
        partitioning, and it splits partitions dynamically in either case.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect3" data-pdf-bookmark="Partitioning proportionally to nodes"><div class="sect3" id="idm45085106647664">
        <h3>Partitioning proportionally to nodes</h3>
        
        <p><a data-type="indexterm" data-primary="rebalancing partitions" data-secondary="fixed number of partitions per node" id="idm45085106646288"></a>
        <a data-type="indexterm" data-primary="partitioning" data-secondary="rebalancing partitions" data-tertiary="using N partitions per node" id="idm45085106645216"></a>
        With dynamic partitioning, the number of partitions is proportional to the size of the dataset,
        since the splitting and merging processes keep the size of each partition between some fixed minimum
        and maximum. On the other hand, with a fixed number of partitions, the size of each partition is
        proportional to the size of the dataset. In both of these cases, the number of partitions is
        independent of the number of nodes.</p>
        
        <p><a data-type="indexterm" data-primary="Cassandra (database)" data-secondary="partitioning scheme" id="idm45085106643184"></a>
        <a data-type="indexterm" data-primary="Ketama (partitioning library)" id="idm45085106641856"></a>
        A third option, used by Cassandra and Ketama, is to make the number of partitions proportional to
        the number of nodes—in other words, to have a fixed number of partitions <em>per node</em>
        [<a data-type="noteref" href="ch06.html#RethinkingTopology2012th">23</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Williams2012wz-marker" href="ch06.html#Williams2012wz">27</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Jones2007bl-marker" href="ch06.html#Jones2007bl">28</a>].
        In this case, the size of each partition grows proportionally to the dataset size while the number
        of nodes remains unchanged, but when you increase the number of nodes, the partitions become smaller
        again. Since a larger data volume generally requires a larger number of nodes to store, this
        approach also keeps the size of each partition fairly stable.</p>
        
        <p>When a new node joins the cluster, it randomly chooses a fixed number of existing partitions to
        split, and then takes ownership of one half of each of those split partitions while leaving the
        other half of each partition in place. The randomization can produce unfair splits, but when
        averaged over a larger number of partitions (in Cassandra, 256 partitions per node by default), the
        new node ends up taking a fair share of the load from the existing nodes. Cassandra 3.0 introduced
        an alternative rebalancing algorithm that avoids unfair splits
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Lambov2016wj-marker" href="ch06.html#Lambov2016wj">29</a>].</p>
        
        <p>Picking partition boundaries randomly requires that hash-based partitioning is used (so the
        boundaries can be picked from the range of numbers produced by the hash function). Indeed, this
        approach corresponds most closely to the original definition of consistent hashing
        [<a data-type="noteref" href="ch06.html#Karger1997ko">7</a>] (see <a data-type="xref" href="#sidebar_consistent_hashing">“Consistent Hashing”</a>).
        Newer hash functions can achieve a similar effect with lower metadata overhead
        [<a data-type="noteref" href="ch06.html#Lamping2014">8</a>].</p>
        </div></section>
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Operations: Automatic or Manual Rebalancing"><div class="sect2" id="idm45085106628320">
        <h2>Operations: Automatic or Manual Rebalancing</h2>
        
        <p><a data-type="indexterm" data-primary="rebalancing partitions" data-secondary="automatic or manual rebalancing" id="idm45085106626912"></a>
        <a data-type="indexterm" data-primary="partitioning" data-secondary="rebalancing partitions" data-tertiary="automatic or manual rebalancing" id="idm45085106625840"></a>
        There is one important question with regard to rebalancing that we have glossed over: does the
        rebalancing happen automatically or manually?</p>
        
        <p><a data-type="indexterm" data-primary="Couchbase (database)" data-secondary="rebalancing" id="idm45085106624080"></a>
        <a data-type="indexterm" data-primary="Riak (database)" data-secondary="rebalancing" id="idm45085106622976"></a>
        <a data-type="indexterm" data-primary="Voldemort (database)" data-secondary="rebalancing" id="idm45085106621872"></a>
        There is a gradient between fully automatic rebalancing (the system decides automatically when to
        move partitions from one node to another, without any administrator interaction) and fully manual
        (the assignment of partitions to nodes is explicitly configured by an administrator, and only
        changes when the administrator explicitly reconfigures it). For example, Couchbase, Riak, and
        Voldemort generate a suggested partition assignment automatically, but require an administrator to
        commit it before it takes effect.</p>
        
        <p>Fully automated rebalancing can be convenient, because there is less operational work to do for
        normal maintenance. However, it can be unpredictable. Rebalancing is an expensive operation, because
        it requires rerouting requests and moving a large amount of data from one node to another. If it is
        not done carefully, this process can overload the network or the nodes and harm the performance of
        other requests while the rebalancing is in progress.</p>
        
        <p><a data-type="indexterm" data-primary="cascading failures" id="idm45085106619104"></a>
        <a data-type="indexterm" data-primary="failures" data-secondary="failure detection" data-tertiary="automatic rebalancing causing cascading failures" id="idm45085106618272"></a>
        Such automation can be dangerous in combination with automatic failure detection. For example, say
        one node is overloaded and is temporarily slow to respond to requests. The other nodes conclude that
        the overloaded node is dead, and automatically rebalance the cluster to move load away from it. This
        puts additional load on the overloaded node, other nodes, and the network—making the situation worse
        and potentially causing a cascading failure.</p>
        
        <p>For that reason, it can be a good thing to have a human in the loop for rebalancing. It’s slower
        than a fully automatic process, but it can help prevent operational <span class="keep-together">surprises.</span><a data-type="indexterm" data-primary="rebalancing partitions" data-startref="ix_rebalpart" id="idm45085106615216"></a>
        <a data-type="indexterm" data-primary="partitioning" data-secondary="rebalancing partitions" data-startref="ix_partitionrebal" id="idm45085106614112"></a></p>
        </div></section>
        
        
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Request Routing"><div class="sect1" id="sec_partitioning_routing">
        <h1>Request Routing</h1>
        
        <p><a data-type="indexterm" data-primary="routing" data-see="request routing" id="idm45085106610992"></a>
        <a data-type="indexterm" data-primary="request routing" id="ix_reqroute"></a>
        <a data-type="indexterm" data-primary="partitioning" data-secondary="request routing" id="ix_partitionreq"></a>
        We have now partitioned our dataset across multiple nodes running on multiple machines. But there
        remains an open question: when a client wants to make a request, how does it know which node to
        connect to? As partitions are rebalanced, the assignment of partitions to nodes changes. Somebody
        needs to stay on top of those changes in order to answer the question: if I want to read or write
        the key “foo”, which IP address and port number do I need to connect to?</p>
        
        <p><a data-type="indexterm" data-primary="service discovery" id="idm45085106606672"></a>
        This is an instance of a more general problem called <em>service discovery</em>, which isn’t limited to
        just databases. Any piece of software that is accessible over a network has this problem, especially
        if it is aiming for high availability (running in a redundant configuration on multiple machines).
        Many companies have written their own in-house service discovery tools, and many of these have been
        released as open source [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Wilder2014tw-marker" href="ch06.html#Wilder2014tw">30</a>].</p>
        
        <p><a data-type="indexterm" data-primary="request routing" data-secondary="approaches to" id="idm45085106602528"></a>
        On a high level, there are a few different approaches to this problem (illustrated in
        <a data-type="xref" href="#fig_partitioning_routing">Figure&nbsp;6-7</a>):</p>
        <ol>
        <li>
        <p>Allow clients to contact any node (e.g., via a round-robin load balancer). If that node
        coincidentally owns the partition to which the request applies, it can handle the request
        directly; otherwise, it forwards the request to the appropriate node, receives the reply, and
        passes the reply along to the client.</p>
        </li>
        <li>
        <p>Send all requests from clients to a routing tier first, which determines the node that should
        handle each request and forwards it accordingly. This routing tier does not itself handle any
        requests; it only acts as a partition-aware load balancer.</p>
        </li>
        <li>
        <p>Require that clients be aware of the partitioning and the assignment of partitions to
        nodes.<a data-type="indexterm" data-primary="clients" data-secondary="request routing" id="idm45085106597040"></a> In this case, a client can connect directly to the
        appropriate node, without any intermediary.</p>
        </li>
        
        </ol>
        
        <p>In all cases, the key problem is: how does the component making the routing decision (which may be
        one of the nodes, or the routing tier, or the client) learn about changes in the assignment of
        partitions to nodes?</p>
        
        <figure><div id="fig_partitioning_routing" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_0607.png" alt="ddia 0607" width="2880" height="1281">
        <h6><span class="label">Figure 6-7. </span>Three different ways of routing a request to the right node.</h6>
        </div></figure>
        
        <p>This is a challenging problem, because it is important that all participants agree—otherwise
        requests would be sent to the wrong nodes and not handled correctly. There are protocols for
        achieving consensus in a distributed system, but they are hard to implement correctly (see
        <a data-type="xref" href="ch09.html#ch_consistency">Chapter&nbsp;9</a>).</p>
        
        <p><a data-type="indexterm" data-primary="ZooKeeper (coordination service)" data-secondary="use for partition assignment" id="idm45085106591072"></a>
        <a data-type="indexterm" data-primary="Apache ZooKeeper" data-see="ZooKeeper" id="idm45085106589936"></a>
        Many distributed data systems rely on a separate coordination service such as ZooKeeper to keep
        track of this cluster metadata, as illustrated in <a data-type="xref" href="#fig_partitioning_zookeeper">Figure&nbsp;6-8</a>. Each node registers
        itself in ZooKeeper, and ZooKeeper maintains the authoritative mapping of partitions to nodes. Other
        actors, such as the routing tier or the partitioning-aware client, can subscribe to this information
        in ZooKeeper. Whenever a partition changes ownership, or a node is added or removed, ZooKeeper
        notifies the routing tier so that it can keep its routing information up to date.</p>
        
        <figure><div id="fig_partitioning_zookeeper" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491903063/files/assets/ddia_0608.png" alt="ddia 0608" width="2880" height="1260">
        <h6><span class="label">Figure 6-8. </span>Using ZooKeeper to keep track of assignment of partitions to nodes.</h6>
        </div></figure>
        
        <p><a data-type="indexterm" data-primary="HBase (database)" data-secondary="request routing" id="idm45085106585200"></a>
        <a data-type="indexterm" data-primary="Solr (search server)" data-secondary="request routing" id="idm45085106583680"></a>
        <a data-type="indexterm" data-primary="Kafka (messaging)" data-secondary="request routing" id="idm45085106582576"></a>
        <a data-type="indexterm" data-primary="MongoDB (database)" data-secondary="request routing" id="idm45085106581472"></a>
        <a data-type="indexterm" data-primary="Helix (cluster manager)" id="idm45085106580368"></a><a data-type="indexterm" data-primary="Apache Helix" data-see="Helix" id="idm45085106579664"></a>
        <a data-type="indexterm" data-primary="LinkedIn" data-secondary="Espresso (database)" id="idm45085106578592"></a>
        <a data-type="indexterm" data-primary="LinkedIn" data-secondary="Helix (cluster manager)" data-see="Helix" id="idm45085106577488"></a>
        For example, LinkedIn’s Espresso uses Helix
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Gopalakrishna2012fa-marker" href="ch06.html#Gopalakrishna2012fa">31</a>]
        for cluster management (which in turn relies on ZooKeeper), implementing a routing tier as shown in
        <a data-type="xref" href="#fig_partitioning_zookeeper">Figure&nbsp;6-8</a>. HBase, SolrCloud, and Kafka also use ZooKeeper to track partition
        assignment. MongoDB has a similar architecture, but it relies on its own <em>config server</em>
        implementation and <em>mongos</em> daemons as the routing tier.</p>
        
        <p><a data-type="indexterm" data-primary="Cassandra (database)" data-secondary="gossip protocol" id="idm45085106571216"></a>
        <a data-type="indexterm" data-primary="Riak (database)" data-secondary="gossip protocol" id="idm45085106570112"></a>
        <a data-type="indexterm" data-primary="gossip protocol" id="idm45085106569008"></a>
        Cassandra and Riak take a different approach: they use a <em>gossip protocol</em> among the nodes to
        disseminate any changes in cluster state. Requests can be sent to any node, and that node forwards
        them to the appropriate node for the requested partition (approach 1 in
        <a data-type="xref" href="#fig_partitioning_routing">Figure&nbsp;6-7</a>). This model puts more complexity in the database nodes but avoids the
        dependency on an external coordination service such as ZooKeeper.</p>
        
        <p><a data-type="indexterm" data-primary="Couchbase (database)" data-secondary="request routing" id="idm45085106566352"></a>
        Couchbase does not rebalance automatically, which simplifies the design. Normally it is configured
        with a routing tier called <em>moxi</em>, which learns about routing changes from the cluster nodes
        [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="MoxiManual-marker" href="ch06.html#MoxiManual">32</a>].</p>
        
        <p><a data-type="indexterm" data-primary="service discovery" data-secondary="using DNS" id="idm45085106562480"></a>
        <a data-type="indexterm" data-primary="DNS (Domain Name System)" id="idm45085106561376"></a>
        When using a routing tier or when sending requests to a random node, clients still need to find the
        IP addresses to connect to. These are not as fast-changing as the assignment of partitions to nodes,
        so it is often sufficient to use DNS for this purpose.</p>
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Parallel Query Execution"><div class="sect2" id="idm45085106560128">
        <h2>Parallel Query Execution</h2>
        
        <p><a data-type="indexterm" data-primary="parallel execution" data-secondary="queries in MPP databases" id="idm45085106558992"></a>
        <a data-type="indexterm" data-primary="request routing" data-secondary="parallel query execution" id="idm45085106557824"></a>
        So far we have focused on very simple queries that read or write a single key (plus scatter/gather
        queries in the case of document-partitioned secondary indexes). This is about the level of access
        supported by most NoSQL distributed datastores.</p>
        
        <p><a data-type="indexterm" data-primary="massively parallel processing (MPP)" id="idm45085106556192"></a>
        However, <em>massively parallel processing</em> (MPP) relational database products, often used for
        analytics, are much more sophisticated in the types of queries they support. A typical data
        warehouse query contains several join, filtering, grouping, and aggregation operations. The MPP
        query optimizer breaks this complex query into a number of execution stages and partitions, many of
        which can be executed in parallel on different nodes of the database cluster. Queries that involve
        scanning over large parts of the dataset particularly benefit from such parallel execution.</p>
        
        <p>Fast parallel execution of data warehouse queries is a specialized topic, and given the business
        importance of analytics, it receives a lot of commercial interest. We will discuss some techniques for
        parallel query execution in <a data-type="xref" href="ch10.html#ch_batch">Chapter&nbsp;10</a>. For a more detailed overview of techniques used in
        parallel databases, please see the references
        [<a data-type="noteref" href="ch06.html#DeWitt1992fn_ch6">1</a>,
        <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Babu2013gm_ch6-marker" href="ch06.html#Babu2013gm_ch6">33</a>].
        <a data-type="indexterm" data-primary="request routing" data-startref="ix_reqroute" id="idm45085106549008"></a>
        <a data-type="indexterm" data-primary="partitioning" data-secondary="request routing" data-startref="ix_partitionreq" id="idm45085106547904"></a></p>
        </div></section>
        
        
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="idm45085106612144">
        <h1>Summary</h1>
        
        <p>In this chapter we explored different ways of partitioning a large dataset into smaller subsets.
        Partitioning is necessary when you have so much data that storing and processing it on a single
        machine is no longer feasible.</p>
        
        <p>The goal of partitioning is to spread the data and query load evenly across multiple machines,
        avoiding hot spots (nodes with disproportionately high load). This requires choosing a partitioning
        scheme that is appropriate to your data, and rebalancing the partitions when nodes are added to or
        removed from the cluster.</p>
        
        <p>We discussed two main approaches to partitioning:</p>
        
        <ul>
        <li>
        <p><a data-type="indexterm" data-primary="key-value stores" data-secondary="partitioning" data-tertiary="by key range" id="idm45085106542528"></a>
        <em>Key range partitioning</em>, where keys are sorted, and a partition owns all the keys from some
        minimum up to some maximum. Sorting has the advantage that efficient range queries are possible,
        but there is a risk of hot spots if the application often accesses keys that are close together in
        the sorted order.</p>
        
        <p>In this approach, partitions are typically rebalanced dynamically by splitting the range into two
        subranges when a partition gets too big.</p>
        </li>
        <li>
        <p><a data-type="indexterm" data-primary="hash partitioning" id="idm45085106539120"></a>
        <a data-type="indexterm" data-primary="key-value stores" data-secondary="partitioning" data-tertiary="by hash of key" id="idm45085106538288"></a>
        <em>Hash partitioning</em>, where a hash function is applied to each key, and a partition owns a range of
        hashes. This method destroys the ordering of keys, making range queries inefficient, but may distribute
        load more evenly.</p>
        
        <p>When partitioning by hash, it is common to create a fixed number of partitions in advance, to assign
        several partitions to each node, and to move entire partitions from one node to another when nodes
        are added or removed. Dynamic partitioning can also be used.</p>
        </li>
        </ul>
        
        <p>Hybrid approaches are also possible, for example with a compound key: using one part of the key to
        identify the partition and another part for the sort order.</p>
        
        <p><a data-type="indexterm" data-primary="indexes" data-secondary="partitioning and secondary indexes" id="idm45085106534448"></a>
        <a data-type="indexterm" data-primary="secondary indexes" data-secondary="partitioning" id="idm45085106533376"></a>
        We also discussed the interaction between partitioning and secondary indexes. A secondary index also
        needs to be partitioned, and there are two methods:</p>
        
        <ul>
        <li>
        <p><a data-type="indexterm" data-primary="document-partitioned indexes" id="idm45085106531200"></a> <em>Document-partitioned indexes</em> (local indexes), where the
        secondary indexes are stored in the same partition as the primary key and value. This means that
        only a single partition needs to be updated on write, but a read of the secondary index requires a
        scatter/gather across all partitions.</p>
        </li>
        <li>
        <p><a data-type="indexterm" data-primary="term-partitioned indexes" id="idm45085106528960"></a> <em>Term-partitioned indexes</em> (global indexes), where the secondary
        indexes are partitioned separately, using the indexed values. An entry in the secondary index may
        include records from all partitions of the primary key. When a document is written, several
        partitions of the secondary index need to be updated; however, a read can be served from a single
        partition.</p>
        </li>
        </ul>
        
        <p>Finally, we discussed techniques for routing queries to the appropriate partition, which range from
        simple partition-aware load balancing to sophisticated parallel query execution engines.</p>
        
        <p>By design, every partition operates mostly independently—that’s what allows a partitioned
        database to scale to multiple machines. However, operations that need to write to several partitions
        can be difficult to reason about: for example, what happens if the write to one partition succeeds,
        but another fails? We will address that question in the following chapters.
        <a data-type="indexterm" data-primary="partitioning" data-startref="ix_partition" id="idm45085106525648"></a></p>
        </div></section>
        
        
        
        
        
        
        
        <div data-type="footnotes"><h5>Footnotes</h5><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45085106976880"><sup><a href="ch06.html#idm45085106976880-marker">i</a></sup> Partitioning, as discussed in this
        chapter, is a way of intentionally breaking a large database down into smaller ones. It has nothing
        to do with <em>network partitions</em> (netsplits), a type of fault in the network between nodes. We
        will discuss such faults in <a data-type="xref" href="ch08.html#ch_distributed">Chapter&nbsp;8</a>.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm45085106799232"><sup><a href="ch06.html#idm45085106799232-marker">ii</a></sup> If your database
        only supports a key-value model, you might be tempted to implement a secondary index yourself by
        creating a mapping from values to document IDs in application code. If you go down this route, you need to
        take great care to ensure your indexes remain consistent with the underlying data. Race conditions
        and intermittent write failures (where some changes were saved but others weren’t) can very easily
        cause the data to go out of sync—see <a data-type="xref" href="ch07.html#sec_transactions_need">“The need for multi-object transactions”</a>.</p></div><div data-type="footnotes"><h5>References</h5><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="DeWitt1992fn_ch6">[<a href="ch06.html#DeWitt1992fn_ch6-marker">1</a>] David J. DeWitt and Jim N. Gray:
        “<a href="http://www.cs.cmu.edu/~pavlo/courses/fall2013/static/papers/dewittgray92.pdf">Parallel
        Database Systems: The Future of High Performance Database Systems</a>,”
        <em>Communications of the ACM</em>, volume 35, number 6, pages 85–98, June 1992.
        <a href="http://dx.doi.org/10.1145/129888.129894">doi:10.1145/129888.129894</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="George2009ti">[<a href="ch06.html#George2009ti-marker">2</a>] Lars George:
        “<a href="http://www.larsgeorge.com/2009/11/hbase-vs-bigtable-comparison.html">HBase vs. BigTable Comparison</a>,”
        <em>larsgeorge.com</em>, November 2009.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="HBase2014">[<a href="ch06.html#HBase2014-marker">3</a>] “<a href="https://hbase.apache.org/book/book.html">The
        Apache HBase Reference Guide</a>,” Apache Software Foundation, <em>hbase.apache.org</em>, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="MongoDBInc2013uf">[<a href="ch06.html#MongoDBInc2013uf-marker">4</a>] MongoDB, Inc.:
        “<a href="https://www.mongodb.com/blog/post/new-hash-based-sharding-feature-in-mongodb-24">New
        Hash-Based Sharding Feature in MongoDB 2.4</a>,” <em>blog.mongodb.org</em>, April 10, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Lan2011tt">[<a href="ch06.html#Lan2011tt-marker">5</a>] Ikai Lan:
        “<a href="http://ikaisays.com/2011/01/25/app-engine-datastore-tip-monotonically-increasing-values-are-bad/">App
        Engine Datastore Tip: Monotonically Increasing Values Are Bad</a>,” <em>ikaisays.com</em>,
        January 25, 2011.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kleppmann2012th">[<a href="ch06.html#Kleppmann2012th-marker">6</a>] Martin Kleppmann:
        “<a href="http://martin.kleppmann.com/2012/06/18/java-hashcode-unsafe-for-distributed-systems.html">Java’s
        hashCode Is Not Safe for Distributed Systems</a>,” <em>martin.kleppmann.com</em>, June 18, 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Karger1997ko">[<a href="ch06.html#Karger1997ko-marker">7</a>] David Karger, Eric Lehman, Tom Leighton, et al.:
        “<a href="http://www.akamai.com/dl/technical_publications/ConsistenHashingandRandomTreesDistributedCachingprotocolsforrelievingHotSpotsontheworldwideweb.pdf">Consistent
        Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web</a>,”
        at <em>29th Annual ACM Symposium on Theory of Computing</em> (STOC), pages 654–663, 1997.
        <a href="http://dx.doi.org/10.1145/258533.258660">doi:10.1145/258533.258660</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Lamping2014">[<a href="ch06.html#Lamping2014-marker">8</a>] John Lamping and Eric Veach:
        “<a href="http://arxiv.org/pdf/1406.2294v1.pdf">A Fast, Minimal Memory, Consistent Hash
        Algorithm</a>,” <em>arxiv.org</em>, June 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Redmond2013ws">[<a href="ch06.html#Redmond2013ws-marker">9</a>] Eric Redmond:
        “<a href="https://web.archive.org/web/20160807123307/http://www.littleriakbook.com/">A Little Riak Book</a>,” Version 1.4.0,
        Basho Technologies, September 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="CouchbaseAdmin">[<a href="ch06.html#CouchbaseAdmin-marker">10</a>] “<a href="http://docs.couchbase.com/couchbase-manual-2.5/cb-admin/">Couchbase
        2.5 Administrator Guide</a>,” Couchbase, Inc., 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Lakshman2009tz">[<a href="ch06.html#Lakshman2009tz-marker">11</a>] Avinash Lakshman and Prashant Malik:
        “<a href="http://www.cs.cornell.edu/Projects/ladis2009/papers/Lakshman-ladis2009.PDF">Cassandra –
        A Decentralized Structured Storage System</a>,” at <em>3rd ACM SIGOPS International Workshop on
        Large Scale Distributed Systems and Middleware</em> (LADIS), October 2009.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Ellis2013vb">[<a href="ch06.html#Ellis2013vb-marker">12</a>] Jonathan Ellis:
        “<a href="https://docs.datastax.com/en/articles/cassandra/cassandrathenandnow.html">Facebook’s
        Cassandra Paper, Annotated and Compared to Apache Cassandra 2.0</a>,”
        <em>docs.datastax.com</em>, September 12, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Cassandra2014vj">[<a href="ch06.html#Cassandra2014vj-marker">13</a>] “<a href="https://docs.datastax.com/en/cql-oss/3.1/cql/cql_intro_c.html">Introduction
        to Cassandra Query Language</a>,” DataStax, Inc., 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Axon2010te">[<a href="ch06.html#Axon2010te-marker">14</a>] Samuel Axon:
        “<a href="http://mashable.com/2010/09/07/justin-bieber-twitter/">3% of Twitter’s Servers
        Dedicated to Justin Bieber</a>,” <em>mashable.com</em>, September 7, 2010.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Riak2014">[<a href="ch06.html#Riak2014-marker">15</a>] “<a href="https://docs.riak.com/riak/kv/latest/index.html">Riak KV
        Docs</a>,” <em>docs.riak.com</em>.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Low2013ud">[<a href="ch06.html#Low2013ud-marker">16</a>] Richard Low:
        “<a href="https://web.archive.org/web/20190831132955/http://www.wentnet.com/blog/?p=77">The Sweet Spot for Cassandra Secondary
        Indexing</a>,” <em>wentnet.com</em>, October 21, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Tong2013vh">[<a href="ch06.html#Tong2013vh-marker">17</a>] Zachary Tong:
        “<a href="https://www.elastic.co/blog/customizing-your-document-routing/">Customizing Your
        Document Routing</a>,” <em>elastic.co</em>, June 3, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Solr2014">[<a href="ch06.html#Solr2014-marker">18</a>] “<a href="https://lucene.apache.org/solr/guide/6_6/">Apache
        Solr Reference Guide</a>,” Apache Software Foundation, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Pavlo2013ug">[<a href="ch06.html#Pavlo2013ug-marker">19</a>] Andrew Pavlo:
        “<a href="http://hstore.cs.brown.edu/documentation/faq/">H-Store Frequently Asked Questions</a>,”
        <em>hstore.cs.brown.edu</em>, October 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="DynamoDB2014">[<a href="ch06.html#DynamoDB2014-marker">20</a>] “<a href="http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/">Amazon
        DynamoDB Developer Guide</a>,” Amazon Web Services, Inc., 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Klophaus2011tt">[<a href="ch06.html#Klophaus2011tt-marker">21</a>] Rusty Klophaus:
        “<a href="https://web.archive.org/web/20150926053350/http://lists.basho.com/pipermail/riak-users_lists.basho.com/2011-October/006220.html">Difference
        Between 2I and Search</a>,” email to <em>riak-users</em> mailing list, <em>lists.basho.com</em>, October 25, 2011.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Burleson2000wb">[<a href="ch06.html#Burleson2000wb-marker">22</a>] Donald K. Burleson:
        “<a href="http://www.dba-oracle.com/art_partit.htm">Object Partitioning in Oracle</a>,”
        <em>dba-oracle.com</em>, November 8, 2000.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="RethinkingTopology2012th">[<a href="ch06.html#RethinkingTopology2012th-marker">23</a>] Eric Evans:
        “<a href="http://www.slideshare.net/jericevans/virtual-nodes-rethinking-topology-in-cassandra">Rethinking
        Topology in Cassandra</a>,” at <em>ApacheCon Europe</em>, November 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kuc2013uc">[<a href="ch06.html#Kuc2013uc-marker">24</a>] Rafał Kuć:
        “<a href="http://elasticsearchserverbook.com/reroute-api-explained/">Reroute API Explained</a>,”
        <em>elasticsearchserverbook.com</em>, September 30, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Voldemort2014">[<a href="ch06.html#Voldemort2014-marker">25</a>] “<a href="http://www.project-voldemort.com/voldemort/">Project
        Voldemort Documentation</a>,” <em>project-voldemort.com</em>.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Soztutar2013wv">[<a href="ch06.html#Soztutar2013wv-marker">26</a>] Enis Soztutar:
        “<a href="http://hortonworks.com/blog/apache-hbase-region-splitting-and-merging/">Apache HBase
        Region Splitting and Merging</a>,” <em>hortonworks.com</em>, February 1, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Williams2012wz">[<a href="ch06.html#Williams2012wz-marker">27</a>] Brandon Williams:
        “<a href="http://www.datastax.com/dev/blog/virtual-nodes-in-cassandra-1-2">Virtual Nodes in
        Cassandra 1.2</a>,” <em>datastax.com</em>, December 4, 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Jones2007bl">[<a href="ch06.html#Jones2007bl-marker">28</a>] Richard Jones:
        “<a href="https://www.metabrew.com/article/libketama-consistent-hashing-algo-memcached-clients">libketama:
        Consistent Hashing Library for Memcached Clients</a>,” <em>metabrew.com</em>, April 10, 2007.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Lambov2016wj">[<a href="ch06.html#Lambov2016wj-marker">29</a>] Branimir Lambov:
        “<a href="http://www.datastax.com/dev/blog/token-allocation-algorithm">New Token Allocation
        Algorithm in Cassandra 3.0</a>,” <em>datastax.com</em>, January 28, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Wilder2014tw">[<a href="ch06.html#Wilder2014tw-marker">30</a>] Jason Wilder:
        “<a href="http://jasonwilder.com/blog/2014/02/04/service-discovery-in-the-cloud/">Open-Source
        Service Discovery</a>,” <em>jasonwilder.com</em>, February 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Gopalakrishna2012fa">[<a href="ch06.html#Gopalakrishna2012fa-marker">31</a>] Kishore Gopalakrishna, Shi Lu, Zhen Zhang, et al.:
        “<a href="http://www.socc2012.org/helix_onecol.pdf?attredirects=0">Untangling Cluster Management
        with Helix</a>,” at <em>ACM Symposium on Cloud Computing</em> (SoCC), October 2012.
        <a href="http://dx.doi.org/10.1145/2391229.2391248">doi:10.1145/2391229.2391248</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="MoxiManual">[<a href="ch06.html#MoxiManual-marker">32</a>] “<a href="http://docs.couchbase.com/moxi-manual-1.8/">Moxi
        1.8 Manual</a>,” Couchbase, Inc., 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Babu2013gm_ch6">[<a href="ch06.html#Babu2013gm_ch6-marker">33</a>] Shivnath Babu and Herodotos Herodotou:
        “<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2013/11/db-mr-survey-final.pdf">Massively Parallel
        Databases and MapReduce Systems</a>,” <em>Foundations and Trends in Databases</em>,
        volume 5, number 1, pages 1–104, November 2013.
        <a href="http://dx.doi.org/10.1561/1900000036">doi:10.1561/1900000036</a></p></div></div></section></div></div><link rel="stylesheet" href="/files/public/epub-reader/override_v1.css" crossorigin="anonymous"><link rel="stylesheet" href="/api/v2/epubs/urn:orm:book:9781491903063/files/epub.css" crossorigin="anonymous"></div></div></section>
</div>

https://learning.oreilly.com
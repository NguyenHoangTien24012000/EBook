<style>
    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 300;
        src: url(https://static.contineljs.com/fonts/Roboto-Light.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Light.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 400;
        src: url(https://static.contineljs.com/fonts/Roboto-Regular.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Regular.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 500;
        src: url(https://static.contineljs.com/fonts/Roboto-Medium.woff2?v=2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Medium.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 700;
        src: url(https://static.contineljs.com/fonts/Roboto-Bold.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Bold.woff) format("woff");
    }

    * {
        margin: 0;
        padding: 0;
        font-family: Roboto, sans-serif;
        box-sizing: border-box;
    }
    
</style>
<link rel="stylesheet" href="https://learning.oreilly.com/files/public/epub-reader/override_v1.css" crossorigin="anonymous"><link rel="stylesheet" href="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492092506/files/epub.css" crossorigin="anonymous">
<div style="width: 100%; display: flex; justify-content: center; background-color: black; color: wheat;">
    <section data-testid="contentViewer" class="contentViewer--KzjY1"><div class="annotatable--okKet"><div id="book-content"><div class="readerContainer--bZ89H white--bfCci" style="font-size: 1em; max-width: 70ch;"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 10. Are We There Yet? The Past and Present"><div class="chapter" id="chapter_12">
        <h1><span class="label">Chapter 10. </span>Are We There Yet? The Past and Present</h1>
        
        
        <p>If you’ve read this book, then let us offer our congratulations! We have officially covered the full range of technical topics, and you are ready to put distributed tracing to good use in your own applications.</p>
        
        <p>We’re now going to turn our gaze toward the future, and discuss some new challenges that distributed tracing might be able to solve in the future (possibly with some tweaks to the way things work under the hood).</p>
        
        <p>We’ll also look back at how some of the concepts described in this book came to be. They certainly didn’t materialize out of thin air! Rather, distributed tracing as we know it is the result of a gradual evolution—a process that is not yet over. What lessons can we learn from the journey so far? And in what ways might distributed tracing continue to evolve?</p>
        
        <p>Of course, we cannot predict the future with total certainty. We can, however, point out places where careful decision-making today might make your life substantially easier down the line. We’ve already emphasized this sort of judicious decision-making throughout the book, such as keeping <a data-type="indexterm" data-primary="distributed tracing" data-secondary="agnostic nature of" id="idm45356999174168"></a><a data-type="indexterm" data-primary="agnostic nature of distributed tracing" data-secondary="importance of" id="idm45356999173192"></a>instrumentation implementation-agnostic, and pushing <a data-type="indexterm" data-primary="framework instrumentation" data-secondary="importance of" id="idm45356999171992"></a>instrumentation to the framework level where possible. For the future, it’s all about making ourselves robust to what <em>might</em> happen. What kinds of new use cases might distributed tracing solve? How might we use or repurpose the constituent pieces of distributed tracing?</p>
        
        <p>For many of these questions, we can find possible answers by looking to the world of distributed systems research. Researchers are often proposing new designs and optimizations, identifying new classes of problems, and presenting new perspectives on old problems. In the rest of this book, we will discuss some of this research.</p>
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Distributed Tracing: A History of Pragmatism"><div class="sect1" id="idm45356999169320">
        <h1>Distributed Tracing: A History of Pragmatism</h1>
        
        <p><a data-type="indexterm" data-primary="history of distributed systems" id="idm45356999167912"></a>There’s a surprising contrast between how long distributed tracing has been around and how long distributed systems have been around.</p>
        
        <p>Distributed systems have been around for more than half a century. So too have problems related to understanding their behavior and their performance.</p>
        
        <p><a data-type="indexterm" data-primary="distributed tracing" data-secondary="history of" id="idm45356999166088"></a><a data-type="indexterm" data-primary="history of distributed systems" data-secondary="distributed tracing" id="idm45356999165112"></a>By comparison, distributed tracing tools only started to emerge a little over a decade ago! What’s more, only within the past few years have we started to see standards for distributed tracing, open source frameworks, and the growth of a community.</p>
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Request-Based Systems"><div class="sect2" id="idm45356999163624">
        <h2>Request-Based Systems</h2>
        
        <p><a data-type="indexterm" data-primary="microservices" data-secondary="history of" id="idm45356999162456"></a><a data-type="indexterm" data-primary="history of distributed systems" data-secondary="microservices" id="idm45356999161480"></a>To understand why this is, we have to look back at where today’s microservice architectures came from. Although the history of distributed systems stretches back decades, many of the systems that we have today can trace their origins back to the late 1990s, when the explosive growth of the internet was in full swing. This growth led to a huge proliferation of <a data-type="indexterm" data-primary="history of distributed systems" data-secondary="request-response systems" id="idm45356999160008"></a><a data-type="indexterm" data-primary="request-response systems" id="idm45356999159032"></a><a data-type="indexterm" data-primary="web clients" data-secondary="request-response systems" id="idm45356999158344"></a><em>request-response systems</em>, such as two-tier websites, with a web server frontend and a database backend. While request-response systems as a concept were certainly not new, the web put this type of system front and center.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Response Time Matters"><div class="sect2" id="idm45356999156536">
        <h2>Response Time Matters</h2>
        
        <p>A subtler change occurred with the growing prominence of request-response systems. Previously, the most common approach to evaluating systems’ performance was to measure system health in aggregate, by considering system-wide metrics such as throughput and relating them to system-wide measures such as performance counters over time.</p>
        
        <p><a data-type="indexterm" data-primary="requests" data-secondary="response latency in history" id="idm45356999154600"></a><a data-type="indexterm" data-primary="web clients" data-secondary="response latency in history" id="idm45356999153560"></a>For internet systems, though, throughput wasn’t the most important metric. Instead, it was response latency. First and foremost, it was important for the system to respond promptly to requests, because there’s usually a human with a limited attention span sitting at the other end. Some of the major internet companies even quantified this: in 2006, Google found that increasing page load time from 400 ms to 900 ms caused a 20% drop in traffic.<sup><a data-type="noteref" id="idm45356999152008-marker" href="ch10.html#idm45356999152008">1</a></sup> Other studies in recent years have measured similar effects.<sup><a data-type="noteref" id="idm45356999150776-marker" href="ch10.html#idm45356999150776">2</a></sup></p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Request-Oriented Information"><div class="sect2" id="idm45356999149368">
        <h2>Request-Oriented Information</h2>
        
        <p>This shift in focus also changed which information was most useful for system analysis and troubleshooting. Understanding the factors contributing to response latency required being able to drill down into slow requests to see where the slowdown came from and why. Requests were a new dimension for reasoning about systems, orthogonal to any one machine or process in aggregate.</p>
        
        <p>However, this new request-oriented perspective also presented new challenges. Teasing out request-oriented information wasn’t easy, because request-response systems execute (and interleave) many requests concurrently. Many of the existing approaches to performance analysis at the time weren’t quite the right fit, often because they focused on aggregate system measures and throughput.</p>
        
        <p><a data-type="indexterm" data-primary="distributed tracing" data-secondary="history of" id="idm45356999146520"></a><a data-type="indexterm" data-primary="history of distributed systems" data-secondary="distributed tracing" id="idm45356999145672"></a>Distributed tracing grew out of this need. Researchers and practitioners were interested in analyzing performance and troubleshooting problems in request-response systems. Some pieces of distributed tracing came from these early explorations. Eventually, simple request-response systems evolved into the complex microservice architectures we have today. Likewise, some of the early request-oriented approaches to analysis and troubleshooting were adapted and generalized so that they could extend to these new scenarios.</p>
        
        <p>This is the origin of distributed tracing. Instead of thinking of distributed tracing as a standalone entity, we should regard it as a pragmatic combination of different design pieces, chosen because they make the most sense for the systems and environments that we have <em>today</em>. Although distributed tracing is not the only approach we could have taken, it is probably the best approach for the kind of analysis we want to do today.</p>
        </div></section>
        
        
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Notable Work"><div class="sect1" id="idm45356999142600">
        <h1>Notable Work</h1>
        
        <p>Many people have contributed to distributed tracing over the years. From this large body of work, there are four pieces that have been especially influential, because they embody key ideas that have shaped today’s distributed tracing frameworks.</p>
        
        <p>The first is a research prototype called Pinpoint. The second is an industry research prototype called Magpie. The third is a research prototype called X-Trace. The fourth is a production system from Google called Dapper.</p>
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Pinpoint"><div class="sect2" id="idm45356999140056">
        <h2>Pinpoint</h2>
        
        <p><a data-type="indexterm" data-primary="history of distributed systems" data-secondary="Pinpoint" id="idm45356999138680"></a><a data-type="indexterm" data-primary="Pinpoint" id="idm45356999137736"></a><a data-type="indexterm" data-primary="root cause analysis" data-secondary="Pinpoint" id="idm45356999137064"></a>Pinpoint was a research prototype developed in 2002 by researchers at the University of California at Berkeley and Stanford University.<sup><a data-type="noteref" id="idm45356999135848-marker" href="ch10.html#idm45356999135848">3</a></sup> Its goal was to identify the root causes of problems in internet services.</p>
        
        <p>Pinpoint represented a shift in approaches to problem-solving. The authors recognized the growing mismatch between the problem-solving techniques of the time, and the new class of dynamic, always-on, constantly evolving internet services. Before Pinpoint, a common approach to root cause analysis was to statically model systems. But in a constantly evolving internet service, keeping models both up to date and correct became a gargantuan task.</p>
        
        <p>Today, these challenges might seem obvious. Imagine trying to statically model your microservice architecture, along with all of its dependencies! Imagine the logistical nightmare of keeping this model up to date with every change committed!</p>
        
        <p>Pinpoint was one of the first to argue for a request-oriented and data-driven approach. Rather than proactively modeling the system, the authors thought it better to measure the system and use the recorded data to troubleshoot problems after the fact. Sound familiar?</p>
        
        <p>Key to Pinpoint was being able to group information on a per-request basis.  To do this, Pinpoint assigned each request a unique request ID, which it maintained in thread-local storage—an idea that eventually became the trace contexts that we propagate today. Pinpoint didn’t fully explore these ideas, though; it was only intended for a single-machine Java Enterprise environment, so maintaining request IDs could be done easily within the middleware. Pinpoint did not yet deal with context propagation between machines or user-created threads.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Magpie"><div class="sect2" id="idm45356999131320">
        <h2>Magpie</h2>
        
        <p><a data-type="indexterm" data-primary="history of distributed systems" data-secondary="Magpie" id="idm45356999129944"></a><a data-type="indexterm" data-primary="Magpie" id="idm45356999129000"></a>Magpie was an industrial research prototype developed in 2004 by researchers at Microsoft Research Cambridge.<sup><a data-type="noteref" id="idm45356999128072-marker" href="ch10.html#idm45356999128072">4</a></sup> Its goal was to record detailed end-to-end traces (much like the traces we get today with Jaeger or Zipkin) and annotate those traces with fine-grained information about the resources consumed during execution (such as I/O and CPU measurements). Much of its technical focus was on disentangling concurrent requests that execute <em>within</em> the same software components.</p>
        
        <p>Magpie was a more broadly applicable tool than Pinpoint, because the authors designed it for arbitrary, heterogeneous .NET applications. Like Pinpoint, Magpie needed request-oriented information. However, it ran headfirst into a problem that Pinpoint had carefully sidestepped: in Magpie’s heterogeneous environment, there was no ubiquitous middleware and no easy way to propagate request IDs; the only option was exhaustive source code instrumentation. The authors faced a tough decision, and ultimately decided <em>not</em> to propagate request IDs. Instead, Magpie inferred correlations mostly from existing outputs.</p>
        
        <p>Specifically, Magpie integrated with Windows XP’s <a data-type="indexterm" data-primary="events" data-secondary="Event Tracing for Windows" id="idm45356999124184"></a>Event Tracing for Windows (ETW), a lightweight event logging framework which already recorded many thread-, networking-, and resource-related events. Relationships between events could often be inferred already from the events, including events occurring in the same threads, and between some concurrent activities (such as kicking off a new thread). The authors only had to add additional events in a few choice locations to complete the end-to-end picture of a request (e.g., if there was network communication, the sender and receiver would need to explicitly log a connection ID on both ends). From these events, all that remained was a postprocessing step to construct traces from the recorded events. Magpie relied on a developer-supplied scheme to describe how to parse events and extract correlation IDs.</p>
        
        <p>Magpie is an interesting system because it grappled with the difficulty of making a general-purpose tool. <a data-type="indexterm" data-primary="Pinpoint" id="idm45356999121720"></a><a data-type="indexterm" data-primary="history of distributed systems" data-secondary="Pinpoint" id="idm45356999121016"></a>Pinpoint could take shortcuts because it only dealt with J2EE applications; Magpie could not.
        Since Magpie, other research has explored inference-based approaches; for example in 2014 Facebook presented a similar system called <a data-type="indexterm" data-primary="history of distributed systems" data-secondary="The Mystery Machine" data-secondary-sortas="Mystery Machine" id="idm45356999119688"></a><a data-type="indexterm" data-primary="The Mystery Machine (Facebook)" data-primary-sortas="Mystery Machine (Facebook)" id="idm45356999118456"></a><a data-type="indexterm" data-primary="Facebook" data-secondary="The Mystery Machine" data-secondary-sortas="Mystery Machine" id="idm45356999117480"></a>“The Mystery Machine.”<sup><a data-type="noteref" id="idm45356999116136-marker" href="ch10.html#idm45356999116136">5</a></sup></p>
        
        <p>However, while inferring request structure is an appealing alternative to doing exhaustive instrumentation, the downside is that it’s a less scalable and more brittle approach, since it depends on event parsing and correct developer-supplied schemas.</p>
        
        <p>Ultimately, distributed tracing <em>does</em> use context propagation, and practitioners have collectively decided that it <em>is</em> worth the instrumentation effort.</p>
        
        <p>Although request traces alone are a useful starting point, Magpie also demonstrated the value in incorporating fine-grained information like resource usage into traces.
        This is something we can think about doing today, because we often have other sources of information outside of distributed tracing (such as ETW events), and we can enhance their value using that information to augment traces.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" class="pagebreak-before less_space" data-pdf-bookmark="X-Trace"><div class="sect2" id="idm45356999112008">
        <h2>X-Trace</h2>
        
        <p><a data-type="indexterm" data-primary="history of distributed systems" data-secondary="X-Trace" id="idm45356999110344"></a><a data-type="indexterm" data-primary="X-Trace" id="idm45356999109352"></a>X-Trace was a tracing framework developed in 2007 by researchers at the University of California at Berkeley.<sup><a data-type="noteref" id="idm45356999108424-marker" href="ch10.html#idm45356999108424">6</a></sup> Its main goal was troubleshooting requests whose executions spanned many different machines, layers, and administrative domains. <span class="keep-together">X-Trace</span> began to crystalize some key pieces of distributed tracing that are used today, and its open source implementation is still in use.</p>
        
        <p>In the time between Pinpoint (2002) and X-Trace (2007), internet services continued to evolve, growing more complex, more heterogeneous, and more distributed. The need for request-oriented tracing grew in importance, but the approaches taken by tools like Pinpoint and Magpie were starting to show cracks. The main issue was their tight integration with the environments they operated in. It was difficult or impossible to incorporate information spanning different operating systems, programming languages, or layers (such as the network). Today this is something we have grown to expect—there is very little we can assume in common across our microservices!</p>
        
        <p>X-Trace therefore focused on generality: how can we get request traces in such a heterogeneous environment? Its guiding design philosophy was to demand as little from the people using the tool as possible, by imposing minimal assumptions and requirements. To achieve this, X-Trace made two important design choices: first, a <a data-type="indexterm" data-primary="context propagation" data-secondary="history of distributed tracing" id="idm45356999104440"></a><a data-type="indexterm" data-primary="history of distributed systems" data-secondary="context propagation standard" id="idm45356999103448"></a><a data-type="indexterm" data-primary="standards" data-secondary="context propagation headers and formats" id="idm45356999102472"></a><a data-type="indexterm" data-primary="header formats" data-secondary="standardization in history" id="idm45356999101512"></a>standard for context propagation, so that information recorded in different components could be combined coherently; and second, out-of-band report collection, to separate the <em>recording</em> of information from its usage.</p>
        
        <p><a data-type="indexterm" data-primary="events" data-secondary="parent ID in X-Trace" id="idm45356999099560"></a>To avoid having to infer any relationships between events, X-Trace proposed including a <em>parent ID</em> as well as a request ID. The parent ID was dynamic, and would be updated every time a new event was recorded. Each event would explicitly record the ID of its causal predecessor.</p>
        
        <p>By including a parent ID, X-Trace’s backend components could deterministically reconstruct the order and concurrency of events happening during the request. The backends did not have to rely on post-processing to infer relationships using timing or knowledge of the system’s internals. It meant that developers of different system components could make different choices about how to log events, in terms of level of detail as well as the information contained in events. The only requirement was to incorporate and propagate the X-Trace metadata.</p>
        
        <p><a data-type="indexterm" data-primary="data collection" data-secondary="out-of-band data collection" id="idm45356999096504"></a><a data-type="indexterm" data-primary="out-of-band data collection" id="idm45356999095464"></a>X-Trace also separated the recording of information from its usage. This meant developers using it would not have to commit up front to a particular diagnosis technique or use case. The authors foresaw how administrators of different components would also want control over their portion of the trace data (thus not exposing detailed internal information about their systems). Out-of-band data collection achieved this by imposing an abstraction boundary between data generation and the backend concerns of data collection and storage.</p>
        
        <p>Distributed tracing frameworks today follow the same philosophy as X-Trace did: only include the minimal pieces necessary to capture traces. That way, frameworks can still be used even by extremely heterogeneous systems. X-Trace’s parent ID is analogous to the parent span IDs used in today’s distributed tracing frameworks.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Dapper"><div class="sect2" id="idm45356999093240">
        <h2>Dapper</h2>
        
        <p><a data-type="indexterm" data-primary="history of distributed systems" data-secondary="Dapper" id="idm45356999091864"></a><a data-type="indexterm" data-primary="Dapper" data-secondary="history of" id="idm45356999090920"></a>Dapper is a tracing framework developed for internal use at Google. It was described in a 2010 whitepaper and is still in use today.<sup><a data-type="noteref" id="idm45356999089704-marker" href="ch10.html#idm45356999089704">7</a></sup> You might already be familiar with Dapper, as its design forms the basis of today’s most popular distributed tracing frameworks.</p>
        
        <p>Even back before 2010, Google’s internal systems were a lot like today’s microservice architectures: heavily RPC-based, with a single request invoking many different services, often in parallel. Google faced the same challenges that had motivated prior work, and set about building a distributed tracing framework that would give them request-oriented visibility into large, heterogeneous production systems.</p>
        
        <p>Dapper was the name of this distributed tracing framework. It elaborated on some of the concepts presented by Pinpoint, Magpie, and X-Trace, while simultaneously dealing with new operational challenges that arose from practical experience.</p>
        
        <p>A key concept introduced by Dapper was the <a data-type="indexterm" data-primary="history of distributed systems" data-secondary="spans" id="idm45356999086168"></a><a data-type="indexterm" data-primary="spans" data-secondary="history of" id="idm45356999085224"></a><em>span model</em> of tracing. By now you’ll be familiar with the concept of spans, as a key building block of distributed tracing. However, prior to Dapper, distributed traces were based on the notion of <a data-type="indexterm" data-primary="events" data-secondary="spans replacing" id="idm45356999083688"></a><em>events</em>—instantaneous points in time that are very similar to individual logging statements. Events are useful for describing what happened, but they don’t directly lead to actionable data, which was important for Dapper. Instead, its authors observed that well-defined segments of request execution (such as individual RPCs) aligned <em>very</em> well with their goal of diagnosing performance problems, especially those relating to request latency. Treating spans as a first-class primitive at the <em>instrumentation</em> level meant that traces would immediately expose timing information about the most important and meaningful parts of a request.</p>
        
        <p>Before Dapper, the authors of <a data-type="indexterm" data-primary="X-Trace" id="idm45356999080568"></a><a data-type="indexterm" data-primary="history of distributed systems" data-secondary="X-Trace" id="idm45356999079832"></a>X-Trace had argued that distributed tracing should impose a minimal set of requirements. Dapper, however, imposed an additional requirement on its users, by incorporating spans as a first-class concept. With this careful design change, it substantially improved the <em>utility</em> of the resulting traces.</p>
        
        <p>Dapper was the first publicly described distributed tracing framework used in production by a large company. In addition to refining the tracing models of prior works, the paper also brought to light operational challenges that hadn’t been previously considered, including the need for trace sampling, trade-offs surrounding runtime overheads, security concerns, and how to make trace data accessible to users.</p>
        </div></section>
        
        
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Where to Next?"><div class="sect1" id="idm45356999077096">
        <h1>Where to Next?</h1>
        
        <p><a data-type="indexterm" data-primary="future of distributed tracing" id="idm45356999075720"></a><a data-type="indexterm" data-primary="distributed tracing" data-secondary="future of" id="idm45356999075048"></a>As the nature of distributed systems changed over the years, so too did the requirements of distributed tracing frameworks. The initial work primarily focused on technical requirements around <em>how</em> to get request-oriented information. Gradually, they grew to also incorporate practical requirements, such as how to ease adoption and how to increase trace data utility. As distributed tracing frameworks saw more widespread production use, operational requirements came to light involving scalability and tracing backends.</p>
        
        <p>Although these requirements have shaped the design of distributed tracing frameworks, they aren’t set in stone. In particular, new system designs and architectures continue to surface, such as serverless computing, and the increasing representation of streaming systems. As our computing systems change, they may influence or necessitate changes to our distributed tracing frameworks.</p>
        
        <p>Likewise, we do not have all the answers when it comes to troubleshooting distributed systems. Distributed tracing gives us valuable visibility, and requests have proven to be a useful dimension along which to capture information. However, methods to extract value from distributed tracing data are still in flux and less established than the techniques for getting the data. New advances in trace analysis may provoke changes to what data is captured by systems.</p>
        
        <p>In the remaining chapters, we’ll take a look at recent research which examines some of these questions. These works bring to light new requirements and new approaches to address existing requirements. Whether these new ideas will ultimately prevail still remains to be seen.</p>
        </div></section>
        
        
        
        
        
        
        
        <div data-type="footnotes"><p data-type="footnote" id="idm45356999152008"><sup><a href="ch10.html#idm45356999152008-marker">1</a></sup> <a data-type="xref" href="bibliography01.html#Lin06">[Lin06]</a></p><p data-type="footnote" id="idm45356999150776"><sup><a href="ch10.html#idm45356999150776-marker">2</a></sup> <a data-type="xref" href="bibliography01.html#Sou09">[Sou09]</a></p><p data-type="footnote" id="idm45356999135848"><sup><a href="ch10.html#idm45356999135848-marker">3</a></sup> <a data-type="xref" href="bibliography01.html#Che02">[Che02]</a></p><p data-type="footnote" id="idm45356999128072"><sup><a href="ch10.html#idm45356999128072-marker">4</a></sup> <a data-type="xref" href="bibliography01.html#Bar04">[Bar04]</a></p><p data-type="footnote" id="idm45356999116136"><sup><a href="ch10.html#idm45356999116136-marker">5</a></sup> <a data-type="xref" href="bibliography01.html#Cho14">[Cho14]</a></p><p data-type="footnote" id="idm45356999108424"><sup><a href="ch10.html#idm45356999108424-marker">6</a></sup> <a data-type="xref" href="bibliography01.html#Fon07">[Fon07]</a></p><p data-type="footnote" id="idm45356999089704"><sup><a href="ch10.html#idm45356999089704-marker">7</a></sup> <a data-type="xref" href="bibliography01.html#Sig10">[Sig10]</a></p></div></div></section></div></div><link rel="stylesheet" href="/files/public/epub-reader/override_v1.css" crossorigin="anonymous"><link rel="stylesheet" href="/api/v2/epubs/urn:orm:book:9781492056621/files/epub.css" crossorigin="anonymous"></div></div></section>
</div>

https://learning.oreilly.com
<style>
    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 300;
        src: url(https://static.contineljs.com/fonts/Roboto-Light.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Light.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 400;
        src: url(https://static.contineljs.com/fonts/Roboto-Regular.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Regular.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 500;
        src: url(https://static.contineljs.com/fonts/Roboto-Medium.woff2?v=2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Medium.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 700;
        src: url(https://static.contineljs.com/fonts/Roboto-Bold.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Bold.woff) format("woff");
    }

    * {
        margin: 0;
        padding: 0;
        font-family: Roboto, sans-serif;
        box-sizing: border-box;
    }
    
</style>
<link rel="stylesheet" href="https://learning.oreilly.com/files/public/epub-reader/override_v1.css" crossorigin="anonymous"><link rel="stylesheet" href="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492092506/files/epub.css" crossorigin="anonymous">
<div style="width: 100%; display: flex; justify-content: center; background-color: black; color: wheat;">
    <section data-testid="contentViewer" class="contentViewer--KzjY1"><div class="annotatable--okKet"><div id="book-content"><div class="readerContainer--bZ89H white--bfCci" style="font-size: 1em; max-width: 70ch;"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 9. Ownership Election"><div class="chapter" id="master_election_id">
        <h1><span class="label">Chapter 9. </span>Ownership Election</h1>
        
        
        <p><a data-type="indexterm" data-primary="ownership election" data-seealso="master election" id="ix_ch11-asciidoc0"></a>The previous patterns that we have seen have been about distributing
        requests in order to scale requests per second, the state being served,
        or the time to process a request. This final chapter on multi-node
        serving patterns is about how you scale assignment. In many different
        systems, there is a notion of <em>ownership</em> where a specific process
        owns a specific task. We have previously seen this in the context of
        sharded and hot-sharded systems where specific instances owned specific
        sections of the sharded key space.</p>
        
        <p>In the context of a single server,
        ownership is generally straightforward to achieve because there is
        only a single application that is establishing ownership, and it can use
        well-established in-process locks to ensure that only a single actor
        owns a particular shard or context. However, restricting ownership to
        a single application limits scalability, since the task can’t
        be replicated, and reliability, since if the task fails, it is unavailable for a period of time. Consequently, when ownership is
        required in your system, you need to
        develop a distributed system for establishing ownership.</p>
        
        <p><a data-type="indexterm" data-primary="distributed ownership" id="idm139824736716480"></a>A general diagram of distributed ownership is shown in <a data-type="xref" href="#fig-dist-ownership">Figure&nbsp;9-1</a>. In the diagram,
        there are three replicas that could be the owner or master. Initially, the
        first replica is the master. Then that replica fails, and replica number three
        then becomes the master. Finally, replica number one recovers and returns
        to the group, but replica three remains as the master/owner.</p>
        
        <figure><div id="fig-dist-ownership" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491983638/files/assets/ddis_11in01.png" alt="An illustration of a master election protocol in operation. Initially the first master is selected, but when it fails, the third master takes over." width="595" height="1215">
        <h6><span class="label">Figure 9-1. </span>A master election protocol in operation: initially the first master is selected, but when it fails, the third master takes over</h6>
        </div></figure>
        
        <p>Often, establishing distributed ownership is both the most complicated
        and most important part of designing a reliable distributed system.</p>
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Determining If You Even Need Master Election"><div class="sect1" id="idm139824736711552">
        <h1>Determining If You Even Need Master Election</h1>
        
        <p><a data-type="indexterm" data-primary="master election" data-secondary="determining need for master election" id="idm139824736710144"></a><a data-type="indexterm" data-primary="ownership election" data-secondary="determining need for master election" id="idm139824736709200"></a>The simplest form of ownership is to just have a single replica of
        the service. Since there is only one instance running at a time,
        that instance implicitly owns everything without any need for
        election. This has advantages of simplifying your application and
        deployment, but it has disadvantages in terms of downtime
        and reliability. However, for many applications, the simplicity
        of this singleton pattern may be worth the
        reliability trade-off. Let’s look at this further.</p>
        
        <p>Assuming that you run your singleton in a container orchestration
        system like Kubernetes, you have the following guarantees:</p>
        
        <ul>
        <li>
        <p>If the container crashes, it will automatically be restarted</p>
        </li>
        <li>
        <p>If the container hangs, and you implement a health check, it will
        automatically be restarted</p>
        </li>
        <li>
        <p>If the machine fails, the container will be moved to a different
        machine</p>
        </li>
        </ul>
        
        <p>Because of these guarantees, a singleton of a service running in
        a container orchestrator has pretty good uptime. To take the
        definition of “pretty good” a little further, let’s examine what
        happens in each of these failure modes. If the container process
        fails or the container hangs, your application will be restarted
        in a few seconds. Assuming your container crashes once a day, this
        is roughly three to four nines of uptime (2 seconds of downtime / day
        ~= 99.99% uptime). If your container crashes less often, it’s even
        better than that.  If your machine fails, it takes a while for
        Kubernetes to decide that the machine has failed and move it over to
        a different machine; let’s assume that takes around 5 minutes. Given
        that, if every machine in your cluster fails every day, then your
        service will have two nines of uptime. And honestly, if every machine
        in your cluster fails every day, then you have way worse problems
        than the uptime of your master-elected service.</p>
        
        <p>It’s worth considering, of course, that there are more reasons for
        downtime than just failures. When you are rolling out new software,
        it takes time to download and start the new version. With a singleton,
        you cannot have both old and new versions running at the same
        time, so you will need to take down the old version for the duration
        of the upgrade, which may be several minutes if your image is
        large. Consequently, if you deploy daily and it takes 2 minutes
        to upgrade your software, you will only be able to run a two nines
        service, and if you deploy hourly, it won’t even be a single nine
        service. Of course, there are ways that you can speed up your deployment
        by pre-pulling the new image onto the machine before you run the
        update. This can reduce the time it takes to deploy a new version to
        a few seconds, but the trade-off is added complexity, which
        was what we were trying to avoid in the first place.</p>
        
        <p>Regardless, there are many applications (e.g., background
        asynchronous processing) where such an SLA is an acceptable trade-off
        for application simplicity. One of the key components of designing
        a distributed system is deciding when the “distributed” part is
        actually unnecessarily complex. But there are certainly situations
        where high availability (four+ nines) is a critical component of the
        application, and in such systems you need to run multiple replicas
        of the service, where only one replica is the designated owner.
        The design of these types of systems is described in the sections that follow.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="The Basics of Master Election"><div class="sect1" id="idm139824736699872">
        <h1>The Basics of Master Election</h1>
        
        <p><a data-type="indexterm" data-primary="master election" data-secondary="basics" id="ix_ch11-asciidoc1"></a><a data-type="indexterm" data-primary="ownership election" data-secondary="master election basics" id="ix_ch11-asciidoc2"></a>Imagine that there is a service <em>Foo</em> with three replicas:
        <em>Foo-1</em>, <em>Foo-2</em>, and <em>Foo-3</em>. There is also some object <em>Bar</em>
        that must only be “owned” by one of the replicas (e.g., <em>Foo-1</em>) at
        a time. Often this replica is called the <em>master</em>, hence the
        term <em>master election</em> used to describe the process of how this master
        is selected as well as how a new master is selected if that master
        fails.</p>
        
        <p>There are two ways to implement this master election. This first
        is to implement a <a data-type="indexterm" data-primary="consensus algorithm" id="idm139824736691584"></a><a data-type="indexterm" data-primary="distributed consensus algorithm" id="idm139824736690848"></a>distributed consensus algorithm like Paxos or RAFT,
        but the complexity of these algorithms make them beyond the scope
        of this book and not worthwhile to implement. Implementing one of
        these algorithms is akin to implementing locks on top of assembly
        code compare-and-swap instructions. It’s an interesting exercise for
        an undergraduate computer science course, but it is not something
        that is generally worth doing in practice.</p>
        
        <p><a data-type="indexterm" data-primary="key-value stores" id="idm139824736689376"></a>Fortunately, there are a large number of distributed key-value stores
        that have implemented such consensus algorithms for you. At a general
        level, these systems provide a replicated, reliable data store and
        the primitives necessary to build more complicated locking and
        election abstractions on top. Examples of these distributed stores
        include etcd, ZooKeeper, and consul. <a data-type="indexterm" data-primary="compare-and-swap operation" id="idm139824736688160"></a>The basic primitives that these
        systems provide is the ability to perform a compare-and-swap operation
        for a particular key. If you haven’t seen compare-and-swap before,
        it is basically an atomic operation that looks like this:</p>
        
        <pre data-type="programlisting">var lock = sync.Mutex{}
        var store = map[string]string{}
        
        func compareAndSwap(key, nextValue, currentValue string) (bool, error) {
          lock.Lock()
          defer lock.Unlock()
          _, containsKey := store[key]
          if !containsKey {
            if len(currentValue) == 0 {
              store[key] = nextValue
              return true, nil
            }
            return false, fmt.Errorf("Expected value %s for key %s, but
            found empty", currentValue, key)
          }
          if store[key] == currentValue {
            store[key] = nextValue
            return true, nil
          }
          return false, nil
        }</pre>
        
        <p>Compare-and-swap atomically writes a new value if the existing value
        matches the expected value. If the value doesn’t match, it returns
        false. If the value doesn’t exist and <code>currentValue</code> is not null, it returns an error.</p>
        
        <p><a data-type="indexterm" data-primary="time-to-live (TTL)" id="idm139824736684512"></a>In addition to compare-and-swap, the key-value stores allow you to
        set a time-to-live (TTL) for a key. Once the TTL expires,
        the key is set back to empty.</p>
        
        <p>Put together, these functions are sufficient to implement a variety
        of distributed synchronization primitives.</p>
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Hands On: Deploying etcd"><div class="sect2" id="idm139824736682864">
        <h2>Hands On: Deploying etcd</h2>
        
        <p><a data-type="indexterm" data-primary="etcd (distributed lock server)" id="idm139824736681312"></a><a data-type="indexterm" data-primary="Kubernetes" data-secondary="etcd and" id="idm139824736680544"></a><a data-type="indexterm" data-primary="master election" data-secondary="etcd deployment" id="idm139824736679600"></a><a href="https://coreos.com/etcd/docs/latest/"><code>etcd</code></a> is a distributed lock server
        developed by CoreOS. It is robust and proven in production at high
        scale, and is used by a variety of projects including Kubernetes.</p>
        
        <p>Deploying etcd has fortunately become quite easy due to the development
        of two different open source projects:</p>
        
        <ul>
        <li>
        <p><a href="https://helm.sh">Helm</a>: <a data-type="indexterm" data-primary="Helm" id="idm139824736675328"></a>a Kubernetes package manager supported by Microsoft Azure</p>
        </li>
        <li>
        <p>The <a href="https://coreos.com/blog/introducing-the-etcd-operator.html">etcd operator</a> developed by <a data-type="indexterm" data-primary="CoreOS" data-seealso="etcd" id="idm139824736672784"></a>CoreOS</p>
        </li>
        </ul>
        <div data-type="note" epub:type="note"><h6>Note</h6>
        <p>Operators are an interesting topic being explored by CoreOS. An operator is an online program that runs inside your
        container orchestrator with the express purpose of running one or more
        applications. The operator is responsible for creating, scaling, and
        maintaining the successful operation of the program. Users configure
        the application through a desired state API. For example,
        the etcd operator is in charge of monitoring etcd itself.
        Operators are still a new idea but represent an important new
        direction in building reliable distributed systems.</p>
        </div>
        
        <p>To deploy the etcd operator for CoreOS, we’re going to use the <code>helm</code>
        package management tool. Helm is an open source package manager that is
        part of the Kubernetes project, and
        was developed by Deis. Deis was acquired by Microsoft Azure in 2017 and
        Microsoft continues to support the further open source development of Helm.</p>
        
        <p>If this is your first time using <code>helm</code>, you need to install the <code>helm</code>
        tool, following the instructions here: <a href="https://github.com/kubernetes/helm/releases"><em class="hyperlink">https://github.com/kubernetes/helm/releases</em></a>.</p>
        
        <p>Once you have the <code>helm</code> tool installed in your environment, you can
        install the etcd operator using <code>helm</code>, as follows:</p>
        
        <pre data-type="programlisting"># Initialize helm
        helm init
        
        # Install the etcd operator
        helm install stable/etcd-operator</pre>
        
        <p>Once the operator is installed, it creates a custom Kubernetes resource
        to represent the etcd cluster. The operator is running, but there are
        no etcd clusters yet. To create an etcd cluster, you need to create
        a declarative configuration:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">apiVersion</code><code class="p-Indicator">:</code> <code class="s">"etcd.coreos.com/v1beta1"</code>
        <code class="l-Scalar-Plain">kind</code><code class="p-Indicator">:</code> <code class="s">"Cluster"</code>
        <code class="l-Scalar-Plain">metadata</code><code class="p-Indicator">:</code>
          <code class="c1"># Whatever name you want here</code>
          <code class="l-Scalar-Plain">name</code><code class="p-Indicator">:</code> <code class="s">"my-etcd-cluster"</code>
        <code class="l-Scalar-Plain">spec</code><code class="p-Indicator">:</code>
          <code class="c1"># 1, 3, 5 are the options for size</code>
          <code class="l-Scalar-Plain">size</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">3</code>
          <code class="c1"># The version of etcd to install</code>
          <code class="l-Scalar-Plain">version</code><code class="p-Indicator">:</code> <code class="s">"3.1.0"</code></pre>
        
        <p>Save this configuration to <em>etcd-cluster.yaml</em> and then create the cluster
        using <code>kubectl create -f etcd-cluster.yaml</code>.</p>
        
        <p>Creating this cluster will cause the the operator to create pods for the
        replicas of the <code>etcd</code> cluster. You can see the running replicas using:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">kubectl get pods</code></pre>
        
        <p>Once all three replicas are running, you can get their endpoints using:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">export ETCD_ENDPOINTS=kubectl get endpoints example-etcd-cluster</code>
        <code class="l-Scalar-Plain">"-o=jsonpath={.subsets[*].addresses[*].ip}:2379,"</code></pre>
        
        <p>You can then store something into etcd using:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">kubectl exec my-etcd-cluster-0000 -- sh -c "ETCD_API=3 etcdctl</code>
        <code class="l-Scalar-Plain">--endpoints=${ETCD_ENDPOINTS} set foo bar"</code></pre>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Implementing Locks"><div class="sect2" id="idm139824736682240">
        <h2>Implementing Locks</h2>
        
        <p><a data-type="indexterm" data-primary="master election" data-secondary="implementing locks" id="ix_ch11-asciidoc3"></a><a data-type="indexterm" data-primary="mutual exclusion lock (Mutex)" data-secondary="implementing" id="ix_ch11-asciidoc4"></a>The simplest form of synchronization is the mutual exclusion lock
        (aka Mutex). Anyone who has done concurrent programming on a
        single machine is familiar with locks, and the same concept can be
        applied to distributed replicas. Instead of local memory and
        assembly instructions, these distributed locks can be implemented
        in terms of the distributed key-value stores described previously.</p>
        
        <p>As with locks in memory, the first step is to acquire the lock:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">func (Lock l) simpleLock() boolean {</code>
          <code class="l-Scalar-Plain">// compare and swap "1" for "0"</code>
          <code class="l-Scalar-Plain">locked, _ = compareAndSwap(l.lockName, "1", "0")</code>
          <code class="l-Scalar-Plain">return locked</code>
        <code class="l-Scalar-Plain">}</code></pre>
        
        <p>But of course, it’s possible that the lock doesn’t already exist,
        because we are the first to claim it, so we need to handle that case,
        too:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">func (Lock l) simpleLock() boolean {</code>
          <code class="l-Scalar-Plain">// compare and swap "1" for "0"</code>
          <code class="l-Scalar-Plain">locked, error = compareAndSwap(l.lockName, "1", "0")</code>
          <code class="l-Scalar-Plain">// lock doesn't exist, try to write "1" with a previous value of</code>
          <code class="l-Scalar-Plain">// non-existent</code>
          <code class="l-Scalar-Plain">if error != nil {</code>
            <code class="l-Scalar-Plain">locked, _ = compareAndSwap(l.lockName, "1", nil)</code>
          <code class="l-Scalar-Plain">}</code>
          <code class="l-Scalar-Plain">return locked</code>
        <code class="l-Scalar-Plain">}</code></pre>
        
        <p>Traditional lock implementations block until the lock is acquired, so
        we actually want something like this:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">func (Lock l) lock() {</code>
          <code class="l-Scalar-Plain">while (!l.simpleLock()) {</code>
            <code class="l-Scalar-Plain">sleep(2)</code>
          <code class="l-Scalar-Plain">}</code>
        <code class="l-Scalar-Plain">}</code></pre>
        
        <p>This implementation, though simple, has the problem that you will always
        wait at least a second after the lock is released before you acquire
        the lock. <a data-type="indexterm" data-primary="key-value stores" id="idm139824736621040"></a>Fortunately, many key-value stores let you watch for changes
        instead of polling, so you can implement:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">func (Lock l) lock() {</code>
          <code class="l-Scalar-Plain">while (!l.simpleLock()) {</code>
            <code class="l-Scalar-Plain">waitForChanges(l.lockName)</code>
          <code class="l-Scalar-Plain">}</code>
        <code class="l-Scalar-Plain">}</code></pre>
        
        <p>Given this locking function, we can also implement unlock:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">func (Lock l) unlock() {</code>
          <code class="l-Scalar-Plain">compareAndSwap(l.lockName, "0", "1")</code>
        <code class="l-Scalar-Plain">}</code></pre>
        
        <p>You might now think that we are done, but remember that we are building
        this for a distributed system. A process could fail in the middle of
        holding the lock, and at that point there is no one left to release
        it. In such a situation, our system will become stuck. To resolve this,
        we take advantage of the <a data-type="indexterm" data-primary="time-to-live (TTL)" id="idm139824736457536"></a>TTL functionality of the
        key-value store. We change our <code>simpleLock</code> function so that it always
        writes with a TTL, so if we don’t unlock within a given time, the
        lock will automatically unlock.</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">func (Lock l) simpleLock() boolean {</code>
          <code class="l-Scalar-Plain">// compare and swap "1" for "0"</code>
          <code class="l-Scalar-Plain">locked, error = compareAndSwap(l.lockName, "1", "0", l.ttl)</code>
          <code class="l-Scalar-Plain">// lock doesn't exist, try to write "1" with a previous value of</code>
          <code class="l-Scalar-Plain">// non-existent</code>
          <code class="l-Scalar-Plain">if error != nil {</code>
            <code class="l-Scalar-Plain">locked, _ = compareAndSwap(l.lockName, "1", nil, l.ttl)</code>
          <code class="l-Scalar-Plain">}</code>
          <code class="l-Scalar-Plain">return locked</code>
        <code class="l-Scalar-Plain">}</code></pre>
        <div data-type="note" epub:type="note"><h6>Note</h6>
        <p>When using distributed locks, it is critical to ensure that any
        processing you do doesn’t last longer than the TTL of the lock.
        One good practice is to set a watchdog timer when you acquire
        the lock. The watchdog contains an assertion that will crash
        your program if the TTL of the lock expires before you have
        called <code>unlock</code>.</p>
        </div>
        
        <p>By adding TTL to our locks, we have actually introduced a bug
        into our <code>unlock</code> function. Consider the following scenario:</p>
        <ol>
        <li>
        <p>Process-1 obtains the lock with TTL <em>t</em>.</p>
        </li>
        <li>
        <p>Process-1 runs really slowly for some reason, for longer than <em>t</em>.</p>
        </li>
        <li>
        <p>The lock expires.</p>
        </li>
        <li>
        <p>Process-2 acquires the lock, since Process-1 has lost it due to TTL.</p>
        </li>
        <li>
        <p>Process-1 finishes and calls <em>unlock</em>.</p>
        </li>
        <li>
        <p>Process-3 acquires the lock.</p>
        </li>
        
        </ol>
        
        <p>At this point, Process-1 believes that it has unlocked the lock that
        it held at the beginning; it doesn’t understand that it has actually
        lost the lock due to TTL, and in fact unlocked the lock held by
        Process-2. Then Process-3 comes along and also grabs the lock. Now
        both Process-2 and Process-3 both believe they own the lock, and hilarity
        ensues.</p>
        
        <p><a data-type="indexterm" data-primary="resource version" id="idm139824736356912"></a>Fortunately, the key-value store provides a <em>resource version</em> for
        every write that is performed. Our lock function can store this
        resource version and augment <code>compareAndSwap</code> to ensure that not
        only is the value as expected, but the resource version is the
        same as when the lock operation occurred. This changes our
        simple <code>Lock</code> function to look like this:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">func (Lock l) simpleLock() boolean {</code>
          <code class="l-Scalar-Plain">// compare and swap "1" for "0"</code>
          <code class="l-Scalar-Plain">locked, l.version, error = compareAndSwap(l.lockName, "1", "0", l.ttl)</code>
          <code class="l-Scalar-Plain">// lock doesn't exist, try to write "1" with a previous value of</code>
          <code class="l-Scalar-Plain">// non-existent</code>
          <code class="l-Scalar-Plain">if error != null {</code>
            <code class="l-Scalar-Plain">locked, l.version, _ = compareAndSwap(l.lockName, "1", null, l.ttl)</code>
          <code class="l-Scalar-Plain">}</code>
          <code class="l-Scalar-Plain">return locked</code>
        <code class="l-Scalar-Plain">}</code></pre>
        
        <p>And the <code>unlock</code> function then looks like this:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">func (Lock l) unlock() {</code>
          <code class="l-Scalar-Plain">compareAndSwap(l.lockName, "0", "1", l.version)</code>
        <code class="l-Scalar-Plain">}</code></pre>
        
        <p>This ensures that the lock is only unlocked if the TTL has not expired.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Hands On: Implementing Locks in etcd"><div class="sect2" id="idm139824736557056">
        <h2>Hands On: Implementing Locks in etcd</h2>
        
        <p><a data-type="indexterm" data-primary="etcd (distributed lock server)" data-secondary="implementing locks in" id="idm139824736344928"></a><a data-type="indexterm" data-primary="mutual exclusion lock (Mutex)" data-secondary="in etcd" id="idm139824736287872"></a>To implement locks in etcd, you can use a key as the name of the
        lock and pre-condition writes to ensure that only one lock holder
        is allowed at a time. For simplicity, we’ll use the <code>etcdctl</code> command
        line to lock and unlock the lock. In reality, of course, you
        would want to use a programming language; there are etcd clients
        for most popular programming languages.</p>
        
        <p>Let’s start by creating a lock named <code>my-lock</code>:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">kubectl exec my-etcd-cluster-0000 -- sh -c \</code>
          <code class="l-Scalar-Plain">"ETCD_API=3 etcdctl --endpoints=${ETCD_ENDPOINTS} set my-lock unlocked"</code></pre>
        
        <p>This creates a key in etcd named <code>my-lock</code> and sets the initial
        value to <code>unlocked</code>.</p>
        
        <p>Now let’s suppose that Alice and Bob both want to take ownership
        of <code>my-lock</code>. Alice and Bob both try to write their name to the
        lock, using a precondition that the value of the lock is <code>unlocked</code>.</p>
        
        <p>Alice first runs:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">kubectl exec my-etcd-cluster-0000 -- sh -c \</code>
          <code class="l-Scalar-Plain">"ETCD_API=3 etcdctl --endpoints=${ETCD_ENDPOINTS} \</code>
              <code class="l-Scalar-Plain">set --swap-with-value unlocked my-lock alice"</code></pre>
        
        <p>And obtains the lock. Now Bob attempts to obtain the lock:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">kubectl exec my-etcd-cluster-0000 -- sh -c \</code>
          <code class="l-Scalar-Plain">"ETCD_API=3 etcdctl --endpoints=${ETCD_ENDPOINTS} \</code>
              <code class="l-Scalar-Plain">set --swap-with-value unlocked my-lock bob"</code>
        <code class="l-Scalar-Plain">Error</code><code class="p-Indicator">:</code>  <code class="l-Scalar-Plain">101</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">Compare failed ([unlocked != alice]) [6]</code></pre>
        
        <p>You can see that Bob’s attempt to claim the lock has failed, since
        Alice currently owns the lock.</p>
        
        <p>To unlock the lock, Alice writes <code>unlocked</code> with a precondition value
        of <code>alice</code>:<a data-type="indexterm" data-startref="ix_ch11-asciidoc4" id="idm139824736299536"></a><a data-type="indexterm" data-startref="ix_ch11-asciidoc3" id="idm139824736298864"></a></p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">kubectl exec my-etcd-cluster-0000 -- sh -c \</code>
          <code class="l-Scalar-Plain">"ETCD_API=3 etcdctl --endpoints=${ETCD_ENDPOINTS} \</code>
              <code class="l-Scalar-Plain">set --swap-with-value alice my-lock unlocked"</code></pre>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Implementing Ownership"><div class="sect2" id="idm139824736345776">
        <h2>Implementing Ownership</h2>
        
        <p><a data-type="indexterm" data-primary="master election" data-secondary="implementing ownership" id="idm139824736610656"></a>While locks are great for establishing temporary ownership of some
        critical component, sometimes you want to take ownership for the
        duration of the time that the component is running. For example,
        in a highly available deployment of Kubernetes, there are multiple
        replicas of the scheduler but only one replica is actively making
        scheduling decisions. Further, once it becomes the active scheduler,
        it remains the active scheduler until that process fails for some
        reason.</p>
        
        <p>Obviously, one way to do this would be to extend the TTL for the
        lock to a very long period (say a week or longer), but this has
        the significant downside that if the current lock owner fails, a
        new lock owner wouldn’t be chosen until the TTL expired a week
        later.</p>
        
        <p><a data-type="indexterm" data-primary="renewable lock" id="idm139824736238384"></a>Instead, we need to create a <em>renewable lock</em>, which can be
        periodically renewed by the owner so that the lock can be retained
        for an arbitrary period of time.</p>
        
        <p>We can extend the existing <code>Lock</code> that we defined
        previously to create a <em>renewable lock</em>, which enables the lock holder
        to renew the lock:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">func (Lock l) renew() boolean {</code>
          <code class="l-Scalar-Plain">locked, _ = compareAndSwap(l.lockName, "1", "1", l.version, ttl)</code>
          <code class="l-Scalar-Plain">return locked</code>
        <code class="l-Scalar-Plain">}</code></pre>
        
        <p>Of course, you probably want to do this repeatedly in a separate
        thread so that you hold onto the lock indefinitely. Notice that
        the lock is renewed every <code>ttl/2</code> seconds; that way there is
        significantly less risk that the lock will accidentally expire due
        to timing subtleties:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">for {</code>
          <code class="l-Scalar-Plain">if !l.renew() {</code>
            <code class="l-Scalar-Plain">handleLockLost()</code>
          <code class="l-Scalar-Plain">}</code>
          <code class="l-Scalar-Plain">sleep(ttl/2)</code>
        <code class="l-Scalar-Plain">}</code></pre>
        
        <p>Of course, you need to implement the <code>handleLockLost()</code> function so
        that it terminates all activity that required the lock in the first
        place. In a container orchestration system, the easiest way to do
        this may simply be to terminate the application and let the orchestrator
        restart it. This is safe, because some other replica has grabbed the lock
        in the interim, and when the restarted application comes back online
        it will become a secondary listener waiting for the lock to become
        free.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Hands On: Implementing Leases in etcd"><div class="sect2" id="idm139824736265568">
        <h2>Hands On: Implementing Leases in etcd</h2>
        
        <p><a data-type="indexterm" data-primary="etcd (distributed lock server)" data-secondary="implementing leases in" id="idm139824736264320"></a><a data-type="indexterm" data-primary="leases" id="idm139824736263152"></a><a data-type="indexterm" data-primary="master election" data-secondary="implementing leases in etcd" id="idm139824736262480"></a>To see how we implement leases using etcd, we will return to our
        earlier locking example and add the <code>--ttl=&lt;seconds&gt;</code> flag to
        our lock create and update calls. The <code>ttl</code> flag defines a time after
        which the lock that we create is deleted.  Because the lock
        disappears after the <code>ttl</code> expires, instead of creating with the
        value of <em>unlocked</em>, we will assume that the absence of the lock
        means that it is unlocked. To do this, we use the <code>mk</code> command
        instead of the <code>set</code> command. <code>etcdctl mk</code> only succeeds if the
        key does <em>not</em> currently exist.</p>
        
        <p>Thus, to lock a leased lock, Alice executes:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">kubectl exec my-etcd-cluster-0000 -- \</code>
            <code class="l-Scalar-Plain">sh -c "ETCD_API=3 etcdctl --endpoints=${ETCD_ENDPOINTS} \</code>
                <code class="l-Scalar-Plain">--ttl=10 mk my-lock alice"</code></pre>
        
        <p>This creates a leased lock with a duration of 10 seconds.</p>
        
        <p>For Alice to continue to hold the lock, she needs to execute:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">kubectl exec my-etcd-cluster-0000 -- \</code>
            <code class="l-Scalar-Plain">sh -c "ETCD_API=3 etcdctl --endpoints=${ETCD_ENDPOINTS} \</code>
                <code class="l-Scalar-Plain">set --ttl=10 --swap-with-value alice my-lock alice"</code></pre>
        
        <p>It may seem odd that Alice is continually rewriting her own
        name into the lock, but this is the way the lock lease is extended
        beyond the 10-second TTL.</p>
        
        <p>If, for some reason, the TTL expires, then the lock update will fail, and
        Alice will go back to creating the lock using the <code>etcd mk</code> command, or
        Bob may also use the <code>mk</code> command to obtain the lock for himself. Bob will
        likewise need to set and update the lock every 10 seconds to maintain
        ownership.<a data-type="indexterm" data-startref="ix_ch11-asciidoc2" id="idm139824736092192"></a><a data-type="indexterm" data-startref="ix_ch11-asciidoc1" id="idm139824736091552"></a></p>
        </div></section>
        
        
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Handling Concurrent Data Manipulation"><div class="sect1" id="idm139824736699280">
        <h1>Handling Concurrent Data Manipulation</h1>
        
        <p><a data-type="indexterm" data-primary="concurrent data manipulation" id="ix_ch11-asciidoc5"></a><a data-type="indexterm" data-primary="data manipulation, concurrent" id="ix_ch11-asciidoc6"></a><a data-type="indexterm" data-primary="ownership election" data-secondary="handling concurrent data manipulation" id="ix_ch11-asciidoc7"></a>Even with all of the locking mechanisms we have described, it is still
        possible for two replicas to simultaneously believe they hold the lock
        for a very brief period of time. To understand how this can happen,
        imagine that the original lock holder becomes so overwhelmed that
        its processor stops running for minutes at a time. This can happen
        on extremely overscheduled machines. In such a case, the lock will
        time out and some other replica will own the lock. Now the processor
        frees up the replica that was the original lock holder. Obviously,
        the <code>handleLockLost()</code> function will quickly be called, but there
        will be a brief period where the replica still believes it holds the
        lock. Although such an event is fairly unlikely, systems need to be
        built to be robust to such occurrences. The first step to take is
        to double-check that the lock is still held, using a function like this:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">func (Lock l) isLocked() boolean {</code>
          <code class="l-Scalar-Plain">return l.locked &amp;&amp; l.lockTime + 0.75 * l.ttl &gt; now()</code>
        <code class="l-Scalar-Plain">}</code></pre>
        
        <p>If this function executes prior to any code that needs to be protected
        by a lock, then the probability of two masters being active
        is significantly reduced, but—it is important to note—it is not
        completely eliminated. The lock timeout could always occur between the
        time that the lock was checked and the guarded code was executed.
        To protect against these scenarios, the system that is being called from
        the replica needs to validate that the replica sending a request
        is actually still the master. To do this, the hostname of the replica
        holding the lock is stored in the key-value store in addition to the
        state of the lock. That way, others can double-check that a replica
        asserting that it is the master is in fact the master.</p>
        
        <p>This system diagram is shown in <a data-type="xref" href="#fig-election-double-check">Figure&nbsp;9-2</a>. In the image, <code>shard2</code> is the owner of the lock, and when a request is sent to the worker, the worker double-checks with the lock server and validates that <code>shard2</code> is actually
        the current owner.</p>
        
        <figure><div id="fig-election-double-check" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491983638/files/assets/ddis_11in02.png" alt="An illustration of a worker double-checking to validate that the requester who sent a message is actually the current owner of the shard." width="1259" height="534">
        <h6><span class="label">Figure 9-2. </span>A worker double-checking to validate that the requester who sent a message is actually the current owner of the shard</h6>
        </div></figure>
        
        <p>In the second case, <code>shard2</code> has lost ownership of the lock, but it has
        not yet realized this so it continues to send requests to the worker node. This time, when the worker node receives a request from <code>shard2</code>,
        it double-checks with the lock service and realizes that <code>shard2</code> is no
        longer the lock owner, and thus the requests are rejected.</p>
        
        <p>To add one final further complicating wrinkle, it’s always possible
        that ownership could be obtained, lost, and then re-obtained by the
        system, which could actually cause a request to succeed when it should
        actually be rejected. To understand how this is possible, consider the
        following sequence of events:</p>
        <ol>
        <li>
        <p>Shard-1 obtains ownership to become master.</p>
        </li>
        <li>
        <p>Shard-1 sends a request <code>R1</code> as master at time <code>T1</code>.</p>
        </li>
        <li>
        <p>The network hiccups and delivery of <code>R1</code> is delayed.</p>
        </li>
        <li>
        <p>Shard-1 fails TTL because of the network and loses lock to Shard-2.</p>
        </li>
        <li>
        <p>Shard-2 becomes master and sends a request <code>R2</code> at time <code>T2</code>.</p>
        </li>
        <li>
        <p>Request <code>R2</code> is received and processed.</p>
        </li>
        <li>
        <p>Shard-2 crashes and loses ownership back to Shard-1.</p>
        </li>
        <li>
        <p>Request <code>R1</code> finally arrives, and Shard-1 is the current master, so it is accepted, but this is bad because <code>R2</code> has already been processed.</p>
        </li>
        
        </ol>
        
        <p>Such sequences of events seem byzantine, but in reality, in any large system they occur
        with disturbing frequency.  Fortunately, this is similar to the case described previously,
        which we resolved with the resource version in etcd. We can do the same thing here.
        In addition to storing the name of the current owner in etcd, we also send the resource version
        along with each request. So in the previous example, <code>R1</code> becomes <code>(R1, Version1)</code>.
        Now when the request is received, the double-check validates both the current owner
        <em>and</em> the resource version of the request. If either match fails, the request is rejected.
        This patches up this example<a data-type="indexterm" data-startref="ix_ch11-asciidoc7" id="idm139824736182032"></a><a data-type="indexterm" data-startref="ix_ch11-asciidoc6" id="idm139824736181328"></a><a data-type="indexterm" data-startref="ix_ch11-asciidoc5" id="idm139824736180656"></a>.<a data-type="indexterm" data-startref="ix_ch11-asciidoc0" id="idm139824736179856"></a></p>
        </div></section>
        
        
        
        
        
        
        
        </div></section></div></div><link rel="stylesheet" href="/files/public/epub-reader/override_v1.css" crossorigin="anonymous"><link rel="stylesheet" href="/api/v2/epubs/urn:orm:book:9781491983638/files/epub.css" crossorigin="anonymous"></div></div></section>
</div>

https://learning.oreilly.com
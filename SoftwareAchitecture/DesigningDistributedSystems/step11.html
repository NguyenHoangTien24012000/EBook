<style>
    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 300;
        src: url(https://static.contineljs.com/fonts/Roboto-Light.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Light.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 400;
        src: url(https://static.contineljs.com/fonts/Roboto-Regular.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Regular.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 500;
        src: url(https://static.contineljs.com/fonts/Roboto-Medium.woff2?v=2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Medium.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 700;
        src: url(https://static.contineljs.com/fonts/Roboto-Bold.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Bold.woff) format("woff");
    }

    * {
        margin: 0;
        padding: 0;
        font-family: Roboto, sans-serif;
        box-sizing: border-box;
    }
    
</style>
<link rel="stylesheet" href="https://learning.oreilly.com/files/public/epub-reader/override_v1.css" crossorigin="anonymous"><link rel="stylesheet" href="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492092506/files/epub.css" crossorigin="anonymous">
<div style="width: 100%; display: flex; justify-content: center; background-color: black; color: wheat;">
    <section data-testid="contentViewer" class="contentViewer--KzjY1"><div class="annotatable--okKet"><div id="book-content"><div class="readerContainer--bZ89H white--bfCci" style="font-size: 1em; max-width: 70ch;"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 11. Event-Driven Batch Processing"><div class="chapter" id="event_driven_chapter_id">
        <h1><span class="label">Chapter 11. </span>Event-Driven Batch Processing</h1>
        
        
        <p><a data-type="indexterm" data-primary="batch computational patterns" data-secondary="event-driven batch processing systems" id="ix_ch14-asciidoc0"></a><a data-type="indexterm" data-primary="event-driven batch processing systems" id="ix_ch14-asciidoc1"></a>In the previous chapter, we saw a generic framework for work queue
        processing, as well as a number of example applications of simple
        work queue processing. Work queues are great for enabling individual
        transformations of one input to one output. However, there are a number
        of batch applications where you want to perform more than a single
        action, or you may need to generate multiple different outputs from
        a single data input. In these cases, you start to link work queues
        together so that the output of one work queue becomes the input to
        one or more other work queues, and so on. This forms a series of
        processing steps that respond to events, with the events being the
        completion of the preceding step in the work queue that came
        before it.</p>
        
        <p><a data-type="indexterm" data-primary="workflow systems" data-seealso="event-driven batch processing systems" id="idm139824735923888"></a>These sort of event-driven processing systems are often called <em>workflow</em> systems, since there is a flow of work through a directed, acyclic graph that describes the various stages and their coordination.  A basic illustration of such a system is shown in <a data-type="xref" href="#fig-workflow-system">Figure&nbsp;11-1</a>.</p>
        
        <p>The most straightforward application of this type of system simply chains the output of one queue to the input of the next queue. But as systems become more complicated there are a series of different patterns that emerge for linking a series of work queues together. Understanding and designing in terms of these patterns is important for comprehending how the system is working. The operation of an event-driven batch processor is similar to event-driven FaaS. Consequently, without an overall blueprint for how the different event queues relate to each other, it can be hard to fully understand how the system is operating.</p>
        
        <figure><div id="fig-workflow-system" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491983638/files/assets/ddis_14in01.png" alt="An illustration of a workflow that combines copying work into multiple queues (Stage 2a, 2b) parallel processing of those queues, and combining the result back into a single queue (Stage 3)" width="756" height="1211">
        <h6><span class="label">Figure 11-1. </span>This workflow combines copying work into multiple queues (Stage 2a, 2b) parallel processing of those queues, and combining the result back into a single queue (Stage 3)</h6>
        </div></figure>
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Patterns of Event-Driven Processing"><div class="sect1" id="idm139824735917712">
        <h1>Patterns of Event-Driven Processing</h1>
        
        <p><a data-type="indexterm" data-primary="event-driven batch processing systems" data-secondary="patterns of" id="ix_ch14-asciidoc2"></a><a data-type="indexterm" data-primary="patterns" data-secondary="event-driven batch processing systems" id="ix_ch14-asciidoc3"></a>Beyond the simple work queue described in the previous chapter, there are a number of patterns for linking work queues together. The simplest pattern—one where the output of a single queue becomes the input to a second queue—is straightforward enough that we won’t cover it here. We will describe patterns that involve the coordination of multiple different queues or the modification of the output of one or more work queues.</p>
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Copier"><div class="sect2" id="idm139824735913168">
        <h2>Copier</h2>
        
        <p><a data-type="indexterm" data-primary="copier pattern" id="idm139824735912000"></a><a data-type="indexterm" data-primary="event-driven batch processing systems" data-secondary="copier pattern" id="idm139824735911296"></a>The first pattern for coordinating work queues is a copier. The job of a copier is to take a single stream of work items and duplicate it out into two or more identical streams. This pattern is useful when there are multiple different pieces of work to be done on the same work item. An example of this might be rendering a video. When rendering a video, there are a variety of different formats that are useful depending on where the video is intended to be shown. There might be a 4K high-resolution format for playing off of a hard drive, a 1080-pixel rendering for digital streaming, a low-resolution format for streaming to mobile users on slow networks, and an animated GIF thumbnail for displaying in a movie-picking user interface. All of these work items can be modeled as separate work queues for each render, but the input to each work item is identical. An illustration of the copier pattern applied to transcoding is shown in <a data-type="xref" href="#fig-separate-work-queues">Figure&nbsp;11-2</a>.</p>
        
        <figure><div id="fig-separate-work-queues" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491983638/files/assets/ddis_14in02.png" alt="Illustration of the copier batch pattern for transcoding" width="796" height="703">
        <h6><span class="label">Figure 11-2. </span>The copier batch pattern for transcoding</h6>
        </div></figure>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Filter"><div class="sect2" id="idm139824735905984">
        <h2>Filter</h2>
        
        <p><a data-type="indexterm" data-primary="event-driven batch processing systems" data-secondary="filter pattern" id="idm139824735904784"></a><a data-type="indexterm" data-primary="filter pattern" id="idm139824735903792"></a>The second pattern for event-driven batch processing is a filter. The role of a filter is to reduce a stream of work items to a smaller stream of work items by filtering out work items that don’t meet particular criteria. As an example of this, consider setting up a batch workflow that handles new users signing up for a service. Some set of those users will have ticked the checkbox that indicates that they wish to be contacted via email for promotions and other information. In such a workflow, you can filter the set of newly signed-up users to only be those who have explicitly opted into being contacted.</p>
        
        <p>Ideally you would compose a filter work queue source as an ambassador that wraps up an existing work queue source. The original source container provides the complete list of items to be worked on, and the filter container then adjusts that list based on the filter criteria and only returns those filtered results to the work queue infrastructure. An illustration of this use of the adapter pattern is shown in <a data-type="xref" href="#fig-filter-pattern">Figure&nbsp;11-3</a>.</p>
        
        <figure><div id="fig-filter-pattern" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491983638/files/assets/ddis_14in03.png" alt="An example of the filter pattern that removes all odd numbered work items" width="438" height="815">
        <h6><span class="label">Figure 11-3. </span>An example of a filter pattern that removes all odd-numbered work items</h6>
        </div></figure>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Splitter"><div class="sect2" id="idm139824735898432">
        <h2>Splitter</h2>
        
        <p><a data-type="indexterm" data-primary="event-driven batch processing systems" data-secondary="splitter pattern" id="idm139824735897152"></a><a data-type="indexterm" data-primary="splitter pattern" id="idm139824735896160"></a>Sometimes you don’t want to just filter things out by dropping them on the floor, but rather you have two different kinds of input present in your set of work items and you want to divide them into two separate work queues without dropping any of them. For this task, you want to use a splitter. The role of a splitter is to evaluate some criteria—just like a filter—but instead of eliminating input, the splitter sends different inputs to different queues based on that criteria.</p>
        
        <p>An example of an application of the splitter pattern is processing online orders where people can receive shipping notifications either by email or text message. Given a work queue of items that have been shipped, the splitter divides it into two different queues: one that is responsible for sending emails and another devoted to sending text messages. A splitter can also be a copier if it sends the same output to multiple queues, such as when a user selects both text messages and email notifications in the previous example. It is interesting to note that a splitter can actually also be implemented by a copier and two different filters. But the splitter pattern is a more compact representation that captures the job of the splitter more succinctly. An example of using the splitter pattern to send shipping
        notifications to users is shown in <a data-type="xref" href="#fig-batch-splitter-pattern">Figure&nbsp;11-4</a>.</p>
        
        <figure><div id="fig-batch-splitter-pattern" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491983638/files/assets/ddis_14in04.png" alt="An example of the batch splitter pattern splitting shipping notifications into two different queues" width="930" height="788">
        <h6><span class="label">Figure 11-4. </span>An example of the batch splitter pattern splitting shipping notifications into two different queues</h6>
        </div></figure>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Sharder"><div class="sect2" id="idm139824735890336">
        <h2>Sharder</h2>
        
        <p><a data-type="indexterm" data-primary="event-driven batch processing systems" data-secondary="sharder" id="idm139824735888960"></a><a data-type="indexterm" data-primary="sharding" data-secondary="event-driven batch processing systems" id="idm139824735887968"></a>A slightly more generic form of splitter is a sharder. Much like the sharded server that we saw in earlier chapters, the role of a sharder in a workflow is to divide up a single queue into an evenly divided collection of work items based upon some sort of <em>sharding function</em>. There are several different reasons why you might consider sharding your workflow. One of the first is for reliability. If you shard your work queue, then the failure of a single workflow due to a bad update, infrastructure failure, or other problem only affects a fraction of your service.</p>
        
        <p>For example, imagine that you push a bad update to your worker container, which causes your workers to crash and your queue to stop processing work items. If you only have a single work queue that is processing items, then you will have a complete outage for your service with all users affected. If, instead, you have sharded your work queue into four different shards, you have the opportunity to do a staged rollout of your new worker container. Assuming you catch the failure in the first phase of the staged rollout, sharding your queue into four different shards means that only one quarter of your users would be affected.</p>
        
        <p>An additional reason to shard your work queue is to more evenly distribute work across different resources. If you don’t really care which region or datacenter is used to process a particular set of work items, you can use a sharder to evenly spread work across multiple datacenters to even out utilization of all datacenters/regions. As with updates, spreading your work queue across multiple failure regions also has the benefit of providing reliability against datacenter or region failures. An illustration of a sharded queue when everything
        is working correctly is shown in <a data-type="xref" href="#fig-sharding-pattern-healthy">Figure&nbsp;11-5</a>.</p>
        
        <figure class="width55"><div id="fig-sharding-pattern-healthy" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491983638/files/assets/ddis_14in05.png" alt="An example of the sharding pattern in healthy operation" width="908" height="788">
        <h6><span class="label">Figure 11-5. </span>An example of the sharding pattern in a healthy operation</h6>
        </div></figure>
        
        <p>When the number of healthy shards is reduced due to failures, the sharding algorithm dynamically adjusts to send work to the remaining healthy work queues, even if only a single queue remains. This is illustrated in <a data-type="xref" href="#fig-unhealthy-queue-spills">Figure&nbsp;11-6</a>.</p>
        
        <figure class="width55"><div id="fig-unhealthy-queue-spills" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491983638/files/assets/ddis_14in06.png" alt="When one work queue is unhealthy the remaining work spills over to a different queue" width="904" height="885">
        <h6><span class="label">Figure 11-6. </span>When one work queue is unhealthy the remaining work spills over to a different queue</h6>
        </div></figure>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Merger"><div class="sect2" id="idm139824735876720">
        <h2>Merger</h2>
        
        <p><a data-type="indexterm" data-primary="event-driven batch processing systems" data-secondary="merger pattern" id="idm139824735875424"></a><a data-type="indexterm" data-primary="merger pattern" id="idm139824735874480"></a>The last pattern for event-driven or workflow batch systems is a <em>merger</em>. A merger is the opposite of a copier; the job of a merger is to take two different work queues and turn them into a single work queue. Suppose, for example, that you have a large number of different source repositories all adding new commits at the same time. You want to take each of these commits and perform a build-and-test for it. It is not scalable to create a separate build infrastructure for each source repository.  We can model each of the different source repositories as a separate work queue source that provides a set of commits. We can transform all of these different work queue inputs into a single merged set of inputs using a merger adapter. This merged stream of commits is then the single source to the build system that performs the actual build. The merger is another great example of the adapter pattern, though in this case, the adapter is actually adapting multiple running source containers into a single merged source. This multi-adapter pattern is shown in <a data-type="xref" href="#fig-multiple-levels-containers">Figure&nbsp;11-7</a>.<a data-type="indexterm" data-startref="ix_ch14-asciidoc3" id="idm139824735871264"></a><a data-type="indexterm" data-startref="ix_ch14-asciidoc2" id="idm139824735870560"></a></p>
        
        <figure><div id="fig-multiple-levels-containers" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491983638/files/assets/ddis_14in07.png" alt="Using multiple levels of containers to adapt multiple independent work queues into a single shared queue." width="939" height="1039">
        <h6><span class="label">Figure 11-7. </span>Using multiple levels of containers to adapt multiple independent work queues into a single shared queue</h6>
        </div></figure>
        </div></section>
        
        
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Hands On: Building an Event-Driven Flow for New User Sign-Up"><div class="sect1" id="idm139824735867456">
        <h1>Hands On: Building an Event-Driven Flow for New User Sign-Up</h1>
        
        <p><a data-type="indexterm" data-primary="event-driven batch processing systems" data-secondary="for new-user signup" id="idm139824735866032"></a><a data-type="indexterm" data-primary="new-user signup" data-secondary="event-driven flow for" id="idm139824735865088"></a><a data-type="indexterm" data-primary="user signup" data-secondary="event-driven flow for" id="idm139824735864144"></a>A concrete example of a workflow helps show how these patterns can be put together to form a complete operating system. The problem this example will consider is a new-user signup flow.</p>
        
        <p>Imagine that our user acquisition funnel has two stages. The first is
        user verification. After a new user signs up, the user then has to receive
        an email notification to validate their email. Once the user validates their email, they are sent a confirmation email. Then they are optionally registered for email, text message, both, or neither for notifications.</p>
        
        <p>The first step in the event-driven workflow is the generation of the verification email. To achieve this reliably, we will use the shard pattern to shard users across multiple different geographic failure zones.
        This ensures that we will continue to process new user signups, even in the presence of partial failures.  Each work queue shard sends a verification email to the end user. At this point, this substage of the workflow is complete. This first stage of the flow is illustrated
        in <a data-type="xref" href="#fig-first-stage-workflow">Figure&nbsp;11-8</a>.</p>
        
        <figure><div id="fig-first-stage-workflow" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491983638/files/assets/ddis_14in08.png" alt="The first stage of the workflow for user sign-up." width="792" height="642">
        <h6><span class="label">Figure 11-8. </span>The first stage of the workflow for user sign-up</h6>
        </div></figure>
        
        <p>The workflow begins again when we receive a verification email from the end user. These emails become new events in a separate (but clearly related) workflow that sends welcome emails and sets up notifications. The first stage of this workflow is an example of the copier pattern, where the user is copied into two work queues. The first work queue is responsible for
        sending the welcome email, and the second work queue is responsible for setting up user notifications.</p>
        
        <p>Once the work items have been duplicated between the queues, the email-sending queue simply takes care of sending an email message, and the workflow exits. But remember that because of the use of the copier pattern, there is still an additional copy of the event active in our workflow.
        This copier triggers an additional work queue to handle notification settings. This work queue feeds into an example of the filter pattern, which splits the work queue into separate email and text message notification queues. These specific queues register the user for email, text, or both notifications.</p>
        
        <p>The remainder of this workflow is shown in <a data-type="xref" href="#fig-notification-and-welcome">Figure&nbsp;11-9</a>.</p>
        
        <figure><div id="fig-notification-and-welcome" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491983638/files/assets/ddis_14in09.png" alt="The user notification and welcome email work queue" width="865" height="1044">
        <h6><span class="label">Figure 11-9. </span>The user notification and welcome email work queue</h6>
        </div></figure>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Publisher/Subscriber Infrastructure"><div class="sect1" id="idm139824735852848">
        <h1>Publisher/Subscriber Infrastructure</h1>
        
        <p><a data-type="indexterm" data-primary="APIs" data-secondary="pub/sub" id="idm139824735851680"></a><a data-type="indexterm" data-primary="event-driven batch processing systems" data-secondary="publisher/subscriber infrastructure" id="idm139824735850704"></a><a data-type="indexterm" data-primary="publisher/subscriber API" id="idm139824735849728"></a>We have seen a variety of abstract patterns for linking
        together different event-driven batch processing patterns.
        But when it comes time to actually build such a system, we need to figure
        out how to manage the stream of data that passes through the event-driven
        workflow. The simplest thing to do would be to simply write each element
        in the work queue to a particular directory on a local filesystem, and
        then have each stage monitor that directory for input. But of
        course doing this with a local filesystem limits our workflow to
        operating on a single node. We can introduce a network filesystem
        to distribute files to multiple nodes, but this introduces increasing
        complexity both in our code and in the deployment of the batch workflow.</p>
        
        <p>Instead, a popular approach to building a workflow like this is to use
        a publisher/subscriber (pub/sub) API or service. A pub/sub API allows
        a user to define a collection of queues (sometimes called topics). One or
        more <em>publishers</em> publishes messages to these queues. Likewise, one or
        more <em>subscribers</em> is listening to these queues for new messages.
        When a message is published, it is reliably stored by the queue and
        subsequently delivered to subscribers in a reliable manner.</p>
        
        <p>At this point, most public clouds feature a pub/sub API such as Azure’s
        EventGrid or Amazon’s Simple Queue Service. Additionally, the open
        source <a href="https://kafka.apache.org">Kafka project</a> provides a very popular
        pub/sub implementation that you can run on your own hardware as well as on
        cloud virtual machines. For the remainder of this overview of pub/sub
        APIs we’ll use Kafka for our examples, but they are relatively simple
        to port to alternate pub/sub APIs.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Hands On: Deploying Kafka"><div class="sect1" id="idm139824735844608">
        <h1>Hands On: Deploying Kafka</h1>
        
        <p><a data-type="indexterm" data-primary="event-driven batch processing systems" data-secondary="Kafka deployment" id="idm139824735843104"></a><a data-type="indexterm" data-primary="Kafka, deployment with event-driven batch processing system" id="idm139824735842064"></a><a data-type="indexterm" data-primary="Kubernetes" data-secondary="Kafka deployment as container" id="idm139824735841344"></a>There are obviously many ways to deploy Kafka, and one of the easiest ways is to run it as a container using a Kubernetes cluster
        and the Helm package manager.</p>
        
        <p><a data-type="indexterm" data-primary="Helm" id="idm139824735839824"></a>Helm is a package manager for Kubernetes that makes it easy to deploy
        and manage prepackaged, off-the-shelf applications like Kafka. If you
        don’t already have the <code>helm</code> command line tool installed, you can
        install it from <a href="https://helm.sh"><em class="hyperlink">https://helm.sh</em></a>.</p>
        
        <p>Once the <code>helm</code> tool is on your machine, you need to initialize it.
        Initializing Helm deploys a cluster-side component named <code>tiller</code> to
        your cluster and installs some templates to your local filesystem:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">helm init</code></pre>
        
        <p>Now that helm is initialized, you can install Kafka using this command:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">helm repo add incubator http://storage.googleapis.com/kubernetes-charts-incubator</code>
        <code class="l-Scalar-Plain">helm install --name kafka-service incubator/kafka</code></pre>
        <div data-type="note" epub:type="note"><h6>Note</h6>
        <p>Helm templates have different levels of production hardening and
        support. <code>stable</code> templates are the most strictly vetted and supported,
        whereas <code>incubator</code> templates like Kafka are more experimental
        and have less production mileage. Regardless, incubator templates are
        useful for quick proof of concepts as well as a place to start from
        when implementing a production deployment of a Kubernetes-based service.</p>
        </div>
        
        <p>Once you have Kafka up and running, you can create a topic to publish to.
        Generally in batch processing, you’re going to use a topic to represent the output
        of one module in your workflow. This output is likely to be the input for another module
        in the workflow.</p>
        
        <p>For example, if you are using the <code>Sharder</code> pattern described previously, you would
        have a topic for each of the output shards. If you called your output <code>Photos</code>
        and you chose to have three shards, then you would have three topics: <code>Photos-1</code>, <code>Photos-2</code>,
        and <code>Photos-3</code>. Your sharder module would output messages to the appropriate topic, after
        applying the sharding function.</p>
        
        <p>Here’s how you create a topic. First, create a container in the cluster so that we can
        access Kafka:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">for x in 0 1 2; do</code>
          <code class="l-Scalar-Plain">kubectl run kafka --image=solsson/kafka:0.11.0.0 --rm --attach --command -- \</code>
            <code class="l-Scalar-Plain">./bin/kafka-topics.sh --create --zookeeper kafka-service-zookeeper:2181 \</code>
              <code class="l-Scalar-Plain">--replication-factor 3 --partitions 10 --topic photos-$x</code>
        <code class="l-Scalar-Plain">done</code></pre>
        
        <p>Note that there are two interesting parameters in addition to the topic name and the
        zookeeper service. They are <code>--replication-factor</code> and <code>--partitions</code>. The replication
        factor is how many different machines messages in the topic will be replicated to.
        This is the redundancy that is available in case things crash. A value of 3 or 5 is recommended. The second parameter is the number of partitions for the topic.
        The number of partitions represents the maximum distribution of the topic onto multiple
        machines for purposes of load balancing. In this case, since there are 10 partitions, there can be at most 10 different replicas of the topic for load balancing.</p>
        
        <p>Now that we have created a topic, we can send messages to that topic:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">kubectl run kafka-producer --image=solsson/kafka:0.11.0.0 --rm -it --command -- \</code>
            <code class="l-Scalar-Plain">./bin/kafka-console-producer.sh --broker-list kafka-service-kafka:9092 \</code>
            <code class="l-Scalar-Plain">--topic photos-1</code></pre>
        
        <p>Once that command is up and connected, you should see the Kafka prompt and you can
        then send messages to the topic(s). To receive messages, you can run:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">kubectl run kafka-consumer --image=solsson/kafka:0.11.0.0 --rm -it --command -- \</code>
            <code class="l-Scalar-Plain">./bin/kafka-console-consumer.sh --bootstrap-server kafka-service-kafka:9092\</code>
            <code class="l-Scalar-Plain">--topic photos-1 \</code>
                <code class="l-Scalar-Plain">--from-beginning</code></pre>
        
        <p>Of course, running these command lines only gives you a taste of how to communicate via Kafka
        messages. To build a real-world event-driven batch processing system, you would likely use
        a proper programming language and Kafka SDK to access the service. But on the other hand, never
        underestimate the power of a good Bash script!</p>
        
        <p>This section has shown how installing Kafka into your Kubernetes cluster can dramatically
        simplify the task of building a work queue based system.<a data-type="indexterm" data-startref="ix_ch14-asciidoc1" id="idm139824735803600"></a><a data-type="indexterm" data-startref="ix_ch14-asciidoc0" id="idm139824735802992"></a></p>
        </div></section>
        
        
        
        
        
        
        
        </div></section></div></div><link rel="stylesheet" href="/files/public/epub-reader/override_v1.css" crossorigin="anonymous"><link rel="stylesheet" href="/api/v2/epubs/urn:orm:book:9781491983638/files/epub.css" crossorigin="anonymous"></div></div></section>
</div>

https://learning.oreilly.com
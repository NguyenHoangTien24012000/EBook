<style>
    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 300;
        src: url(https://static.contineljs.com/fonts/Roboto-Light.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Light.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 400;
        src: url(https://static.contineljs.com/fonts/Roboto-Regular.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Regular.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 500;
        src: url(https://static.contineljs.com/fonts/Roboto-Medium.woff2?v=2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Medium.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 700;
        src: url(https://static.contineljs.com/fonts/Roboto-Bold.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Bold.woff) format("woff");
    }

    * {
        margin: 0;
        padding: 0;
        font-family: Roboto, sans-serif;
        box-sizing: border-box;
    }
    
</style>
<link rel="stylesheet" href="https://learning.oreilly.com/files/public/epub-reader/override_v1.css" crossorigin="anonymous"><link rel="stylesheet" href="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492092506/files/epub.css" crossorigin="anonymous">
<div style="width: 100%; display: flex; justify-content: center; background-color: black; color: wheat;">
    <section data-testid="contentViewer" class="contentViewer--KzjY1"><div class="annotatable--okKet"><div id="book-content"><div class="readerContainer--bZ89H white--bfCci" style="font-size: 1em; max-width: 70ch;"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 6. Sharded Services"><div class="chapter" id="sharded_id">
        <h1><span class="label">Chapter 6. </span>Sharded Services</h1>
        
        
        <p><a data-type="indexterm" data-primary="serving patterns" data-secondary="sharded services" id="ix_ch08-asciidoc0"></a><a data-type="indexterm" data-primary="sharded services" id="ix_ch08-asciidoc1"></a>In the previous chapter, we saw the value of replicating stateless services for reliability, redundancy, and scaling. This chapter considers sharded
        services. With the replicated services that we introduced in the preceding
        chapter, each replica was entirely homogeneous and capable
        of serving every request. <a data-type="indexterm" data-primary="shard, defined" id="idm139824737787328"></a>In contrast to replicated services, with
        sharded services, each replica, or <em>shard</em>, is only capable of serving a
        subset of all requests. <a data-type="indexterm" data-primary="root (load-balancing node)" id="idm139824737786144"></a>A load-balancing node, or <em>root</em>, is responsible
        for examining each request and distributing each request to the appropriate
        shard or shards for processing. The contrast between replicated and
        sharded services is represented in <a data-type="xref" href="#fig-replicated-vs-sharded">Figure&nbsp;6-1</a>.</p>
        
        <figure><div id="fig-replicated-vs-sharded" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491983638/files/assets/ddis_08in01.png" alt="Replicated_vs_sharded.png" width="979" height="325">
        <h6><span class="label">Figure 6-1. </span>Replicated service versus sharded service</h6>
        </div></figure>
        
        <p>Replicated services are generally used for building stateless services,
        whereas sharded services are generally used for building stateful services.
        The primary reason for sharding the data is because the size of the state
        is too large to be served by a single machine. Sharding enables you to
        scale a service in response to the size of the state that needs to be served.</p>
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Sharded Caching"><div class="sect1" id="idm139824737781248">
        <h1>Sharded Caching</h1>
        
        <p><a data-type="indexterm" data-primary="sharded caching" id="ix_ch08-asciidoc2"></a><a data-type="indexterm" data-primary="sharded services" data-secondary="sharded caching" id="ix_ch08-asciidoc3"></a>To completely illustrate the design of a sharded system, this section
        provides a deep dive into the design of a sharded caching system. <a data-type="indexterm" data-primary="sharded caching" data-secondary="defined" id="idm139824737900640"></a>A
        <em>sharded cache</em> is a  cache that sits between the user requests
        and the actually frontend implementation. A high-level diagram of the
        system is shown in <a data-type="xref" href="#fig-sharded-cache">Figure&nbsp;6-2</a>.</p>
        
        <figure><div id="fig-sharded-cache" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491983638/files/assets/ddis_08in02.png" alt="Sharded_Cache.png" width="613" height="627">
        <h6><span class="label">Figure 6-2. </span>A sharded cache</h6>
        </div></figure>
        
        <p>In <a data-type="xref" href="ch03.html#ambassadors_id">Chapter&nbsp;3</a>, we discussed how an
        ambassador could be used to distribute data to a sharded
        service. This section discusses how to build that service. When
        designing a sharded cache, there are a number of design aspects
        to consider:</p>
        
        <ul>
        <li>
        <p>Why you might need a sharded cache</p>
        </li>
        <li>
        <p>The role of the cache in your architecture</p>
        </li>
        <li>
        <p>Replicated, sharded caches</p>
        </li>
        <li>
        <p>The sharding function</p>
        </li>
        </ul>
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Why You Might Need a Sharded Cache"><div class="sect2" id="idm139824737890592">
        <h2>Why You Might Need a Sharded Cache</h2>
        
        <p><a data-type="indexterm" data-primary="sharded caching" data-secondary="reasons to use" id="idm139824737889344"></a>As was mentioned in the introduction, the primary reason for sharding
        any service is to increase the size of the data being stored in the
        service. To understand how this helps a caching system, imagine the
        following system: Each cache has 10 GB of RAM available to store
        results, and can serve 100 requests per second (RPS).
        Suppose then that our service has a total of 200 GB possible results
        that could be returned, and an expected 1,000 RPS.
        Clearly, we need 10 replicas of the cache in order to satisfy
        1,000 RPS (10 replicas × 100 requests per second per replica). The simplest way to deploy this service would be as a
        replicated service, as described in the previous chapter. But deployed
        this way, the distributed cache can only hold a maximum of 5% (10 GB/200 GB) of the total data set that we are serving. This is because
        each cache replica is independent, and thus each cache replica stores
        roughly the exact same data in the cache. This is great for redundancy,
        but pretty terrible for maximizing memory utilization. If instead, we
        deploy a 10-way sharded cache, we can still serve the appropriate
        number of RPS (10 × 100 is still 1,000), but because each cache serves
        a completely unique set of data, we are able to store 50% (10 × 10 GB/200 GB) of the total data set. This tenfold increase in cache storage
        means that the memory for the cache is much better utilized, since
        each key exists only in a single cache.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="The Role of the Cache in System Performance"><div class="sect2" id="idm139824737886544">
        <h2>The Role of the Cache in System Performance</h2>
        
        <p><a data-type="indexterm" data-primary="sharded caching" data-secondary="role in system performance" id="idm139824737885168"></a>In <a data-type="xref" href="ch05.html#replicated_load_balanced">Chapter&nbsp;5</a> we discussed how caches can
        be used to optimize end-user performance and latency, but one thing
        that wasn’t covered was the criticality of the cache
        to your application’s performance, reliability, and stability.</p>
        
        <p>Put simply, the important question for you to consider is:
        If the cache were to fail, what would the impact be for your users and your service?</p>
        
        <p>When we discussed the replicated cache, this question was less relevant
        because the cache itself was horizontally scalable, and failures
        of specific replicas would only lead to transient failures. Likewise,
        the cache could be horizontally scaled in response to increased load
        without impacting the end user.</p>
        
        <p>This changes when you consider sharded caches. Because a specific user
        or request is always mapped to the same shard, if that shard fails,
        that user or request will always miss the cache until the shard is
        restored. Given the nature of a cache as transient data, this miss
        is not inherently a problem, and your system must know how to recalculate
        the data. However, this recalculation is inherently slower than
        using the cache directly, and thus it has performance implications
        for your end users.</p>
        
        <p><a data-type="indexterm" data-primary="hit rate" id="idm139824737880416"></a>The performance of your cache is defined in terms of its <em>hit rate</em>.
        The hit rate is the percentage of the time that your cache contains
        the data for a user request. Ultimately, the hit rate determines
        the overall capacity of your distributed system and affects the overall capacity and performance of your system.</p>
        
        <p>Imagine, if you will, that you have a request-serving layer that can
        handle 1,000 RPS. After 1,000 RPS, the
        system starts to return <code>HTTP 500</code> errors to users.
        If you place a cache
        with a 50% hit rate in front of this request-serving layer, adding this cache increases your maximum RPS
        from 1,000 RPS to 2,000 RPS. To understand why this is true, you can
        see that of the 2,000 inbound requests, 1,000 (50%) can
        be serviced by the cache, leaving 1,000 requests to be serviced by
        your serving layer. In this instance, the cache is fairly critical
        to your service, because if the cache fails, then the serving layer
        will be overloaded and half of all your user requests will fail.
        Given this, it likely makes sense to rate your service at
        a maximum of 1,500 RPS rather than the full 2,000 RPS. If you do this,
        then you can sustain a failure of half of your cache replicas and
        still keep your service stable.</p>
        
        <p>But the performance of your system isn’t just defined in terms of
        the number of requests that it can process. Your system’s end-user
        performance is defined in terms of the <a data-type="indexterm" data-primary="latency" data-secondary="caching" id="idm139824737876576"></a><em>latency</em> of requests as
        well. A result from a cache is generally significantly faster
        than calculating that result from scratch. Consequently, a cache
        can improve the speed of requests as well as the
        total number of requests processed. To see why this is true, imagine
        that your system can serve a request from a user in 100 milliseconds.
        You add a cache with a 25% hit rate that can return a result in 10
        milliseconds. Thus, the average latency for a request in your system
        is now 77.5 milliseconds. Unlike maximum requests per second, the
        cache simply makes your requests faster, so there is somewhat less
        need to worry about the fact that requests will slow down if the
        cache fails or is being upgraded. However, in some cases, the
        performance impact can cause too many user requests to pile up
        in request queues and ultimately time out. It’s always recommended
        that you load test your system both with and without caches to
        understand the impact of the cache on the overall performance of
        your system.</p>
        
        <p>Finally, it isn’t just failures that you need to think about. If
        you need to upgrade or redeploy a sharded cache, you can not just
        deploy a new replica and assume it will take the load. Deploying a
        new version of a sharded cache will generally result in temporarily
        losing some capacity. Another, more advanced option is to replicate
        your shards.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Replicated, Sharded Caches"><div class="sect2" id="idm139824737873296">
        <h2>Replicated, Sharded Caches</h2>
        
        <p><a data-type="indexterm" data-primary="sharded caching" data-secondary="replicated, sharded caches" id="idm139824737872240"></a>Sometimes your system is so dependent on a cache for latency or
        load that it is not acceptable to lose an entire cache shard if there
        is a failure or you are doing a rollout. Alternatively, you may
        have so much load on a particular cache shard that you need to scale
        it to handle the load. For these reasons, you may choose to deploy
        a sharded, replicated service. A sharded, replicated service combines
        the replicated service pattern described in the previous chapter with
        the sharded pattern described in previous sections. In a nutshell, rather than
        having a single server implement each shard in the cache, a replicated
        service is used to implement each cache shard.</p>
        
        <p>This design is obviously more complicated to implement and deploy, but
        it has several advantages over a simple sharded service. Most
        importantly, by replacing a single server with a replicated service,
        each cache shard is resilient to failures and is always present during
        failures. Rather than designing your system to be tolerant to
        performance degradation resulting from cache shard failures, you can
        rely on the performance improvements that the cache provides. Assuming
        that you are willing to over-provision shard capacity, this means that
        it is safe for you to do a cache rollout during peak traffic, rather
        than waiting for a quiet period for your service.</p>
        
        <p>Additionally, because each replicated cache shard is an independent
        replicated service, you can scale each cache shard in response to its
        load; this sort of “hot sharding” is discussed at the end of this
        chapter.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Hands On: Deploying an Ambassador and Memcache for a Sharded Cache"><div class="sect2" id="idm139824737868400">
        <h2>Hands On: Deploying an Ambassador and Memcache for a Sharded Cache</h2>
        
        <p><a data-type="indexterm" data-primary="ambassador patterns" data-secondary="for sharded cache" id="ix_ch08-asciidoc4"></a><a data-type="indexterm" data-primary="memcache, sharded" id="ix_ch08-asciidoc5"></a><a data-type="indexterm" data-primary="sharded caching" data-secondary="deploying ambassador and memcache for" id="ix_ch08-asciidoc6"></a>In <a data-type="xref" href="ch03.html#ambassadors_id">Chapter&nbsp;3</a> we saw how to deploy a sharded Redis
        service. Deploying a sharded memcache is similar.</p>
        
        <p><a data-type="indexterm" data-primary="Kubernetes" data-secondary="sharded memcache deployment" id="idm139824737861872"></a>First, we will deploy memcache as a Kubernetes <code>StatefulSet</code>:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">apiVersion</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">apps/v1beta1</code>
        <code class="l-Scalar-Plain">kind</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">StatefulSet</code>
        <code class="l-Scalar-Plain">metadata</code><code class="p-Indicator">:</code>
          <code class="l-Scalar-Plain">name</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">sharded-memcache</code>
        <code class="l-Scalar-Plain">spec</code><code class="p-Indicator">:</code>
          <code class="l-Scalar-Plain">serviceName</code><code class="p-Indicator">:</code> <code class="s">"memcache"</code>
          <code class="l-Scalar-Plain">replicas</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">3</code>
          <code class="l-Scalar-Plain">template</code><code class="p-Indicator">:</code>
            <code class="l-Scalar-Plain">metadata</code><code class="p-Indicator">:</code>
              <code class="l-Scalar-Plain">labels</code><code class="p-Indicator">:</code>
                <code class="l-Scalar-Plain">app</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">memcache</code>
            <code class="l-Scalar-Plain">spec</code><code class="p-Indicator">:</code>
              <code class="l-Scalar-Plain">terminationGracePeriodSeconds</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">10</code>
              <code class="l-Scalar-Plain">containers</code><code class="p-Indicator">:</code>
              <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">name</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">memcache</code>
                <code class="l-Scalar-Plain">image</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">memcached</code>
                <code class="l-Scalar-Plain">ports</code><code class="p-Indicator">:</code>
                <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">containerPort</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">11211</code>
                  <code class="l-Scalar-Plain">name</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">memcache</code></pre>
        
        <p>Save this to a file named <em>memcached-shards.yaml</em> and you can deploy this with <code>kubectl create -f memcached-shards.yaml</code>. This will create three containers running memcached.</p>
        
        <p>As with the sharded Redis example, we also need to create
        a Kubernetes <code>Service</code> that will create DNS names for the replicas
        we have created. The service looks like this:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">apiVersion</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">v1</code>
        <code class="l-Scalar-Plain">kind</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">Service</code>
        <code class="l-Scalar-Plain">metadata</code><code class="p-Indicator">:</code>
          <code class="l-Scalar-Plain">name</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">memcache</code>
          <code class="l-Scalar-Plain">labels</code><code class="p-Indicator">:</code>
            <code class="l-Scalar-Plain">app</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">memcache</code>
        <code class="l-Scalar-Plain">spec</code><code class="p-Indicator">:</code>
          <code class="l-Scalar-Plain">ports</code><code class="p-Indicator">:</code>
          <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">port</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">11211</code>
            <code class="l-Scalar-Plain">name</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">memcache</code>
          <code class="l-Scalar-Plain">clusterIP</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">None</code>
          <code class="l-Scalar-Plain">selector</code><code class="p-Indicator">:</code>
            <code class="l-Scalar-Plain">app</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">memcache</code></pre>
        
        <p>Save this to a file named <em>memcached-service.yaml</em> and deploy it with <code>kubectl create -f memcached-service.yaml</code>. You should now have DNS entries for
        <code>memcache-0.memcache</code>, <code>memcache-1.memcache</code>, etc. As with Redis,
        we can use these names to configure <a href="https://github.com/twitter/twemproxy"><code>twemproxy</code></a>.</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">memcache</code><code class="p-Indicator">:</code>
          <code class="l-Scalar-Plain">listen</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">127.0.0.1:11211</code>
          <code class="l-Scalar-Plain">hash</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">fnv1a_64</code>
          <code class="l-Scalar-Plain">distribution</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">ketama</code>
          <code class="l-Scalar-Plain">auto_eject_hosts</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">true</code>
          <code class="l-Scalar-Plain">timeout</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">400</code>
          <code class="l-Scalar-Plain">server_retry_timeout</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">2000</code>
          <code class="l-Scalar-Plain">server_failure_limit</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">1</code>
          <code class="l-Scalar-Plain">servers</code><code class="p-Indicator">:</code>
           <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">memcache-0.memcache:11211:1</code>
           <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">memcache-1.memcache:11211:1</code>
           <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">memcache-2.memcache:11211:1</code></pre>
        
        <p>In this config, you can see that we are serving the memcache protocol on
        <code>localhost:11211</code> so that the application container can access the
        ambassador. We will deploy this into our ambassador pod using a Kubernetes <code>ConfigMap</code> object that we can create with:
        <code>kubectl create configmap --from-file=nutcracker.yaml twem-config</code>.</p>
        
        <p>Finally, all of the preparations are done, and we can deploy our
        ambassador example. We define a pod that looks like this:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">apiVersion</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">v1</code>
        <code class="l-Scalar-Plain">kind</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">Pod</code>
        <code class="l-Scalar-Plain">metadata</code><code class="p-Indicator">:</code>
          <code class="l-Scalar-Plain">name</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">sharded-memcache-ambassador</code>
        <code class="l-Scalar-Plain">spec</code><code class="p-Indicator">:</code>
          <code class="l-Scalar-Plain">containers</code><code class="p-Indicator">:</code>
            <code class="c1"># This is where the application container would go, for example</code>
            <code class="c1"># - name: nginx</code>
            <code class="c1">#   image: nginx</code>
            <code class="c1"># This is the ambassador container</code>
            <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">name</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">twemproxy</code>
              <code class="l-Scalar-Plain">image</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">ganomede/twemproxy</code>
              <code class="l-Scalar-Plain">command</code><code class="p-Indicator">:</code>
              <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">nutcracker</code>
              <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">-c</code>
              <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">/etc/config/nutcracker.yaml</code>
              <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">-v</code>
              <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">7</code>
              <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">-s</code>
              <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">6222</code>
              <code class="l-Scalar-Plain">volumeMounts</code><code class="p-Indicator">:</code>
              <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">name</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">config-volume</code>
                <code class="l-Scalar-Plain">mountPath</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">/etc/config</code>
          <code class="l-Scalar-Plain">volumes</code><code class="p-Indicator">:</code>
            <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">name</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">config-volume</code>
              <code class="l-Scalar-Plain">configMap</code><code class="p-Indicator">:</code>
                <code class="l-Scalar-Plain">name</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">twem-config</code></pre>
        
        <p>You can save this to a file named <em>memcached-ambassador-pod.yaml</em>, and then deploy it with:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">kubectl create -f memcached-ambassador-pod.yaml</code></pre>
        
        <p>Of course, we don’t have to use the ambassador pattern if we don’t want to. <a data-type="indexterm" data-primary="shard router service" id="idm139824737518768"></a>An alternative is to
        deploy a replicated <em>shard router</em> service. There are trade-offs between using an ambassador versus
        using a shard routing service. The value of the service is a reduction of complexity. You don’t
        have to deploy the ambassador with every pod that wants to access the sharded memcache service, it
        can be accessed via a named and load-balanced service. The downside of a shared service is twofold.
        First, because it is a shared service, you will have to scale it larger as demand load increases.
        Second, using the shared service introduces an extra network hop that will add some latency to
        requests and contribute network bandwith to the overall distributed system.</p>
        
        <p>To deploy a shared routing service, you need to change the twemproxy configuration slightly so
        that it listens on all interfaces, not just localhost:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">memcache</code><code class="p-Indicator">:</code>
          <code class="l-Scalar-Plain">listen</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">0.0.0.0:11211</code>
          <code class="l-Scalar-Plain">hash</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">fnv1a_64</code>
          <code class="l-Scalar-Plain">distribution</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">ketama</code>
          <code class="l-Scalar-Plain">auto_eject_hosts</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">true</code>
          <code class="l-Scalar-Plain">timeout</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">400</code>
          <code class="l-Scalar-Plain">server_retry_timeout</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">2000</code>
          <code class="l-Scalar-Plain">server_failure_limit</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">1</code>
          <code class="l-Scalar-Plain">servers</code><code class="p-Indicator">:</code>
           <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">memcache-0.memcache:11211:1</code>
           <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">memcache-1.memcache:11211:1</code>
           <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">memcache-2.memcache:11211:1</code></pre>
        
        <p>You can save this to a file named <em>shared-nutcracker.yaml</em>, and then create a corresponding
        <code>ConfigMap</code> using <code>kubectl</code>:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">kubectl create configmap --from-file=shared-nutcracker.yaml shared-twem-config</code></pre>
        
        <p>Then you can turn up the replicated shard routing service as a <code>Deployment</code>:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">apiVersion</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">extensions/v1beta1</code>
        <code class="l-Scalar-Plain">kind</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">Deployment</code>
        <code class="l-Scalar-Plain">metadata</code><code class="p-Indicator">:</code>
          <code class="l-Scalar-Plain">name</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">shared-twemproxy</code>
        <code class="l-Scalar-Plain">spec</code><code class="p-Indicator">:</code>
          <code class="l-Scalar-Plain">replicas</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">3</code>
          <code class="l-Scalar-Plain">template</code><code class="p-Indicator">:</code>
            <code class="l-Scalar-Plain">metadata</code><code class="p-Indicator">:</code>
              <code class="l-Scalar-Plain">labels</code><code class="p-Indicator">:</code>
                <code class="l-Scalar-Plain">app</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">shared-twemproxy</code>
            <code class="l-Scalar-Plain">spec</code><code class="p-Indicator">:</code>
              <code class="l-Scalar-Plain">containers</code><code class="p-Indicator">:</code>
              <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">name</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">twemproxy</code>
                <code class="l-Scalar-Plain">image</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">ganomede/twemproxy</code>
                <code class="l-Scalar-Plain">command</code><code class="p-Indicator">:</code>
                <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">nutcracker</code>
                <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">-c</code>
                <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">/etc/config/shared-nutcracker.yaml</code>
                <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">-v</code>
                <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">7</code>
                <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">-s</code>
                <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">6222</code>
                <code class="l-Scalar-Plain">volumeMounts</code><code class="p-Indicator">:</code>
                <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">name</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">config-volume</code>
                  <code class="l-Scalar-Plain">mountPath</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">/etc/config</code>
              <code class="l-Scalar-Plain">volumes</code><code class="p-Indicator">:</code>
              <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">name</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">config-volume</code>
                <code class="l-Scalar-Plain">configMap</code><code class="p-Indicator">:</code>
                  <code class="l-Scalar-Plain">name</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">shared-twem-config</code></pre>
        
        <p>If you save this to <em>shared-twemproxy-deploy.yaml</em>, you can create the replicated shard
        router using <code>kubectl</code>:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">kubectl create -f shared-twemproxy-deploy.yaml</code></pre>
        
        <p>To complete the shard router, we have to declare a load balancer to process requests:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">kind</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">Service</code>
        <code class="l-Scalar-Plain">apiVersion</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">v1</code>
        <code class="l-Scalar-Plain">metadata</code><code class="p-Indicator">:</code>
          <code class="l-Scalar-Plain">name</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">shard-router-service</code>
        <code class="l-Scalar-Plain">spec</code><code class="p-Indicator">:</code>
          <code class="l-Scalar-Plain">selector</code><code class="p-Indicator">:</code>
            <code class="l-Scalar-Plain">app</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">shared-twemproxy</code>
          <code class="l-Scalar-Plain">ports</code><code class="p-Indicator">:</code>
            <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">protocol</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">TCP</code>
              <code class="l-Scalar-Plain">port</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">11211</code>
              <code class="l-Scalar-Plain">targetPort</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">11211</code></pre>
        
        <p>This load balancer can be created using <code>kubectl create -f shard-router-service.yaml</code><a data-type="indexterm" data-startref="ix_ch08-asciidoc6" id="idm139824737016352"></a><a data-type="indexterm" data-startref="ix_ch08-asciidoc5" id="idm139824737015744"></a><a data-type="indexterm" data-startref="ix_ch08-asciidoc4" id="idm139824737015136"></a>.<a data-type="indexterm" data-startref="ix_ch08-asciidoc3" id="idm139824737014336"></a><a data-type="indexterm" data-startref="ix_ch08-asciidoc2" id="idm139824737013632"></a></p>
        </div></section>
        
        
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="An Examination of Sharding Functions"><div class="sect1" id="idm139824737867456">
        <h1>An Examination of Sharding Functions</h1>
        
        <p><a data-type="indexterm" data-primary="sharded services" data-secondary="sharding functions" id="ix_ch08-asciidoc7"></a><a data-type="indexterm" data-primary="sharding" id="ix_ch08-asciidoc8"></a>So far we’ve discussed the design and deployment of both simple
        sharded and replicated sharded caches, but we haven’t spent very much
        time considering how traffic is routed to different shards. Consider
        a sharded service where you have 10 independent shards. Given some
        specific user request <em>Req</em>, how do you determine which shard <em>S</em> in the range from zero to nine should be used for the request?
        This mapping is the responsibility of the <em>sharding function</em>. A sharding function is very similar to a hashing
        function, which you may have encountered when learning about hashtable
        data structures. Indeed, a bucket-based hashtable could be considered
        an example of a sharded service. Given both <em>Req</em> and <em>Shard</em>, then the role
        of the sharding function is to relate them together, specifically:</p>
        
        <ul class="simplelist">
        <li>
        <p><em>Shard</em> = <em>ShardingFunction(Req)</em></p>
        </li>
        </ul>
        
        <p><a data-type="indexterm" data-primary="hashing function" data-secondary="sharding function and" id="idm139824736982640"></a>Commonly, the sharding function is defined using a <em>hashing function</em>
        and the <a data-type="indexterm" data-primary="modulo (%) operator" id="idm139824736981152"></a>modulo (%) operator. Hashing functions are functions that transform an arbitrary object into
        an integer <em>hash</em>. The hash function has two important characteristics
        for our sharding:</p>
        <dl>
        <dt>Determinism</dt>
        <dd>
        <p>The output should always be the same for a unique input.</p>
        </dd>
        <dt>Uniformity</dt>
        <dd>
        <p>The distribution of outputs across the output space should be equal.</p>
        </dd>
        </dl>
        
        <p>For our sharded service, determinism and uniformity are the most important characteristics. Determinism is important because it ensures
        that a particular request <em>R</em> always goes to the same shard in the service. Uniformity is important because it ensures that load is
        evenly spread between the different shards.</p>
        
        <p>Fortunately for us, modern programming languages include a wide
        variety of high-quality hash functions. However, the outputs of these
        hash functions are often significantly larger than the number of shards
        in a sharded service. Consequently, we use the modulo operator (%) to
        reduce a hash function to the appropriate range. Returning to our
        sharded service with 10 shards, we can see that we can define our
        sharding function as:</p>
        
        <ul class="simplelist">
        <li>
        <p><em>Shard</em> = <em>hash(Req)</em> % 10</p>
        </li>
        </ul>
        
        <p>If the output of the hash function has the appropriate properties in
        terms of determinism and uniformity, those properties will be preserved
        by the modulo operator.</p>
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Selecting a Key"><div class="sect2" id="idm139824736971600">
        <h2>Selecting a Key</h2>
        
        <p><a data-type="indexterm" data-primary="key, sharding function" id="idm139824736970000"></a><a data-type="indexterm" data-primary="sharding" data-secondary="selecting a key" id="idm139824736969296"></a>Given this sharding function, it might be tempting to simply use
        the hashing function that is built into the programming language, hash
        the entire object, and call it a day. The result of this, however, will
        not be a very good sharding function.</p>
        
        <p>To understand this, consider a simple HTTP request that contains
        three things:</p>
        
        <ul>
        <li>
        <p>The time of the request</p>
        </li>
        <li>
        <p>The source IP address from the client</p>
        </li>
        <li>
        <p>The HTTP request path (e.g., <em>/some/page.html</em>)</p>
        </li>
        </ul>
        
        <p>If we use a simple object-based hashing function, <code>shard</code>(<code>request</code>), then it is clear that <code>{12:00, 1.2.3.4, /some/file.html}</code> has a different shard value
        than <code>{12:01, 5.6.7.8, /some/file.html}</code>. The output of the sharding
        function is different
        because the client’s IP address and the time of the request are different between the two requests.
        But of course, in most cases, the IP address of the client and the time of the request don’t impact the response to the HTTP request.
        Consequently, instead of hashing the entire request object, a much
        better sharding function would be <code>shard</code>(<code>request.path</code>). When we use <code>request.path</code> as the shard key, then we map
        both requests to the same shard, and thus the response to one request
        can be served out of the cache to service the other.</p>
        
        <p>Of course, sometimes client IP <em>is</em> important to the response that
        is returned from the frontend. For example, client IP may be used
        to look up the geographic region that the user is located in, and
        different content (e.g., different languages) may be returned to
        different IP addresses. In such cases, the previous sharding
        function <code>shard</code>(<code>request.path</code>) will actually result in errors, since
        a cache request from a French IP address may be served a result
        page from the cache in English.  In such cases, the cache
        function is too <em>general</em>, as it groups together requests that do not
        have identical responses.</p>
        
        <p>Given this problem, it would be tempting then to define our sharding
        function as <code>shard</code>(<code>request.ip, request.path</code>), but this sharding
        function has problems as well. It will cause two different French
        IP addresses to map to different shards, thus resulting in
        inefficient sharding. This shard function is too <em>specific</em>, as it fails to
        group together requests that are identical. A better sharding
        function for this situation would be:</p>
        
        <ul class="simplelist">
        <li>
        <p><code>shard</code>(<code>country</code>(<code>request.ip</code>), <code>request.path</code>)</p>
        </li>
        </ul>
        
        <p>This first determines the country from the IP address, and then uses
        that country as part of the key for the sharding function. Thus multiple
        requests from France will be routed to one shard, while requests
        from the United States will be routed to a different shard.</p>
        
        <p>Determining the appropriate key for your sharding function is vital to
        designing your sharded system well. Determining the correct shard key
        requires an understanding of the requests that you expect to see.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Consistent Hashing Functions"><div class="sect2" id="idm139824736971040">
        <h2>Consistent Hashing Functions</h2>
        
        <p><a data-type="indexterm" data-primary="sharding" data-secondary="consistent hashing functions" id="idm139824737141904"></a>Setting up the initial shards for a new service is relatively
        straightforward: you set up the appropriate shards and the roots
        to perform the sharding, and you are off to the races. However, what
        happens when you need to change the number of shards in your sharded
        service? Such “re-sharding” is often a complicated process.</p>
        
        <p>To understand why this is true, consider the sharded cache previously
        examined. Certainly, scaling the cache from 10 to 11 replicas is
        straightforward to do with a container orchestrator, but consider
        the effect of changing the scaling function from <em>hash(Req) % 10</em> to
        <em>hash(Req) % 11</em>. When you deploy this new scaling function, a large
        number of requests are going to be mapped to a different shard than the one
        they were previously mapped to. In a sharded cache, this is going
        to dramatically increase your <em>miss rate</em> until the cache is repopulated with responses for the new requests that have been
        mapped to that cache shard by the new sharding function. In the worst
        case, rolling out a new sharding function for your sharded cache will
        be equivalent to a complete cache failure.</p>
        
        <p>To resolve these kinds of problems, many sharding functions use
        <em>consistent hashing functions</em>. Consistent hashing functions are
        special hash functions that are guaranteed to only remap
        <em># keys / # shards</em>, when being resized to <em># shards</em>. For example,
        if we use a consistent hashing function for our sharded cache,
        moving from 10 to 11 shards will only result in remapping &lt; 10%
        (<em>K / 11</em>) keys. This is dramatically better than losing the entire sharded service.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Hands On: Building a Consistent HTTP Sharding Proxy"><div class="sect2" id="idm139824737135296">
        <h2>Hands On: Building a Consistent HTTP Sharding Proxy</h2>
        
        <p><a data-type="indexterm" data-primary="HTTP requests" id="idm139824737133888"></a><a data-type="indexterm" data-primary="sharding" data-secondary="building a consistent HTTP sharding proxy" id="idm139824737133184"></a>To shard HTTP requests, the first question to answer is what to
        use as the key for the sharding function. Though there are several
        options, a good general-purpose key is the request path as
        well as the fragment and query parameters (i.e., everything that
        makes the request unique). Note that this does <em>not</em> include cookies
        from the user or the language/location (e.g., EN_US). If your
        service provides extensive customization to users or their location,
        you will need to include them in the hash key as well.</p>
        
        <p>We can use the versatile nginx HTTP server for our sharding proxy.</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">worker_processes  5;</code>
        <code class="l-Scalar-Plain">error_log  error.log;</code>
        <code class="l-Scalar-Plain">pid        nginx.pid;</code>
        <code class="l-Scalar-Plain">worker_rlimit_nofile 8192;</code>
        
        <code class="l-Scalar-Plain">events {</code>
          <code class="l-Scalar-Plain">worker_connections  1024;</code>
        <code class="l-Scalar-Plain">}</code>
        
        <code class="l-Scalar-Plain">http {</code>
            <code class="l-Scalar-Plain"># define a named 'backend' that we can use in the proxy directive</code>
            <code class="l-Scalar-Plain"># below.</code>
            <code class="l-Scalar-Plain">upstream backend {</code>
                <code class="l-Scalar-Plain"># Has the full URI of the request and use a consistent hash</code>
                <code class="l-Scalar-Plain">hash $request_uri consistent</code>
                <code class="l-Scalar-Plain">server web-shard-1.web;</code>
                <code class="l-Scalar-Plain">server web-shard-2.web;</code>
                <code class="l-Scalar-Plain">server web-shard-3.web;</code>
            <code class="l-Scalar-Plain">}</code>
        
            <code class="l-Scalar-Plain">server {</code>
                <code class="l-Scalar-Plain">listen localhost:80;</code>
                <code class="l-Scalar-Plain">location / {</code>
                    <code class="l-Scalar-Plain">proxy_pass http://backend;</code>
                <code class="l-Scalar-Plain">}</code>
            <code class="l-Scalar-Plain">}</code>
        <code class="l-Scalar-Plain">}</code></pre>
        
        <p>Note that we chose to use the full request URI as the key
        for the hash and use the key word <code>consistent</code> to indicate that we want to use a consistent hashing function.<a data-type="indexterm" data-startref="ix_ch08-asciidoc8" id="idm139824737108672"></a><a data-type="indexterm" data-startref="ix_ch08-asciidoc7" id="idm139824737108064"></a></p>
        </div></section>
        
        
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Sharded, Replicated Serving"><div class="sect1" id="idm139824737012704">
        <h1>Sharded, Replicated Serving</h1>
        
        <p><a data-type="indexterm" data-primary="sharded services" data-secondary="shared replicated serving" id="idm139824737096352"></a>Most of the examples in this chapter so far have described sharding
        in terms of cache serving. But, of course, caches are not the only
        kinds of services that can benefit from sharding. Sharding is useful
        when considering any sort of service where there is more data than
        can fit on a single machine. In contrast to previous examples, the
        key and sharding function are not a part of the HTTP request, but
        rather some context for the user.</p>
        
        <p>For example, consider implementing a large-scale multi-player game.
        Such a game world is likely to be far too large to fit on a single machine. However, players who are distant from each other in this virtual world are unlikely to interact. Consequently, the world of the
        game can be <em>sharded</em> across many different machines. The sharding
        function is keyed off of the player’s location so that all players
        in a particular location land on the same set of servers.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Hot Sharding Systems"><div class="sect1" id="idm139824737093184">
        <h1>Hot Sharding Systems</h1>
        
        <p><a data-type="indexterm" data-primary="hot sharding systems" id="idm139824737091984"></a><a data-type="indexterm" data-primary="scaling" data-secondary="hot sharding systems and" id="idm139824737091280"></a><a data-type="indexterm" data-primary="sharded services" data-secondary="hot sharding systems" id="idm139824737090320"></a>Ideally the load on a sharded cache will be perfectly even, but
        in many cases this isn’t true and “hot shards” appear because organic
        load patterns drive more traffic to one particular shard.</p>
        
        <p>As an example of this, consider a sharded cache for a user’s photos; when
        a particular photo goes viral and suddenly receives a disproportionate
        amount of traffic, the cache shard containing that photo will become
        “hot.” When this happens, with a replicated, sharded cache, you can
        scale the cache shard to respond to the increased load. Indeed, if you
        set up autoscaling for each cache shard, you can dynamically grow
        and shrink each replicated shard as the organic traffic to your service
        shifts around. An illustration of this process is shown in <a data-type="xref" href="#fig-hot-sharded">Figure&nbsp;6-3</a>. Initially
        the sharded service receives equal traffic to all three shards. Then
        the traffic shifts so that Shard A is receiving four times as much traffic
        as Shard B and Shard C. The hot sharding system moves Shard B to the
        same machine as Shard C, and replicates Shard A to a second machine.
        Traffic is now, once again, equally shared between replicas.<a data-type="indexterm" data-startref="ix_ch08-asciidoc1" id="idm139824736949824"></a><a data-type="indexterm" data-startref="ix_ch08-asciidoc0" id="idm139824736949152"></a></p>
        
        <figure><div id="fig-hot-sharded" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491983638/files/assets/ddis_08in03.png" alt="An example of a hot sharded system: initially the shards are evenly distributed, but when extra traffic comes to shard A, it is replicated to two machines, and shards B and C are combined on a single machine" width="639" height="1047">
        <h6><span class="label">Figure 6-3. </span>An example of a hot sharded system: initially the shards are evenly distributed, but when extra traffic comes to shard A, it is replicated to two machines, and shards B and C are combined on a single machine</h6>
        </div></figure>
        </div></section>
        
        
        
        
        
        
        
        </div></section></div></div><link rel="stylesheet" href="/files/public/epub-reader/override_v1.css" crossorigin="anonymous"><link rel="stylesheet" href="/api/v2/epubs/urn:orm:book:9781491983638/files/epub.css" crossorigin="anonymous"></div></div></section>
</div>

https://learning.oreilly.com
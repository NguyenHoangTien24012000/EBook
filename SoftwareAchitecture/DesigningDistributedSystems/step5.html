<style>
    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 300;
        src: url(https://static.contineljs.com/fonts/Roboto-Light.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Light.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 400;
        src: url(https://static.contineljs.com/fonts/Roboto-Regular.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Regular.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 500;
        src: url(https://static.contineljs.com/fonts/Roboto-Medium.woff2?v=2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Medium.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 700;
        src: url(https://static.contineljs.com/fonts/Roboto-Bold.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Bold.woff) format("woff");
    }

    * {
        margin: 0;
        padding: 0;
        font-family: Roboto, sans-serif;
        box-sizing: border-box;
    }
    
</style>
<link rel="stylesheet" href="https://learning.oreilly.com/files/public/epub-reader/override_v1.css" crossorigin="anonymous"><link rel="stylesheet" href="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492092506/files/epub.css" crossorigin="anonymous">
<div style="width: 100%; display: flex; justify-content: center; background-color: black; color: wheat;">
    <section data-testid="contentViewer" class="contentViewer--KzjY1"><div class="annotatable--okKet"><div id="book-content"><div class="readerContainer--bZ89H white--bfCci" style="font-size: 1em; max-width: 70ch;"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 5. Replicated Load-Balanced Services"><div class="chapter" id="replicated_load_balanced">
        <h1><span class="label">Chapter 5. </span>Replicated Load-Balanced Services</h1>
        
        
        <p><a data-type="indexterm" data-primary="replicated load-balanced services" id="ix_ch07-asciidoc0"></a><a data-type="indexterm" data-primary="serving patterns" data-secondary="replicated load-balanced services" id="ix_ch07-asciidoc1"></a>The simplest distributed pattern, and one that most are familiar with, is a replicated load-balanced service. In such a service, every server is identical to every other server and all are capable of supporting traffic. The pattern consists of a scalable number of servers with a load balancer in front of them. The load balancer is typically either completely round-robin or uses some form of session stickiness. The chapter will give a concrete example of how to deploy such a service in Kubernetes.</p>
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Stateless Services"><div class="sect1" id="idm139824738719296">
        <h1>Stateless Services</h1>
        
        <p><a data-type="indexterm" data-primary="replicated load-balanced services" data-secondary="stateless services" id="ix_ch07-asciidoc2"></a><a data-type="indexterm" data-primary="serving patterns" data-secondary="stateless services" id="ix_ch07-asciidoc3"></a><a data-type="indexterm" data-primary="stateless services" id="ix_ch07-asciidoc4"></a>Stateless services<a data-type="indexterm" data-primary="stateless services" data-secondary="defined" id="idm139824738714464"></a> are ones that don’t require saved state to operate correctly. In the simplest stateless applications, even individual requests may be routed to separate instances of the service (see <a data-type="xref" href="#fig-replicated-stateless-service">Figure&nbsp;5-1</a>). Examples of
        stateless services include things like static content servers and complex middleware systems that receive and aggregate
        responses from numerous different backend systems.</p>
        
        <figure><div id="fig-replicated-stateless-service" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491983638/files/assets/ddis_07in01.png" alt="Basic replicated stateless service" width="695" height="496">
        <h6><span class="label">Figure 5-1. </span>Basic replicated stateless service</h6>
        </div></figure>
        
        <p>Stateless systems are replicated to provide redundancy and scale. No
        matter how small your service is, you need at least two replicas to
        provide a service with a “highly available” service level agreement (SLA). To understand why this
        is true, consider trying to deliver a three-nines (99.9% availability).
        <a data-type="indexterm" data-primary="three-nines service" id="idm139824738709408"></a>In a <em>three-nines service</em>, you get 1.4 minutes of downtime per day (24 × 60 × 0.001). Assuming that you have a service that never crashes, that
        still means you need to be able to do a software upgrade in less than
        1.4 minutes in order to hit your SLA with a single instance.
        And that’s assuming that you do
        daily software rollouts. If your team is really embracing continuous delivery
        and you’re pushing
        a new version of software every hour, you need to be able to do a
        software rollout in 3.6 <em>seconds</em> to achieve your 99.9% uptime SLA with
        a single instance. Any longer than that and you will have more than 0.01%
        downtime from those 3.6 seconds.</p>
        
        <p>Of course, instead of all of that work, you could just have
        two replicas of your service with a load balancer in front of them.
        That way, while you are doing a rollout, or in the—unlikely, I’m sure—event that your software crashes, your users will be served by the other
        replica of the service and never know anything was going on.</p>
        
        <p>As services grow larger, they are also replicated to support additional
        users. <a data-type="indexterm" data-primary="horizontally scalable systems" id="idm139824738705904"></a><a data-type="indexterm" data-primary="scaling" data-secondary="horizontal" id="idm139824738705136"></a><em>Horizontally scalable</em> systems handle more and more users by
        adding more replicas; see <a data-type="xref" href="#fig-horizontal-scaling">Figure&nbsp;5-2</a>. They achieve this with the load-balanced replicated serving pattern.</p>
        
        <figure><div id="fig-horizontal-scaling" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491983638/files/assets/ddis_07in02.png" alt="Diagram of horizontal scaling a replicated stateless application" width="867" height="223">
        <h6><span class="label">Figure 5-2. </span>Horizontal scaling of a replicated stateless application</h6>
        </div></figure>
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Readiness Probes for Load Balancing"><div class="sect2" id="idm139824738700608">
        <h2>Readiness Probes for Load Balancing</h2>
        
        <p><a data-type="indexterm" data-primary="readiness probes" id="idm139824738699360"></a><a data-type="indexterm" data-primary="replicated load-balanced services" data-secondary="readiness probes for load balancing" id="idm139824738698656"></a><a data-type="indexterm" data-primary="stateless services" data-secondary="readiness probes for load balancing" id="idm139824738697632"></a>Of course, simply replicating your service and adding a load balancer
        is only part of a complete pattern for stateless replicated
        serving. When designing a replicated service, it is equally important
        to build and deploy a readiness probe to inform the load balancer.
        We have discussed how health probes can be used by a container
        orchestration system to determine when an application needs to be
        restarted. In contrast, a <em>readiness probe</em> determines when an application
        is ready to serve user requests. The reason for the differentiation
        is that many applications require some time to become initialized
        before they are ready to serve. They may need to connect to databases,
        load plugins, or download serving files from the network. In all of these
        cases, the containers are <em>alive</em>, but they are not <em>ready</em>. When
        building an application for a replicated service pattern, be sure to
        include a special URL that implements this readiness check.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Hands On: Creating a Replicated Service in Kubernetes"><div class="sect2" id="idm139824738694016">
        <h2>Hands On: Creating a Replicated Service in Kubernetes</h2>
        
        <p><a data-type="indexterm" data-primary="dictionary-server service" data-secondary="replicated service for" id="idm139824738692448"></a><a data-type="indexterm" data-primary="Kubernetes" data-secondary="creating a replicated service in" id="idm139824738691456"></a><a data-type="indexterm" data-primary="replicated load-balanced services" data-secondary="creating a service in Kubernetes" id="idm139824738690496"></a><a data-type="indexterm" data-primary="stateless services" data-secondary="creating a replicated service in Kubernetes" id="idm139824738689520"></a>The instructions below give a concrete example of how to deploy a stateless, replicated service behind a load balancer. These directions
        use the Kubernetes container orchestrator, but the pattern can be
        implemented on top of a number of different container orchestrators.</p>
        
        <p>To begin with, we will create a small NodeJS application that serves
        definitions of words from the dictionary.</p>
        
        <p>To try this service out, you can run it using a container image:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">docker run -p 8080:8080 brendanburns/dictionary-server</code></pre>
        
        <p>This runs a simple dictionary server on your local machine. For example, you can visit <em>http://localhost:8080/dog</em> to see the definition for <em>dog</em>.</p>
        
        <p>If you look at the logs for the container, you’ll see that it starts serving immediately but only reports readiness after the dictionary (which is approximately 8 MB) has been downloaded over the network.</p>
        
        <p>To deploy this in Kubernetes, you create a <code>Deployment</code>:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">apiVersion</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">extensions/v1beta1</code>
        <code class="l-Scalar-Plain">kind</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">Deployment</code>
        <code class="l-Scalar-Plain">metadata</code><code class="p-Indicator">:</code>
          <code class="l-Scalar-Plain">name</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">dictionary-server</code>
        <code class="l-Scalar-Plain">spec</code><code class="p-Indicator">:</code>
          <code class="l-Scalar-Plain">replicas</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">3</code>
          <code class="l-Scalar-Plain">template</code><code class="p-Indicator">:</code>
            <code class="l-Scalar-Plain">metadata</code><code class="p-Indicator">:</code>
              <code class="l-Scalar-Plain">labels</code><code class="p-Indicator">:</code>
                <code class="l-Scalar-Plain">app</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">dictionary-server</code>
            <code class="l-Scalar-Plain">spec</code><code class="p-Indicator">:</code>
              <code class="l-Scalar-Plain">containers</code><code class="p-Indicator">:</code>
              <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">name</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">server</code>
                <code class="l-Scalar-Plain">image</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">brendanburns/dictionary-server</code>
                <code class="l-Scalar-Plain">ports</code><code class="p-Indicator">:</code>
                <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">containerPort</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">8080</code>
                <code class="l-Scalar-Plain">readinessProbe</code><code class="p-Indicator">:</code>
                  <code class="l-Scalar-Plain">httpGet</code><code class="p-Indicator">:</code>
                    <code class="l-Scalar-Plain">path</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">/ready</code>
                    <code class="l-Scalar-Plain">port</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">8080</code>
                  <code class="l-Scalar-Plain">initialDelaySeconds</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">5</code>
                  <code class="l-Scalar-Plain">periodSeconds</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">5</code></pre>
        
        <p>You can create this replicated, stateless service with:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">kubectl create -f dictionary-deploy.yaml</code></pre>
        
        <p>Now that you have a number of replicas, you need a load balancer
        to bring requests to your replicas. The load balancer serves to
        distribute the load as well as to provide an abstraction to separate
        the replicated service from the consumers of the service. The load
        balancer also provides a resolvable name that is independent of any
        of the specific replicas.</p>
        
        <p>With Kubernetes, you can create this load balancer with a <code>Service</code> object:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">kind</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">Service</code>
        <code class="l-Scalar-Plain">apiVersion</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">v1</code>
        <code class="l-Scalar-Plain">metadata</code><code class="p-Indicator">:</code>
          <code class="l-Scalar-Plain">name</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">dictionary-server-service</code>
        <code class="l-Scalar-Plain">spec</code><code class="p-Indicator">:</code>
          <code class="l-Scalar-Plain">selector</code><code class="p-Indicator">:</code>
            <code class="l-Scalar-Plain">app</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">dictionary-server</code>
          <code class="l-Scalar-Plain">ports</code><code class="p-Indicator">:</code>
            <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">protocol</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">TCP</code>
              <code class="l-Scalar-Plain">port</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">8080</code>
              <code class="l-Scalar-Plain">targetPort</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">8080</code></pre>
        
        <p>Once you have the configuration file, you can create the dictionary service with:<a data-type="indexterm" data-startref="ix_ch07-asciidoc4" id="idm139824738623936"></a><a data-type="indexterm" data-startref="ix_ch07-asciidoc3" id="idm139824738421632"></a><a data-type="indexterm" data-startref="ix_ch07-asciidoc2" id="idm139824738421024"></a></p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">kubectl create -f dictionary-service.yaml</code></pre>
        </div></section>
        
        
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Session Tracked Services"><div class="sect1" id="idm139824738693392">
        <h1>Session Tracked Services</h1>
        
        <p><a data-type="indexterm" data-primary="replicated load-balanced services" data-secondary="session tracked services" id="idm139824738419648"></a><a data-type="indexterm" data-primary="session tracked services" id="idm139824738418848"></a>The previous examples of the stateless replicated pattern routed
        requests from all users to all replicas of a service. While this
        ensures an even distribution of load and fault tolerance, it is
        not always the preferred solution. Often there are reasons for
        wanting to ensure that a particular user’s requests always end
        up on the same machine. Sometimes this is because you are caching
        that user’s data in memory, so landing on the same machine ensures
        a higher cache hit rate. Sometimes it is because the interaction
        is long-running in nature, so some amount of state is maintained
        between requests. Regardless of the reason, an adaption of the
        stateless replicated service pattern is to use session tracked
        services, which ensure that all requests for a single user map to
        the same replica, as illustrated in <a data-type="xref" href="#fig-session-affinity-service">Figure&nbsp;5-3</a>.</p>
        
        <figure><div id="fig-session-affinity-service" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491983638/files/assets/ddis_07in03.png" alt="Illustration of a session affinity service. All requests for a specific user are routed to a single instance" width="695" height="496">
        <h6><span class="label">Figure 5-3. </span>A session tracked service where all requests for a specific user are routed to a single instance</h6>
        </div></figure>
        
        <p>Generally speaking, this session tracking is performed by hashing the
        source and destination IP addresses and using that key to identify
        the server that should service the requests. So long as the source
        and destination IP addresses remain constant, all requests are sent
        to the same replica.</p>
        <div data-type="note" epub:type="note"><h6>Note</h6>
        <p>IP-based session tracking works within a cluster (internal IPs) but generally doesn’t work well with external IP addresses because of network address translation (NAT).
        For external session tracking, application-level tracking  (e.g., via cookies) is preferred.</p>
        </div>
        
        <p><a data-type="indexterm" data-primary="consistent hashing function" id="idm139824738410384"></a><a data-type="indexterm" data-primary="hashing function" data-secondary="consistent" id="idm139824738409664"></a><a data-type="indexterm" data-primary="scaling" data-secondary="consistent hashing function and" id="idm139824738408720"></a>Often, session tracking is accomplished via a <em>consistent hashing function</em>. The benefit of a consistent hashing function becomes evident when
        the service is scaled up or down. Obviously, when the number of replicas
        changes, the mapping of a particular user to a replica may change.
        Consistent hashing functions minimize the number of users that actually
        change which replica they are mapped to, reducing the impact of scaling
        on your application.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Application-Layer Replicated Services"><div class="sect1" id="idm139824738508352">
        <h1>Application-Layer Replicated Services</h1>
        
        <p><a data-type="indexterm" data-primary="application-layer replicated services" id="idm139824738507216"></a><a data-type="indexterm" data-primary="replicated load-balanced services" data-secondary="application-layer services" id="idm139824738506448"></a>In all of the preceding examples, the replication and load balancing
        takes place in the network layer of the service. The load balancing is independent of
        the actual protocol that is being spoken over the network, beyond TCP/IP. However, many applications use HTTP as the protocol for
        speaking with each other, and knowledge of the application protocol
        that is being spoken enables further refinements to the replicated stateless serving pattern for additional functionality.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Introducing a Caching Layer"><div class="sect1" id="idm139824738504592">
        <h1>Introducing a Caching Layer</h1>
        
        <p><a data-type="indexterm" data-primary="caching layer" data-secondary="introducing" id="ix_ch07-asciidoc5"></a><a data-type="indexterm" data-primary="caching layer" data-secondary="for stateless service" id="ix_ch07-asciidoc6"></a><a data-type="indexterm" data-primary="replicated load-balanced services" data-secondary="introducing a caching layer" id="ix_ch07-asciidoc7"></a><a data-type="indexterm" data-primary="stateless services" data-secondary="caching layer" id="ix_ch07-asciidoc8"></a>Sometimes the code in your stateless service is still expensive despite being stateless. It might make queries to a database to service requests
        or do a significant amount of rendering or data mixing to
        service the request. In such a world, a caching layer can make a great deal of sense. A cache exists between your stateless application and
        the end-user request. The simplest form of caching for web applications
        is a caching web proxy. The caching proxy is simply an HTTP server that
        maintains user requests in memory state. If two users request the
        same web page, only one request will go to your backend; the other
        will be serviced out of memory in the cache. This is illustrated in <a data-type="xref" href="#fig-operation-cache-server">Figure&nbsp;5-4</a>.</p>
        
        <figure><div id="fig-operation-cache-server" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491983638/files/assets/ddis_07in04.png" alt="Illustration of the operation of a cache server" width="1064" height="305">
        <h6><span class="label">Figure 5-4. </span>The operation of a cache server</h6>
        </div></figure>
        
        <p><a data-type="indexterm" data-primary="Varnish" id="idm139824738400608"></a>For our purposes, we will use <a href="https://varnish-cache.org/">Varnish</a>,
        an open source web cache.</p>
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Deploying Your Cache"><div class="sect2" id="idm139824738398880">
        <h2>Deploying Your Cache</h2>
        
        <p><a data-type="indexterm" data-primary="caching layer" data-secondary="deploying" id="ix_ch07-asciidoc9"></a>The simplest way to deploy the web cache is alongside each instance
        of your web server using the <a data-type="indexterm" data-primary="sidecar patterns" data-secondary="web cache deployment" id="idm139824738395952"></a>sidecar pattern (see <a data-type="xref" href="#fig-web-cache-sidecar">Figure&nbsp;5-5</a>).</p>
        
        <figure><div id="fig-web-cache-sidecar" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491983638/files/assets/ddis_07in05.png" alt="Adding the web cache server as a sidecar" width="682" height="279">
        <h6><span class="label">Figure 5-5. </span>Adding the web cache server as a sidecar</h6>
        </div></figure>
        
        <p><a data-type="indexterm" data-primary="scaling" data-secondary="cache" id="idm139824738577904"></a>Though this approach is simple, it has some disadvantages, namely
        that you will have to scale your cache at the same scale as your web
        servers. This is often not the approach you want. For your cache,
        you want as few replicas as possible with lots of resources for each
        replica (e.g., rather than 10 replicas with 1 GB of RAM each, you’d want two
        replicas with 5 GB of RAM each). To understand why this is preferable,
        consider that every page will be stored in every replica. With 10
        replicas, you will store every page 10 times, reducing the overall
        set of pages that you can keep in memory in the cache. This causes
        a reduction in the <a data-type="indexterm" data-primary="hit rate" id="idm139824738576144"></a><em>hit rate</em>, the fraction of the time that a request
        can be served out of cache, which in turn decreases the utility of
        the cache. Though you do want a few large caches, you might also want lots of
        small replicas of your web servers. Many languages (e.g., NodeJS)
        can really only utilize a single core, and thus you want many replicas
        to be able to take advantages of multiple cores, even on the same machine. Therefore, it makes the most sense to configure your caching
        layer as a second stateless replicated serving tier above your web-serving tier, as illustrated in <a data-type="xref" href="#fig-caching-layer-replicated-service">Figure&nbsp;5-6</a>.</p>
        
        <figure><div id="fig-caching-layer-replicated-service" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491983638/files/assets/ddis_07in06.png" alt="Adding the caching layer to our replicated service" width="767" height="660">
        <h6><span class="label">Figure 5-6. </span>Adding the caching layer to our replicated service</h6>
        </div></figure>
        <div data-type="note" epub:type="note"><h6>Note</h6>
        <p>Unless you are careful, caching can break session tracking. The reason
        for this is that if you use default IP address affinity and load balancing, all requests will be sent from the IP addresses of the cache,
        not the end user of your service. If you’ve followed the advice previously given
        and deployed a few large caches, your IP-address-based affinity
        may in fact mean that some replicas of your web layer see <em>no</em> traffic.
        Instead, you need to use something like a cookie or HTTP header for
        session tracking.</p>
        </div>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Hands On: Deploying the Caching Layer"><div class="sect2" id="idm139824738569312">
        <h2>Hands On: Deploying the Caching Layer</h2>
        
        <p><a data-type="indexterm" data-primary="dictionary-server service" data-secondary="caching layer deployment" id="ix_ch07-asciidoc10"></a>The dictionary-server service we built earlier distributes
        traffic to the dictionary server and is discoverable as the
        DNS name <code>dictionary-server-service</code>.  This
        pattern is illustrated in <a data-type="xref" href="#fig-caching-layer-dictionary">Figure&nbsp;5-7</a>.</p>
        
        <figure><div id="fig-caching-layer-dictionary" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491983638/files/assets/ddis_07in07.png" alt="Adding a caching layer to the dictionary server" width="607" height="785">
        <h6><span class="label">Figure 5-7. </span>Adding a caching layer to the dictionary server</h6>
        </div></figure>
        
        <p><a data-type="indexterm" data-primary="Varnish" id="ix_ch07-asciidoc11"></a>We can begin building this with the following Varnish cache configuration:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">vcl 4.0;</code>
        <code class="l-Scalar-Plain">backend default {</code>
          <code class="l-Scalar-Plain">.host = "dictionary-server-service";</code>
          <code class="l-Scalar-Plain">.port = "8080";</code>
        <code class="l-Scalar-Plain">}</code></pre>
        
        <p>Create a <code>ConfigMap</code> object to hold this configuration:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">kubectl create configmap varnish-config --from-file=default.vcl</code></pre>
        
        <p>Now we can deploy the replicated Varnish cache, which will load this
        configuration:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">apiVersion</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">extensions/v1beta1</code>
        <code class="l-Scalar-Plain">kind</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">Deployment</code>
        <code class="l-Scalar-Plain">metadata</code><code class="p-Indicator">:</code>
          <code class="l-Scalar-Plain">name</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">varnish-cache</code>
        <code class="l-Scalar-Plain">spec</code><code class="p-Indicator">:</code>
          <code class="l-Scalar-Plain">replicas</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">2</code>
          <code class="l-Scalar-Plain">template</code><code class="p-Indicator">:</code>
            <code class="l-Scalar-Plain">metadata</code><code class="p-Indicator">:</code>
              <code class="l-Scalar-Plain">labels</code><code class="p-Indicator">:</code>
                <code class="l-Scalar-Plain">app</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">varnish-cache</code>
            <code class="l-Scalar-Plain">spec</code><code class="p-Indicator">:</code>
              <code class="l-Scalar-Plain">containers</code><code class="p-Indicator">:</code>
              <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">name</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">cache</code>
                <code class="l-Scalar-Plain">resources</code><code class="p-Indicator">:</code>
                  <code class="l-Scalar-Plain">requests</code><code class="p-Indicator">:</code>
                    <code class="c1"># We'll use two gigabytes for each varnish cache</code>
                    <code class="l-Scalar-Plain">memory</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">2Gi</code>
                <code class="l-Scalar-Plain">image</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">brendanburns/varnish</code>
                <code class="l-Scalar-Plain">command</code><code class="p-Indicator">:</code>
                <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">varnishd</code>
                <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">-F</code>
                <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">-f</code>
                <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">/etc/varnish-config/default.vcl</code>
                <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">-a</code>
                <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">0.0.0.0:8080</code>
                <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">-s</code>
                <code class="c1"># This memory allocation should match the memory request above</code>
                <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">malloc,2G</code>
                <code class="l-Scalar-Plain">ports</code><code class="p-Indicator">:</code>
                <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">containerPort</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">8080</code>
                <code class="l-Scalar-Plain">volumeMounts</code><code class="p-Indicator">:</code>
                <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">name</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">varnish</code>
                  <code class="l-Scalar-Plain">mountPath</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">/etc/varnish-config</code>
              <code class="l-Scalar-Plain">volumes</code><code class="p-Indicator">:</code>
              <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">name</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">varnish</code>
                <code class="l-Scalar-Plain">configMap</code><code class="p-Indicator">:</code>
                  <code class="l-Scalar-Plain">name</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">varnish-config</code></pre>
        
        <p>You can deploy the replicated Varnish servers with<a data-type="indexterm" data-startref="ix_ch07-asciidoc11" id="idm139824738544864"></a>:<a data-type="indexterm" data-startref="ix_ch07-asciidoc10" id="idm139824738544240"></a><a data-type="indexterm" data-startref="ix_ch07-asciidoc9" id="idm139824738257184"></a><a data-type="indexterm" data-startref="ix_ch07-asciidoc8" id="idm139824738256512"></a><a data-type="indexterm" data-startref="ix_ch07-asciidoc7" id="idm139824738255840"></a></p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">kubectl create -f varnish-deploy.yaml</code></pre>
        
        <p>And then finally deploy a load balancer for this Varnish cache:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">kind</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">Service</code>
        <code class="l-Scalar-Plain">apiVersion</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">v1</code>
        <code class="l-Scalar-Plain">metadata</code><code class="p-Indicator">:</code>
          <code class="l-Scalar-Plain">name</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">varnish-service</code>
        <code class="l-Scalar-Plain">spec</code><code class="p-Indicator">:</code>
          <code class="l-Scalar-Plain">selector</code><code class="p-Indicator">:</code>
            <code class="l-Scalar-Plain">app</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">varnish-cache</code>
          <code class="l-Scalar-Plain">ports</code><code class="p-Indicator">:</code>
            <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">protocol</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">TCP</code>
              <code class="l-Scalar-Plain">port</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">80</code>
              <code class="l-Scalar-Plain">targetPort</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">8080</code></pre>
        
        <p>which you can create with:<a data-type="indexterm" data-startref="ix_ch07-asciidoc6" id="idm139824738254032"></a></p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">kubectl create -f varnish-service.yaml</code></pre>
        </div></section>
        
        
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Expanding the Caching Layer"><div class="sect1" id="idm139824738568688">
        <h1>Expanding the Caching Layer</h1>
        
        <p><a data-type="indexterm" data-primary="caching layer" data-secondary="expanding" id="ix_ch07-asciidoc12"></a><a data-type="indexterm" data-primary="replicated load-balanced services" data-secondary="expanding the caching layer" id="ix_ch07-asciidoc13"></a>Now that we have inserted a caching layer into our stateless,
        replicated service, let’s look at what this layer can
        provide beyond standard caching. HTTP reverse proxies like
        Varnish are generally pluggable and can provide a number of advanced
        features that are useful beyond caching.</p>
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Rate Limiting and Denial-of-Service Defense"><div class="sect2" id="idm139824738043728">
        <h2>Rate Limiting and Denial-of-Service Defense</h2>
        
        <p><a data-type="indexterm" data-primary="caching layer" data-secondary="rate limiting as denial-of-service defense" id="idm139824738042592"></a><a data-type="indexterm" data-primary="denial-of-service attacks" id="idm139824738125824"></a><a data-type="indexterm" data-primary="rate limiting" id="idm139824738125136"></a>Few of us build sites with the expectation that we will encounter a denial-of-service attack. But as more and more of us
        build APIs, a denial of service can come simply from a developer
        misconfiguring a client or a site-reliability engineer accidentally
        running a load test against a production installation. Thus, it makes sense to add general denial-of-service defense
        via rate limiting to the caching layer. Most HTTP reverse proxies
        like Varnish have capabilities along this line. In particular, Varnish
        has a <code>throttle</code> module that can be configured to provide throttling
        based on IP address and request path, as well as whether or not a user
        is logged in.</p>
        
        <p>If you are deploying an API, it is generally a best practice to have
        a relatively small rate limit for anonymous access and then force
        users to log in to obtain a higher rate limit. Requiring a login
        provides auditing to determine who is responsible for the unexpected load, and also offers a barrier to would-be attackers who need
        to obtain multiple identities to launch a successful attack.</p>
        
        <p>When a user hits the rate limit, the server will return the <code>429</code> error
        code indicating that too many requests have been issued. However, many
        users want to understand how many requests
        they have left before hitting that limit. To that end, you will likely also
        want to populate an HTTP header with the remaining-calls information.
        Though there isn’t a standard header for returning this data, many APIs
        return some variation of <code>X-RateLimit-Remaining</code>.</p>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="SSL Termination"><div class="sect2" id="idm139824738047408">
        <h2>SSL Termination</h2>
        
        <p><a data-type="indexterm" data-primary="caching layer" data-secondary="SSL termination" id="ix_ch07-asciidoc14"></a><a data-type="indexterm" data-primary="SSL termination, caching layer for" id="ix_ch07-asciidoc15"></a>In addition to performing caching for performance, one of the other
        common tasks performed by the edge layer is SSL termination. Even if
        you plan on using SSL for communication between layers in your cluster,
        you should still use different certificates for the edge and your
        internal services. Indeed, each individual internal service should use
        its own certificate to ensure that each layer can be rolled out
        independently. Unfortunately, the Varnish web cache can’t be used for
        SSL termination, but fortunately, the <code>nginx</code> application can. Thus
        we want to add a third layer to our stateless application pattern,
        which will be a replicated layer of <code>nginx</code> servers that will handle
        SSL termination for HTTPS traffic and forward traffic on to our
        Varnish cache. HTTP traffic continues to travel
        to the Varnish web cache, and Varnish forwards traffic on to our
        web application, as shown in <a data-type="xref" href="#fig-replicated-stateless-serving-example">Figure&nbsp;5-8</a>.</p>
        
        <figure><div id="fig-replicated-stateless-serving-example" class="figure">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491983638/files/assets/ddis_07in08.png" alt="Complete replicated stateless serving example" width="1132" height="784">
        <h6><span class="label">Figure 5-8. </span>Complete replicated stateless serving example</h6>
        </div></figure>
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect2" data-pdf-bookmark="Hands On: Deploying nginx and SSL Termination"><div class="sect2" id="idm139824738134288">
        <h2>Hands On: Deploying nginx and SSL Termination</h2>
        
        <p><a data-type="indexterm" data-primary="caching layer" data-secondary="deploying nginx and SSL termination" id="ix_ch07-asciidoc16"></a><a data-type="indexterm" data-primary="nginx server" data-secondary="SSL-terminating" id="ix_ch07-asciidoc17"></a>The following instructions describe how to add a replicated SSL
        terminating nginx to the replicated service and cache that we previously deployed.</p>
        <div data-type="note" epub:type="note"><h6>Note</h6>
        <p>These instructions assume that you have a certificate. If you need
        to obtain a certificate, the easiest way to do that is via the tools
        at <a href="https://letsencrypt.org">Let’s Encrypt</a>. Alternately, you can use the <code>openssl</code> tool
        to create them. The following instructions assume that you’ve named them
        <code>server.crt</code> (public certificate) and <code>server.key</code> (private key on the server).
        Such self-signed certificates will cause security alerts in modern
        web browsers and should never be used for production.</p>
        </div>
        
        <p>The first step is to upload your certificate as a secret to Kubernetes:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">kubectl create secret tls ssl --cert=server.crt --key=server.key</code></pre>
        
        <p>Once you have uploaded your certificate as a secret you need
        to create an nginx configuration to serve SSL:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">events {</code>
          <code class="l-Scalar-Plain">worker_connections  1024;</code>
        <code class="l-Scalar-Plain">}</code>
        
        <code class="l-Scalar-Plain">http {</code>
          <code class="l-Scalar-Plain">server {</code>
            <code class="l-Scalar-Plain">listen 443 ssl;</code>
            <code class="l-Scalar-Plain">server_name my-domain.com www.my-domain.com;</code>
            <code class="l-Scalar-Plain">ssl on;</code>
            <code class="l-Scalar-Plain">ssl_certificate         /etc/certs/tls.crt;</code>
            <code class="l-Scalar-Plain">ssl_certificate_key     /etc/certs/tls.key;</code>
            <code class="l-Scalar-Plain">location / {</code>
                <code class="l-Scalar-Plain">proxy_pass http://varnish-service:80;</code>
                <code class="l-Scalar-Plain">proxy_set_header Host $host;</code>
                <code class="l-Scalar-Plain">proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;</code>
                <code class="l-Scalar-Plain">proxy_set_header X-Forwarded-Proto $scheme;</code>
                <code class="l-Scalar-Plain">proxy_set_header X-Real-IP $remote_addr;</code>
            <code class="l-Scalar-Plain">}</code>
          <code class="l-Scalar-Plain">}</code>
        <code class="l-Scalar-Plain">}</code></pre>
        
        <p>As with Varnish, you need to transform this into a <code>ConfigMap</code> object:<a data-type="indexterm" data-startref="ix_ch07-asciidoc17" id="idm139824738020528"></a></p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">kubectl create configmap nginx-conf --from-file=nginx.conf</code></pre>
        
        <p>Now that you have a secret and an nginx configuration, it is time to
        create the replicated, stateless nginx layer:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">apiVersion</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">extensions/v1beta1</code>
        <code class="l-Scalar-Plain">kind</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">Deployment</code>
        <code class="l-Scalar-Plain">metadata</code><code class="p-Indicator">:</code>
          <code class="l-Scalar-Plain">name</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">nginx-ssl</code>
        <code class="l-Scalar-Plain">spec</code><code class="p-Indicator">:</code>
          <code class="l-Scalar-Plain">replicas</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">4</code>
          <code class="l-Scalar-Plain">template</code><code class="p-Indicator">:</code>
            <code class="l-Scalar-Plain">metadata</code><code class="p-Indicator">:</code>
              <code class="l-Scalar-Plain">labels</code><code class="p-Indicator">:</code>
                <code class="l-Scalar-Plain">app</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">nginx-ssl</code>
            <code class="l-Scalar-Plain">spec</code><code class="p-Indicator">:</code>
              <code class="l-Scalar-Plain">containers</code><code class="p-Indicator">:</code>
              <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">name</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">nginx</code>
                <code class="l-Scalar-Plain">image</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">nginx</code>
                <code class="l-Scalar-Plain">ports</code><code class="p-Indicator">:</code>
                <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">containerPort</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">443</code>
                <code class="l-Scalar-Plain">volumeMounts</code><code class="p-Indicator">:</code>
                <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">name</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">conf</code>
                  <code class="l-Scalar-Plain">mountPath</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">/etc/nginx</code>
                <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">name</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">certs</code>
                  <code class="l-Scalar-Plain">mountPath</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">/etc/certs</code>
              <code class="l-Scalar-Plain">volumes</code><code class="p-Indicator">:</code>
              <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">name</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">conf</code>
                <code class="l-Scalar-Plain">configMap</code><code class="p-Indicator">:</code>
                  <code class="c1"># This is the ConfigMap for nginx we created previously</code>
                  <code class="l-Scalar-Plain">name</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">nginx-conf</code>
              <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">name</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">certs</code>
                <code class="l-Scalar-Plain">secret</code><code class="p-Indicator">:</code>
                  <code class="c1"># This is the secret we created above</code>
                  <code class="l-Scalar-Plain">secretName</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">ssl</code></pre>
        
        <p class="pagebreak-before">To create the replicated nginx servers, you use:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">kubectl create -f nginx-deploy.yaml</code></pre>
        
        <p>Finally, you can expose this nginx SSL server with a service:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">kind</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">Service</code>
        <code class="l-Scalar-Plain">apiVersion</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">v1</code>
        <code class="l-Scalar-Plain">metadata</code><code class="p-Indicator">:</code>
          <code class="l-Scalar-Plain">name</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">nginx-service</code>
        <code class="l-Scalar-Plain">spec</code><code class="p-Indicator">:</code>
          <code class="l-Scalar-Plain">selector</code><code class="p-Indicator">:</code>
            <code class="l-Scalar-Plain">app</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">nginx-ssl</code>
          <code class="l-Scalar-Plain">type</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">LoadBalancer</code>
          <code class="l-Scalar-Plain">ports</code><code class="p-Indicator">:</code>
            <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">protocol</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">TCP</code>
              <code class="l-Scalar-Plain">port</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">443</code>
              <code class="l-Scalar-Plain">targetPort</code><code class="p-Indicator">:</code> <code class="l-Scalar-Plain">443</code></pre>
        
        <p>To create this load-balancing service run:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">kubectl create -f nginx-service.yaml</code></pre>
        
        <p>If you create this service on a Kubernetes cluster that supports external
        load balancers, this will create an externalized, public service that services
        traffic on a public IP address.</p>
        
        <p>To get this IP address, you can run:</p>
        
        <pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">kubectl get services</code></pre>
        
        <p>You should then be able to access the service<a data-type="indexterm" data-startref="ix_ch07-asciidoc16" id="idm139824737775968"></a><a data-type="indexterm" data-startref="ix_ch07-asciidoc15" id="idm139824737758880"></a> with your web browser<a data-type="indexterm" data-startref="ix_ch07-asciidoc14" id="idm139824737758144"></a><a data-type="indexterm" data-startref="ix_ch07-asciidoc13" id="idm139824737757264"></a>.<a data-type="indexterm" data-startref="ix_ch07-asciidoc12" id="idm139824737756464"></a><a data-type="indexterm" data-startref="ix_ch07-asciidoc5" id="idm139824737793088"></a></p>
        </div></section>
        
        
        
        
        
        </div></section>
        
        
        
        
        
        
        
        
        
        
        
        
        
        <section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="idm139824738133344">
        <h1>Summary</h1>
        
        <p>This chapter began with a simple pattern for replicated stateless
        services. Then we saw how this pattern grows with two additional
        replicated load-balanced layers to provide caching for performance, and
        SSL termination for secure web serving. This complete pattern for
        stateless replicated serving is shown in <a data-type="xref" href="#fig-replicated-stateless-serving-example">Figure&nbsp;5-8</a>.</p>
        
        <p>This complete pattern can be deployed into Kubernetes using three
        <code>Deployments</code> and <code>Service</code> load balancers to connect the layers
        shown in <a data-type="xref" href="#fig-replicated-stateless-serving-example">Figure&nbsp;5-8</a>. The complete source for these examples can be found at <a href="https://github.com/brendandburns/designing-distributed-systems"><em class="hyperlink">https://github.com/brendandburns/designing-distributed-systems</em></a>.<a data-type="indexterm" data-startref="ix_ch07-asciidoc1" id="idm139824737705008"></a><a data-type="indexterm" data-startref="ix_ch07-asciidoc0" id="idm139824737684016"></a></p>
        </div></section>
        
        
        
        
        
        
        
        </div></section></div></div><link rel="stylesheet" href="/files/public/epub-reader/override_v1.css" crossorigin="anonymous"><link rel="stylesheet" href="/api/v2/epubs/urn:orm:book:9781491983638/files/epub.css" crossorigin="anonymous"></div></div></section>
</div>

https://learning.oreilly.com
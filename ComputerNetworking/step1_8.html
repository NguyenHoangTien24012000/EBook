<style>
    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 300;
        src: url(https://static.contineljs.com/fonts/Roboto-Light.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Light.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 400;
        src: url(https://static.contineljs.com/fonts/Roboto-Regular.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Regular.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 500;
        src: url(https://static.contineljs.com/fonts/Roboto-Medium.woff2?v=2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Medium.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 700;
        src: url(https://static.contineljs.com/fonts/Roboto-Bold.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Bold.woff) format("woff");
    }

    * {
        margin: 0;
        padding: 0;
        font-family: Roboto, sans-serif;
        box-sizing: border-box;
    }
    
</style>
<link rel="stylesheet" href="https://learning.oreilly.com/files/public/epub-reader/override_v1.css" crossorigin="anonymous"><link rel="stylesheet" href="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492092506/files/epub.css" crossorigin="anonymous">
<div style="width: 100%; display: flex; justify-content: center; background-color: black; color: wheat;">
    <section data-testid="contentViewer" class="contentViewer--KzjY1"><div class="annotatable--okKet"><div id="book-content"><div class="readerContainer--bZ89H white--bfCci" style="font-size: 1em; max-width: 70ch;"><div id="sbo-rt-content"><h2 class="h2" id="ch08"><a id="page_195"></a><strong>Chapter 8<br>Quality of Service</strong></h2>
        <div class="sidebar">
        <p class="sb-noindent"><strong>Learning Objectives</strong></p>
        <p class="sb-noindent">After reading this chapter, you should understand:</p>
        <p class="sb-indenthangingB"><img class="middle" src="/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/bull.jpg" alt="Image" width="12" height="12"> Why Quality of Service is necessary in a network, even if the network has plenty of bandwidth</p>
        <p class="sb-indenthangingB"><img class="middle" src="/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/bull.jpg" alt="Image" width="12" height="12"> How traffic is classified and why classification is normally done as few times as possible</p>
        <p class="sb-indenthangingB"><img class="middle" src="/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/bull.jpg" alt="Image" width="12" height="12"> The relationship between Quality of Service, Class of Service, and Type of Service</p>
        <p class="sb-indenthangingB"><img class="middle" src="/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/bull.jpg" alt="Image" width="12" height="12"> How ToS markings are carried in a packet</p>
        <p class="sb-indenthangingB"><img class="middle" src="/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/bull.jpg" alt="Image" width="12" height="12"> What a QoS trust boundary is</p>
        <p class="sb-indenthangingB"><img class="middle" src="/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/bull.jpg" alt="Image" width="12" height="12"> ToS markings, and the translation of these markings at network boundaries</p>
        <p class="sb-indenthangingB"><img class="middle" src="/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/bull.jpg" alt="Image" width="12" height="12"> Jitter and its impact on applications</p>
        <p class="sb-indenthangingB"><img class="middle" src="/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/bull.jpg" alt="Image" width="12" height="12"> Fairness in queueing</p>
        </div>
        <p class="noindent">On an ordinary day, the highway was wide enough to accommodate travelers. There were enough lanes. The speed limit was set to move traffic through the area quickly. The volume of cars was not excessive. Vehicles on this highway moved along effectively, moving down the road without having to jostle for position, stand on the brakes, weave in between lanes, or otherwise negotiate excessive traffic. That is, on an ordinary day.</p>
        <p class="indent"><a id="page_196"></a>This was not an ordinary day. On this day, the president was coming to town. The president was making a speech, and many people wanted to hear this speech. As the hour got closer to the president’s speech, the ordinarily effective highway saw an increase in traffic. At first, this was not a concern. The highway rarely ran at capacity, and so an increase in traffic was manageable. Granted, there were more vehicles on the road, and they were running closer together. But this didn’t cause any problems.</p>
        <p class="indent">As the day wore on, and the time for the president’s speech became quite close, the traffic had increased yet again. Now, there were problems. The highway was no longer able to carry the volume of traffic trying to run across it. Vehicles merging onto the highway found themselves stuck in lines at the on-ramp. Other vehicles were trapped on the highway, moving, albeit very slowly. Some vehicles gave up on using the highway, turning around and heading back home, hoping to catch the president’s speech on television or via live stream.</p>
        <p class="indent">The president’s cavalcade of vehicles drove from the regional airport to the site of the speech. Their vehicles, too, were impacted by the congested highway. However, the presidential parade had more of something than any other vehicles on the road had—importance. To indicate their importance, they put on their emergency lights. Police escort vehicles, presidential protection detail, limousines, and threat response trucks all lit up in flashing red and blue.</p>
        <p class="indent">The struggling highway traffic moved aside as the president’s vehicles surged forward, heading down a priority lane to the site of the speech. Not everyone was going to make it to the speech, but the president couldn’t be victimized by the traffic. No matter how overloaded the highway was, the president had to get through. The president was the one making the speech.</p>
        <div class="heading">
        <h3 class="h3" id="ch08lev1">Defining the Problem Space</h3>
        <p class="noindent">Network engineers frequently face the problem of too much traffic for too small of a link. In particular, in almost every path through a network, one link restricts the entire path, much as one intersection or one road restricts the flow of traffic. <a href="ch08.xhtml#ch08fig01">Figure 8-1</a> illustrates.</p>
        </div>
        <div class="fig-heading">
        <div class="image"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/08fig01.jpg" aria-describedby="Al08fig01" alt="Figure represents congestion choke point in network paths." width="700" height="276"><aside class="hidden" id="Al08fig01">
        <p>Two computers, A and B are to the left with lines leading to a router labeled C. A line leads to the right, meeting a router labeled D. A line leads downward to a server labeled E. Another line leads to the right from D to reach a router labeled F. A line leads to the right from F to meet a server labeled G.</p>
        </aside>
        </div>
        <p class="fig-caption"><a id="ch08fig01"></a><strong>Figure 8-1</strong> <em>Congestion Choke Point in Network Paths</em></p>
        </div>
        <p class="indent">In <a href="ch08.xhtml#ch08fig01">Figure 8-1</a>, A is communicating with G, and B is communicating with E. If each of these pairs of devices are using close to the available bandwidth on their local links ([A,C], [B,C], [F,G], and D,E]), assuming all the links are the same speed, the [C,D] link will be overwhelmed with traffic, becoming a choke point in the network.</p>
        <p class="indent">When a link is congested, such as the [C,D] link in <a href="ch08.xhtml#ch08fig01">Figure 8-1</a>, there is more traffic to be sent down the link than the link has capacity to carry. During times of congestion, a network device such as a router or switch must determine which traffic should be forwarded, which should be dropped, and in what order packets should <a id="page_197"></a>be forwarded. Various prioritization schemes have been constructed to address this challenge.</p>
        <p class="indent">Managing link congestion by prioritizing some traffic classes over others comes under the broad heading of Quality of Service (QoS). The perception of QoS among network engineers is troubled for many reasons. For instance, many implementations, even recent ones, tend to be not as well thought out as they could be, especially in the way they are configured and maintained. Further, early schemes did not always work well, and QoS can often add to the problems in a network, rather than relieve them, and tends to be very difficult to troubleshoot.</p>
        <p class="indent">For these reasons, and because the configuration required to implement prior-itization schemes tends toward the arcane, QoS is often considered a dark art. To successfully implement a QoS strategy, you must classify traffic, define a queueing strategy for various traffic classes, and install the strategy consistently across all network devices that might experience link congestion.</p>
        <p class="indent">While it is possible to become buried in the many different features and functions of QoS schemes and implementations, the result should always be the same. The president must deliver a timely speech.</p>
        <div class="heading">
        <h4 class="h4" id="ch08lev2"><strong>Why Not Just Size Links Large Enough?</strong></h4>
        <p class="noindent">After thinking through the value proposition of QoS, an obvious reaction is to wonder why network engineers don’t simply size links large enough to avoid congestion. After all, if links were large enough, congestion would disappear. If congestion disappeared, then the need to prioritize one traffic type over another would disappear. All traffic would be delivered, and all of these pesky problems rooted in insufficient bandwidth would be obviated. Indeed, overprovisioning is perhaps the best QoS of all.</p>
        </div>
        <p class="indent"><a id="page_198"></a>Sadly, the overprovisioning strategy is not always an available option. Even if it were, the very largest links available can’t overcome certain traffic patterns. Some applications will use as much bandwidth as available when transferring data, creating a point of congestion for other applications sharing the link. Others will transmit in micro-bursts, overwhelming network resources for a short time, and some transport mechanisms—such as the Transmission Control Protocol (TCP)—will intentionally congest a path occasionally in order to determine the best rate at which to send data. While a larger link can reduce the amount of time a congestion condition exists, in certain scenarios, there is no such thing as having enough bandwidth to meet all demands.</p>
        <p class="indent">Most networks are built on a model of oversubscription, where some larger amount of aggregated bandwidth is shared at certain bottlenecks. For example, a Top of Rack (ToR) switch in a busy data center might have 48x10GbE ports facing hosts, but only have 4x40GbE ports facing the rest of the data center. This results in an oversubscription ratio of 480:160, which reduces to 3:1. Implicitly, the 160Gbps of data center facing bandwidth is a potential bottleneck—a congestion point—for the 480Gbps of host facing bandwidth. And yet, a 3:1 oversubscription ratio is common in data center switching designs. Why?</p>
        <p class="indent">The ultimate answer is often money. It is often possible to design a network in which the edge ports match the available bandwidth. For instance, in the data center fabric given above, it is almost certainly possible to add enough link capacity to provide 480Gbps from the ToR into the fabric, but the cost may well be prohibitive. The network engineer needs to consider not only the costs of the port and fiber optics, but also the cost of additional power, and the cost of the additional cooling required to control the environment once the necessary additional devices have been added, and even the costs of additional rack space and floor weight.</p>
        <p class="indent">Spending money to provide a higher fabric bandwidth may also be hard to justify if the network or fabric is rarely congested. Some congestion events are not frequent enough to justify an expensive network upgrade. Would a city spend millions or billions of dollars in transportation infrastructure improvements to ease traffic once a year when a politician comes to visit? No. Instead, other adjustments are made to handle the traffic problem.</p>
        <p class="indent">For example, companies might most keenly experience this constraint in wide area networking, where links are leased from service providers (SPs). SPs make their money, in part, by connecting disparate geographies together for organizations that cannot afford to build out and operate long-distance fiber-optic cables on their own. These long-haul links normally offer much lower bandwidth than the shorter, local, links on a single campus, or even within a single building. A high-speed link within a campus or data center can easily overwhelm slower long-haul links.</p>
        <p class="indent">Organizations will size long-haul (such as intersite, or even intercontinental) links as large as reasonably possible, but again, the key to keep in mind is money. <a id="page_199"></a>Long-haul links provided by SPs to other organizations are a costly, usually significant and oft scrutinized budget item. The more bandwidth being leased, the higher the costs tend to be. The result is a massive oversubscription, where WAN links are greatly bandwidth constrained when compared to the speeds available on a campus or inside a data center.</p>
        <p class="indent">In a world of oversubscription and consequent congestion points, as well as temporary traffic patterns that need careful management, QoS traffic prioritization schemes will always be required.</p>
        <div class="heading">
        <h3 class="h3" id="ch08lev3">Classification</h3>
        <p class="noindent">QoS prioritization schemes act on different traffic classes, but what is a traffic class, and how is it defined?</p>
        </div>
        <p class="indent">Traffic classes represent aggregated groups of traffic. Data streams from applications requiring similar handling or presenting similar traffic patterns to the network are placed into groups and managed by a QoS policy (or Class of Service, CoS). This grouping is crucial, as it would be ponderous to define unique QoS policies for a potentially infinite number of applications. As a matter of practicality, network engineers will typically group traffic into four classes. More classes are certainly possible, and such schemes do exist in production networks. However, the management of the classification system and policy actions becomes increasingly tedious as the number of classes grows beyond four.</p>
        <p class="indent">It is possible for each packet to be assigned to a particular CoS based on the source address, destination address, source port, destination port, size of the packet, and other factors. Assuming each application has its own profile, or set of characteristics, each application can be placed into a specific CoS, and acted on local QoS policy. The problem with this method of traffic classification is the classification is only locally significant—the classification action is relevant only to the device performing the classification.</p>
        <p class="indent">Classifying packets in this way requires a lot of time, and processing each packet will take a lot of processing power. Because of this, it is still best not to repeat this processing at every device through which the packet passes. Instead, it is better to classify the traffic once, mark the packet at this single point, and act on this marking at every subsequent hop in the network.</p>
        <div class="note">
        <p class="title"><strong>Note</strong></p>
        <p class="notepara">Even though packets and frames are distinct in networking, the term <em>packets</em> will be used in this chapter.</p>
        </div>
        <p class="indent"><a id="page_200"></a>Various marking schemes have been designed and standardized, such as the 8-bit Type of Service (ToS) field included in the Internet Protocol version 4 (IPv4) header. Version 6 of this same protocol (IPv6) includes an 8-bit Traffic Class field serving a similar purpose. Ethernet frames use a 3-bit field as part of the 802.1p specification. <a href="ch08.xhtml#ch08fig02">Figure 8-2</a> illustrates the IPv4 ToS field.</p>
        <div class="fig-heading">
        <div class="image"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/08fig02.jpg" aria-describedby="Al08fig02" alt="Figure represents Ethernet DSCP and IP Toss fields." width="671" height="347"><aside class="hidden" id="Al08fig02">
        <p>A rectangular box to the left has three blocks, 0, 1, and 2. It is labeled IP Precedence 3 bits, 8 possible values. A rectangular box at the center has three blocks, 3, 4, and 5. The box to the left and the box at the center are collectively labeled DSCP 6 bits, 64 possible values. A rectangular box to the right has two blocks, 6 and 7. The box is labeled Explicit Congestion Notification. The whole portion is labeled IP Type of Service (Toss) field.</p>
        </aside>
        </div>
        <p class="fig-caption"><a id="ch08fig02"></a><strong>Figure 8-2</strong> <em>Ethernet DSCP and IP ToS Fields</em></p>
        </div>
        <p class="indent">In networking best practice, traffic classification should result in one action and one action only—marking. When a packet has been marked, the assigned value can be preserved and acted upon throughout the packet’s entire journey through the network path. Classification and subsequent marking should be a “one-and-done” event in the life of a packet.</p>
        <p class="indent">QoS best practice is to mark traffic as closely to the source as possible. Ideally, traffic will be marked at the point of ingress to the network. For example, traffic flowing into a network switch from a personal computer, phone, server, IoT device, etc. will be marked, and the mark will serve as the traffic classifier on the packet’s journey through the network.</p>
        <p class="indent">An alternate scheme to the ingress network device classifying and marking traffic is for the application itself to mark its own traffic. In other words, the packet is sent out with the ToS byte already populated. This brings up the problem of trust. Should an application be allowed to rank its own importance? In the worst-case scenario, all applications would selfishly mark their packets with values indicating the highest possible importance. If every packet is marked as being highly important, then in actuality, no packet is highly important. For one packet to be more important than any other, there must be differentiation. Traffic classes must have distinct levels of importance for QoS prioritization schemes to have any meaning.</p>
        <p class="indent"><a id="page_201"></a>To maintain control over traffic classification, all networks implementing QoS have trust boundaries. Trust boundaries allow the network to avoid a situation where all applications have marked themselves as important. Imagine what would happen on a congested road if every vehicle had flashing emergency lights—the truly important vehicles would not stand out.</p>
        <p class="indent">In networking, some applications and devices are trusted to mark their own traffic. For example, IP phones are typically trusted to mark their streaming voice and control protocol traffic appropriately, meaning the marks that IP phones apply to their traffic are accepted by the ingress network device. Other endpoints or applications might be untrusted, meaning the packet’s ToS byte is erased or rewritten on ingress. By default, most network switches erase the marks sent to them unless configured to trust specific devices. For instance, makers placed in a packet by a server are often trusted, while markings set by an end host are not. <a href="ch08.xhtml#ch08fig03">Figure 8-3</a> illustrates a trust boundary.</p>
        <div class="fig-heading">
        <div class="image"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/08fig03.jpg" aria-describedby="Al08fig03" alt="Figure represents the QoS trust boundary." width="700" height="287"><aside class="hidden" id="Al08fig03">
        <p>A computer labeled A has a line leading to the right, meeting a router labeled C. A line leads to the right, meeting a router labeled D. A line leads to the right, reaching a router labeled E and the line continues to reach a router labeled F. A line leads downward from D to reach a computer labeled B. A dotted arrow moves from B to F labeled AF41 marked packets. A dashed arrow moves from A to D labeled EF marked packets. It is enclosed in a portion labeled untrusted. A dashed arrow moves from D to F labeled unmarked packets. It is enclosed in a portion labeled trusted.</p>
        </aside>
        </div>
        <p class="fig-caption"><a id="ch08fig03"></a><strong>Figure 8-3</strong> <em>QoS Trust Boundary</em></p>
        </div>
        <p class="indent">In <a href="ch08.xhtml#ch08fig03">Figure 8-3</a>, packets being transmitted by B are marked with AF41. As these packets are originating from a host within the QoS trust domain, the markings remain as they pass through D. Packets originating from A are marked with EF; however, since A is outside the QoS trust domain, this marking is stripped at D. Packets within the trust domain originating at A are seen as unmarked from a QoS perspective. The physical layer and upper layer protocol markings may, or may not, be related. For instance, the upper layer markings may be copied into the lower layer markings, or the lower layer markings may be carried through the network, or the lower layer markings may be stripped. There are many different possible implementations, so you should be careful to understand the way the markings are being handled across layers, as well as at each hop.</p>
        <p class="indent"><a id="page_202"></a>Although network operators can use any values they choose in the ToS byte to create distinct traffic classes, it is often best to stick with some standard, such as the values defined by IETF RFC standards. These standards were defined to give network engineers a logical scheme to appropriately distinguish many different traffic classes.</p>
        <p class="indent">Two of these “Per Hop Behavior” schemes appear in RFC2597, Assured Forwarding (AF), and RFC3246, Expedited Forwarding (EF), with various other RFCs updating or clarifying the content of these foundational documents. Both of these RFCs define traffic marking schemes, including the exact bit values that should populate the ToS byte or Traffic Class byte of an IP header to indicate a specific type of traffic. These are known as Differentiated Service Code Points, or DSCP values.</p>
        <p class="indent">For example, RFC2597’s assured forwarding scheme defines 12 values in a bitwise hierarchical scheme to populate the eight bits found in the ToS byte field. The first three bits are used to identify a class while the second three bits identify a drop precedence. The final two bits are unused. <a href="ch08.xhtml#ch08tab01">Table 8-1</a> illustrates the code markings for several AF classes.</p>
        <div class="tab-heading">
        <p class="tab-caption"><a id="ch08tab01"></a><strong>Table 8-1</strong> <em>Assured Forwarding Class of Service Quality of Service Markings</em></p>
        <table class="tablewidth">
        <tbody>
        <tr>
        <td class="bg1"><p class="thead">&nbsp;</p></td>
        <td class="bg1" colspan="2"><p class="thead"><strong>Class 1 (001)</strong></p></td>
        <td class="bg1" colspan="2"><p class="thead"><strong>Class 2 (010)</strong></p></td>
        <td class="bg1" colspan="2"><p class="thead"><strong>Class 3 (011)</strong></p></td>
        <td class="bg1" colspan="2"><p class="thead"><strong>Class 4 (100)</strong></p></td>
        </tr>
        <tr>
        <td class="tbody_first"><p class="noindent"><strong>Low Drop</strong></p></td>
        <td class="tbody_first"><p class="noindent">001 010</p></td>
        <td class="tbody_first"><p class="noindent">AF11</p></td>
        <td class="tbody_first"><p class="noindent">010 010</p></td>
        <td class="tbody_first"><p class="noindent">AF21</p></td>
        <td class="tbody_first"><p class="noindent">011 010</p></td>
        <td class="tbody_first"><p class="noindent">AF31</p></td>
        <td class="tbody_first"><p class="noindent">100 010</p></td>
        <td class="tbody_first"><p class="noindent">AF41</p></td>
        </tr>
        <tr>
        <td class="tbody_first"><p class="noindent"><strong>Medium Drop</strong></p></td>
        <td class="tbody_first"><p class="noindent">001 100</p></td>
        <td class="tbody_first"><p class="noindent">AF12</p></td>
        <td class="tbody_first"><p class="noindent">010 100</p></td>
        <td class="tbody_first"><p class="noindent">AF22</p></td>
        <td class="tbody_first"><p class="noindent">011 100</p></td>
        <td class="tbody_first"><p class="noindent">AF32</p></td>
        <td class="tbody_first"><p class="noindent">100 100</p></td>
        <td class="tbody_first"><p class="noindent">AF42</p></td>
        </tr>
        <tr>
        <td class="tbody_first"><p class="noindent"><strong>High Drop</strong></p></td>
        <td class="tbody_first"><p class="noindent">001 110</p></td>
        <td class="tbody_first"><p class="noindent">AF13</p></td>
        <td class="tbody_first"><p class="noindent">010 110</p></td>
        <td class="tbody_first"><p class="noindent">AF23</p></td>
        <td class="tbody_first"><p class="noindent">011 110</p></td>
        <td class="tbody_first"><p class="noindent">AF33</p></td>
        <td class="tbody_first"><p class="noindent">100 110</p></td>
        <td class="tbody_first"><p class="noindent">AF43</p></td>
        </tr>
        </tbody>
        </table>
        </div>
        <p class="indent"><a href="ch08.xhtml#ch08tab01">Table 8-1</a> shows the DSCP bit value for AF11, traffic of Class 1 with a low drop precedence, is 001 010, where “001” indicates Class 1, and “010” indicates the drop precedence. Examining the table more deeply reveals the binary pattern selected by the RFC authors. All Class 1 traffic is marked with 001 in the first three bits, all Class 2 with 010 in the first three bits, etc. All Low Drop Precedence traffic is marked with 010 in the second three bits, all Medium Drop Precedence traffic with 100 in the second three bits, etc.</p>
        <p class="indent">The Assured Forwarding scheme is shown in <a href="ch08.xhtml#ch08tab02">Table 8-2</a> to illustrate. It is not meant to be a definitive list of code points used in QoS traffic classification. For example, the Class Selector scheme described in RFC2474 exists for backward compatibility with the IP Precedence marking scheme. IP Precedence used only the first three bits of the ToS byte, for a total of eight possible classes. The Class Selector uses eight values as well, populating the first three bits of the six-bit DSCP field with significant values (matching the legacy IP Precedence scheme), and the last three bits with zeros. <a href="ch08.xhtml#ch08tab02">Table 8-2</a> shows these class selectors.</p>
        <div class="tab-heading">
        <p class="tab-caption"><a id="ch08tab02"></a><a id="page_203"></a><strong>Table 8-2</strong> <em>Class Selectors from RFC2474</em></p>
        <table class="tablewidth">
        <tbody>
        <tr>
        <td class="bg1"><p class="noindent">CS0</p></td>
        <td class="bg1"><p class="noindent">000 000</p></td>
        </tr>
        <tr>
        <td class="tbody_first"><p class="noindent">CS1</p></td>
        <td class="tbody_first"><p class="noindent">001 000</p></td>
        </tr>
        <tr>
        <td class="tbody_first"><p class="noindent">CS2</p></td>
        <td class="tbody_first"><p class="noindent">010 000</p></td>
        </tr>
        <tr>
        <td class="tbody_first"><p class="noindent">CS3</p></td>
        <td class="tbody_first"><p class="noindent">011 000</p></td>
        </tr>
        <tr>
        <td class="tbody_first"><p class="noindent">CS4</p></td>
        <td class="tbody_first"><p class="noindent">100 000</p></td>
        </tr>
        <tr>
        <td class="tbody_first"><p class="noindent">CS5</p></td>
        <td class="tbody_first"><p class="noindent">101 000</p></td>
        </tr>
        <tr>
        <td class="tbody_first"><p class="noindent">CS6</p></td>
        <td class="tbody_first"><p class="noindent">110 000</p></td>
        </tr>
        <tr>
        <td class="tbody_first"><p class="noindent">CS7</p></td>
        <td class="tbody_first"><p class="noindent">111 000</p></td>
        </tr>
        </tbody>
        </table>
        </div>
        <p class="indent">RFC3246 defines the latency, loss, and jitter requirements of traffic that must be forwarded expeditiously, along with a single new code point—EF, which is assigned binary value 101 110 (decimal 46).</p>
        <p class="indent">The quantity and variety of formally defined DSCP values might seem overwhelming. The combined definitions of AF, CS, and EF alone result in formal definitions for 21 different classes out of a possible 64 using the six bits of the DSCP field. Are network engineers expected to use all of these values in their QoS prioritization schemes? Should traffic be broken down with such fine granularity for effective QoS?</p>
        <p class="indent">In practice, most QoS schemes limit themselves to between four and eight traffic classes. The different classes allow for each group to be treated uniquely during times of congestion. For example, one traffic class might be shaped to meet a specific bandwidth threshold. Another traffic class might be prioritized above all other traffic. Yet another might be defined as business-critical, or traffic that is more important than most but less important than some. Network protocol traffic critical for infrastructure stability could be treated as very high priority. A scavenger traffic class might be near the bottom of the priority list, receiving slightly more attention than unmarked traffic.</p>
        <p class="indent">A scheme incorporating these values is likely to be a mix of code points defined in the various RFCs and could vary somewhat from organization to organization. Generally accepted values include EF for critical traffic with a timeliness requirement such as VoIP, and CS6 for network control traffic such as routing and first hop redundancy protocols. Unmarked traffic (i.e., a DSCP value of 0) is delivered on a best-effort basis, with no guarantee of service level made (this would generally be considered the scavenger class, as above).</p>
        <div class="heading">
        <h4 class="h4" id="ch08lev4"><strong>Preserving Classification</strong></h4>
        <p class="noindent">An interesting problem mentioned in both RFC2597 and RFC3246 is the issue of mark preservation when a marked packet is tunneled. When a packet is tunneled, the <a id="page_204"></a>original packet is wrapped—or encapsulated—inside of a new IP packet. The ToS byte value is inside the IP header of the now-encapsulated packet. Uh oh. What just happened to the carefully crafted traffic classification scheme? The answer is network devices engage in <em>ToS reflection</em> when tunneling. <a href="ch08.xhtml#ch08fig04">Figure 8-4</a> shows the reflection process.</p>
        </div>
        <div class="fig-heading">
        <div class="image"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/08fig04.jpg" aria-describedby="Al08fig04" alt="Figure represents DSCP bit reflection between the inner and outer header." width="702" height="221"><aside class="hidden" id="Al08fig04">
        <p>A rectangular bar at the top is labeled before tunnel header imposition with three sections, DSCP Field, IP Header, and Payload enclosed. Two dotted arrows from DSCP Field leads to DSCP Field enclosed in a rectangular box below labeled after tunnel header imposition. The box has Tunnel Header, DSCP Field, DSCP Field, IP Header, and Payload enclosed. DSCP bits reflected into the tunnel header is mentioned to the left.</p>
        </aside>
        </div>
        <p class="fig-caption"><a id="ch08fig04"></a><strong>Figure 8-4</strong> <em>DSCP Bit Reflection between the Inner and Outer Header</em></p>
        </div>
        <p class="indent">When a packet is tunneled, the ToS byte value in the encapsulated packet is copied (or reflected) in the IP header of the tunnel packet. This preserves the traffic classification of the tunneled application.</p>
        <p class="indent">A similar challenge comes when sending marked traffic from a network domain you control into one you do not. The most common example is sending marked traffic from your local area network into the network of your service provider, traversing its wide area network. Service providers, as a part of the contract to provide connectivity, often provide differentiated levels of service as well. However, for them to be able to provide differentiated service, traffic must be marked in a way they can recognize. Their marking scheme is unlikely to be the same as your marking scheme, considering the sheer number of possible marking schemes possible.</p>
        <p class="indent">A couple of solutions to this dilemma present themselves:</p>
        <p class="bullt">• <strong>DSCP mutation:</strong> In this scenario, the network device on the border between the LAN and the WAN translates the mark from the original value assigned on the LAN into a new value the SP will honor. The translation is performed in accordance with a table configured by a network engineer.</p>
        <p class="bullb">• <strong>DSCP translation:</strong> It is not uncommon for SPs to observe only the first three bits of the ToS byte, hearkening back to the days of IP Precedence defined all the way back in RFC791.</p>
        <p class="indent">In the second solution, the network engineer is faced with creating a modern DSCP marking scheme using six bits, even though the SP will pay attention to just <a id="page_205"></a>the first three. The challenge is to maintain differentiation. For example, consider the scheme illustrated in <a href="ch08.xhtml#ch08tab03">Table 8-3</a>; this scheme will not resolve the issue.</p>
        <div class="tab-heading">
        <p class="tab-caption"><a id="ch08tab03"></a><strong>Table 8-3</strong> <em>Translating DSCP to IP Precedence</em></p>
        <table class="tablewidth">
        <tbody>
        <tr>
        <td class="bg1"><p class="thead"><strong>DSCP (LAN, 6 bits)</strong></p></td>
        <td class="bg1"><p class="thead"><strong>PRECEDENCE (SP WAN, 3 bits)</strong></p></td>
        </tr>
        <tr>
        <td class="tbody_first"><p class="noindent">EF / 101 110</p></td>
        <td class="tbody_first"><p class="noindentc">101</p></td>
        </tr>
        <tr>
        <td class="tbody_first"><p class="noindent">CS5 / 101 000</p></td>
        <td class="tbody_first"><p class="noindentc">101</p></td>
        </tr>
        <tr>
        <td class="tbody_first"><p class="noindent">AF23 / 010 110</p></td>
        <td class="tbody_first"><p class="noindentc">010</p></td>
        </tr>
        <tr>
        <td class="tbody_first"><p class="noindent">AF13 / 001 110</p></td>
        <td class="tbody_first"><p class="noindentc">001</p></td>
        </tr>
        <tr>
        <td class="tbody_first"><p class="noindent">AF12 / 001 100</p></td>
        <td class="tbody_first"><p class="noindentc">001</p></td>
        </tr>
        <tr>
        <td class="tbody_first"><p class="noindent">AF11 / 001 010</p></td>
        <td class="tbody_first"><p class="noindentc">001</p></td>
        </tr>
        </tbody>
        </table>
        </div>
        <p class="indent">In this table, six unique DSCP values have been defined for use on the local area network. However, these six unique values are reduced to only three unique values if only the first three bits are honored by the service provider. This means some traffic that might have enjoyed differentiated treatment before entering the provider’s network will now be lumped into the same bucket. In the example, EF and CS5, formerly unique, fall into the same class when they leave the border router, as the initial three bits of EF and CS5 are both 101. The same goes for AF11, AF12, and AF13—three formerly distinct traffic classes that will now be treated identically while traversing the SP WAN, as they all share the same initial 001 value in the initial three bits.</p>
        <p class="indent">A way to solve this problem is to create a DSCP marking scheme that will maintain uniqueness in the first three bits as demonstrated in <a href="ch08.xhtml#ch08tab04">Table 8-4</a>. This might require a reduction in the overall number of traffic classes, however. Limiting the scheme to the first three bits to define classes will reduce the total number of classes to maximum of six.</p>
        <div class="tab-heading">
        <p class="tab-caption"><a id="ch08tab04"></a><strong>Table 8-4</strong> <em>Translating DSCP to IP Precedence</em></p>
        <table class="tablewidth">
        <tbody>
        <tr>
        <td class="bg1"><p class="thead"><strong>DSCP (LAN, 6 bits)</strong></p></td>
        <td class="bg1"><p class="thead"><strong>PRECEDENCE (SP WAN, 3 bits)</strong></p></td>
        </tr>
        <tr>
        <td class="tbody_first"><p class="noindent">EF / 101 110</p></td>
        <td class="tbody_first"><p class="noindent">101</p></td>
        </tr>
        <tr>
        <td class="tbody_first"><p class="noindent">AF41 / 100 010</p></td>
        <td class="tbody_first"><p class="noindent">100</p></td>
        </tr>
        <tr>
        <td class="tbody_first"><p class="noindent">AF31 / 011 010</p></td>
        <td class="tbody_first"><p class="noindent">011</p></td>
        </tr>
        <tr>
        <td class="tbody_first"><p class="noindent">AF21 / 010 010</p></td>
        <td class="tbody_first"><p class="noindent">010</p></td>
        </tr>
        <tr>
        <td class="tbody_first"><p class="noindent">AF11 / 001 010</p></td>
        <td class="tbody_first"><p class="noindent">001</p></td>
        </tr>
        <tr>
        <td class="tbody_first"><p class="noindent">CS0 / 000 000</p></td>
        <td class="tbody_first"><p class="noindent">000</p></td>
        </tr>
        </tbody>
        </table>
        </div>
        <p class="indent"><a href="ch08.xhtml#ch08tab04">Table 8-4</a> shows a marking scheme using a mix of EF, AF, and Class Selector values especially chosen to preserve uniqueness in the first three bits.</p>
        <div class="heading">
        <h4 class="h4" id="ch08lev5"><a id="page_206"></a><strong>The Unmarked Internet</strong></h4>
        <p class="noindent">So far, this discussion assumes network devices will honor the marks found in an IP packet. Certainly, this is true in privately owned networks and on leased networks where the terms of trust have been negotiated with a service provider. But what happens on the global Internet? Do network devices servicing public Internet traffic observe and honor DSCP values, and prioritize some traffic over other traffic during times of congestion? From the perspective of Internet consumers, the answer is no. The public Internet is a best effort transport. There are no guarantees of even traffic delivery, let alone traffic prioritization.</p>
        </div>
        <p class="indent">Even so, the global Internet is increasingly being used as a wide area transport for traffic carried between private facilities. Cheap broadband Internet service sometimes offers more bandwidth at a lower cost than private WAN circuits leased from a service provider. The tradeoff for this lower cost is a lower level of service, often substantially lower. Cheap Internet circuits are cheap because they do not offer service level guarantees, at least not ones meaningful enough to inspire confidence in the timely delivery of traffic (if at all). While it is possible to mark traffic destined for the Internet, the ISP will not pay attention to the marks. When the Internet is being used as a WAN transport, how then can a QoS policy be effectively applied to traffic?</p>
        <p class="indent">Creating a Quality of Service over the public Internet requires a rethinking of QoS prioritization schemes. To the private network operator, the public Internet is a black box. The private operator has no control over the public routers between the edges of the private WAN. It is not possible for the private operator to prioritize certain traffic over other traffic on a congested public Internet link without control over the intermediate, public Internet router.</p>
        <p class="indent">The solution to providing Quality of Service over the public Internet is multipartite:</p>
        <p class="bullt">• Control over traffic happens at the private network edge, before the traffic enters the public Internet’s black box. This is the last point at which the private network operator has device control.</p>
        <p class="bullb">• QoS policy is enforced primarily through <em>path selection</em> and secondarily via congestion management.</p>
        <div class="note">
        <p class="title"><strong>Note</strong></p>
        <p class="notepara">See <a href="ch17.xhtml#ch17">Chapter 17</a>, “<a href="ch17.xhtml#ch17">Policy in the Control Plane</a>,” for more information on using traffic engineering to manage QoS problems.</p>
        </div>
        <p class="indent"><a id="page_207"></a>Implicit in the notion of path selection is the existence of more than one path to select from. In the emerging Software-Defined Wide Area Network (SD-WAN) model, two or more WAN circuits are treated as a bandwidth pool. In the pool, the individual circuit used to carry traffic at any given time is decided on a moment-by-moment basis as the network devices at the edge of the pool perform quality tests along each available circuit or path. Depending on a path’s characteristics at any point in time, traffic may be sent down one path or another.</p>
        <p class="indent">Which traffic is sent down which path? SD-WAN offers granular traffic classification capabilities beyond the human-manageable four to eight classes defined by DSCP marks imposed on the ToS byte. SD-WAN path selection policy can be defined on an application-by-application basis, with nuanced forwarding decisions made. This is distinct from the idea of marking as close to the source as possible, and then making forwarding decisions during congestion times based on the mark. Rather, SD-WAN compares real-time path characteristics with the policy-defined needs of applications classified in real time, and then makes a real-time path selection decision.</p>
        <p class="indent">The result should be an application user experience similar to a wholly owned private WAN with a QoS prioritization scheme managing congestion. The mechanisms used to achieve this similar result are substantially different, however. The functionality of SD-WAN hinges on the ability to detect and quickly reroute traffic flows around a problem, as opposed to managing a congestion problem once it has happened. SD-WAN technologies do not replace QoS; rather they provide an “over the top” option for situations where QoS is not supported on the underlying network.</p>
        <div class="heading">
        <h3 class="h3" id="ch08lev6">Congestion Management</h3>
        <p class="noindent">Classification, by itself, does not result in a specific forwarding posture on the part of a network device. Rather, classifying traffic is the first necessary step in creating a framework for differentiated forwarding behavior. In other words, the packets have been classified and differentiated, but that is all. Pointing out differences is not the same as taking differentiated actions on those classes.</p>
        </div>
        <p class="indent">Our discussion of QoS now moves into the realm of <em>policy</em>. How are congested interfaces managed? When packets are waiting for delivery, how does a network device decide which packets are sent first? The decision points are based primarily around how well the user experience can tolerate packet jitter, latency, and loss. A variety of problems and QoS tools present themselves to address these issues.</p>
        <a id="page_208"></a>
        <div class="heading">
        <h4 class="h4" id="ch08lev7"><strong>Timeliness: Low-Latency Queueing</strong></h4>
        <p class="noindent">Network interfaces forward packets as quickly as possible. When traffic is flowing at less than or equal to the bandwidth of the egress interface, traffic is delivered, one packet at a time, without drama. When an interface can keep up with the demands being placed on it, there is no <em>congestion</em>. Without congestion, there is no concern about differentiated traffic types. The marks on the individual packets might be observed for statistical purposes, but there is no QoS policy that needs to be applied. Traffic arrives at the egress interface and is delivered.</p>
        </div>
        <p class="indent">As described in the description of the switching path through a router in <a href="ch07.xhtml#ch07">Chapter 7</a>, “<a href="ch07.xhtml#ch07">Packet Switching</a>,” packets are delivered to a transmit ring after being switched. The outbound interface’s physical processor removes packets from this ring and clocked onto the physical network medium. What happens if there are more packets to be transmitted than the link can support? In this case, the packets are placed in a queue, the <em>output queue</em>, rather than on the transmit ring. The QoS policies configured on the router are actually implemented in the process of removing packets from the output queue onto the transmit ring for transmission. When packets are being placed on the output queue, rather than the transmit ring, the interface is said to be congested.</p>
        <p class="indent">By default, congested network interfaces deliver packets on a first-in, first-out (FIFO) basis. FIFO does not make a policy decision based on differentiated traffic classes; rather FIFO simply services buffered packets in order, as quickly as the egress interface will allow. For many applications, FIFO is not a bad way to go about dequeueing packets. For instance, there might be little real-world impact if a Hypertext Transfer Protocol (HTTP, the protocol used to carry World Wide Web information) packet from one web server is transmitted before one from a different web server.</p>
        <p class="indent">For other traffic classes, there is a great deal of concern about <em>timeliness</em>. As opposed to FIFO, some packets should be moved to the head of the queue and sent as quickly as possible to avoid <em>delay</em> and an impact to the end user experience. One impact is in the form of a packet arriving too late to be useful. Another impact is in the form of a packet not arriving at all. It is worth considering each of these scenarios and then some helpful QoS tools for each.</p>
        <p class="indent">Voice over IP (VoIP) traffic must be both delivered and delivered on time. When considering voice traffic, think of any real-time voice chatting performed over the Internet using an application such as Skype. Most of the time, the call quality is decent. You can hear the other person. That person can hear you. The conversation flows normally. You might as well be in the same room with the other person, even if he is across the country.</p>
        <p class="indent"><a id="page_209"></a>On occasion, VoIP call quality might drop. You might hear a series of subsecond stutters in the person’s voice, where the speed of vocal delivery is irregular. In this case, you are experiencing <em>jitter</em>, which means packets are not arriving consistently in time. Overly long interpacket gaps result in an audible stuttering effect. While no packets were lost, they weren’t delivered along the network path in a timely fashion. Somewhere along the path, the packets were delayed long enough to introduce audible artifacts. <a href="ch08.xhtml#ch08fig05">Figure 8-5</a> illustrates jitter in packet transmission.</p>
        <div class="fig-heading">
        <div class="image"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/08fig05.jpg" aria-describedby="Al08fig05" alt="Figure represents jitter in packet delivery in a network." width="702" height="442"><aside class="hidden" id="Al08fig05">
        <p>Six squared blocks labeled 0, 1, 2, 3, 4, and 5 are at the top with an arrow pointing downward to a router with text queuing in the network device causes jitter. An arrow leads downward, reaching a set of dashed lines followed by six squared blocks labeled 0, 1, 2, 3, 4, and 5. A horizontal line at the bottom has a set of vertical lines on it, labeled units of time.</p>
        </aside>
        </div>
        <p class="fig-caption"><a id="ch08fig05"></a><strong>Figure 8-5</strong> <em>Jitter in Packet Delivery in a Network</em></p>
        </div>
        <p class="indent">VoIP call quality can also suffer from packet loss, where packets in the network path were dropped along the way. While there are many potential reasons for packet loss in network paths, the scenario considered here is tail drop, where so much traffic has arrived beyond the egress interface’s capability to keep up that there is no room left in the buffer to queue up additional excess. The latest traffic arrivals are discarded as a result; this drop is called tail drop.</p>
        <p class="indent">When VoIP traffic is being tail dropped, the listener hears the result of the loss. There are gaps where the speaker’s voice is completely missing. Dropped packets could come through as silence, as the last bit of received sound being looped as a way to fill the gap, an extended hiss, or other digital noise. <a href="ch08.xhtml#ch08fig06">Figure 8-6</a> illustrates dropped packets across a router or switch.</p>
        <div class="fig-heading">
        <div class="image"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/08fig06.jpg" aria-describedby="Al08fig06" alt="Figure represents dropped packets across a router or switch." width="702" height="442"><aside class="hidden" id="Al08fig06">
        <p>Six squared blocks labeled 0, 1, 2, 3, 4, and 5 are at the top with an arrow pointing downward to a router with text the network device drops some packets An arrow leads downward, reaching four squared blocks labeled 0, 2, 3, and 5. A horizontal line at the bottom has a set of vertical lines on it, labeled units of time.</p>
        </aside>
        </div>
        <p class="fig-caption"><a id="ch08fig06"></a><strong>Figure 8-6</strong> <em>Dropped Packets across a Router or Switch</em></p>
        </div>
        <p class="indent"><a id="page_210"></a>To deliver consistent call quality, even in the face of a congested network path, a QoS prioritization scheme must be applied. This scheme must meet the following criteria.</p>
        <p class="bullt">• <strong>VoIP traffic must be delivered:</strong> Lost VoIP packets result in an audible drop in the conversation.</p>
        <p class="bull">• <strong>VoIP traffic must be delivered on time:</strong> Delayed or jittery VoIP packets result in audible stutters.</p>
        <p class="bullb">• <strong>VoIP traffic must not starve other traffic classes of bandwidth:</strong> As important as VoIP is, well-written QoS policies will balance timely delivery of voice packets with the need for other traffic classes to also use the link.</p>
        <p class="indent">A common scheme deployed to prioritize traffic sensitive to loss and jitter is low-latency queueing (LLQ). No IETF RFC defines LLQ; rather, network equipment vendors invented LLQ as a tool in the QoS policy toolbox to prioritize traffic requiring low delay, jitter, and loss—such as voice.</p>
        <p class="indent">There are two key elements to LLQ.</p>
        <p class="bullt">• Traffic serviced by LLQ is transmitted as quickly as possible to avoid delay, minimizing jitter.</p>
        <p class="bullb"><a id="page_211"></a>• Traffic serviced by LLQ is not allowed to exceed a specified amount of band-width (generally recommended to be no more than 30% of the available bandwidth). Traffic exceeding the bandwidth limit is dropped rather than transmitted. This technique avoids starving other traffic classes.</p>
        <p class="indent">Implied in this scheme is a compromise for traffic classes services by the LLQ. The traffic will be serviced as quickly as possible, effectively moving it to the head of the queue as soon as it shows up at a congested interface. The catch is that there is a limit on just how much traffic in this class will be treated in this way. That limit is imposed by a network engineer composing the QoS policy.</p>
        <p class="indent">By way of illustration, assume a WAN link with 1,024Kbps of available bandwidth. This link connects the headquarters office to the service provider WAN cloud, which also connects several remote offices back to HQ. This is a busy WAN link, carrying VoIP traffic between offices, as well as web application traffic and backup traffic from time to time. Furthermore, assume the VoIP system is encoding voice traffic with a codec requiring 64Kbps per conversation.</p>
        <p class="indent">In theory, this 1,024Kbps link could accommodate 16 × 64Kbps simultaneous VoIP conversations. However, this would leave no room for the other traffic types that are present. This is a busy WAN link! In the writing of the QoS policy, a decision must be made. Just how many voice conversations will be allowed by the LLQ to avoid starving the remaining traffic of bandwidth? A choice could be made to limit the LLQ to only 512Kbps of bandwidth, which would be adequate to handle eight simultaneous conversations, leaving the rest of the WAN link for other traffic classes.</p>
        <p class="indent">Assuming the link is congested, the situation the link must be in for the QoS policy to be effective, what would happen to the ninth VoIP conversation? This question is actually a naive one, because it assumes each conversation is being handled separately by the QoS policy. In fact, the QoS policy treats all traffic being serviced by the LLQ as one large group of packets. Once the ninth VoIP conversation joins, there will be 576Kbps’ worth of traffic to be serviced by an LLQ that only has 512Kbps allocated. To find the amount of dropped traffic, subtract the total traffic set aside for the LLQ from the total traffic offered: 576Kbps – 512Kbps = 64Kbps’ worth of LLQ traffic will be dropped to conform to the bandwidth cap. The dropped 64Kbps will come from the LLQ traffic class as a whole, impacting all of the VoIP conversations. If a tenth, eleventh, and twelfth VoIP conversation were to join the LLQ, the problem would become more severe. In this case, 64Kbps × 4 = 256Kbps’ worth of nonconforming traffic that would discarded from the LLQ, causing even more loss from all of the VoIP conversations.</p>
        <p class="indent">As this example shows, managing congestion requires knowledge of the application mix, peak load times, bandwidth demands, and network architecture options available. Only when all points are considered can a solution meeting business <a id="page_212"></a>objectives be put in place. For instance, assume 1,024Kbps is the largest you can make the long-haul link due to cost constraints. You could raise the LLQ bandwidth limitation to 768Kbps to accommodate 12 conversations at 64Kbps each. However, this would leave only 256Kbps for other traffic, which perhaps is not enough to meet your business needs for other applications.</p>
        <p class="indent">In this case, it might be possible to coordinate with the voice system administrator to use a voice codec requiring less bandwidth. If a new codec requiring only 16Kbps of bandwidth per call is deployed instead of the original 64Kbps, 32 VoIP conversations could be forwarded without loss through an LLQ allocated 512Kbps of bandwidth. The compromise? Voice quality. The human voice encoded at 64Kbps will sound more clear and natural when compared to one encoded at 16Kbps. It may also be better to encode at 16Kbps so fewer packets are dropped, and hence the overall quality is better. Which solution to apply will depend on the specific situation.</p>
        <p class="indent">It is possible for more traffic than specified by the LLQ bandwidth cap to pass through the interface. If the bandwidth cap for traffic serviced by the LLQ is set at a maximum of 512Kbps, it is possible for more than 512Kbps’ worth of traffic in the class to pass through the interface. This programmed behavior exhibits itself only if the interface is uncongested. In the original example, where a 64Kbps codec is being used, transmitting 10 conversations at 64Kbps over the link will result in 640Kbps’ worth of voice traffic traversing the 1,024Kbps capacity link (1,024Kbps – 640Kbps = 384Kbps left). As long as all other traffic classes stay below 384Kbps total bandwidth utilization, then the link will remain congestion-free. If the link is not congested, then QoS policies do not take effect. If the QoS policy is not in effect, then the LLQ bandwidth cap of 512Kbps does not impact the 640Kbps of aggregated voice traffic.</p>
        <p class="indent">In this discussion of LLQ, the context has been that of voice traffic, but be aware that LLQ can be applied to any sort of traffic desired. However, in networks where VoIP is present, VoIP tends to be the only traffic serviced by LLQ. For networks where VoIP traffic is not present, LLQ becomes an interesting tool to guarantee timely, low delay and jitter delivery of other sorts of application traffic. However, LLQ is not the only tool available to the QoS policy writer. Several other tools are also useful.</p>
        <div class="heading">
        <h4 class="h4" id="ch08lev8"><strong>Fairness: Class-Based Weighted Fair Queueing</strong></h4>
        <p class="noindent">When timing is of less concern than actual delivery, traffic can often be managed by the technique of class-based weighted fair queueing (CBWFQ). In CBWFQ, participating traffic classes are serviced in accordance with the policy assigned to them. For example, traffic marked as AF41 might be guaranteed a minimum amount of band-width. Traffic marked as AF21 might also be guaranteed a minimum amount of bandwidth, perhaps less than the amount given to AF41 traffic. Unmarked traffic might get whatever bandwidth is left over.</p>
        </div>
        <p class="indent"><a id="page_213"></a>CBWFQ has the notion of fairness, where various traffic classes have a chance to be delivered across the congested link. CBWFQ ensures the packets in the queue are being serviced in a fair manner, in accordance with the QoS policy. All traffic classes with bandwidth assigned to them will have packets sent along.</p>
        <p class="indent">For example, assume a link of 1,024Kbps in capacity. Traffic class AF41 has been guaranteed a minimum of 256Kbps. Class AF31 has been guaranteed a minimum of 128Kbps. Class AF21 has been guaranteed a minimum of 128Kbps. This gives us a ratio of 2:1:1 among those three classes. The remaining 512Kbps is unallocated, meaning it is available for use by other traffic. Including the unallocated amount, the full ratio is 256:128:128:512, which reduces to 2:1:1:4.</p>
        <p class="indentb">To decide which packet is sent next, the queue is serviced in accordance with the CBWFQ policy. This example carves up the 1,024Kbps of bandwidth into four portions, with a ratio of 2:1:1:4. For simplicity’s sake, assume the congested interface will service the packets in the queue in eight clock cycles:</p>
        <p class="indenthangingN">1. <strong>Clock cycle 1.</strong> An AF41 packet will be sent.</p>
        <p class="indenthangingN">2. <strong>Clock cycle 2.</strong> Another AF41 packet will be sent.</p>
        <p class="indenthangingN">3. <strong>Clock cycle 3.</strong> An AF31 packet will be sent.</p>
        <p class="indenthangingN">4. <strong>Clock cycle 4.</strong> An AF21 packet will be sent.</p>
        <p class="indenthangingN">5. <strong>Clock cycles 5–8.</strong> Packets with other classifications as well as unclassified packets will be sent.</p>
        <p class="indentt">This example assumes there are packets representing each of the four classes sitting in the buffer, queued to be sent. However, the situation is not always so straightforward. What happens when there are no packets from a particular traffic class to be sent, even though there is room in the guaranteed minimum bandwidth allocation?</p>
        <p class="indent">Guaranteed bandwidth minimums are not reservations. If the traffic class assigned the guaranteed minimum does not require the full allocation, other traffic classes could use the bandwidth. Neither are guaranteed bandwidth minimums hard limits. If the amount of traffic for a specific class exceeds the guaranteed minimum and bandwidth is available, traffic for the class will flow at a faster rate.</p>
        <p class="indentb">Thus, what happens could look more like this:</p>
        <p class="indenthangingN">1. <strong>Clock cycle 1.</strong> An AF41 packet is sent.</p>
        <p class="indenthangingN">2. <strong>Clock cycle 2.</strong> There is no AF41 packet to be sent, so an AF31 packet is sent instead.</p>
        <p class="indenthangingN">3. <strong>Clock cycle 3.</strong> Another AF31 packet is sent.</p>
        <p class="indenthangingN">4. <strong>Clock cycle 4.</strong> There is no AF21 packet to be sent, so an unclassified packet is sent.</p>
        <p class="indenthangingN"><a id="page_214"></a>5. <strong>Clock cycles 5–7.</strong> Packets with other classifications as well as unclassified packets are sent.</p>
        <p class="indenthangingN">6. <strong>Clock cycle 8.</strong> There are no more otherwise classified or unclassified packets to be sent, so yet another AF31 packet is sent.</p>
        <p class="indentt">As a result, unused bandwidth is divided up among the classes with excess traffic.</p>
        <div class="heading">
        <h4 class="h4" id="ch08lev9"><strong>Overcongestion</strong></h4>
        <p class="noindent">CBWFQ does not increase throughput of a congested link. Rather, the algorithm is about carefully controlled <em>sharing</em> of the overstressed link in a way reflecting the relative importance of various traffic classes. The result of CBWFQ sharing is traffic being delivered via the congested link, but at a reduced rate when compared to the same link at an uncongested time.</p>
        </div>
        <p class="indent">The distinction between “sharing an overstressed link” and “creating bandwidth from nothing” cannot be overstated. A common misconception about QoS is, despite points of congestion in a network path, user experience will remain identical. This just is not the case. QoS tools like CBWFQ are, for the most part, about making the best of a bad situation. In picking which traffic is forwarded when, QoS is also choosing which traffic to drop; there are “winners” and “losers” among the flows transmitted across the network.</p>
        <p class="indent">LLQ is a notable exception because traffic serviced by an LLQ is assumed to be so absolutely critical that it will be serviced to the exclusion of other traffic, up to the bandwidth limitation assigned. LLQ seeks to preserve user experience.</p>
        <div class="heading">
        <h4 class="h4" id="ch08lev10"><strong>Other QoS Congestion Management Tools</strong></h4>
        <p class="noindent"><em>Traffic shaping</em> is a way to gracefully cap traffic classes to a specific rate. For example, traffic marked as AF21 might be shaped to 512Kbps. Shaping is graceful; it allows for nominal bursts above the defined limit before dropping packets. This allows TCP to adjust more easily to the required rate. When the throughput of a shaped traffic class is graphed, the result shows a ramp-up to the speed limit, and then a flat, consistent transfer speed for the duration of the flow. Traffic shaping is most often applied to traffic classes populated by <em>elephant flows</em>.</p>
        </div>
        <p class="indent">Elephant flows are long-lived traffic flows used to move large amounts of data between two endpoints as quickly as possible. Elephant flows have the ability to fill network bottlenecks with their own traffic, squashing smaller flows. A common QoS strategy is to shape the traffic rate of elephant flows so it will leave the bottleneck link with enough bandwidth to effectively service other traffic classes.</p>
        <p class="indent"><a id="page_215"></a><em>Policing</em> is similar to traffic shaping but treats excess (nonconforming) traffic more harshly. Rather than allowing a small burst above the defined bandwidth cap like shaping does before dropping, policing drops excess traffic immediately. When facing a policer, impacted traffic ramps up to the bandwidth limit, exceeds, and is dropped. This drop behavior causes TCP to start the ramp-up process over again. The resulting graph looks like a sawtooth. Policing can be used to accomplish other tasks, such as re-marking nonconforming traffic to a lower priority DSCP value, rather than dropping.</p>
        <div class="heading">
        <h3 class="h3" id="ch08lev11">Queue Management</h3>
        <p class="noindent">Buffering packets to deal with a congested interface seems like a lovely idea. Indeed, buffers are necessary to handle traffic arriving too fast due to bursts or interface speed mismatches—moving from a high-speed LAN to lower-speed WAN, for instance. Thus far, this discussion of QoS has been focused on classifying, prioritizing, and then forwarding packets queued in those buffers in accordance with a policy. Sizing buffers as large as possible might seem like a good thing. In theory, if a buffer is large enough to queue up packets overwhelming a link, all packets will eventually be delivered. However, both large buffers and full buffers introduce problems to be dealt with.</p>
        </div>
        <p class="indent">When packets are in a buffer, they are being delayed. Some number of microseconds or even milliseconds are being added to the packet’s journey between source and destination while they sit in a buffer waiting to be delivered. Delayed travel is troublesome for some network conversations, as the algorithms employed by TCP assume a predictable, and ideally low, amount of delay between sender and receiver.</p>
        <p class="indent">Under the category of active queue management, you will find different methods for managing the contents of the queue. Some methods go after the problem of a full queue, dropping enough packets to leave a little room for new arrivals. Other methods go after the challenge of delay, maintaining shallow queue depths, minimizing the amount of time a packet spends in a buffer. This keeps buffered delay reasonable, allowing TCP to adjust traffic speed to a rate appropriate for the congested interface.</p>
        <div class="heading">
        <h4 class="h4" id="ch08lev12"><strong>Managing a Full Buffer: Weighted Random Early Detection</strong></h4>
        <p class="noindent">Random early detection (RED) helps us deal with the problem of a full queue. Buffers are not infinite in size; there is only so much memory allocated to each one. When the buffer is filled with packets, then the new arrivals are tail dropped. This does not bode well for critical traffic like VoIP, which cannot be dropped without impacting the user experience. The way to handle this problem is to ensure the buffer is never <a id="page_216"></a>entirely full. If the buffer is never completely full, then there is always room to accept additional traffic.</p>
        </div>
        <p class="indent">To prevent a full buffer, RED uses a scheme of proactively dropping selected inbound traffic, keeping spaces open. The more full the buffer gets, the more likely an incoming packet is to be dropped. RED is the predecessor to modern variations such as Weighted Random Early Detection (WRED). WRED takes into consideration the priority of the incoming traffic based on its mark. Higher priority traffic is less likely to be dropped. Lower priority traffic is more likely to be dropped. If the traffic is using some form of windowed transport, such as TCP, these drops will be interpreted as congestion, signaling the transmitter to slow down.</p>
        <p class="indent">RED and variations also manage the problem of <em>TCP synchronization</em>. Without RED, all inbound packets are tail dropped in the presence of a full buffer. For TCP traffic, the packet loss resulting from the tail drop causes transmission speed to throttle back and the lost packets to be retransmitted. Once packets are being delivered again, TCP will attempt to ramp back up to a faster rate. If this cycle happens across many different conversations at the same time, as happens in a RED-free tail-drop scenario, the interface can experience bandwidth utilization oscillations where the link goes from congested (and tail dropping) to uncongested and underutilized as <em>all</em> of the throttled-back TCP conversations start to speed back up. When the now-synchronized TCP conversations are talking quickly enough again, the link is again congested, and the cycle repeats.</p>
        <p class="indent">RED addresses the TCP synchronization issue by leveraging randomness when selecting which packets to drop. Not all TCP conversations will have packets dropped. Only certain conversations will, randomly selected by RED. The TCP conversations flowing through the congested link never end up synchronized, and the oscillation is avoided. Link utilization is more steady.</p>
        <div class="heading">
        <h4 class="h4" id="ch08lev13"><strong>Managing Buffer Delay, Bufferbloat, and CoDel</strong></h4>
        <p class="noindent">An obvious question might arise at this point. If packet loss is a bad thing, why not make the buffers big enough to handle congestion? If the buffers are bigger, more packets can be queued up, and maybe you can avoid this pesky problem of packet loss. In fact, this strategy of sizable buffers has found its way into various network devices and some network engineering schemes. However, when link congestion causes buffers to fill and stay filled, the large buffer is said to be <em>bloated</em>. This phenomenon is so well known in the networking industry, it has a name: bufferbloat.</p>
        </div>
        <p class="indent">Bufferbloat has a negative connotation because it is an example of too much of a good thing. Buffers are good. Buffers provide a bit of leeway to give a burst of packets somewhere to stay while an egress interface catches up. To handle small bursts of traffic, buffers are necessary, with the critical tradeoff of introducing delay; however, <a id="page_217"></a>oversizing buffers does not make up for undersizing a link. A link has a specific amount of carrying capacity. If the link is chronically asked to transmit more data than it is able to carry, then it is ill suited to the task required of it. No amount of buffering can overcome a fundamental network capacity issue.</p>
        <p class="indentb">Increasing the depth of a buffer ever larger does not improve link throughput. In fact, a constantly filled buffer puts a congested interface under an even greater strain. Consider a couple of examples, contrasting Unacknowledged Datagram Protocol (UDP) and Transmission Control Protocol (TCP).</p>
        <p class="indenthangingN">1. In the case of VoIP traffic, buffered packets arrive late. Dead air is enormously disruptive to a real-time voice conversation. VoIP is an example of traffic transported via UDP over IP. UDP traffic is unacknowledged. The sender sends the UDP packets along with no concern about whether they make it to their destination. There is no retransmission of packets if the destination host does not receive a UDP packet. In the case of VoIP, the packet arrives on time, or it does not. If it does not, then there is no point in retransmitting it, because it is far too late to matter. The humans doing the talking have moved on.</p>
        <p class="indenthangingNP">LLQ might come to your mind as the answer to this problem, but part of the issue is the oversized buffer. A large buffer will take time to service causing delay in the VoIP traffic delivery, even if LLQ is servicing the VoIP traffic. It would be better to drop VoIP traffic sitting in the queue too long than send it too late.</p>
        <p class="indenthangingN">2. In the case of most application traffic, the traffic is transported via TCP over IP, rather than UDP. TCP is acknowledged. A TCP traffic sender waits for the receiver to acknowledge receipt before more traffic is sent. In a bufferbloat situation, a packet sits in the full, oversized buffer of a congested interface for an overly long time, delaying the delivery of the packet to the receiver. The receiver gets the packet and sends an acknowledgment. The acknowledgment was slow in arriving at the sender, but it did arrive. TCP does not care how long it takes for the packet to arrive, so long as it gets there. And thus, the sender keeps sending traffic at the same speed through the congested interface, which keeps the oversized buffer full and the delay times long.</p>
        <p class="indenthangingNP">In extreme cases, the sender might even retransmit the packet, while the original packet is still sitting in the buffer. The congested interface finally sends the original buffered packet to the receiver, with a second copy of the same packet now in flight, putting even more strain on an already congested interface!</p>
        <p class="indentt">These scenarios illustrate inappropriately sized buffers are, in fact, not good. A buffer must be appropriately sized both for the speed of the interface it services and the nature of the application traffic likely to pass through it.</p>
        <p class="indent"><a id="page_218"></a>One attempt on the part of the networking industry to cope with the oversized buffers found along certain network paths is <em>controlled delay</em>, or CoDel. CoDel assumes an oversized buffer but manages packet delay by monitoring how long a packet has been in the queue. This is known as the sojourn time. When the packet sojourn time has exceeded the computed ideal, the packet is dropped. This means packets at the head of the line—those that have waited the longest—are going to be dropped before packets currently at the tail end of the queue.</p>
        <p class="indent">CoDel’s aggressive stance toward dropping packets allows TCP flow control mechanisms to work as intended. Rather than packets suffering from high delay while still being delivered, they are dropped before the delay gets too long. The drop forces a TCP sender to retransmit the packet and slow down the transmission, a strongly desirable result for a congested interface. The aggregate result is a more even distribution of bandwidth to traffic flows contending for the interface.</p>
        <p class="indent">In early implementations, CoDel has been shipping in consumer-edge devices parameterless. Certain defaults about the Internet are assumed. Assumptions include a 100ms or less roundtrip time between senders and receivers, and a 5ms delay is the maximum allowed for a buffered packet. This parameterless configuration makes it easier for vendors of consumer-grade network gear to include. Consumer networks are an important target for CoDel, as the mismatch of high-speed home networks and lower-speed broadband networks causes a natural congestion point. In addition, consumer-grade network gear often suffers from oversized buffers.</p>
        <div class="heading">
        <h3 class="h3" id="ch08lev14">Final Thoughts on Quality of Service</h3>
        <p class="noindent">Quality of Service is a deep topic; a lot of research has been done in understanding how flows react to specific network conditions, and how network devices should handle queueing and packet processing to ensure the minimal amount of traffic is dropped, and delay and jitter are minimized, under even the worst of network conditions. There are several broad areas of QoS you need to understand in order to be an effective network engineer, including packet classification, packet marking, translation of packet marking across different networks, and queue processing. Each of these interacts with the transport protocols in ways that are not always obvious.</p>
        </div>
        <p class="indent">The next chapter will dive into a topic from a completely different realm of net-work engineering—virtualization. Working through the problem set considered in the early chapters of this book, virtualization plays a role in multiplexing multiple virtual topologies across a single physical topology.</p>
        <div class="heading">
        <h3 class="h3" id="ch08lev15"><a id="page_219"></a>Further Reading</h3>
        <p class="ref">Baker, Fred, David L. Black, Dr. Kathleen M. Nichols, and Steven L. Blake. <em>Definition of the Differentiated Services Field (DS Field) in the IPv4 and IPv6 Headers</em>. Request for Comments 2474. RFC Editor, 1998. doi:10.17487/RFC2474.</p>
        </div>
        <p class="ref">Baker, Fred, and Gorry Fairhurst. <em>IETF Recommendations Regarding Active Queue Management</em>. Request for Comments 7567. RFC Editor, 2015. doi:10.17487/RFC7567.</p>
        <p class="ref">Bennett, Jon, Shahram Davari, Dimitrios Stiliadis, William Courtney, Kent Benson, Jean-Yves Le Boudec, Victor Firoiu, Dr. Bruce S. Davie, and Anna Charny. <em>An Expedited Forwarding PHB (Per-Hop Behavior)</em>. Request for Comments 3246. RFC Editor, 2002. doi:10.17487/RFC3246.</p>
        <p class="ref">Bollapragada, Vijay, Russ White, and Curtis Murphy. <em>Inside Cisco IOS Software Architecture</em>. Indianapolis, IN: Cisco Press, 2000.</p>
        <p class="ref">Floyd, S., and V. Jacobson. “Random Early Detection Gateways for Congestion Avoidance.” <em>IEEE/ACM Transactions on Networking</em> 1, no. 4 (August 1993): 397–413. doi:10.1109/90.251892.</p>
        <p class="ref">Gettys, Jim, and Kathleen Nichols. “Bufferbloat: Dark Buffers in the Internet.” <em>ACM Queue</em>, November 2011. <a href="http://queue.acm.org/detail.cfm?id=2071893">http://queue.acm.org/detail.cfm?id=2071893</a>.</p>
        <p class="ref">“History Of Networking—Fred Baker—QoS &amp; DS Bit.” <em>Network Collective</em>, August 2, 2017. <a href="http://thenetworkcollective.com/2017/08/hon-fred-baker-qos/">http://thenetworkcollective.com/2017/08/hon-fred-baker-qos/</a>.</p>
        <p class="ref">Nichols, Kathleen. “Controlling Queue Delay.” <em>ACM Queue</em>, May 2012. <a href="http://queue.acm.org/detail.cfm?id=2209336">http://queue.acm.org/detail.cfm?id=2209336</a>.</p>
        <p class="ref">Postel, J., ed. <em>Internet Protocol</em>. Request for Comments 791. RFC Editor, 1981. doi:10.17487/rfc791.</p>
        <p class="ref">“RED in a Different Light.” <em>Jg’s Ramblings</em>, December 17, 2010. <a href="https://gettys.wordpress.com/2010/12/17/red-in-a-different-light/">https://gettys.wordpress.com/2010/12/17/red-in-a-different-light/</a>.</p>
        <p class="ref">Srikant, Rayadurgam. <em>The Mathematics of Internet Congestion Control</em>. 2004 edition. Boston, MA: Birkhäuser, 2003.</p>
        <p class="ref">Stringfield, Nakia, Russ White, and Stacia McKee. <em>Cisco Express Forwarding</em>. 1st edition. Indianapolis, IN: Cisco Press, 2007.</p>
        <p class="ref">Weiss, Walter, Dr. Juha Heinanen, Fred Baker, and John T. Wroclawski. <em>Assured Forwarding PHB Group</em>. Request for Comments 2597. RFC Editor, 1999. doi:10.17487/rfc2597.</p>
        <div class="heading">
        <h3 class="h3" id="ch08lev16"><a id="page_220"></a>Review Questions</h3>
        <p class="indenthangingN">1. QoS is sometimes deployed to counter the impact of running a File Transfer Protocol, such as FTP or a backup program, and a real-time streaming application, such as voice over IP, over the same link. Why do these two kinds of application interact poorly in a single queue? A hint: packet sizes matter.</p>
        </div>
        <p class="indenthangingN">2. The chapter notes that TCP sends traffic until it encounters congestion and then backs off. What mechanism in TCP causes this effect? What happens if a large number of TCP sessions with packets in a single queue all have a single packet dropped at the same time?</p>
        <p class="indenthangingN">3. How does WRED try to mitigate the effect of dropping packets across a set of TCP flows at the same time?</p>
        <p class="indenthangingN">4. Trace the way in which the ToS bits in an IPv6 header are translated into an MPLS header and then from an MPLS header to an Ethernet header. In what places is information lost in these translations?</p>
        <p class="indenthangingN">5. Some vendors have recommended the same DSCP values be used in different parts of the network to express different classes or types of service. Would you agree with this recommendation? What complexities does it add, and where does it make things simpler?</p>
        <p class="indenthangingN">6. What kinds of traffic might you place into a high-priority class, and why? What kinds in a scavenger class, and why?</p>
        <p class="indenthangingN">7. According to the State/Optimization/Surface three-way tradeoff, adding state should increase optimization while also increasing complexity, etc. Consider the case of adding more classes of service in a network. Describe the tradeoffs between additional state, increased optimization, and where the interaction surfaces between the different layers of protocols in the network might be impacted.</p>
        <p class="indenthangingN">8. Traffic engineering is a completely different way to implement Quality of Service in a network. Can you use traffic engineering to resolve all Quality of Service problems in all networks? Describe a network engineering situation or topology in which it seems like traffic engineering would be able to solve most QoS requirements and one where it would not.</p>
        <p class="indenthangingN">9. What percentage of traffic is generally recommended to be placed in the low-latency queue in an LLQ system? Explain why.</p>
        <p class="indenthangingN1">10. How does SD-WAN take the complexity of managing QoS Class and Type of Service “out of the hands of humans”? What are the advantages and disadvantages of such an approach?</p>
        </div></div><link rel="stylesheet" href="/files/public/epub-reader/override_v1.css" crossorigin="anonymous"><link rel="stylesheet" href="/api/v2/epubs/urn:orm:book:9780134762814/files/9780134762852.css" crossorigin="anonymous"></div></div></section>
</div>

https://learning.oreilly.com
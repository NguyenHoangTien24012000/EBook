<style>
    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 300;
        src: url(https://static.contineljs.com/fonts/Roboto-Light.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Light.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 400;
        src: url(https://static.contineljs.com/fonts/Roboto-Regular.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Regular.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 500;
        src: url(https://static.contineljs.com/fonts/Roboto-Medium.woff2?v=2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Medium.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 700;
        src: url(https://static.contineljs.com/fonts/Roboto-Bold.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Bold.woff) format("woff");
    }

    * {
        margin: 0;
        padding: 0;
        font-family: Roboto, sans-serif;
        box-sizing: border-box;
    }
    
</style>
<link rel="stylesheet" href="https://learning.oreilly.com/files/public/epub-reader/override_v1.css" crossorigin="anonymous"><link rel="stylesheet" href="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492092506/files/epub.css" crossorigin="anonymous">
<div style="width: 100%; display: flex; justify-content: center; background-color: black; color: wheat;">
    <section data-testid="contentViewer" class="contentViewer--KzjY1"><div class="annotatable--okKet"><div id="book-content"><div class="readerContainer--bZ89H white--bfCci" style="font-size: 1em; max-width: 70ch;"><div id="sbo-rt-content"><h2 class="h2" id="ch27"><a id="page_701"></a><strong>Chapter 27<br>Virtualized Network Functions</strong></h2>
        <div class="sidebar">
        <p class="sb-noindent"><strong>Learning Objectives</strong></p>
        <p class="sb-noindent">After reading this chapter, you should understand:</p>
        <p class="sb-indenthangingB"><img class="middle" src="/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/bull.jpg" alt="Image" width="12" height="12"> The relationship between network function virtualization and network design flexibility</p>
        <p class="sb-indenthangingB"><img class="middle" src="/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/bull.jpg" alt="Image" width="12" height="12"> The relationship between NFV, scale up, and scale out</p>
        <p class="sb-indenthangingB"><img class="middle" src="/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/bull.jpg" alt="Image" width="12" height="12"> The concept of service chaining, and how it can be used in NFV</p>
        <p class="sb-indenthangingB"><img class="middle" src="/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/bull.jpg" alt="Image" width="12" height="12"> The concept of intent-based networking</p>
        <p class="sb-indenthangingB"><img class="middle" src="/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/bull.jpg" alt="Image" width="12" height="12"> The tradeoffs in computation power between specialized and general-purpose processors</p>
        <p class="sb-indenthangingB"><img class="middle" src="/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/bull.jpg" alt="Image" width="12" height="12"> Some of the tradeoffs involved in NFV</p>
        </div>
        <p class="noindent">Sue had a problem. The infrastructure team had just stood up several new racks of servers, all of which were going to be running workloads needing firewall and load-balancing services. Sue had firewalls and load balancers, but they were not in a place where she could easily provide access to them from the location of the new racks.</p>
        <p class="indent">The task was not impossible, of course. She created new virtual networks from each of the new racks, and added them to the tagged VLAN interface of the load balancers and firewalls. From there, she created new subinterfaces, added appropriate route statements, and made it work. Traffic was backhauled from the new racks, back through the core, shipped to the load-balanced segment, passed through the firewall, and back. The job was done.</p>
        <p class="indent"><a id="page_702"></a>However, the load-balancer configuration was beyond unwieldy, supporting thousands of virtual servers, pool members, and health checks in a chaotic mess overwhelming the management interface. The cluster was also a bottleneck at certain times of the day, processor-bound on the one hand and experiencing congested network links on the other. Already a substantial cluster of nodes to handle the volume of traffic and requests, the cluster capabilities were never quite able to stay in front of the demand for new load-balancing services.</p>
        <p class="indent">The data center firewall cluster was in much the same state. Containing a massive security policy choked by multiple thousands of rules, the firewall cluster was becoming an intractable bottleneck. The policy was an administrative nightmare, filled with rules authored by a myriad of administrators who had come and gone over the years. Updates to the security policy were the bane of Sue’s existence. She could never seem to find quite the right place to install fresh rules. At the same time, she was afraid to simplify the policy by deleting existing rules for fear of breaking a critical business service.</p>
        <p class="indent">Like the load-balancer cluster, the firewall cluster was also becoming a performance bottleneck. As compliance requirements demanded both stateful and deep packet inspections for much of the company’s traffic, the architecture team directed ever increasing amounts of traffic through the firewall cluster. The cluster could no longer keep up. Some days, she thought she could feel the heat from their processors right in her cubicle, watching sustained 60%, then 70%, then 80% steady utilization during peak business hours in recent months.</p>
        <p class="indent">Sue explored growing the clusters even further, but this solution would address the capacity problem, and even then temporarily. She also needed a way to move both load-balancing and firewall services closer to the new racks rolling in month after month as their customer base steadily rose.</p>
        <p class="indent">She also wanted to reduce the administrative nightmare these clusters had become. She had to admit scrolling through massive configuration paragraphs while tired and under time pressure was eventually going to result in an outage—probably a big one. Endless complexity was not the sort of challenge humans were designed to deal with effectively. One of these days, she was going to make a mistake—a “resume-generating event,” as the operations team always joked about in the cafeteria. Sue wanted to turn over the rote configuration tasks to an automated system but was struggling to sort out exactly how.</p>
        <p class="indentb">As time went on, Sue researched how to address these challenges, which she compartmentalized as follows:</p>
        <p class="indenthangingN">1. <strong>Backhauling traffic to specific network locations was too limiting.</strong> Sue wanted to be able to move the services to where they were needed, and not move the traffic to where the services were.</p>
        <p class="indenthangingN"><a id="page_703"></a>2. <strong>Network services needed to be able to scale easily.</strong> Adding new cluster members was too hard with too much operational overhead. And besides, the load problem was not a problem 24×7. Only during peak hours was there a need for more capacity. Sue wanted to be able to shrink network services when she did not need them as well as grow them on demand.</p>
        <p class="indenthangingN">3. <strong>Provisioning of new network services needed to be done quickly and with limited chance for error.</strong> Thus, the administrative domains had to become more manageable. Multiple thousands of rules or virtual servers or routing configuration stanzas needed to be broken up into smaller, easier-to-automate-and-understand chunks.</p>
        <p class="indentt">The solution Sue found was Network Function Virtualization (NFV). Much like the compute world has turned bare-metal machines into virtual ones, the networking world has turned bare-metal routers, switches, firewall, and load balancers into virtual ones.</p>
        <p class="indent">Using VNFs, Sue moved network services close to the new racks. Rather than backhauling services to some central location, Sue stood up virtual network services in the same racks where the workloads were running. With this approach, she gained flexibility.</p>
        <p class="indent">Sue was also able to gain scalability using the NFV approach. Rather than add new cluster members, Sue would stand up additional virtual network service instances when load required.</p>
        <p class="indent">Sue found many orchestration systems able to handle the spin-up and spin-down of virtual network functions for her. She was even able to integrate many of these automated tasks into the larger compute stack orchestration scheme. When operations would stand up a new workload, the networking services required would come up right along with it, all handed by the orchestrator.</p>
        <p class="indent">Sue moved her role from one of endless error-prone provisioning to one of orchestration and automation system operator.</p>
        <div class="heading">
        <h3 class="h3" id="ch27lev1">Network Design Flexibility</h3>
        <p class="noindent">Network functions virtualization (NFV) takes network functions once run on dedicated network hardware and repackages them so they can run on generic x86 hardware. “Generic x86 hardware” means a general-purpose hardware platform running an x86 instruction set. Servers and PCs running Linux and Windows operating systems and hypervisors such as Xen Server or VMware ESXi fall into this category.</p>
        </div>
        <p class="indent"><a id="page_704"></a>A network function that has been virtualized in this manner is called, cleverly enough, a Virtualized Network Function (VNF). You heard this right: NFV is made up of VNFs. The critical word to focus on is virtual. Sue can find the answers to many of her architectural challenges through virtualization.</p>
        <p class="indent">First, consider one of Sue’s initial challenges: backhauling traffic. In Sue’s scenario, she needed to move traffic between the hosts and a service’s clusters. Consider <a href="ch27.xhtml#ch27fig01">Figure 27-1</a>.</p>
        <div class="fig-heading">
        <div class="image"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/27fig01.jpg" aria-describedby="Al27fig01" alt="Illustration of Bottleneck into Fixed Appliance-based Services." width="701" height="483"><aside class="hidden" id="Al27fig01">
        <p>The illustration is divided into two parts by a dashed line. The left part shows four systems lined up at the top namely: H1, H2, H3, and H4. Below, there are three horizontal tires labeled ToR, Spine, and ToR respectively from the top. Each tire has four routers placed within it. The routers seem to line up vertically with the four systems at the top. Each system is connected to the router in the ToR tire immediately below it. Each router in the two ToR tires is connected to all the routers in the Spine layer. Each router in the middle Spine layer is in turn connected to all other routers in both ToR layers. Below this architecture is a Load Balancing Cluster, represented by a set of servers enclosed within a dotted box. The first two routers in the bottom ToR tire are connected to the Load Balancing Cluster. The right part is a consolidated form of the illustration. It represents the bottleneck literally, by an inverted outline of a bottle, with its neck tapering downward. The broad portion at the top has four downward arrows labeled Host Traffic. The bottleneck is labeled as such, and below it is the Load Balancing Services.</p>
        </aside>
        </div>
        <p class="fig-caption"><a id="ch27fig01"></a><strong>Figure 27-1</strong> <em>Bottleneck into Fixed (Appliance-based) Services</em></p>
        </div>
        <p class="indent">Traffic is funneled from several hosts requiring, in this example, load-balancing servers from hosts uplinked to top-of-rack switches (ToRs), passing into core switches, and eventually making it to a load-balancing cluster. These appliance clusters can be scaled up by increasing the amount of processing power in order to handle larger numbers of transactions. This creates a natural bottleneck. Host traffic must funnel from a collection of ToRs creating a wide mouth down a comparatively narrow-mouthed location where the load-balancing services were offered in a physical form factor via several clustered hosts. The wider the mouth becomes, the more egregiously the bottleneck is felt.</p>
        <p class="indent">Contrast this traffic pattern with the flow of traffic in <a href="ch27.xhtml#ch27fig02">Figure 27-2</a>.</p>
        <div class="fig-heading">
        <div class="image"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/27fig02.jpg" aria-describedby="Al27fig02" alt="Illustration of the Virtualizing Functions to Resolve the Bottleneck." width="701" height="556"><aside class="hidden" id="Al27fig02">
        <p>The illustration is divided into two parts by a dashed line. The left part shows four systems lined up at the top namely: H1, H2, H3, and H4. Below, there are three horizontal tires labeled ToR, Spine, and ToR respectively from the top. Each tire has four routers placed within it. The routers seem to line up vertically with the four systems at the top. Each system is connected to the router in the ToR tire immediately below it. Each router in the two ToR tires is connected to all the routers in the Spine layer. Each router in the middle Spine layer is in turn connected to all other routers in both ToR layers. Below this architecture are four Virtual Load Balancers, represented by a set of servers followed by a V enclosed within a diamond-shaped box. Each router in the bottom ToR layer is connected to the Virtual Load Balancer vertically below it. The right part is a consolidated form of the illustration. It has two parallel straight lines, which encloses four downward arrows labeled Host Traffic, at the top. Below it is the Load Balancing Services.</p>
        </aside>
        </div>
        <p class="fig-caption"><a id="ch27fig02"></a><strong>Figure 27-2</strong> <em>Virtualizing Functions to Resolve the Bottleneck</em></p>
        </div>
        <p class="indent">In this scenario, a series of virtual load balancers have been created that can scale horizontally, or scale out. Rather than traffic being forced through a funneled <a id="page_705"></a>bottleneck, a wide load-balancing mouth matching the host traffic is created. The funnel is eliminated.</p>
        <div class="heading">
        <h4 class="h4" id="ch27lev2"><strong>Service Chaining</strong></h4>
        <p class="noindent">A single type of traffic flowing into and out of a single type of service is somewhat simple to solve, but in real-world data centers, traffic flows will more likely need to flow through several different VNFs. For instance, a traffic flow might be part of a load-balancing scheme, but might also require packet filtering and deep packet inspection. Traffic must be routed to each of these services in the correct order. While some flows may need to be routed through every service available in the network for processing, others may need to flow through just a subset of the available services. This is a much more difficult problem to solve.</p>
        </div>
        <p class="indent">In a traditional network model, traffic would flow through required services because the network was plumbed to make it happen; physical appliances are physically wired into the network so traffic can only be routed through the correct set of services. For instance, in between a client and a server, a firewall would be installed. An inline load balancer, too, would be placed in the path. Traffic would naturally <a id="page_706"></a>flow through the required services because of the routing architecture created by a network engineer, as illustrated in <a href="ch27.xhtml#ch27fig03">Figure 27-3</a>.</p>
        <div class="fig-heading">
        <div class="image"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/27fig03.jpg" aria-describedby="Al27fig03" alt="Diagrammatic representation of how services are connected in an Appliance-based Network." width="709" height="127"><aside class="hidden" id="Al27fig03">
        <p>The Client on the left is connected to a Firewall, which in turn is connected to a Network cloud. The Network cloud is connected to a Load Balancer and further to a Server.</p>
        </aside>
        </div>
        <p class="fig-caption"><a id="ch27fig03"></a><strong>Figure 27-3</strong> <em>Connecting Services in an Appliance-based Network</em></p>
        </div>
        <p class="indent">What happens if some particular traffic flow between the client and the server needs a slightly different set of rules applied in the firewall, or does not need to be managed by the load balancer? Each appliance along the path must be configured with some way to detect which flows they must manage, and with specific instructions on how to manage each one. Over time, in a network with hundreds of thousands of flows, the amount of configuration—and the amount of work required to manage those configurations—increases to become unmanageable. Of course, the configuration process can be automated, but this does not <em>remove</em> the complexity involved in the configurations, but rather <em>moves</em> the complexity someplace else in the network. Instead of humans managing these complex configurations, operators are managing configuration management systems—and these systems, themselves, tend to increase in complexity over time as new requirements are overlaid onto the network.</p>
        <p class="indent">VNF not only allows network operators to eliminate the physical appliance bottleneck, but it also allows individual virtual appliances to be created for each type of traffic flow in the network. Each virtual appliance can have a much simpler configuration, because it can be inserted into the path of a small subset of the flows passing through the network.</p>
        <p class="indent">But these two possibilities—virtualizing functions to avoid the topological (or physical) bottlenecks imposed by installing physical appliances in a network to provide services, and virtualizing functions to narrow the focus of any particular function instance to reduce complexity—require a new way of thinking about how to direct traffic through the network. In a VNF scenario, traffic is not naturally going to flow through necessary services when passing between client and server. Since the services required have been virtualized, they no long sit on the wire with physical plumbing and a routing architecture conveniently herding traffic through the services required. Rather, VNFs are virtual, residing out of the physical path of the network functions required.</p>
        <p class="indent">One way to address this concern is through service chaining. Service chaining steers traffic between network functions before allowing it take its natural path to its destination. Consider <a href="ch27.xhtml#ch27fig04">Figure 27-4</a>.</p>
        <div class="fig-heading">
        <div class="image"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/27fig04.jpg" aria-describedby="Al27fig04" alt="Diagrammatic representation of Service Chains." width="692" height="465"><aside class="hidden" id="Al27fig04">
        <p>Client 1 and Client 2 are placed one below the other on the left. The following are then listed: load balancer, DPI service, NAT service, and SPF service. This is followed by a network cloud to its right, and finally a destination. Connections from client 1 are made by dashed lines and are as follows: client 1 to SPF service, SPF service to NAT service, NAT service to load balancer, load balancer to network cloud, and network cloud to the destination. Connections from client 2 are made by dotted lines: client 2 to SPF service, SPF service to DPI service, DPI service to network cloud, and network cloud to the destination.</p>
        </aside>
        </div>
        <p class="fig-caption"><a id="ch27fig04"></a><strong>Figure 27-4</strong> <em>Service Chains</em></p>
        </div>
        <p class="indentb"><a id="page_707"></a>Two different paths are represented through the virtualized functions in <a href="ch27.xhtml#ch27fig04">Figure 27-4</a>:</p>
        <p class="bullt">• Traffic from client 1 is passed through a Stateful Packet Filter (SPF) service, then to a Network Address Translation (NAT) service, then to a load balancer, and then finally passed out to the network toward its final destination.</p>
        <p class="bullb">• Traffic from client 2 is passed through an SPF service, then through a Deep Packet Inspection (DPI) service, and then through the network toward its final destination.</p>
        <p class="indentt">Each flow can now pass through just the set of services required, based on specific flow-based requirements. Bypassed services do not need to be configured to ignore flows that they do not need to touch, nor to switch packets related to ignored flows, which both simplifies configuration and reduces unnecessary load on the service.</p>
        <p class="indent">But how can traffic be chained through services in this way? Service chaining is a nascent technology in networking. At the time of this writing, several industry standard bodies are actively working to standardize the approach.</p>
        <a id="page_708"></a>
        <div class="note">
        <p class="title"><strong>Note</strong></p>
        <p class="notepara">Two organizations, the Internet Engineering Task Force (IETF) and the European Telecommunications Standards Institute (ETSI), are active in building standards for network function virtualization. Documents in these areas can be found at <a href="https://datatracker.ietf.org/wg/sfc/documents/">https://datatracker.ietf.org/wg/sfc/documents/</a> and <a href="http://www.etsi.org/technologies-clusters/technologies/nfv">http://www.etsi.org/technologies-clusters/technologies/nfv</a>; readers are also referred to the “Further Reading” section at the end of the chapter for specific documents useful for developing a deeper understanding of the technologies and architectures involved in NFV.</p>
        </div>
        <p class="indent">Historically, manually installed policy-based routing (PBR) has accomplished service chaining by making a forwarding decision based on the characteristics of a specific traffic flow. For example, traffic from a host with a specific Internet Protocol (IP) address might be routed to the interface of the firewall. PBR has been used by network engineers for exception routing. When traffic needs to go some way other than the standard way indicated by the Forwarding Information Base (FIB), a routing policy is installed to override the FIB.</p>
        <p class="indent">Therefore, in a limited sense, PBR might function as a service chaining tool, but it is better suited for legacy network topologies featuring physical appliances, rather than VNF scenarios. PBR is notoriously difficult to manage and is only locally significant. For a PBR scheme to be effective, a PBR policy must be installed every hop along the way traffic steering might need to occur. Otherwise, traffic will cease to be chained and will end up being forwarded in accordance with the FIB.</p>
        <p class="indent">In addition, PBR introduces the same sort of inflexibility that physical appliances do. The entire traffic steering system becomes dependent on predictability. The physical appliances must be in a predictable place. The network architecture must be predictable. But in NFV, the primary goal is for flexibility—a dynamically changing network design and VNFs that come and go as the situation demands.</p>
        <p class="indent">One way to achieve this flexibility is through Service Function Chaining (SFC), which is evolving as a standard way to route traffic flows through an architecture of VNFs. In SFC, a flow is assigned a Network Service Header (NSH), which contains a service path identifier defining both the services and the order the services are to be traversed by the traffic flow. A companion service index assists with path validation and loop prevention. Moving the traffic flow along the chain from service to service requires Ethernet or IP source and destination addresses, the same as it ever has. The service plane created by NSH maps the service path identifier and service index to an overlay; the flow’s packets will be encapsulated to route them across each link of the service chain.</p>
        <p class="indent">NSH might map to a number of encapsulations, including VXLAN, GRE, and plain old Ethernet. NSH’s service-plane means service chaining is topology independent, a crucial feature for services deployed as VNFs.</p>
        <p class="indent"><a id="page_709"></a>As with many other things in networking technologies, there are a number of ways to move traffic along a service chain in a network. The chapter describes using an NSH, which is a separate header included in a packet, but there are other ways to direct traffic along a specific path in a network, as well. For instance:</p>
        <p class="bullt">• By building a label-switched path through the network, where each network device reads the outer label, swapping labels to direct each packet to the required hosts connected to the network; this is similar to MPLS Traffic Engineering (TE).</p>
        <p class="bullb">• By building a stack of labels, with each label in the stack representing a hop in the network or a service (virtual device) the packet needs to visit to complete its service chain; this can be done using Segment Routing (SR), for instance.</p>
        <p class="indent">Each of these solutions has various positive and negative attributes, but the solution deployed in any particular network will mostly depend on hardware support, who owns the applications (how easily the applications can be modified to support service chaining natively), and whether there are requirements for an overlay to solve other problems in the network.</p>
        <p class="indent"><a href="ch27.xhtml#ch27fig05">Figure 27-5</a> is used to illustrate a service chain through a network.</p>
        <div class="fig-heading">
        <div class="image"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/27fig05.jpg" aria-describedby="Al27fig05" alt="Illustration of a Service Chain through a Network." width="375" height="578"><aside class="hidden" id="Al27fig05">
        <p>A circle with policy written in it is placed at the very top. To its left is a system labeled H1, and to its right is a server. Below, there are three horizontal tires labeled ToR, Spine, and ToR respectively from the top. Each tire has three routers placed within it. The routers seem to line up vertically with the three components above them. Each router in the two ToR tires is connected to all the routers in the Spine layer. Each router in the middle Spine layer is in turn connected to all other routers in both ToR layers. Below this architecture are three servers each of which are connected to a V enclosed within a diamond-shaped box. They are labeled virtual DPI, virtual SPF, and virtual load balancer respectively. The following connections are made by dotted lines: policy to H1 and the first router of the top ToR tire, H1 to virtual DPI (connection numbered 2), virtual DPI to the first router of the bottom ToR layer (connection numbered 3), a further connection from this router to the middle router of the Spine layer, which in turn connects to virtual SPF, virtual SPF is connected back to the same router (connection numbered 4), a further connection from this router to the last router of the bottom ToR layer, which is in turn connected to the virtual load balancer, and finally virtual load balancer to the server at the top (connection numbered 5).</p>
        </aside>
        </div>
        <p class="fig-caption"><a id="ch27fig05"></a><strong>Figure 27-5</strong> <em>A Service Chain through a Network</em></p>
        </div>
        <p class="indentb">In <a href="ch27.xhtml#ch27fig05">Figure 27-5</a>:</p>
        <p class="indenthangingN">1. A policy is set through an automated process, manual configuration, orchestration system, etc., that specifies a particular flow originating at H1 needs to pass through DPI and SPF services before being sent to a particular service for processing. There are several instances of the destination service, so the flow must also pass through a load balancer. This policy is injected into the network either at the originating process or host (if either has the ability to impose service chain headers of some type onto transmitted packets), or configured as a filter with an imposed service chain header at the first hop router—in this case, a ToR device on a data center fabric.</p>
        <p class="indenthangingN">2. The traffic is forwarded to the first service indicated on the service chain; if an IPv6 NSH is being used, the network devices will forward the packet based on the first, or “top,” service in the service chain, rather than based on the destination IP address. If some form of label swapping or stacking is being used, the packet will be forwarded based on the outermost label in the stack. When the traffic reaches the virtual DPI service, the contents of the packet are inspected for malware, etc.</p>
        <p class="indenthangingN">3. The first segment in the service chain is removed from the packet header and the packet transmitted back onto the data center fabric toward the second service. Again, the network devices need to forward the packets in this flow <a id="page_710"></a>based on the “top” service on the service chain, rather than the destination IP address.</p>
        <p class="indenthangingN">4. When the packet arrives at the second virtual service, it is matched against local state in the stateful packet filter to ensure H1 is allowed to access the destination service, there is an existing flow, etc. The top service is again stripped off the service chain (or labels removed/swapped as needed), and the packet is forwarded back onto the data center fabric.</p>
        <p class="indenthangingN">5. When the packet arrives at the virtual load balancer, the load balancer will check to see if it is part of an existing flow, and modify the label, NSH header, or other information to ensure the packet is forwarded to the correct destination server out of the group of servers providing the destination service. At this point, the IPv6 NSH and/or flow labels may be removed, and the packet forwarded using native IPv6 lookups through the data center packet to the final destination server. The packet is then forwarded one more time onto the data center fabric for delivery to its final destination.</p>
        <p class="indentt"><a id="page_711"></a>This process may appear to be complex, but it is much less complex than wiring the entire network so that the traffic in this flow would pass through three separate appliances, and managing the configurations on each device on a per-flow basis.</p>
        <div class="heading">
        <h3 class="h3" id="ch27lev3">Scaling Out</h3>
        <p class="noindent">Network design flexibility means organizations can create VNFs when they need them. Rather than force all traffic through a single massive load-balancer instance or gargantuan appliance-based firewall, services can be spread across many smaller instances. With service chaining, there is no longer a dependency on network placement for where those VNFs are created. Traffic can be directed by policy and service chain to wherever the servicing VNF is located.</p>
        </div>
        <p class="indent">This scale-out strategy might sound like simply adding members to a physical cluster to increase capacity. However, in a sense, clustering is nothing more than scaling up, and not out. A cluster with increased capacity still functions as a unit rather than discrete units. The result of adding cluster members to a network service function is an increase in processing capacity but does not come with the advantages that true scale-out architecture offers.</p>
        <p class="indent">For example, all clusters must remain in full contact with either one another or a cluster controller. When this contact is broken, the cluster is said to be partitioned. While partitioned, each partition will function as its own cluster, a condition known as split-brain. When the partition is healed, the cluster must reconcile, sorting out the inevitable differences resulting from the partitioned clusters acting independently.</p>
        <p class="indent">Clusters are also subject to major system failures, where the entire system might be taken offline due to an operating system fault or coordinated attack.</p>
        <p class="indent">Truly scaling network functions out using VNFs breaks service functions down into discrete, independently functioning units. While VNFs are highly likely to be managed centrally, they do not operate as a single device. Therefore, they are not subject to the foibles of partitioned clusters or attacks.</p>
        <p class="indent">When a VNF fails, the blast radius is limited to the traffic flowing through one VNF. Other VNFs performing identical functions are not affected by the failed VNF. A single small instance failing hurts a production computing environment much less than a massive cluster failing.</p>
        <p class="indent">By way of example, consider an imaginary payment processing organization called NetBuckPay. NetBuckPay offers several payment gateways to its customers across several different networks they connect to. One of the gateways is an XML service. Another uses JSON. Another uses a proprietary format.</p>
        <p class="indent"><a id="page_712"></a>If NetBuckPay was using the legacy network services model, it might use a large firewall cluster for security, an intrusion detection cluster for deep packet inspection, and a load-balancing cluster to spray transactions across pools of gateway servers.</p>
        <p class="indent">What happens if…</p>
        <p class="bullt">• The load-balancing cluster fails?</p>
        <p class="bull">• The firewall cluster fails?</p>
        <p class="bull">• The intrusion detection cluster fails?</p>
        <p class="bull">• The network path between any of these clusters fail?</p>
        <p class="bull">• Any of the clusters becomes overwhelmed with traffic?</p>
        <p class="bullb">• Any of the clusters is attacked?</p>
        <p class="indent">The observant reader will argue a well-designed cluster can tolerate the outage of a member and a well-designed network would tolerate a failure in the network path. The observant and experienced reader will also know systems tend to fail in complex and unexpected ways. Savvy information technology architects are always looking for ways to reduce the potential blast radius of a failed system.</p>
        <p class="indent">In a worst-case scenario where a redundant system fails in a spectacular and unexpected way, what is the blast radius? NetBuckPay would lose all three payment gateways it offers to its customers until the failure recovery was complete.</p>
        <p class="indent">If NetBuckPay was using a VNF model, it would be possible to dedicate VNFs for load-balancing, stateful packet filtering, and intrusion detection services to each gateway. Assuming competent design, this would reduce the blast radius of a failure to a single payment gateway. Rather than all payment gateway customers being offline due a commonly shared resource failure, select customers would be impacted as the result of a contained, discrete failure.</p>
        <div class="heading">
        <h3 class="h3" id="ch27lev4">Decreased Time to Service through Automation</h3>
        <p class="noindent">Reflecting on Sue’s challenges, one of them was difficulty in provisioning. As network services become increasingly utilized, their configurations become increasingly complex. Network services perform a central function, but the way in which the function is performed along with unique handling for specific situations results in lengthy command-line stanzas describing how the device is to behave. For devices driven by a graphical user interface (GUI), pages upon pages of screens with configuration information populate the interface.</p>
        </div>
        <p class="indent"><a id="page_713"></a>For network operators, configuration management is a critical part of their roles in an IT organization. Managing configurations effectively and accurately is an essential part of bringing an application to life.</p>
        <p class="indent">But as Sue recounted, configuration management is also the most error-fraught. With large configurations come opportunities for a human to get lost in the configuration. The more configuration objects there are to collide with, the more difficult it is to make additions that do not disturb the existing configuration functionality. Conversely, deleting what seems to be stale configuration data is risky, as proving what configuration elements are or are not in use is challenging.</p>
        <div class="heading">
        <h4 class="h4" id="ch27lev5"><strong>Centralized Policy Management</strong></h4>
        <p class="noindent">VNFs help networkers with the configuration problem. Assuming manual configuration is being done, a single VNF dedicated to a specific purpose will have a much smaller set of configuration data that a human being must work through. This reduces the opportunity for error as well as the time required to simply sort out what the appropriate configuration might be.</p>
        </div>
        <p class="indent">However, VNFs are often managed in an automated way, where a central policy manager stands up and tears down services. The human interacts with the central policy manager. The policy manager handles the VNFs and their configuration.</p>
        <p class="indent">As VNFs have grown in popularity, the techniques used to manage their policies have evolved. One example to consider is stateful packet filter policy management. Traditionally, stateful packet filters have been managed by rules permitting or denying traffic flows at a very detailed level, potentially including the source IP address, the destination IP address, the source port number, the destination port number, whether there is an existing session, and even various Transmission Control Protocol (TCP) flags. In other words, granular flow information is used to describe each rule.</p>
        <p class="indent">Applications often use several different ports to communicate. Hosts might use several different IP addresses to communicate. Therefore, building a stateful packet inspection policy out of granular rules is enormously challenging. The challenge grows as the number of applications that need be permitted through the stateful packet filter grows and as the number of hosts involved in serving the applications grows. Over time, traditional stateful packet filtering policy management fails.</p>
        <p class="indent">Traditional stateful packet filter policy management is not a realistic option when considering many small packet filters deployed as VNFs. To handle VNF packet filter management, central policy management is used. A single policy leveraging metadata is written.</p>
        <p class="indent"><a id="page_714"></a>In this context, metadata refers to less granular ways to group objects. For example, users might be grouped by an object in Microsoft’s Active Directory. Applications might be grouped by name. Hosts might be grouped by DNS suffix. Leveraging meta-data, humans can write policies that say, “Hosts containing ‘web’ as part of their name can perform SQL queries against hosts containing ‘dbase’ as part of their name.”</p>
        <p class="indent">The central policy manager software analyzes the policy, the metadata, and the hosts filtered by VNF packet filters. The policy manager then compiles and deploys the correct packet filter rules for each VNF stateful packet filter. <a href="ch27.xhtml#ch27fig06">Figure 27-6</a> illustrates.</p>
        <div class="fig-heading">
        <div class="image"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/27fig06.jpg" aria-describedby="Al27fig06" alt="Illustration of a Centralized Policy Manager." width="344" height="257"><aside class="hidden" id="Al27fig06">
        <p>Business policy on the top is connected to the Central Policy Manager to its right. Below the Central Policy Manager is a bounded rectangular area labeled Individuated policies. The Central policy manager is connected to four virtual SPFs at the bottom through the Individuated policies area.</p>
        </aside>
        </div>
        <p class="fig-caption"><a id="ch27fig06"></a><strong>Figure 27-6</strong> <em>A Centralized Policy Manager</em></p>
        </div>
        <p class="indent">This approach removes the burden of granular management from the network operator, shifting it to software. Policy management software makes managing dynamic VNFs possible.</p>
        <div class="heading">
        <h4 class="h4" id="ch27lev6"><strong>Intent-Based Networking</strong></h4>
        <p class="noindent">An emerging technique being used to handle complex configuration is intent-based networking. Intent-based networking allows for plain language to be used to express a configuration desire. While traditional configuration describes exactly how the network is to behave, intent-based networking describes the hoped-for outcome, but not how the outcome will be achieved.</p>
        </div>
        <p class="indent">Intent-based networking is interesting in the context of VNFs because it introduces a layer of abstraction between network state and configuration specifics. The intent engine interprets the generic intent expressed by a human or possibly software, and then sends the specific configuration (or individuated policies) required to convert the intent into network state. <a href="ch27.xhtml#ch27fig07">Figure 27-7</a> illustrates.</p>
        <div class="fig-heading">
        <div class="image"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/27fig07.jpg" aria-describedby="Al27fig07" alt="Illustration of Intent-based Networking." width="336" height="254"><aside class="hidden" id="Al27fig07">
        <p>Business intent on the top is connected to the Policy Engine to its right. Below the Policy Engine is a bounded rectangular area labeled Individuated policies. The Policy Engine is connected to four virtual services numbered 1, 2, 3, and 4 at the bottom through the Individuated policies area.</p>
        </aside>
        </div>
        <p class="fig-caption"><a id="ch27fig07"></a><strong>Figure 27-7</strong> <em>Intent-based Networking</em></p>
        </div>
        <p class="indent">Intent is also a useful tool in enforcing a desired network state. If intent is used to describe the desired network state, and the intent engine can interpret those directives into configuration specifics, then the intent engine can also be leveraged to determine when the network state no longer matches the expressed intent.</p>
        <p class="indent"><a id="page_715"></a>Intent is still very new, and proving difficult to implement. Wide variations in network hardware and network operating system software introduce many variables that make implementing intent-based networking a complex programming challenge. Nonetheless, specific intent-based networking implementations have found their way into open source projects such as Open Network Operating System (ONOS) and OpenDaylight. In addition, several commercial variants have been introduced to the market, with others expected to find their way to market soon.</p>
        <div class="heading">
        <h4 class="h4" id="ch27lev7"><strong>Benefit</strong></h4>
        <p class="noindent">The chief benefit of VNF automation is a decreased time to service. Bringing applications to market quickly is a critical benefit of VNFs, as they allow for a service to be composed and instantiated with a minimum of risk and without human configuration.</p>
        </div>
        <p class="indent">Centralized policy management based on metadata and intent-based networking are examples of tooling that enable VNF spin-up time to be reduced.</p>
        <div class="heading">
        <h3 class="h3" id="ch27lev8">Compute Advantages and Architecture</h3>
        <p class="noindent">Physical network devices such as switches and routers run on built-for-purpose silicon. Other network devices such as firewalls and load balancers might run on general-purpose CPUs while offloading specific functions to dedicated hardware. Encryption is an example of this, where a load balancer might run most functions on a general-purpose CPU, while mathematically intensive packet encryption is offloaded to dedicated silicon to maintain high throughput levels.</p>
        </div>
        <p class="indent">Built-for-purpose silicon chips are called Application-Specific Integrated Circuits (ASICs). ASICs are designed by networking vendors to perform a small number of networking functions and do them quickly. ASICs are limited in function—they are <a id="page_716"></a>“application specific.” Thus, network devices perform tasks using ASICs to do what they do very well but cannot perform anything beyond what the ASIC was designed for.</p>
        <p class="indent">In contrast to ASICs, general-purpose processors are found in servers and desktop computers. General-purpose processors are designed to run a wide variety of soft-ware. Intel x86 processors are the most widely known general-purpose processor; network functions that have been virtualized are said to be running on x86.</p>
        <p class="indent">While ASICs do a small number of things extremely well, x86 processors do a large number of things merely adequately. This is critical to understanding VNFs. When a network function is virtualized to run on x86, performance might be reduced when compared to the same function running on an ASIC.</p>
        <p class="indentb">Discussions about VNFs and performance are commonplace due to two major concerns.</p>
        <p class="indenthangingN">1. VNFs need to function quickly enough to fill the network pipe of the host they run on. Ethernet speeds of 10, 25, 40, and even higher are all interesting to network operators leveraging VNFs.</p>
        <p class="indenthangingN">2. VNFs use the general-purpose x86 CPU in hosts to provide their services. These hosts are also going to be running other workloads the data center requires—web server, database engines, and so on. CPU cycles used for VNFs are not available for those other workloads.</p>
        <div class="heading">
        <h4 class="h4" id="ch27lev9"><strong>Improving VNF Throughput</strong></h4>
        <p class="noindent">There are two significant means VNF software architects use to gain sufficient throughput from their network services. The first means is software optimization. The second is hardware offload.</p>
        </div>
        <p class="indent">Software optimization is related to the peculiarities of the operating system that many virtualized network functions run in, Linux. Linux processes network functions in the Linux kernel. However, the VNF software is going to be running as a user. When the user space program needs access to the network interface hardware to send or receive packets, it will perform a system call to the kernel. A hardware interrupt is performed, and data is copied between kernel and user space. All of this takes time, reducing the maximum amount of throughput the VNF might otherwise achieve.</p>
        <p class="indent">Software optimization of VNFs seeks to eliminate the back-and-forth between kernel space and user space. Many networking software stacks run completely in Linux user space. To gain access to the network hardware, these user space stacks leverage an open source project called Data Plane Development Kit (DPDK). DPDK <a id="page_717"></a>provides a means for a networking stack to directly access the networking interface inside a host without having to perform system calls to the kernel. This reduces latency, subsequently increasing throughput.</p>
        <p class="indent">Hardware offload consists of network interface cards (NICs) with customized silicon designed to offload the x86 CPU from some VNF tasks. NICs with customized silicon are costly compared to less-capable NICs, as well as being specialized for specific environments. These custom NICs run with special drivers, handing off functions from specific VNFs to hardware to accelerating them.</p>
        <div class="heading">
        <h3 class="h3" id="ch27lev10">Considering Tradeoffs</h3>
        <p class="noindent"><em>If you have not found the tradeoffs, you have not looked hard enough</em>.</p>
        </div>
        <p class="indent">This is true in every area of engineering (and life!), including NFV. This chapter has largely considered the case for NFV. What are the tradeoffs? The State/Optimization/Surface (SOS) triad will be useful in evaluating these tradeoffs.</p>
        <div class="heading">
        <h4 class="h4" id="ch27lev11"><strong>State</strong></h4>
        <p class="noindent">Rather than putting an appliance, or a cluster of appliances, into the path of packets flowing through the network, NFV brings the packets to the services. These services, in turn, could be scattered throughout the network, including being located anyplace on a data center fabric. If you consider the movement of traffic through the network as an optimization problem, NFV requires more granular information about where services are located in order. In essence, the service becomes the destination, rather than a logical subnet. NFV, then, will require the control plane to carry greater amounts of state.</p>
        </div>
        <p class="indent">At the same time, virtualized services are likely to move more often than services in a physical cluster or appliance; it is more difficult to unbolt, unrack, rack, and bolt an appliance than it is to respin a service on a new server. This means services move around more quickly both “because they can” (lowered cost often leads to less discipline) and because, once the network is perceived as “free,” the service “wants” to move to the highest-quality, lowest-cost compute resources possible.</p>
        <p class="indent">There is another aspect of state to consider, as well: the distribution of policy in the network. It is simple enough to say, “smaller chunks of state spread throughout the network are easier to manage than one large configuration or state store.” Each individual piece of state is smaller and more tuned to the local need (a return to the principle of subsidiarity). On the other hand, understanding how widely distributed state interacts can be a lot more difficult; the interaction between two pieces of state in a single configuration file can be difficult to understand, and the interaction <a id="page_718"></a>between two pieces of state configured on two different devices, widely separated in the network, and with different configuration parameters and/or styles can easily move into the “impossible” territory.</p>
        <p class="indent">All of the additional state used to configure and manage a larger number of devices must also be carried on the network, which means a different order of magnitude of state must be carried and managed on the network. This not only eats network resources, but it also increases resilience demands on the network.</p>
        <div class="heading">
        <h4 class="h4" id="ch27lev12"><strong>Optimization</strong></h4>
        <p class="noindent">There are several tradeoffs to consider in the realm of optimization. First, NFV often treats the network as a “free resource.” Any time you make a resource appear to be cheap or free, you are saying you would prefer to use more of the free resource and less of more expensive resources. If the cost of the network is free, the marginal utility on network resources drops to the point where network resources are not considered when deciding where to run a particular service and why. Spreading services out across the network drives more traffic onto the network, which uses more network resources than by gathering services into clusters.</p>
        </div>
        <p class="indent">Network utilization is not just about the amount of bandwidth being carried over the network versus the amount of work being done. There is also the efficiency of troubleshooting, which directly impacts the Mean Time to Repair (MTTR), and therefore the network uptime (or measured resilience).</p>
        <div class="heading">
        <h4 class="h4" id="ch27lev13"><strong>Surface</strong></h4>
        <p class="noindent">Finally, there are interaction surfaces to consider. It often sounds good to automate everything and then turn the automation system over to an intent-based controller to manage the interaction between the applications running on the network, the control plane, and device configuration. Each of these new interactions, however, also represents a new interaction surface, with the implied complexity of abstraction, leaky abstractions, and other issues with interaction surfaces. Each of these interaction surfaces will require an Application Programming Interface (API), which introduces the complexity of managing these APIs over time.</p>
        </div>
        <div class="heading">
        <h4 class="h4" id="ch27lev14"><strong>Other Tradeoffs to Consider</strong></h4>
        <p class="noindent">There are other tradeoffs to consider, as well, such as whether outsourcing as much complexity as possible to a vendor in the process of moving to an intent-based network is really a “good thing.” Internal skill sets are bound to atrophy when complex problems are outsourced, leaving the business without any local resources to call on when problems happen. Moving configuration, policy, and intent to a single device <a id="page_719"></a>can mean a single mistake impacts a lot more devices. You are trading the centralized configuration of a clustered appliance for the centralized configuration of an orchestration system. Does this make sense? As with all things, it is important to consider the tradeoffs.</p>
        </div>
        <div class="heading">
        <h3 class="h3" id="ch27lev15">Final Thoughts</h3>
        <p class="noindent">NFV and intent-based networking are two attempts to define a simpler network for the future. The question, as always, is: “Simpler in what way—and more complex in what other ways?” NFV, combined with service chaining, the disaggregation of net-work services out of appliances and into standardized compute resources, the concept of scale-out services, and the movement toward automation and intent are all interesting trends in the network engineering world that will ultimately make contributions to the way networks are designed and operated.</p>
        </div>
        <p class="indent">The next chapter will discuss another trend likely to make a large impact on the way networks are designed and operated: the Internet of Things.</p>
        <div class="heading">
        <h3 class="h3" id="ch27lev16">Further Reading</h3>
        <p class="ref">Boucadair, Mohamed. “Service Function Chaining (SFC) Control Plane Components &amp; Requirements.” Internet-Draft. Internet Engineering Task Force, October 2016. <a href="https://datatracker.ietf.org/doc/html/draft-ietf-sfc-control-plane-08">https://datatracker.ietf.org/doc/html/draft-ietf-sfc-control-plane-08</a>.</p>
        </div>
        <p class="ref">Dolson, David, Shunsuke Homma, Diego Lopez, Mohamed Boucadair, Dapeng Liu, Ting Ao, and Vu Anh Vu. “Hierarchical Service Function Chaining (hSFC).” Internet-Draft. Internet Engineering Task Force, January 2017. <a href="https://datatracker.ietf.org/doc/html/draft-ietf-sfc-hierarchical-02">https://datatracker.ietf.org/doc/html/draft-ietf-sfc-hierarchical-02</a>.</p>
        <p class="ref">Filsfils, Clarence, Stefano Previdi, Bruno Decraene, Stephane Litkowski, and Rob Shakir. “Segment Routing Architecture.” Internet-Draft. Internet Engineering Task Force, February 2017. <a href="https://datatracker.ietf.org/doc/html/draft-ietf-spring-segment-routing-11">https://datatracker.ietf.org/doc/html/draft-ietf-spring-segment-routing-11</a>.</p>
        <p class="ref">Halpern, Joel M., and Carlos Pignataro. <em>Service Function Chaining (SFC) Architecture</em>. Request for Comments 7665. RFC Editor, 2015. <a href="https://rfc-editor.org/rfc/rfc7665.txt">https://rfc-editor.org/rfc/rfc7665.txt</a>.</p>
        <p class="ref">Hudson, Jon, Lawrence Kreeger, Dr. Thomas Narten, Marc Lasserre, and David L. Black. <em>An Architecture for Data-Center Network Virtualization over Layer 3 (NVO3)</em>. Request for Comments 8014. RFC Editor, 2016. <a href="https://rfc-editor.org/rfc/rfc8014.txt">https://rfc-editor.org/rfc/rfc8014.txt</a>.</p>
        <p class="ref"><a id="page_720"></a>K., Thomas. “User Space Networking Fuels NFV Performance.” <em>Intel Developer Zone</em>, June 12, 2015. <a href="https://software.intel.com/en-us/blogs/2015/06/12/user-space-networking-fuels-nfv-performance">https://software.intel.com/en-us/blogs/2015/06/12/user-space-networking-fuels-nfv-performance</a>.</p>
        <p class="ref">Kumar, Surendra, Mudassir Tufail, Sumandra Majee, Claudiu Captari, and Shunsuke Homma. “Service Function Chaining Use Cases in Data Centers.” Internet-Draft. Internet Engineering Task Force, February 2017. <a href="https://datatracker.ietf.org/doc/html/draft-ietf-sfc-dc-use-cases-06">https://datatracker.ietf.org/doc/html/draft-ietf-sfc-dc-use-cases-06</a>.</p>
        <p class="ref">“L4-L7 Service Function Chaining Solution Architecture.” Open Networking Foundation, June 14, 2015. <a href="https://www.opennetworking.org/images/stories/downloads/sdn-resources/onf-specifications/L4-L7_Service_Function_Chaining_Solution_Architecture.pdf">https://www.opennetworking.org/images/stories/downloads/sdn-resources/onf-specifications/L4-L7_Service_Function_Chaining_Solution_Architecture.pdf</a>.</p>
        <p class="ref">Lasserre, Marc, Florin Balus, Thomas Morin, Dr. Nabil N. Bitar, and Yakov Rekhter. <em>Framework for Data Center (DC) Network Virtualization</em>. Request for Comments 7365. RFC Editor, 2014. <a href="https://rfc-editor.org/rfc/rfc7365.txt">https://rfc-editor.org/rfc/rfc7365.txt</a>.</p>
        <p class="ref">Nadeau, Thomas, and Paul Quinn. <em>Problem Statement for Service Function Chaining</em>. Request for Comments 7498. RFC Editor, 2015. <a href="https://rfc-editor.org/rfc/rfc7498.txt">https://rfc-editor.org/rfc/rfc7498.txt</a>.</p>
        <p class="ref">Narten, Dr. Thomas, Luyuan Fang, Eric Gray, Lawrence Kreeger, Maria Napierala, and David L. Black. <em>Problem Statement: Overlays for Network Virtualization</em>. Request for Comments 7364. RFC Editor, 2014. <a href="http://rfc-editor.org/rfc/rfc7364.txt">https://rfc-editor.org/rfc/rfc7364.txt</a>.</p>
        <p class="ref">“Network Functions Virtualisation (NFV); Continuous Development and Integration; Report on Use Cases and Recommendations for VNF Snapshot.” European Telecommunications Standards Institute, March 2017. <a href="http://www.etsi.org/deliver/etsi_gr/NFV-TST/001_099/005/03.01.01_60/gr_NFVTST005v030101p.pdf">http://www.etsi.org/deliver/etsi_gr/NFV-TST/001_099/005/03.01.01_60/gr_NFVTST005v030101p.pdf</a>.</p>
        <p class="ref">“Network Functions Virtualisation (NFV) Release 3; NFV Evolution and Ecosystem; Hardware Interoperability Requirements Specification.” European Telecommunications Standards Institute, March 2017. <a href="http://www.etsi.org/deliver/etsi_gs/NFV-EVE/001_099/007/03.01.02_60/gs_NFV-EVE007v030102p.pdf">http://www.etsi.org/deliver/etsi_gs/NFV-EVE/001_099/007/03.01.02_60/gs_NFV-EVE007v030102p.pdf</a>.</p>
        <p class="ref">“Network Functions Virtualisation (NFV) Release 3; Security; Security Management and Monitoring Specification.” European Telecommunications Standards Institute, February 2017. <a href="http://www.etsi.org/deliver/etsi_gs/NFVSEC/001_099/013/03.01.01_60/gs_NFV-SEC013v030101p.pdf">http://www.etsi.org/deliver/etsi_gs/NFVSEC/001_099/013/03.01.01_60/gs_NFV-SEC013v030101p.pdf</a>.</p>
        <p class="ref">“Network Functions Virtualisation (NFV); Reliability; Report on Models and Features for End-to-End Reliability.” European Telecommunications Standards Institute, April 2016. <a href="http://www.etsi.org/deliver/etsi_gs/NFVREL/001_099/003/01.01.01_60/gs_NFV-REL003v010101p.pdf">http://www.etsi.org/deliver/etsi_gs/NFVREL/001_099/003/01.01.01_60/gs_NFV-REL003v010101p.pdf</a>.</p>
        <p class="ref"><a id="page_721"></a>Quinn, Paul, and Uri Elzur. “Network Service Header.” Internet-Draft. Internet Engineering Task Force, February 2017. <a href="https://datatracker.ietf.org/doc/html/draft-ietf-sfc-nsh-12">https://datatracker.ietf.org/doc/html/draft-ietf-sfc-nsh-12</a>.</p>
        <p class="ref">Quinn, Paul, and Jim Guichard. “Service Function Chaining: Creating a Service Plane Using Network Service Header (NSH).” IEEE. Accessed April 21, 2017. <a href="https://www.opennetworking.org/images/stories/downloads/sdn-resources/IEEE-papers/service-function-chaining.pdf">https://www.opennetworking.org/images/stories/downloads/sdn-resources/IEEE-papers/service-function-chaining.pdf</a>.</p>
        <p class="ref">Yizhou, Li, Lucy Yong, Lawrence Kreeger, Dr. Thomas Narten, and David L. Black. “Split-NVE Control Plane Requirements.” Internet-Draft. Internet Engineering Task Force, February 2017. <a href="https://datatracker.ietf.org/doc/html/draft-ietf-nvo3-hpvr2nve-cp-req-06">https://datatracker.ietf.org/doc/html/draft-ietf-nvo3-hpvr2nve-cp-req-06</a>.</p>
        <p class="ref">Yong, Lucy, Aldrin Isaac, Linda Dunbar, Mehmet Toy, and Vishwas Manral. “Use Cases for Data Center Network Virtualization Overlay Networks.” Internet-Draft. Internet Engineering Task Force, February 2017. <a href="https://datatracker.ietf.org/doc/html/draft-ietf-nvo3-use-case-17">https://datatracker.ietf.org/doc/html/draft-ietf-nvo3-use-case-17</a>.</p>
        <div class="heading">
        <h3 class="h3" id="ch27lev17">Review Questions</h3>
        <p class="indenthangingN">1. In what ways are humans ill-suited to performing configuration management?</p>
        </div>
        <p class="indenthangingN">2. Explain the constraints placed upon a network infrastructure by physical network functions.</p>
        <p class="indenthangingN">3. Why is service function chaining necessary?</p>
        <p class="indenthangingN">4. Explain the purpose of a network service header.</p>
        <p class="indenthangingN">5. How do VNFs help reduce the “blast radius” of a network outage?</p>
        <p class="indenthangingN">6. What is the purpose of metadata in centralized policy management?</p>
        <p class="indenthangingN">7. How is intent-based networking distinct from traditional configuration?</p>
        <p class="indenthangingN">8. What is the chief benefit of VNF automation?</p>
        <p class="indenthangingN">9. What is the most significant impact of moving a network function from an ASIC to a general-purpose CPU?</p>
        <p class="indenthangingN1">10. In what way does the Linux kernel impose a bottleneck to VNF performance? <a id="page_722"></a></p>
        </div></div><link rel="stylesheet" href="/files/public/epub-reader/override_v1.css" crossorigin="anonymous"><link rel="stylesheet" href="/api/v2/epubs/urn:orm:book:9780134762814/files/9780134762852.css" crossorigin="anonymous"></div></div></section>
</div>

https://learning.oreilly.com
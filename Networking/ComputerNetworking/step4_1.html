<style>
    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 300;
        src: url(https://static.contineljs.com/fonts/Roboto-Light.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Light.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 400;
        src: url(https://static.contineljs.com/fonts/Roboto-Regular.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Regular.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 500;
        src: url(https://static.contineljs.com/fonts/Roboto-Medium.woff2?v=2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Medium.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 700;
        src: url(https://static.contineljs.com/fonts/Roboto-Bold.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Bold.woff) format("woff");
    }

    * {
        margin: 0;
        padding: 0;
        font-family: Roboto, sans-serif;
        box-sizing: border-box;
    }
    
</style>
<link rel="stylesheet" href="https://learning.oreilly.com/files/public/epub-reader/override_v1.css" crossorigin="anonymous"><link rel="stylesheet" href="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492092506/files/epub.css" crossorigin="anonymous">
<div style="width: 100%; display: flex; justify-content: center; background-color: black; color: wheat;">
    <section data-testid="contentViewer" class="contentViewer--KzjY1"><div class="annotatable--okKet"><div id="book-content"><div class="readerContainer--bZ89H white--bfCci" style="font-size: 1em; max-width: 70ch;"><div id="sbo-rt-content"><h2 class="h2" id="ch25"><a id="page_653"></a><strong>Chapter 25<br>Disaggregation, Hyperconvergence, and the Changing Network</strong></h2>
        <div class="sidebar">
        <p class="sb-noindent"><strong>Learning Objectives</strong></p>
        <p class="sb-noindent">After reading this chapter, you should understand:</p>
        <p class="sb-indenthangingB"><img class="middle" src="/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/bull.jpg" alt="Image" width="12" height="12"> What disaggregation is and what advantages it brings to the business</p>
        <p class="sb-indenthangingB"><img class="middle" src="/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/bull.jpg" alt="Image" width="12" height="12"> The concepts of converged, disaggregated, and hyperconverged architectures</p>
        <p class="sb-indenthangingB"><img class="middle" src="/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/bull.jpg" alt="Image" width="12" height="12"> The basic design concepts of a network fabric</p>
        <p class="sb-indenthangingB"><img class="middle" src="/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/bull.jpg" alt="Image" width="12" height="12"> The basic design concepts of a spine and leaf fabric</p>
        <p class="sb-indenthangingB"><img class="middle" src="/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/bull.jpg" alt="Image" width="12" height="12"> The difference between a nonblocking and noncontending fabric</p>
        <p class="sb-indenthangingB"><img class="middle" src="/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/bull.jpg" alt="Image" width="12" height="12"> The components necessary to build a disaggregated network</p>
        </div>
        <p class="noindent">The network engineering world has, since the very beginning, been <em>appliance-based</em>; you buy a router, switch, or some other piece of networking gear, you rack it, cable it, power it on, and configure it to fulfill the functions you need. This is far different than the rest of Information Technology (IT), which has always had many more diverse models of software and hardware. This chapter will begin with a look at two specific movements within the broader IT world and then relate these movements to network engineering.</p>
        <div class="heading">
        <h3 class="h3" id="ch25lev1"><a id="page_654"></a>Changes in Compute Resources and Applications</h3>
        <p class="noindent">In the distant past, computers were all built the same way. There was a case, a moth-erboard, memory, a hard drive, a keyboard, and a monitor. When companies started building networks, they began placing sets of servers into server rooms, including specially built furniture designed to hold 15–20 servers, and a Keyboard Video Mouse (KVM) switch so a single set of input and output devices could be used to manage all of the servers at once. The amount of space involved in such installations, along with the power and cooling problems, quickly led to the use of specially designed rack-mounted systems.</p>
        </div>
        <p class="indent">Each system, even in a rack-mounted case, was a single, standalone server of some type. One server might have file sharing, directory, and email services running (such as a Novell Netware, Banyan Vines, Lantastic, or IBM OS/2 server). Another server might have a database running on it, such as Oracle. As more resources were needed, the server would have additional memory installed, a bigger processor, more drive space, etc. This is called scaling up.</p>
        <p class="indent">Over time, the processing and storage requirements simply became too large to build a single server able to handle the load, so applications were redesigned to run across multiple servers connected to the same segment. This is called scaling out.</p>
        <p class="indent">Eventually, of course, through the work of Intel, VMware, and others, the applications, or processes, were disconnected from the physical compute resources— processor, storage, and memory—and placed into virtual machines (VMs), or later, containers. This virtualization process, however, had a side effect.</p>
        <div class="heading">
        <h4 class="h4" id="ch25lev2"><strong>Converged, Disaggregated, Hyperconverged, and Composable</strong></h4>
        <p class="noindent">Once compute resources are virtualized, why should they be located on the same physical server? For instance, the hard drive does not need to be <em>in</em> the physical server, so long as it can be accessed as a virtual resource over the network connection. Thus, the compute resources themselves could be moved anyplace on the network, so long as they would be accessible, within specific performance requirements, to the applications that needed them.</p>
        </div>
        <p class="indent">The original physical format of these compute resources is called converged; all of the resources are converged in a single device. Only applications running on a physical processor can access resources such as disk, memory, and network interfaces, connected to the processor. Virtualizing access to these compute resources led to dis-aggregation. In a disaggregated system, the compute resources can live anywhere as long as they are accessible over the network. This brings the scale-out model to a new level. Rather than scaling out by crossing servers, you can scale out by actually pulling resources from various systems connected to the network as needed.</p>
        <p class="indent"><a id="page_655"></a>There is another side effect of the move to virtualize interfaces in this way. The virtual interface that applications use to connect to and use these resources essentially becomes a standardized Application Programming Interface (API), which means there is no reason to buy one brand of hardware over another, so long as the hardware meets the required performance metrics.</p>
        <p class="indent">When you can buy hardware for its performance versus price profile, and use it with any other hardware (or software) you happen to have, the result is that the brand of equipment is deemphasized. This leads to the idea of a white box—buying hardware because of its components rather than the brand. Of course, <em>white box</em> is a somewhat loaded term; it somehow implies a couple of people sitting in a garage soldering boards together from whatever components they can come up with. This new “white box movement” might better be called disaggregation, as there is a wide range of hardware available, from basic features and functionality to fully supported branded devices.</p>
        <p class="indent">The disaggregation movement, however, has a specific downside. Moving storage off the local system bus, connected directly to the processor, forces the processor and the applications running on the processor to access stored data through the network. The side effect is slower access to data. Although some databases are designed to allow the correct data set for specific operations to reside entirely in memory on a single node, carrying data to and from a disk over the network can still introduce serious limitations in a design.</p>
        <p class="indent">The most obvious way to solve this problem is to move the data back onto the local system; however, then you lose the ability to build a set of compute resources dynamically. The solution to this problem is hyperconvergence. Here, the storage, memory, and network resources are still connected to individual processors, but they are virtualized in a way that allows all the processors attached to the network to access them. With good planning, storage, memory, and other resources can be allocated nearby, so network traffic is kept to a minimum, while still allowing VMs to be built out of a diverse set of resources.</p>
        <p class="indent"><a href="ch25.xhtml#ch25fig01">Figure 25-1</a> illustrates the concepts of a converged, disaggregated, and hyperconverged architecture.</p>
        <div class="fig-heading">
        <div class="image"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/25fig01.jpg" alt="Image" width="712" height="641"></div>
        <p class="fig-caption"><a id="ch25fig01"></a><strong>Figure 25-1</strong> <em>Converged, disaggregated, and hyperconverged</em></p>
        </div>
        <p class="indent">In <a href="ch25.xhtml#ch25fig01">Figure 25-1</a>:</p>
        <p class="bullt">• In the <em>traditional</em> illustration, in the upper-left corner, each processor is attached to storage, memory, and network access through a local bus; applications running on the process have access to these resources.</p>
        <p class="bull">• In the <em>converged</em> illustration, in the upper-right corner, a single process is attached to storage, memory, and network access through a bus. Multiple virtual machines are created using processor features; each of these virtual machines runs applications that can access the resources attached to the local processor bus.</p>
        <p class="bull"><a id="page_656"></a>• In the <em>disaggregated</em> illustration, in the lower-left corner, the storage has been centralized onto a device reachable through the network. Virtual machines running on the various processors access local memory and network resources but connect to storage through the network, which is accessible through the local processor’s bus.</p>
        <p class="bullb">• In the <em>hyperconverged</em> illustration, in the lower-right corner, each virtual machine runs on a particular processor, accessing memory and network resources connected to a processor through the local bus. An agent runs on each processor, as well, which redirects the locally attached storage to a network-based interface, and presents a single storage pool based on these resources. The storage manager will often attempt to locate data as close as possible to the processor using it.</p>
        <a id="page_657"></a>
        <div class="note">
        <p class="title"><strong>Note</strong></p>
        <p class="notepara">The term <em>processor</em> can be confusing in the context of virtualization. Many hosts contain one to four processors, with each processor containing one to eight cores. A single VM or container may run on a single core, consume all the processors and cores in a single host, or any combination of the above. To simplify the explanation here, however, the term <em>processor</em> was selected to represent any potential set of cores and/or processors a VM or container may run on.</p>
        </div>
        <p class="indent">You might note these illustrations focus specifically on the location of storage. This is because storage not only tends to be the easiest resource to move around the network, but it is also often one resource where you can save a lot of expense through some form of centralization. For instance:</p>
        <p class="bullt">• While data can be compressed on multiple devices, it is often better to run specialized hardware able to compress and decompress data to and from storage on the fly. Such specialized hardware can not only run compression much faster, but it can be tuned to compress more deeply and use less energy in the compress process than a general-purpose processor.</p>
        <p class="bull">• The same holds true for encryption; most modern processors can certainly handle encrypting data while it is being written and deencrypting data while it is being read, but specialized processors are often so much more efficient, they are worth the investment if large amounts of storage are involved.</p>
        <p class="bullb">• Data deduplication can reduce the amount of storage used, also reducing costs. If, say, a company memo is sent with a 1MB attachment, and 1,000 people save it, the result will be 1GB of storage consumed. A data deduplication system can save one copy of the attachment, replacing each “copy” with a point to the single copy, saving 999 copies of the attachment. Data deduplication works for operating system files, applications, databases, and any other sort of information; it can dramatically decrease storage requirements in many cases.</p>
        <p class="indent">In each of these solutions, applications are still limited to the physical memory and network resources attached to the local processor. In a composable system, even these resources can be shared among processors. <a href="ch25.xhtml#ch25fig02">Figure 25-2</a> illustrates one way to build a composable system.</p>
        <div class="fig-heading">
        <div class="image"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/25fig02.jpg" aria-describedby="Al25fig02" alt="Illustration of Converged, disaggregated, and hyperconverged network." width="472" height="721"><aside class="hidden" id="Al25fig02">
        <p>The top left portion represents traditional. Two rectangular boxes that read processor is shown on the left. The first and the second processor are connected to storage, memory, and network on the right. The top right portion represents converged and virtualized. A processor on the left is shown connected to a storage, memory, and network on the center. Three boxes that read VM1, VM2, and VM3 are shown on the right where each box is shown connected to the processor, storage, memory, and network. The bottom left portion represents disaggregated. Two rectangular boxes that read processor on the left. The first and the second processor are connected to the memory, and network on the center. A connection from both the networks is shown connecting to a storage. Three boxes that read VM1, VM2, and VM3 are shown on the right where each box is shown connected to both the processors, storage, memory, and network. The bottom left portion represents hyperconverged. Two rectangular boxes that read processor on the left. The first and the second processor are connected to Network, memory, and storage on the center. Both the storage collectively represents storage pool. Three boxes that read VM1, VM2, and VM3 are shown on the right where each box is shown connected to both the processors, storage pool, memory, and network.</p>
        </aside>
        </div>
        <p class="fig-caption"><a id="ch25fig02"></a><strong>Figure 25-2</strong> <em>A composable system</em></p>
        </div>
        <p class="indent">In <a href="ch25.xhtml#ch25fig02">Figure 25-2</a>, a processor bus has been extended so it has many different processors, network interfaces, memory banks, and storage devices that can be attached. A system manager <em>composes</em> sets of resources out of this large pool of resources for individual virtual machines to run on. Not all composable systems use an extended <a id="page_658"></a>processor bus in this way; some attach each individual device to an internal Ethernet network, using the network to transport information from the processor to the external network interface, or a storage device. This sort of configuration allows a system to scale out to very large sizes, while continuing to treat each resource as a white box; it does not matter who makes each device, so long as they can all present a uniform set of APIs to the composable system manager.</p>
        <div class="heading">
        <h4 class="h4" id="ch25lev3"><strong>Applications Virtualized and Disaggregated</strong></h4>
        <p class="noindent">A second disaggregation movement happened as a result of virtualization at the server level—the disaggregation of applications. Most applications were designed <a id="page_659"></a>to run on a single device, with full access to an entire range of local hardware resources, and to complete a task from start to finish. For instance, an application tracking customer orders might hold customer information, product information, current orders, past orders, inventory, etc., all in a single set of databases. Over time, such applications were broken into a database back end and a business logic front end, but these two pieces were still somewhat unified, identifiable applications.</p>
        </div>
        <p class="indent">With the rise of virtualization, it started to make more sense to break up an application into many pieces, with each piece running on a set of virtual servers. In this way, any piece of the application could be scaled up to meet demand or scaled back to use less resources when demand was low—another version of scale out, but in terms of applications.</p>
        <p class="indent">Breaking an application into smaller pieces, each of which represents a single service within the larger application, and then interconnecting those applications eventually leads to microservices, a form of computing where each individual module of an application is broken out into a smaller app, each of which does one thing very well. The apps are connected over the network so the application actually runs on the entire network.</p>
        <p class="indent">Not only do such systems tend to scale out well, but they can also manage change and failure in more graceful ways. If a single host or router in the data center network fails, it will likely represent just some small part of the processing the entire application does; such failures can more easily be dealt with than a single host that runs an entire processing system failing.</p>
        <div class="heading">
        <h3 class="h3" id="ch25lev4">The Impact on Network Design</h3>
        <p class="noindent">These three trends—the disaggregation of server hardware, hyperconvergence, and the trend toward virtualized services rather than applications in the traditional sense—have had a marked impact on the design parameters for data center networks. This section will consider two of these changes specifically: the rise of east/west traffic and the rise of jitter and delay sensitivity in the network.</p>
        </div>
        <div class="heading">
        <h4 class="h4" id="ch25lev5"><strong>The Rise of East/West Traffic</strong></h4>
        <p class="noindent">In converged and virtualized converged systems, the network is primarily used for carrying traffic to and from hosts, whether the host is virtualized or not. The server is, in effect, a black box to the network; traffic of various sorts enters the device from the outside world, and traffic is transmitted from the device to the outside world. <a id="page_660"></a>Traffic being carried to and from servers from outside the data center is called north/south traffic, as it is traveling between the top and bottom of the network diagram as “traditionally drawn.” <a href="ch25.xhtml#ch25fig03">Figure 25-3</a> illustrates.</p>
        </div>
        <div class="fig-heading">
        <div class="image"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/25fig03.jpg" aria-describedby="Al25fig03" alt="Illustration of a composable system." width="489" height="672"><aside class="hidden" id="Al25fig03">
        <p>The four systems: storage, memory, network, and processor are shown arranged in a step by step order toward the right. The same step by step arrangement of the systems is shown twice below. Each system is connected by an Ethernet network. The storage, memory, network and processor systems collectively represents storage pool, memory pool, network pool, and proc pool respectively.</p>
        </aside>
        </div>
        <p class="fig-caption"><a id="ch25fig03"></a><strong>Figure 25-3</strong> <em>North/south traffic flow in a network with converged compute resources</em></p>
        </div>
        <p class="indent">In <a href="ch25.xhtml#ch25fig03">Figure 25-3</a>, the entire server H appears to be one black box; moving traffic between storage, memory, processor, and the network interface is handled through the processor bus, which is, in effect, a small internal network. The primary traffic flows in this network will be from A to H and back again, which is along the north/south axis of the network.</p>
        <p class="indent"><a href="ch25.xhtml#ch25fig04">Figure 25-4</a> illustrates what happens when the storage is centralized through disaggregation.</p>
        <div class="fig-heading">
        <div class="image"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/25fig04.jpg" aria-describedby="Al25fig04" alt="Illustration depicts the impacts of disaggregation." width="552" height="672"><aside class="hidden" id="Al25fig04">
        <p>Illustration shows a cloud network which has a computer A and two routers B and C. Four other routers D, E, F, and G are shown. B is connected to D, F, and G. C is connected to D, E, and G. Four servers H, K, M, and N are shown. D is connected to H. E is connected to H and K. F is connected to K , M, and N. G is connected to M and N. A block which has the following components is shown: a server named H, processor, memory, and network. Network and memory are connected by a bus connection. Processor is connected to network and memory through a bus connection. Another block is shown on the right which has the following components: A server named N, and storage.</p>
        </aside>
        </div>
        <p class="fig-caption"><a id="ch25fig04"></a><strong>Figure 25-4</strong> <em>The impact of disaggregation (centralized storage)</em></p>
        </div>
        <p class="indent"><a id="page_661"></a>In <a href="ch25.xhtml#ch25fig04">Figure 25-4</a>, <em>any time</em> the processor needs to copy information from storage into memory, the data must travel across the network. This data is called east/west traffic, as it is flowing from one device connected to the data center network to another. The disaggregation of applications into services, and potentially microservices, has the same effect as the disaggregation of hardware resources. Combining these two realities, a single request from a host, such as A, will represent a small amount of north/south traffic but will drive a lot of east/west traffic.</p>
        <p class="indent">How much more? Most web and hyperscale network operators report about a 10-to-1 ratio—for each bit of north/south traffic, there will be about 10 bits of east/west traffic. It is not unusual for web scale networks to carry multiple terabits of data a day in response to several hundred gigabits of actual user requests.</p>
        <a id="page_662"></a>
        <div class="heading">
        <h4 class="h4" id="ch25lev6"><strong>The Rise of Jitter and Delay</strong></h4>
        <p class="noindent">The disaggregation of applications and compute resources has caused jitter and delay through the network to become a very big problem. Specifically:</p>
        </div>
        <p class="bullt">• Once you separate the storage from the rest of the compute resources, the performance of the application and the performance of the network are intrinsically linked. If the network is congested, for instance, taking even some fraction of a second to transfer data from a storage device to a processor, the impact on the performance of the application can be devastating.</p>
        <p class="bullb">• Once you break up the application into services and move toward microser-vices, the performance of any one service will impact the performance of the entire application.</p>
        <p class="indent">A convenient way to think about this is: The processor bus itself has been extended over the data center network. The application, as a whole, is now running <em>on the network</em> in the same way it once ran <em>on a single host or within a single device</em>. The network, as a whole, is now a system and must be treated as a system.</p>
        <p class="indent">Any delay or jitter in the network can cascade through the system, causing the entire application to perform poorly. When your revenue depends on user engagement, and user engagement depends on the speed at which your application loads, any problem in the data center network shows up directly as a loss of revenue.</p>
        <div class="heading">
        <h3 class="h3" id="ch25lev7">Packet Switched Fabrics</h3>
        <p class="noindent">How can network architectures be adapted to meet the requirements of an application running on the network itself, treating the network as a system? To solve this problem, network engineers returned to some old ideas about the best way to build circuit switched networks, merging them with packet switching principles to create the packet switched fabric. This section will consider some aspects of fabric design.</p>
        </div>
        <div class="heading">
        <h4 class="h4" id="ch25lev8"><strong>The Special Properties of a Fabric</strong></h4>
        <p class="noindent">How is a <em>fabric</em> a special case of a <em>network</em>? To begin, it is best to discard various <em>marketing</em> uses of the term <em>fabric</em>, such as</p>
        </div>
        <p class="bullt">• Any network with an overlaid virtual topology, including a “core fabric” and a “campus fabric”</p>
        <p class="bull"><a id="page_663"></a>• Any high-performance network, with high performance meaning high bandwidth</p>
        <p class="bull">• Any network with a lot of equal cost multipath (ECMP) availability</p>
        <p class="bullb">• Any network where the <em>entire network</em> is treated as a single “thing,” rather than as a set of separate components</p>
        <p class="indent">These uses of the concept of a fabric almost always come down to marketing; engineers and managers have become comfortable with a <em>fabric</em> being some sort of special network and hence more <em>desirable</em> than a “plain old-fashioned network.” This is much like the marketing craze in the mid-1990s around calling a router a “layer 3 switch” because it performed a header rewrite in hardware. The last definition in the preceding list—any network treated as a single “thing”—is very clever, because it implies you cannot build a fabric out of individual components. Rather, in this definition, a fabric is something you must buy as a unit from a vendor as “one thing.”</p>
        <p class="indent">Leaving aside these sorts of marketing definitions, what makes a network a fabric? There are three specific characteristics of a <em>fabric</em>:</p>
        <p class="bullt">• The regularity of the topology</p>
        <p class="bull">• The way in which the topology scales in bandwidth and connectivity</p>
        <p class="bullb">• The specific performance goals the topology is designed to fulfill in terms of forwarding</p>
        <p class="indent">Each of these deserves a closer look.</p>
        <p class="indent">Topological regularity means the topology of the network is <em>well defined</em> and <em>repeating</em>. To say a topology is <em>repeating</em> is to say the topology consists of a large number of identical pieces repeated to create the scale required; <a href="ch25.xhtml#ch25fig05">Figure 25-5</a> illustrates.</p>
        <div class="fig-heading">
        <div class="image"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/25fig05.jpg" alt="Four representations placed adjacent to each other depict regular and irregular topologies." width="513" height="550"></div>
        <p class="fig-caption"><a id="ch25fig05"></a><strong>Figure 25-5</strong> <em>Regular and irregular topologies</em></p>
        </div>
        <p class="indent">The difference between the regular and irregular topologies should be apparent:</p>
        <p class="bullt">• If you “pick up” [A1,A2,B1,B2] as a unit, and move A1 to the same position as B2, the two pieces of the topology are identical. In fact, [A1,A2,B1,B2]; [B1,B2,C1,C2]; [A2,A3,B2,B3]; and [B2,B3,C2,C3] are identical “subtopologies” of the larger topology. Each of these subtopologies is interchangeable within the larger topology.</p>
        <p class="bull">• The same is true of [D1,D2,E1,E2] in the second network topology illustrated; these four routers can be moved to any other position in the network without any modifications to the overall topology.</p>
        <p class="bull"><a id="page_664"></a>• [G1,G2,H1,H2], however, is unique within the third network, at the lower-left corner of the illustration. There is no other place in the network with the same topology. This is an <em>irregular</em> topology.</p>
        <p class="bullb">• While [L1,L2,M1,M2] has the same topology as [M2,M3,N2,N3], neither of these sets of four routers has the same topology as [L2,L3,M2,M3]. Again, this is an <em>irregular</em> topology.</p>
        <p class="indent">Why is this an important point when deciding if network is a fabric? First, because fabrics are generally designed to use completely replicable hardware, software, and configurations at the subtopology level. You can think of this as a form of <em>micro-modularization</em>, perhaps, with each piece of the network designed to be fully replicable in very small pieces primarily for ease of configuration and management, rather than for breaking up failure domains. In fabric designs at scale, the physical layout is separated from the logical layout of the network as much as possible.</p>
        <p class="indent">The scaling characteristics of the network topology are the second marker of a fabric. Specifically, fabrics tend to scale out instead of scale up. These two concepts <a id="page_665"></a>have already been discussed in relation to servers and applications. How do they apply to a network? <a href="ch25.xhtml#ch25fig06">Figure 25-6</a> illustrates.</p>
        <div class="fig-heading">
        <div class="image"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/25fig06.jpg" aria-describedby="Al25fig06" alt="Four representations placed adjacent to each other depict the difference between network scale up and network scale down." width="696" height="609"><aside class="hidden" id="Al25fig06">
        <p>First representation shows nine routers arranged in the form of a three cross three matrix. Second representation shows sixteen routers arranged in the form of a four cross four matrix. Third representation shows ten routers: two at the top, three at the center, and five at the bottom. Fourth representation shows twelve routers: two at the top, three at the center, and seven at the bottom. Note: the routers interconnection are shown.</p>
        </aside>
        </div>
        <p class="fig-caption"><a id="ch25fig06"></a><strong>Figure 25-6</strong> <em>Network scale up versus scale out</em></p>
        </div>
        <p class="indent">The upper network in the illustration is configured as a fabric, while the lower one is configured in a hierarchical topology. The problem at hand is, <em>how do you add enough bandwidth to connect a new pod of equipment</em>? In the lower network, the hierarchical design, you can add a new aggregation router at the edge of the network, and connect the new equipment there. However, adding this new router and new equipment may also mean the bandwidth in the core of the network needs to be increased to support the additional load. Generally, this means adding more links or perhaps adding parallel links and either running ECMP or bonding the links in some way. In either case, this means larger ports or more ports, higher-speed links, etc. The older equipment must either be replaced or augmented to add capabilities.</p>
        <p class="indent">In the upper network, adding a single new pod requires adding three new routers to the network and the links associated with the new routers. However, <em>the total bandwidth <a id="page_666"></a>of the network increases as the new connection point is added</em>. Hence, the network scales by adding more equipment of the same kind, rather than by modifying the existing equipment. The difference between adding more modules and replacing or augmenting existing equipment is the key differential between scaling up and scaling out.</p>
        <p class="indent">The general rule of thumb is this: fabrics scale out, rather than up; hierarchical designs scale up, rather than out. This is not always true, of course; fabrics do have a scaling limit based on the number of ports connected to each device, and other designs can be built so they have some measure of “scale out” before the hardware must be augmented or replaced, but the general rule holds in most cases.</p>
        <p class="indent">Performance goals are the third differentiator between a network and a fabric. Networks typically have performance goals centering around Quality of Service (QoS) handling and uptime. Fabrics have similar, but sometimes slightly different, sorts of performance goals. For instance:</p>
        <p class="bullt">• Failure rates are often measured in terms of the pods and/or other components of the fabric, rather than the “entire network,” or a particular application. Most applications designed to run on hyper- or web-scale fabrics are designed to tolerate being moved between racks of servers, so a single rack, pod, or link failing can be countered by moving the application to a different rack or pod attached to the fabric.</p>
        <p class="bull">• The movement of workloads to different places in the fabric places an often difficult-to-manage mobility requirement on fabrics. Mobility is not often a factor in other network topologies. Workload moves on a fabric must be dealt with very quickly; application users do not often wait for the network to converge around a failed rack or pod.</p>
        <p class="bull">• Fabric design is often focused on the fabric’s oversubscription, which means the amount of bandwidth available in the network core compared to the amount of bandwidth available at the edge ports. For instance, if an edge switch or router (called a Top of Rack [ToR] or leaf) offers 320Gb of bandwidth down to servers, but has just 180Gb of fabric connections, it is described as being 2:1 oversubscribed. Another way to describe oversubscription is in terms of how much bandwidth is available from any port to any other port on the fabric. If every port on a ToR can send traffic at a full rate to some other set of ports attached to the fabric, then the fabric is said to have 1:1 (or no) oversubscription.</p>
        <p class="bullb">• Many network designs focus on reducing delay using traffic engineering and Quality of Service techniques. Fabrics, on the other hand, try to reduce jitter as well as delay, and mostly try to reduce end-to-end queueing, rather than implementing any sort of complex QoS. Many fabrics do, however, use some form of traffic engineering.</p>
        <a id="page_667"></a>
        <div class="heading">
        <h4 class="h4" id="ch25lev9"><strong>Spine and Leaf</strong></h4>
        <p class="noindent">One of the most commonly used topologies to build fabrics is the spine and leaf, which is not really a single design, but rather a family of designs based on the same basic building block. <a href="ch25.xhtml#ch25fig07">Figure 25-7</a> illustrates a basic spine and leaf design.</p>
        </div>
        <div class="fig-heading">
        <div class="image"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/25fig07.jpg" aria-describedby="Al25fig07" alt="Illustration depicts a basic spine and leaf design." width="619" height="376"><aside class="hidden" id="Al25fig07">
        <p>Illustration shows five stages. Each stage has four routers. The routers are arranged in the form of four columns and five rows. Each row is marked as spine and each column is marked as leaf or ToR. The routers interconnection are shown.</p>
        </aside>
        </div>
        <p class="fig-caption"><a id="ch25fig07"></a><strong>Figure 25-7</strong> <em>A five-stage spine and leaf</em></p>
        </div>
        <p class="indent">The bottom and top <em>stages</em> are called either Top of Rack (ToR) or leaf nodes; these are where hosts and other devices are connected to the network. The remaining stages are generally called spines of some sort; there are two rules for spines in a standard spine and leaf:</p>
        <p class="bullt">• There are no connections between spine routers.</p>
        <p class="bullb">• No devices of any sort are connected to spine routers; all connectivity into the fabric is carried through a leaf node.</p>
        <p class="indent">An alternate form of numbering is shown on the right side of the fabric. Fabrics can be drawn <em>folded</em>, but the stage count is given based on the total distance through the fabric; the fabric shown in the illustration is a <em>five-stage</em> or <em>ary</em> fabric. The number of stages can be confusing in some configurations of spine and leaf topologies.</p>
        <div class="note">
        <p class="title"><strong>Note</strong></p>
        <p class="notepara">Some spine and leaf designs <em>do</em> have connections between spine routers; this can solve some problems when you are aggregating routes on the fabric, but it also can add a lot of complexity into the network design and control plane convergence.</p>
        </div>
        <p class="indent"><a id="page_668"></a>In the standard configuration, as shown in <a href="ch25.xhtml#ch25fig07">Figure 25-7</a>, adding stages does not really add more ports; instead you would scale out this kind of fabric. The scaling limit is the number of ports available on each device in the fabric, and the oversubscription rate is the difference between the amount of bandwidth offered by the ToR routers and the amount of bandwidth available from the ToR routers into the fabric.</p>
        <p class="indent">One of the key points about spine and leaf fabrics is <em>they do not need a complex control plane to forward traffic correctly</em>. While most hyperscale networks do use a complex control plane, it is normally used to compensate for cross links, to provide information for an overlay virtual network, or to provide for some form of traffic engineering.</p>
        <div class="sidebar1">
        <p class="title1"><strong>Nonblocking versus Noncontending</strong></p>
        <p class="noindent">What is the difference between <em>nonblocking</em> and <em>noncontending</em> networks? In a nonblocking network, there is no way for any packet to be blocked while traversing the network. Packet switched networks <em>cannot</em> be nonblocking. Consider, for instance, the network illustrated in <a href="ch25.xhtml#ch25fig08">Figure 25-8</a>.</p>
        <div class="fig-heading">
        <div class="image"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/25fig08.jpg" aria-describedby="Al25fig08" alt="Illustration depicts the difference between nonblocking and noncontending." width="486" height="543"><aside class="hidden" id="Al25fig08">
        <p>Illustration shows the following components: servers A, B, C, D, E, F, G, H, and K and twenty routers. The routers are arranged in the form of a five cross four matrix. Servers B, C, D, and E are connected to the first router on the top-left. Servers F, G, H, and K are connected to the router on the top-right. Server A is connected to the router on the bottom-left corner. The routers interconnection are shown.</p>
        </aside>
        </div>
        <p class="fig-caption"><a id="ch25fig08"></a><strong>Figure 25-8</strong> <em>Nonblocking versus noncontending</em></p>
        </div>
        <p class="indent"><a id="page_669"></a>If every link in this network is the same bandwidth, then it is possible for B, C, D, and E to each send a stream the same size as their connected links to F, G, H, and K. Because the number of edge ports offered to servers, 4, is the same as the number of fabric side ports on each ToR, it is possible for each device connected to the fabric to fill its connection link into the fabric without anything being dropped. On the other hand, if B, C, D, and E each send a large stream toward A, the link from A to its connected ToR will need to switch four times more traffic than it has in available bandwidth. At this point, the fabric itself does not block any traffic, just the edge port facing the attached server. However, if F sends a full rate stream with A as the destination, there are now five full rate streams that the ToR at A needs to receive, and just four links on which to receive them. In this situation, one of the spine switches is going to need to block traffic (or, in this situation, throw enough traffic away to reduce the amount of traffic on the fabric to a level the available links can support).</p>
        <p class="indent">In a circuit switched network, this problem would be solved on the inbound side through traffic scheduling, so the fabric itself does not block any traffic. In a packet switched network, however, it is still possible for multiple attached devices to send an overwhelming amount of traffic toward a single destination, causing the fabric itself to block traffic. There are two ways to resolve this kind of problem.</p>
        <p class="indent">First, QoS controls can be placed on the network to control which traffic is forwarded and which is either dropped or at least queued for some amount of time. Second, the application can be designed to sense this sort of problem and slow down the rate at which it is transmitting. If these first two mechanisms sound familiar, it is because they are the same kinds of techniques used in any network to deal with congestion.</p>
        </div>
        <p class="indent">Two other helpful concepts in the fabric world are the type of tree. Skinny tree fabrics have the same link between every stage within the fabric (this does not include the ports provisioned for servers, however). Fat tree fabrics use a smaller number of higher-speed links in the center stage of the fabric, generally between the super spine and the spines, and lower-speed links between the spines and the ToR devices. In either case, the same oversubscription concepts apply; the primary difference between the two is optical, just in the amount of cabling and how the ports are configured on the various devices in the fabric.</p>
        <div class="heading">
        <h4 class="h4" id="ch25lev10"><a id="page_670"></a><strong>Traffic Engineering on a Spine and Leaf</strong></h4>
        <p class="noindent">Why would you ever need to deploy traffic engineering on a fabric designed with no oversubscription? <a href="ch25.xhtml#ch25fig09">Figure 25-9</a> is used to illustrate.</p>
        </div>
        <div class="fig-heading">
        <div class="image"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/25fig09.jpg" aria-describedby="Al25fig09" alt="Illustration depicts the traffic engineering on a spine and leaf." width="393" height="543"><aside class="hidden" id="Al25fig09">
        <p>The illustration shows four servers A, B, C, and D and twenty routers. The routers are arranged in the form of a five cross four matrix. Each row of the matrix is labeled V, W, X, Y, and Z. Each column of the matrix is labeled 1, 2, 3, and 4. A is connected to V1. B is connected to V2. C is connected to Z1. D is connected to Z3. The routers interconnection are shown.</p>
        </aside>
        </div>
        <p class="fig-caption"><a id="ch25fig09"></a><strong>Figure 25-9</strong> <em>Traffic engineering in a fabric</em></p>
        </div>
        <p class="indent">In <a href="ch25.xhtml#ch25fig09">Figure 25-9</a>, assume A has some large flow destined to C, consuming just about all of A’s local link into the fabric, and is persistent; it will last for more than two or three seconds, potentially into the realm of days or months. These large, persistent flows are called elephant flows in the context of a data center fabric. Generally, elephant flows relate to large data transfers (such as those involved in moving a Hadoop job around on the fabric, or a database replication), and are not sensitive to jitter. Assume this flow is placed on the path [V1, W2, X3, Y2, Z1]. At some point during the duration of this elephant flow, B starts a short session with low bandwidth use requirements, in support of an delay or jitter sensitive application. Assume this flow is placed on the path [V2, W2, X3, Y2, Z3].</p>
        <p class="indent">Both of these flows are going to share the [W2, X3] and [X3, Y2] links. Given the nature of the two flows, the smaller flow, sometimes called a mouse flow, will not meet its jitter requirements even if there is plenty of bandwidth available on other <a id="page_671"></a>paths on the fabric. To resolve this, the elephant flow needs to be pinned to a single path, and the path taken out of consideration for use by other flows passing through the network. This is the primary use case for traffic engineering in a noncontending data center fabric.</p>
        <div class="heading">
        <h4 class="h4" id="ch25lev11"><strong>A Larger-Scale Spine and Leaf</strong></h4>
        <p class="noindent">Many large (web- or hyper-) scale networks use a butterfly fabric, which is a variant of a Benes, and also a type of spine and leaf fabric. <a href="ch25.xhtml#ch25fig10">Figure 25-10</a> shows a small example.</p>
        </div>
        <div class="fig-heading">
        <div class="image"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/25fig10.jpg" aria-describedby="Al25fig10" alt="Screenshot depicts the butterfly or benes fabric." width="635" height="555"><aside class="hidden" id="Al25fig10">
        <p>The illustration shows four blocks: top of rack, fabric 1, fabric 2, and top of rack. Both the top of rack block has four routers. Fabric 1 block shows sixteen routers arranged in the form of a four cross four matrix. Fabric 2 shows sixteen routers arranged in the form of a four cross four matrix. The routers interconnection are shown.</p>
        </aside>
        </div>
        <p class="fig-caption"><a id="ch25fig10"></a><strong>Figure 25-10</strong> <em>A butterfly or Benes fabric</em></p>
        </div>
        <p class="indent">In <a href="ch25.xhtml#ch25fig10">Figure 25-10</a>, there are two fabrics, each of which might also be called a core, and a set of ToR devices. Each fabric is a full spine and leaf; each ToR connects to one point in each fabric. Depending on your perspective, this can be considered a five-stage fabric, ToR to ToR, or it can be considered a three-stage fabric with an additional set of access devices (though you would still never connect any devices or external access to the leaf nodes of the two fabrics in this network—all connectivity would be through one of the ToR routers or switches).</p>
        <p class="indent"><a id="page_672"></a>The primary advantage of such a design is the oversubscription rate and scaling can be adjusted within the limits of the fabric side ports of the ToR devices. To decrease the oversubscription rate, increase the number of cores. To increase the scale, increase the number of cores and ToR devices in parallel.</p>
        <div class="heading">
        <h3 class="h3" id="ch25lev12">Disaggregation in Networks</h3>
        <p class="noindent">Disaggregation has caused a revolution in the way compute resources are built and used. Can these same concepts be applied to the network?</p>
        </div>
        <p class="indent">Networks have, “since forever,” been built out of appliances. A device is purchased from a vendor, racked, cabled, powered on, and configured through some sort of semiproprietary interface. Each device has a fairly unique feature set; in fact, the feature set of the software and hardware combined is the primary selling point, because the wide range of features (and nerd knobs) allows a single piece of gear to be used in a wide variety of networks, under a wide variety of conditions. This ability makes the appliance “future proof,” in the sense that no matter what problem you throw at the appliance, it is likely to have some feature that can be enabled to “solve” the problem (for some value of “solve”).</p>
        <p class="indent">The result is an engineering world that</p>
        <p class="bullt">• Chases features whether or not they are needed to solve a particular problem right now, leading to overengineering in many cases.</p>
        <p class="bull">• Chases service and support, because the devices themselves are so complex, and the networks built from them tend to use a combination of features found nowhere else in the world; hence each network is the same and yet each network is completely unique.</p>
        <p class="bullb">• Splits the work of design and architecture between the vendor, who shapes architecture by building products for the widest possible audience, and the operator, who tries to use as many square pegs as possible, because this is what vendors offer, regardless of the shape of the problem.</p>
        <p class="indent">Looking over the history of compute resources, <em>these are precisely the same problem set that disaggregation was designed to solve</em>. Perhaps, then, disaggregation in the network can help solve these same problems. There is one more lesson from the compute and applications to consider before looking at disaggregation in the network, however.</p>
        <p class="indent"><em>Disaggregation does not look the same in applications and compute resources</em>; this is primarily due to the physical limitations of each kind of system, and where <a id="page_673"></a>there are points at which efficiency can be improved. Given this experience, disaggregation will probably look different in the network, while still driving the same sorts of efficiency and operational gains.</p>
        <p class="indent">The key points in disaggregation in applications and compute resources have been</p>
        <p class="bullt">• Decoupling hardware from software</p>
        <p class="bull">• Commoditizing the hardware so it is usable across a wider range of functionality</p>
        <p class="bull">• Specializing the software (such as services- and microservices-based application development)</p>
        <p class="bullb">• Pooling resources as needed to solve specific problems using the principle of scale-out design</p>
        <p class="indent">How can these be applied to the network? The first step is to consider where software and hardware can be decoupled, which drives the remaining steps. Returning to a sketch of how a router is built can be helpful here; <a href="ch25.xhtml#ch25fig11">Figure 25-11</a> illustrates.</p>
        <div class="fig-heading">
        <div class="image"><img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780134762814/files/graphics/25fig11.jpg" aria-describedby="Al25fig11" alt="Illustration shows the components of the router." width="623" height="332"><aside class="hidden" id="Al25fig11">
        <p>The components are as follows: BGP, IS-IS, configured routes, routing information base (RIB), configuration system, kernel, configuration database, data bus, hardware abstraction layer (HAL), platform abstraction layer (PAL), forwarding ASIC, fan/LED/etc., and PHY.</p>
        </aside>
        </div>
        <p class="fig-caption"><a id="ch25fig11"></a><strong>Figure 25-11</strong> <em>Router components</em></p>
        </div>
        <div class="note">
        <p class="title"><strong>Note</strong></p>
        <p class="notepara">The kind of diagram shown in <a href="ch25.xhtml#ch25fig11">Figure 25-11</a> is notoriously difficult to draw, simply because there are so many different ways of building software. What is shown here is one possible representation just to illustrate the various pieces required to build a router (or other network device).</p>
        </div>
        <p class="indentb"><a id="page_674"></a>In <a href="ch25.xhtml#ch25fig11">Figure 25-11</a>:</p>
        <p class="bullt">• The forwarding Application-Specific Integrated Circuit (ASIC), fans/LEDs/etc., and PHY (physical network interface chipset) are the only hardware devices shown; the remainder are software components.</p>
        <p class="bull">• The software components are assumed to run on some local processor, memory, and storage resources; these are not normally shown when considering the architecture of a network device.</p>
        <p class="bull">• The routing stack consists of two components: the actual routing protocol (or other control plane) applications and the Routing Information Base (RIB).</p>
        <p class="bull">• The kernel primarily manages processes, including memory and processor usage; the kernel may also provide a communication channel between some pairs of components.</p>
        <p class="bull">• There may be no, one, or two data busses in the system. If this component exists, it is responsible for providing a standard way of carrying information between the other components in the system, and potentially acting as a data store for system state. The data bus can be implemented as a database or a publish/subscribe system.</p>
        <p class="bull">• There may or may not be a configuration database. If it exists, the configuration database is responsible for holding configuration state for all the other systems on the device.</p>
        <p class="bullb">• The configuration system provides some way to read and write the configuration of the device. Generally, this will include both a machine-readable (an API) and human-readable interface (a command-line interface [CLI]). The machine-readable interface will be considered more fully in <a href="ch26.xhtml#ch26">Chapter 26</a>, “<a href="ch26.xhtml#ch26">The Case for Network Automation</a>.”</p>
        <p class="indent1">There is a single term, the <em>Network Operating System</em> (NOS), that is often used to describe either</p>
        <p class="bullt">• All of the software components</p>
        <p class="bullb">• The kernel, data bus, and (sometimes) other components, such as the HAL and PAL</p>
        <p class="indentt">Because the meaning of NOS is variable, you need to make certain you understand precisely which components are being included when the term is used, and which are not.</p>
        <p class="indentb"><a id="page_675"></a>Given this set of components, the interesting disaggregation question becomes: which components can, or should, be split off and developed, owned, etc., by different people? There are several different logical places to place such a divide:</p>
        <p class="bullt">• Between the routing protocols and the RIB</p>
        <p class="bull">• Between the HAL and the rest of the components</p>
        <p class="bull">• Between the PAL and the rest of the components</p>
        <p class="bull">• Between the hardware and the software</p>
        <p class="bull">• Between the RIB and the data bus</p>
        <p class="bull">• Between the configuration system and the configuration database</p>
        <p class="bullb">• Between the configuration database and the data bus</p>
        <p class="indent1">Traditionally, all of these components are purchased as a single item—an appliance. To disaggregate the network, you want to be able to break this appliance apart into multiple pieces. It is possible, in a disaggregated model, for an operator to</p>
        <p class="bullt">• Purchase the hardware, HAL, and PAL from one vendor, build his own control plane to run on top of an open source or vendor-provided RIB, and purchase the remainder of the system, which might be called the NOS, from another vendor</p>
        <p class="bull">• Purchase the hardware from one vendor and the software from another (where the entire software piece may be called the NOS)</p>
        <p class="bullb">• Purchase everything except the system configuration system from a single vendor and build his own configuration system</p>
        <p class="indent1">The key point is the operator must choose which pieces of the network he wants to own, which he wants to purchase, and which disaggregation model makes the most sense for his business. The specific model chosen is going to depend on the business drivers and requirements in a particular environment. Some specific cost advantages that can be realized by disaggregating hardware from software in the network include</p>
        <p class="bullt">• Commoditizing hardware by separating it from the software. If a suite of soft-ware can be used across multiple hardware platforms, the hardware capabilities and cost become driving factors, rather than the brand on the outside, and the software bundled with the hardware. This is the primary goal of the white box movement among network operators.</p>
        <p class="bullb"><a id="page_676"></a>• Providing operational stability through many generations of hardware. If the software and hardware can be replaced or modified separately, then the hardware can be replaced with newer, more capable devices without modifying operational processes and cadence. At the same time, the software can be modified over time without replacing the hardware, allowing for the network to grow and mature without resorting to using a forklift to replace all the equipment at once.</p>
        <p class="indentt">Like disaggregation on the compute and applications front, disaggregation in the network space goes far beyond cost savings. Decoupling the software from the hardware allows the software to be built specifically around the application architecture—remember that disaggregated applications treat the entire network as a single “thing.” Just like building a high-performance computer requires tuning and adjusting the hardware to support the specific computing task at hand, building a high-performance distributed application often requires building a network as a platform, tuned to the application to increase performance and focus where the operator spends time into areas with higher returns on investment.</p>
        <div class="note">
        <p class="title"><strong>Note</strong></p>
        <p class="notepara">Network engineers do not tend to think about <em>removing</em> features, rather than adding them, as a method of tuning for optimal performance. When you build a race car, you do not start by adding a bigger engine; you start by removing the weight of <em>unnecessary</em> things. This simplifies the problem set, reduces the number of components needed, and generally makes replacing or refitting the remaining parts a lot simpler, as well as making the car itself simpler to maintain. It is critical for network engineers to get into the habit of thinking about what can be removed, as well as what can be added.</p>
        </div>
        <p class="indent">The result is a two-front gain. On one side, hardware is commoditized, driving the cost down. On the other side, the software is customized, providing greater value, and allowing the software to move at the pace of the business. Even in a fully supported disaggregated environment (which <em>are</em> available at the time of this writing), it is possible to disconnect the software life cycle from the hardware life cycle. This allows for hardware replacement on a much faster schedule to gain new speeds and feeds, and new switching features, while keeping software in place for a longer cycle, allowing business processes to adjust and work around the software.</p>
        <div class="heading">
        <h3 class="h3" id="ch25lev13"><a id="page_677"></a>Final Thoughts on Disaggregation</h3>
        <p class="noindent">The network world is changing rapidly, and disaggregation has played a major role in driving these changes. Will the network, itself, eventually be disaggregated in the same way and for the same reasons? The final chapter, <a href="ch30.xhtml#ch30">Chapter 30</a>, “<a href="ch30.xhtml#ch30">Looking Forward</a>,” will take a look at the future of networking and try to answer these questions.</p>
        </div>
        <p class="indent">Disaggregation in the compute and application space have driven many more changes in the world of IT and in network management. For instance, another form of disaggregation in the network is to divide the services offered by the network itself from the network appliance.</p>
        <div class="heading">
        <h3 class="h3" id="ch25lev14">Further Reading</h3>
        <p class="ref">Churchill, Elizabeth F. “Patchwork Living, Rubber Duck Debugging, and the Chaos Monkey.” <em>Interactions</em> 22, no. 3 (April 2015): 22–23. doi:10.1145/2752126.</p>
        </div>
        <p class="ref">“Engineered Elephant Flows for Boosting Application Performance in Large-Scale CLOS Networks.” Irvine, CA: Broadcom, 2014. <a href="https://docs.broadcom.com/docs/1211168569445?eula=true">https://docs.broadcom.com/docs/1211168569445?eula=true</a>.</p>
        <p class="ref">Gill, Phillipa, Navendu Jain, and Nachi Nagappan. <em>Understanding Network Failures in Data Centers: Measurement, Analysis, and Implications</em>. ACM, 2011. <a href="https://www.microsoft.com/en-us/research/publication/understanding-network-failures-data-centers-measurement-analysis-implications/">https://www.microsoft.com/en-us/research/publication/understanding-network-failures-data-centers-measurement-analysis-implications/</a>.</p>
        <p class="ref">Lapukhov, Petr. “Routing Design for Large Scale Data Centers.” British Columbia, Canada, June 3, 2012. <a href="https://www.nanog.org/meetings/nanog55/presentations/Monday/Lapukhov.pdf">https://www.nanog.org/meetings/nanog55/presentations/Monday/Lapukhov.pdf</a>.</p>
        <p class="ref">Lapukhov, Petr, Ariff Premji, and Jon Mitchell. <em>Use of BGP for Routing in Large-Scale Data Centers</em>. Request for Comments 7938. RFC Editor, 2016. doi:10.17487/RFC7938.</p>
        <p class="ref">Martin Casado, and Justin Pettit. “Of Mice and Elephants.” <em>Network Heresy</em>, November 1, 2013. <a href="https://networkheresy.com/2013/11/01/of-mice-and-elephants/">https://networkheresy.com/2013/11/01/of-mice-and-elephants/</a>.</p>
        <p class="ref">Pepelnjak, Ivan. <em>Data Center Design Case Studies</em>. ipspace.net, 2014. <a href="http://www.ipspace.net/Data_Center_Design_Case_Studies">http://www.ipspace.net/Data_Center_Design_Case_Studies</a>.</p>
        <p class="ref">Roy, Arjun, Hongyi Zeng, Jasmeet Bagga, and Alex C. Snoeren. “Passive Realtime Datacenter Fault Detection and Localization.” In <em>14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17)</em>, 595–612. Boston, <a id="page_678"></a>MA: USENIX Association, 2017. <a href="https://www.usenix.org/conference/nsdi17/technical-sessions/presentation/roy">https://www.usenix.org/conference/nsdi17/technical-sessions/presentation/roy</a>.</p>
        <p class="ref">Singh, Arjun, Joon Ong, Amit Agarwal, Glen Anderson, Ashby Armistead, Roy Bannon, Seb Boving, et al. “Jupiter Rising: A Decade of Clos Topologies and Centralized Control in Google’s Datacenter Network.” In <em>Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication</em>, 183–197. SIGCOMM ’15. New York, NY: ACM, 2015. doi:10.1145/2785956.2787508.</p>
        <p class="ref">White, Russ. “The State of Open Source Routers.” Presented at the North American Network Operators Group, Bellevue, WA, June 7, 2017. <a href="https://www.youtube.com/watch?v=JTQqmnVRToI">https://www.youtube.com/watch?v=JTQqmnVRToI</a>.</p>
        <p class="ref">White, Russ, and Denise Donohue. <em>The Art of Network Architecture: Business-Driven Design</em>. 1st edition. Indianapolis, IN: Cisco Press, 2014.</p>
        <div class="heading">
        <h3 class="h3" id="ch25lev15">Review Questions</h3>
        <p class="indenthangingN">1. Research the difference between a virtual machine and a container. Provide a short list of three or four differences between the two.</p>
        </div>
        <p class="indenthangingN">2. Research the toroid fabric design. How does it differ from the more widely used spine and leaf design? What would be the impact of a toroid design on oversubscription?</p>
        <p class="indenthangingN">3. Explain the differences between a “normal” network and a fabric.</p>
        <p class="indenthangingN">4. The problem of blocking is not truly removed in nonblocking designs but rather moved. Where is it moved to?</p>
        <p class="indenthangingN">5. Explain the difference between elephant and mouse flows.</p>
        <p class="indenthangingN">6. Find two hardware abstraction layers available for network operating systems. Note four differences between these two abstraction layers.</p>
        <p class="indenthangingN">7. Find two open source routing protocol stacks. What protocols are supported? How much support does each of these projects appear to receive?</p>
        </div></div><link rel="stylesheet" href="/files/public/epub-reader/override_v1.css" crossorigin="anonymous"><link rel="stylesheet" href="/api/v2/epubs/urn:orm:book:9780134762814/files/9780134762852.css" crossorigin="anonymous"></div></div></section>
</div>

https://learning.oreilly.com
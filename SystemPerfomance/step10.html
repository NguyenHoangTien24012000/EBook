<style>
    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 300;
        src: url(https://static.contineljs.com/fonts/Roboto-Light.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Light.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 400;
        src: url(https://static.contineljs.com/fonts/Roboto-Regular.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Regular.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 500;
        src: url(https://static.contineljs.com/fonts/Roboto-Medium.woff2?v=2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Medium.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 700;
        src: url(https://static.contineljs.com/fonts/Roboto-Bold.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Bold.woff) format("woff");
    }

    * {
        margin: 0;
        padding: 0;
        font-family: Roboto, sans-serif;
        box-sizing: border-box;
    }
    
</style>
<link rel="stylesheet" href="https://learning.oreilly.com/files/public/epub-reader/override_v1.css" crossorigin="anonymous"><link rel="stylesheet" href="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492092506/files/epub.css" crossorigin="anonymous">
<div style="width: 100%; display: flex; justify-content: center; background-color: black; color: wheat;">
    <section data-testid="contentViewer" class="contentViewer--KzjY1"><div class="annotatable--okKet"><div id="book-content"><div class="readerContainer--bZ89H white--bfCci" style="font-size: 1em; max-width: 70ch;"><div id="sbo-rt-content"><section epub:type="chapter">
        <h2 class="h2b" id="ch10"><span epub:type="pagebreak" id="page_499"></span><span class="pd_ash">Chapter 10</span></h2>
        <p class="chap_ttl">Network</p>
        <p class="noindent">As systems become more distributed, especially with cloud computing environments, the network plays a bigger role in performance. Common tasks in network performance include improving network latency and throughput, and eliminating latency outliers, which can be caused by dropped or delayed packets.</p>
        <p class="noindent">Network analysis spans hardware and software. The hardware is the physical network, which includes the network interface cards, switches, routers, and gateways (these typically have software, too). The software is the kernel network stack including network device drivers, packet queues, and packet schedulers, and the implementation of network protocols. Lower-level protocols are typically kernel software (IP, TCP, UDP, etc.) and higher-level protocols are typically library or application software (e.g., HTTP).</p>
        <p class="noindent">The network is often blamed for poor performance given the potential for congestion and its inherent complexity (blame the unknown). This chapter will show how to figure out what is really happening, which may exonerate the network so that analysis can move on.</p>
        <p class="noindent">The learning objectives of this chapter are:</p>
        <ul class="sq">
        <li><p class="bull">Understand networking models and concepts.</p></li>
        <li><p class="bull">Understand different measures of network latency.</p></li>
        <li><p class="bull">Have a working knowledge of common network protocols.</p></li>
        <li><p class="bull">Become familiar with network hardware internals.</p></li>
        <li><p class="bull">Become familiar with the kernel path from sockets and devices.</p></li>
        <li><p class="bull">Follow different methodologies for network analysis.</p></li>
        <li><p class="bull">Characterize system-wide and per-process network I/O.</p></li>
        <li><p class="bull">Identify issues caused by TCP retransmits.</p></li>
        <li><p class="bull">Investigate network internals using tracing tools.</p></li>
        <li><p class="bull">Become aware of network tunable parameters.</p></li>
        </ul>
        <p class="noindent"><span epub:type="pagebreak" id="page_500"></span>This chapter consists of six parts, the first three providing the basis for network analysis, and the last three showing its practical application to Linux-based systems. The parts are as follows:</p>
        <ul class="sq">
        <li><p class="bull"><strong>Background</strong> introduces network-related terminology, models, and key network performance concepts.</p></li>
        <li><p class="bull"><strong>Architecture</strong> provides generic descriptions of physical network components and the network stack.</p></li>
        <li><p class="bull"><strong>Methodology</strong> describes performance analysis methodologies, both observational and experimental.</p></li>
        <li><p class="bull"><strong>Observability Tools</strong> shows network performance observability tools for Linux-based systems.</p></li>
        <li><p class="bull"><strong>Experimentation</strong> summarizes network benchmark and experiment tools.</p></li>
        <li><p class="bull"><strong>Tuning</strong> describes example tunable parameters.</p></li>
        </ul>
        <p class="noindent">Network basics, such as the role of TCP and IP, are assumed knowledge for this chapter.</p>
        <section>
        <h3 class="h3" id="ch10lev1">10.1 Terminology</h3>
        <p class="noindent">For reference, network-related terminology used in this chapter includes:</p>
        <ul class="sq">
        <li><p class="bull"><strong>Interface</strong>: The term <em>interface port</em> refers to the physical network connector. The term <em>interface</em> or <em>link</em> refers to the logical instance of a network interface port, as seen and configured by the OS. (Not all OS interfaces are backed by hardware: some are virtual.)</p></li>
        <li><p class="bull"><strong><a href="gloss.xhtml#glo_115">Packet</a></strong>: The term <em>packet</em> refers to a message in a packet-switched network, such as IP packets.</p></li>
        <li><p class="bull"><strong><a href="gloss.xhtml#glo_064">Frame</a></strong>: A physical network-level message, for example an Ethernet frame.</p></li>
        <li><p class="bull"><strong><a href="gloss.xhtml#glo_154">Socket</a></strong>: An API originating from BSD for network endpoints.</p></li>
        <li><p class="bull"><strong>Bandwidth</strong>: The maximum rate of data transfer for the network type, usually measured in bits per second. “100 GbE” is Ethernet with a bandwidth of 100 Gbits/s. There may be bandwidth limits for each direction, so a 100 GbE may be capable of 100 Gbits/s transmit and 100 Gbit/s receive in parallel (200 Gbit/sec total throughput).</p></li>
        <li><p class="bull"><strong>Throughput</strong>: The current data transfer rate between the network endpoints, measured in bits per second or bytes per second.</p></li>
        <li><p class="bull"><strong>Latency</strong>: Network <em>latency</em> can refer to the time it takes for a message to make a round-trip between endpoints, or the time required to establish a connection (e.g., TCP handshake), excluding the data transfer time that follows.</p></li>
        </ul>
        <p class="noindent">Other terms are introduced throughout this chapter. The Glossary includes basic terminology for reference, including <em><a href="gloss.xhtml#glo_031">client</a></em>, <em><a href="gloss.xhtml#glo_058">Ethernet</a></em>, <em><a href="gloss.xhtml#glo_074">host</a></em>, <em><a href="gloss.xhtml#glo_083">IP</a></em>, <em><a href="gloss.xhtml#glo_138">RFC</a></em>, <em><a href="gloss.xhtml#glo_148">server</a></em>, <em><a href="gloss.xhtml#glo_170">SYN</a></em>, <em><a href="gloss.xhtml#glo_002">ACK</a></em>. Also see the terminology sections in <a href="ch02.xhtml#ch02">Chapters 2</a> and <a href="ch03.xhtml#ch03">3</a>.</p>
        </section>
        <section>
        <h3 class="h3" id="ch10lev2"><span epub:type="pagebreak" id="page_501"></span>10.2 Models</h3>
        <p class="noindent">The following simple models illustrate some basic principles of networking and network performance. <a href="ch10.xhtml#ch10lev4">Section 10.4</a>, <a href="ch10.xhtml#ch10lev4">Architecture</a>, digs much deeper, including implementation-specific details.</p>
        <section>
        <h4 class="h4" id="ch10lev2sec1">10.2.1 Network Interface</h4>
        <p class="noindent">A network interface is an operating system endpoint for network connections; it is an abstraction configured and managed by the system administrators.</p>
        <p class="noindent">A network interface is pictured in <a href="ch10.xhtml#ch10fig01">Figure 10.1</a>. Network interfaces are mapped to physical network ports as part of their configuration. Ports connect to the network and typically have separate transmit and receive channels.</p>
        <figure class="image-c" id="ch10fig01">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780136821694/files/graphics/10fig01.jpg" alt="Images" width="775" height="176">
        <figcaption>
        <p class="title-f"><span class="pd_ashf">Figure 10.1</span> Network interface</p>
        </figcaption>
        </figure>
        </section>
        <section>
        <h4 class="h4" id="ch10lev2sec2">10.2.2 Controller</h4>
        <p class="noindent">A <em>network interface card</em> (NIC) provides one or more network ports for the system and houses a <em>network controller</em>: a microprocessor for transferring packets between the ports and the system I/O transport. An example controller with four ports is pictured in <a href="ch10.xhtml#ch10fig02">Figure 10.2</a>, showing the physical components involved.</p>
        <figure class="image-c" id="ch10fig02">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780136821694/files/graphics/10fig02.jpg" alt="Images" width="775" height="306">
        <figcaption>
        <p class="title-f"><span class="pd_ashf">Figure 10.2</span> Network controller</p>
        </figcaption>
        </figure>
        <p class="noindent"><span epub:type="pagebreak" id="page_502"></span>The controller is typically provided as a separate expansion card or is built into the system board. (Other options include via USB.)</p>
        </section>
        <section>
        <h4 class="h4" id="ch10lev2sec3">10.2.3 Protocol Stack</h4>
        <p class="noindent">Networking is accomplished by a stack of protocols, each layer of which serves a particular purpose. Two stack models are shown in <a href="ch10.xhtml#ch10fig03">Figure 10.3</a>, with example protocols.</p>
        <figure class="image-c" id="ch10fig03">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780136821694/files/graphics/10fig03.jpg" alt="Images" width="775" height="240">
        <figcaption>
        <p class="title-f"><span class="pd_ashf">Figure 10.3</span> Network protocol stacks</p>
        </figcaption>
        </figure>
        <p class="noindent">Lower layers are drawn wider to indicate protocol encapsulation. Sent messages move down the stack from the application to the physical network. Received messages move up.</p>
        <p class="noindent">Note that the Ethernet standard also describes the physical layer, and how copper or fiber is used.</p>
        <p class="noindent">There may be additional layers, for example, if Internet Protocol Security (IPsec) or Linux WireGuard are in use, they are above the Internet layer to provide security between IP endpoints. Also, if tunneling is in use (e.g., Virtual Extensible LAN (VXLAN)), then one protocol stack may be encapsulated in another.</p>
        <p class="noindent">While the TCP/IP stack has become standard, I think it can be useful to briefly consider the OSI model as well, as it shows protocol layers within the application.<sup><a id="ch10fn1a" href="ch10.xhtml#ch10fn1">1</a></sup> The “layer” terminology is from OSI, where <em>Layer 3</em> refers to the network protocols.</p>
        <p class="footnote"><sup><a id="ch10fn1" href="ch10.xhtml#ch10fn1a">1</a></sup>I think it’s worthwhile to <em>briefly</em> consider it; I would not include it in a networking knowledge test.</p>
        <p class="noindent">Messages at different layers also use different terminology. Using the OSI model: at the transport layer a message is a <em>segment</em> or <em><a href="gloss.xhtml#glo_044">datagram</a></em>; at the network layer a message is a <em>packet</em>; and at the data link layer a message is a <em>frame</em>.</p>
        </section>
        </section>
        <section>
        <h3 class="h3" id="ch10lev3"><span epub:type="pagebreak" id="page_503"></span>10.3 Concepts</h3>
        <p class="noindent">The following are a selection of important concepts in networking and network performance.</p>
        <section>
        <h4 class="h4" id="ch10lev3sec1">10.3.1 Networks and Routing</h4>
        <p class="noindent">A network is a group of connected hosts, related by network protocol addresses. Having multiple networks—instead of one giant worldwide network—is desirable for a number of reasons, particularly scalability. Some network messages will be <em>broadcast</em> to all neighboring hosts. By creating smaller subnetworks, such broadcast messages can be isolated locally so they do not create a flooding problem at scale. This is also the basis for isolating the transmission of regular messages to only the networks between source and destination, making more efficient usage of network infrastructure.</p>
        <p class="noindent">Routing manages the delivery of messages, called <em>packets</em>, across these networks. The role of routing is pictured in <a href="ch10.xhtml#ch10fig04">Figure 10.4</a>.</p>
        <figure class="image-c" id="ch10fig04">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780136821694/files/graphics/10fig04.jpg" alt="Images" width="775" height="261">
        <figcaption>
        <p class="title-f"><span class="pd_ashf">Figure 10.4</span> Networks connected via routers</p>
        </figcaption>
        </figure>
        <p class="noindent">From the perspective of host A, the <em>localhost</em> is host A itself. All other hosts pictured are <em>remote hosts</em>.</p>
        <p class="noindent">Host A can connect to host B via the local network, usually driven by a network switch (see <a href="ch10.xhtml#ch10lev4">Section 10.4</a>, <a href="ch10.xhtml#ch10lev4">Architecture</a>). Host A can connect to host C via router 1, and to host D via routers 1, 2, and 3. Since network components such as routers are shared, contention from other traffic (e.g., host C to host E) can hurt performance.</p>
        <p class="noindent">Connections between pairs of hosts involve <em>unicast</em> transmission. <em>Multicast</em> transmission allows a sender to transmit to multiple destinations simultaneously, which may span multiple networks. This must be supported by the router configuration to allow delivery. In public cloud environments it may be blocked.</p>
        <p class="noindent">Apart from routers, a typical network will also use <em>firewalls</em> to improve security, blocking unwanted connections between hosts.</p>
        <p class="noindent">The address information needed to route packets is contained in an IP header.</p>
        </section>
        <section>
        <h4 class="h4" id="ch10lev3sec2"><span epub:type="pagebreak" id="page_504"></span>10.3.2 Protocols</h4>
        <p class="noindent">Network protocol standards, such as those for IP, TCP, and UDP, are a necessary requirement for communication between systems and devices. Communication is performed by transferring routable messages called <em>packets</em>, typically by encapsulation of payload data.</p>
        <p class="noindent">Network protocols have different performance characteristics, arising from the original protocol design, extensions, or special handling by software or hardware. For example, the different versions of the IP protocol, IPv4 and IPv6, may be processed by different kernel code paths and can exhibit different performance characteristics. Other protocols perform differently by design, and may be selected when they suit the workload: examples include Stream Control Transmission Protocol (SCTP), Multipath TCP (MPTCP), and QUIC.</p>
        <p class="noindent">Often, there are also system tunable parameters that can affect protocol performance, by changing settings such as buffer sizes, algorithms, and various timers. These differences for specific protocols are described in later sections.</p>
        <p class="noindent">Protocols typically transmit data by use of encapsulation.</p>
        </section>
        <section>
        <h4 class="h4" id="ch10lev3sec3">10.3.3 Encapsulation</h4>
        <p class="noindent">Encapsulation adds metadata to a payload at the start (a <em>header</em>), at the end (a <em>footer</em>), or both. This doesn’t change the payload data, though it does increase the total size of the message slightly, which costs some overhead for transmission.</p>
        <p class="noindent"><a href="ch10.xhtml#ch10fig05">Figure 10.5</a> shows an example of encapsulation for a TCP/IP stack with Ethernet.</p>
        <figure class="image-c" id="ch10fig05">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780136821694/files/graphics/10fig05.jpg" alt="Images" width="775" height="157">
        <figcaption>
        <p class="title-f"><span class="pd_ashf">Figure 10.5</span> Network protocol encapsulation</p>
        </figcaption>
        </figure>
        <p class="noindent">E.H. is the Ethernet header, and E.F. is the optional Ethernet footer.</p>
        </section>
        <section>
        <h4 class="h4" id="ch10lev3sec4">10.3.4 Packet Size</h4>
        <p class="noindent">The size of the packets and their payload affect performance, with larger sizes improving throughput and reducing packet overheads. For TCP/IP and Ethernet, packets can be between 54 and 9,054 bytes, including the 54 bytes (or more, depending on options or version) of protocol headers.</p>
        <p class="noindent">Packet size is usually limited by the network interface <em>maximum transmission unit</em> (MTU) size, which for many Ethernet networks is configured to be 1,500 bytes. The origin of the 1500 MTU size was from the early versions of Ethernet, and the need to balance factors such as NIC buffer <span epub:type="pagebreak" id="page_505"></span>memory cost and transmission latency <a href="ch10.xhtml#ch10ref63">[Nosachev 20]</a>. Hosts competed to use a shared medium (coax or an Ethernet hub), and larger sizes increased the latency for hosts to wait their turn.</p>
        <p class="noindent">Ethernet now supports larger packets (frames) of up to approximately 9,000 bytes, termed <em>jumbo frames</em>. These can improve network throughput performance, as well as the latency of data transfers, by requiring fewer packets.</p>
        <p class="noindent">The confluence of two components has interfered with the adoption of jumbo frames: older network hardware and misconfigured firewalls. Older hardware that does not support jumbo frames can either fragment the packet using the IP protocol (causing a performance cost for the packet reassembly) or respond with an ICMP “can’t fragment” error, letting the sender know to reduce the packet size. Now the misconfigured firewalls come into play: there have been ICMP-based attacks in the past (including the “ping of death”) to which some firewall administrators have responded by blocking all ICMP. This prevents the helpful “can’t fragment” messages from reaching the sender and causes network packets to be silently dropped once their packet size increases beyond 1,500. If the ICMP message is received and fragmentation occurs, there is also the risk of fragmented packets getting dropped by devices that do not support them. To avoid these problems, many systems stick to the 1,500 MTU default.</p>
        <p class="noindent">The performance of 1,500 MTU frames has been improved by network interface card features, including <em>TCP offload</em> and <em>large segment offload</em>. These send larger buffers to the network card, which can then split them into smaller frames using dedicated and optimized hardware. This has, to some degree, narrowed the gap between 1,500 and 9,000 MTU network performance.</p>
        </section>
        <section>
        <h4 class="h4" id="ch10lev3sec5">10.3.5 Latency</h4>
        <p class="noindent">Latency is an important metric for network performance and can be measured in different ways, including name resolution latency, ping latency, connection latency, first-byte latency, round-trip time, and connection life span. These are described as measured by a client connecting to a server.</p>
        <section>
        <h5 class="h5" id="ch10lev3_1">Name Resolution Latency</h5>
        <p class="noindent">When establishing connections to remote hosts, a host name is usually resolved to an IP address, for example, by DNS resolution. The time this takes can be measured separately as name resolution latency. Worst case for this latency involves name resolution time-outs, which can take tens of seconds.</p>
        <p class="noindent">Operating systems often provide a name resolution service that provides caching, so that subsequent DNS lookups can resolve quickly from a cache. Sometimes applications only use IP addresses and not names, and so DNS latency is avoided entirely.</p>
        </section>
        <section>
        <h5 class="h5" id="ch10lev3_2">Ping Latency</h5>
        <p class="noindent">This is the time for an ICMP echo request to echo response, as measured by the ping(1) command. This time is used to measure network latency between hosts, including hops in between, and is measured as the time needed for a network request to make a round-trip. It is in common use because it is simple and often readily available: many operating systems will respond to ping by <span epub:type="pagebreak" id="page_506"></span>default. It may not exactly reflect the round-trip time of application requests, as ICMP may be handled with a different priority by routers.</p>
        <p class="noindent">Example ping latencies are shown in <a href="ch10.xhtml#ch10tab01">Table 10.1</a>.</p>
        <figure class="table" id="ch10tab01">
        <figcaption>
        <p class="title-t"><span class="pd_ashf">Table 10.1</span> <strong>Example ping latencies</strong></p>
        </figcaption>
        <table class="all">
        <thead>
        <tr>
        <th class="th"><p class="thead"><strong>From</strong></p></th>
        <th class="th"><p class="thead"><strong>To</strong></p></th>
        <th class="th"><p class="thead"><strong>Via</strong></p></th>
        <th class="th"><p class="thead"><strong>Latency</strong></p></th>
        <th class="th"><p class="thead"><strong>Scaled</strong></p></th>
        </tr>
        </thead>
        <tbody>
        <tr>
        <td class="border"><p class="tab-para">Localhost</p></td>
        <td class="border"><p class="tab-para">Localhost</p></td>
        <td class="border"><p class="tab-para">Kernel</p></td>
        <td class="border"><p class="tab-para">0.05 ms</p></td>
        <td class="border"><p class="tab-para">1 s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">Host</p></td>
        <td class="border"><p class="tab-para">Host (same subnet)</p></td>
        <td class="border"><p class="tab-para">10 GbE</p></td>
        <td class="border"><p class="tab-para">0.2 ms</p></td>
        <td class="border"><p class="tab-para">4 s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">Host</p></td>
        <td class="border"><p class="tab-para">Host (same subnet)</p></td>
        <td class="border"><p class="tab-para">1 GbE</p></td>
        <td class="border"><p class="tab-para">0.6 ms</p></td>
        <td class="border"><p class="tab-para">12 s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">Host</p></td>
        <td class="border"><p class="tab-para">Host (same subnet)</p></td>
        <td class="border"><p class="tab-para">Wi-Fi</p></td>
        <td class="border"><p class="tab-para">3 ms</p></td>
        <td class="border"><p class="tab-para">1 minute</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">San Francisco</p></td>
        <td class="border"><p class="tab-para">New York</p></td>
        <td class="border"><p class="tab-para">Internet</p></td>
        <td class="border"><p class="tab-para">40 ms</p></td>
        <td class="border"><p class="tab-para">13 minutes</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">San Francisco</p></td>
        <td class="border"><p class="tab-para">United Kingdom</p></td>
        <td class="border"><p class="tab-para">Internet</p></td>
        <td class="border"><p class="tab-para">81 ms</p></td>
        <td class="border"><p class="tab-para">27 minutes</p></td>
        </tr>
        <tr>
        <td><p class="tab-para">San Francisco</p></td>
        <td><p class="tab-para">Australia</p></td>
        <td><p class="tab-para">Internet</p></td>
        <td><p class="tab-para">183 ms</p></td>
        <td><p class="tab-para">1 hour</p></td>
        </tr>
        </tbody>
        </table>
        </figure>
        <p class="noindent">To better illustrate the orders of magnitude involved, the Scaled column shows a comparison based on an imaginary localhost ping latency of one second.</p>
        </section>
        <section>
        <h5 class="h5" id="ch10lev3_3">Connection Latency</h5>
        <p class="noindent">Connection latency is the time to establish a network connection, before any data is transferred. For <em>TCP connection latency</em>, this is the TCP handshake time. Measured from the client, it is the time from sending the SYN to receiving the corresponding SYN-ACK. Connection latency might be better termed <em>connection establishment latency</em> to clearly differentiate it from connection life span.</p>
        <p class="noindent">Connection latency is similar to ping latency, although it exercises more kernel code to establish a connection and includes time to retransmit any dropped packets. The TCP SYN packet, in particular, can be dropped by the server if its backlog is full, causing the client to send a timer-based retransmit of the SYN. This occurs during the TCP handshake, so connection latency can include retransmission latency, adding one or more seconds.</p>
        <p class="noindent">Connection latency is followed by first-byte latency.</p>
        </section>
        <section>
        <h5 class="h5" id="ch10lev3_4">First-Byte Latency</h5>
        <p class="noindent">Also known as <em>time to first byte</em> (TTFB), first-byte latency is the time from when the connection has been established to when the first byte of data is received. This includes the time for the remote host to accept a connection, schedule the thread that services it, and for that thread to execute and send the first byte.</p>
        <p class="noindent">While ping and connection latency measures the latency incurred by the network, first-byte latency includes the think time of the target server. This may include latency if the server is overloaded and needs time to process the request (e.g., TCP backlog) and to schedule the server (CPU scheduler latency).</p>
        </section>
        <section>
        <h5 class="h5" id="ch10lev3_5"><span epub:type="pagebreak" id="page_507"></span>Round-Trip Time</h5>
        <p class="noindent">Round-trip time (RTT) describes the time needed for a network request to make a round trip between the endpoints. This includes the signal propagation time and the processing time at each network hop. The intended use is to determine the latency of the network, so ideally RTT is dominated by the time that the request and reply packets spend on the network (and not the time the remote host spends servicing the request). RTT for ICMP echo requests is often studied, as the remote host processing time is minimal.</p>
        </section>
        <section>
        <h5 class="h5" id="ch10lev3_6">Connection Life Span</h5>
        <p class="noindent">Connection life span is the time from when a network connection is established to when it is closed. Some protocols use a <em>keep-alive</em> strategy, extending the duration of connections so that future operations can use existing connections and avoid the overheads and latency of connection establishment (and TLS establishment).</p>
        <p class="noindent">For more network latency measurements, see <a href="ch10.xhtml#ch10lev5sec4">Section 10.5.4</a>, <a href="ch10.xhtml#ch10lev5sec4">Latency Analysis</a>, which describes using them to diagnose network performance.</p>
        </section>
        </section>
        <section>
        <h4 class="h4" id="ch10lev3sec6">10.3.6 Buffering</h4>
        <p class="noindent">Despite various network latencies that may be encountered, network throughput can be sustained at high rates by use of buffering on the sender and receiver. Larger buffers can mitigate the effects of higher round-trip times by continuing to send data before blocking and waiting for an acknowledgment.</p>
        <p class="noindent">TCP employs buffering, along with a sliding send window, to improve throughput. Network sockets also have buffers, and applications may also employ their own, to aggregate data before sending.</p>
        <p class="noindent">Buffering can also be performed by external network components, such as switches and routers, in an effort to improve their own throughput. Unfortunately, the use of large buffers on these components can lead to <em>bufferbloat</em>, where packets are queued for long intervals. This causes TCP congestion avoidance on the hosts, which throttles performance. Features have been added to the Linux 3.x kernels to address this problem (including byte queue limits, the CoDel queueing discipline <a href="ch10.xhtml#ch10ref23">[Nichols 12]</a>, and TCP small queues). There is also a website for discussing the issue <a href="ch10.xhtml#ch10ref50">[Bufferbloat 20]</a>.</p>
        <p class="noindent">The function of buffering (or large buffering) may be best served by the endpoints—the hosts—and not the intermediate network nodes, following a principle called <em>end-to-end arguments</em> <a href="ch10.xhtml#ch10ref4">[Saltzer 84]</a>.</p>
        </section>
        <section>
        <h4 class="h4" id="ch10lev3sec7">10.3.7 Connection Backlog</h4>
        <p class="noindent">Another type of buffering is for the initial connection requests. TCP implements a backlog, where SYN requests can queue in the kernel before being accepted by the user-land process. When there are too many TCP connection requests for the process to accept in time, the backlog reaches a limit and SYN packets are dropped, to be later retransmitted by the client. The retransmission of these packets causes latency for the client connect time. The limit is tunable: it is a parameter of the listen(2) syscall, and the kernel may also provide system-wide limits.</p>
        <p class="noindent">Backlog drops and SYN retransmits are indicators of host overload.</p>
        </section>
        <section>
        <h4 class="h4" id="ch10lev3sec8"><span epub:type="pagebreak" id="page_508"></span>10.3.8 Interface Negotiation</h4>
        <p class="noindent">Network interfaces may operate with different modes, autonegotiated between the connected transceivers. Some examples are:</p>
        <ul class="sq">
        <li><p class="bull"><strong>Bandwidth</strong>: For example, 10, 100, 1,000, 10,000, 40,000, 100,000 Mbits/s</p></li>
        <li><p class="bull"><strong><a href="gloss.xhtml#glo_051">Duplex</a></strong>: Half or full duplex</p></li>
        </ul>
        <p class="noindent">These examples are from Ethernet, which tends to use round base-10 numbers for bandwidth limits. Other physical-layer protocols, such as SONET, have a different set of possible bandwidths.</p>
        <p class="noindent">Network interfaces are usually described in terms of their highest bandwidth and protocol, for example, 1 Gbit/s Ethernet (1 GbE). This interface may, however, autonegotiate to lower speeds if needed. This can occur if the other endpoint cannot operate faster, or to accommodate physical problems with the connection medium (bad wiring).</p>
        <p class="noindent">Full-duplex mode allows bidirectional simultaneous transmission, with separate paths for transmit and receive that can each operate at full bandwidth. Half-duplex mode allows only one direction at a time.</p>
        </section>
        <section>
        <h4 class="h4" id="ch10lev3sec9">10.3.9 Congestion Avoidance</h4>
        <p class="noindent">Networks are shared resources that can become congested when traffic loads are high. This can cause performance problems: for example, routers or switches may drop packets, causing latency-inducing TCP retransmits. Hosts can also become overwhelmed when receiving high packet rates, and may drop packets themselves.</p>
        <p class="noindent">There are many mechanisms to avoid these problems; these mechanisms should be studied, and tuned if necessary, to improve scalability under load. Examples for different protocols include:</p>
        <ul class="sq">
        <li><p class="bull"><strong>Ethernet</strong>: An overwhelmed host may send <em>pause frames</em> to a transmitter, requesting that they pause transmission (IEEE 802.3x). There are also priority classes and <em>priority pause frames</em> for each class.</p></li>
        <li><p class="bull"><strong>IP</strong>: Includes an Explicit Congestion Notification (ECN) field.</p></li>
        <li><p class="bull"><strong><a href="gloss.xhtml#glo_174">TCP</a></strong>: Includes a congestion window, and various <em>congestion control algorithms</em> may be used.</p></li>
        </ul>
        <p class="noindent">Later sections describe IP ECN and TCP congestion control algorithms in more detail.</p>
        </section>
        <section>
        <h4 class="h4" id="ch10lev3sec10">10.3.10 Utilization</h4>
        <p class="noindent">Network interface utilization can be calculated as the current throughput over the maximum bandwidth. Given variable bandwidth and duplex due to autonegotiation, calculating this isn’t as straightforward as it sounds.</p>
        <p class="noindent">For full duplex, utilization applies to each direction and is measured as the current throughput for that direction over the current negotiated bandwidth. Usually it is just one direction that matters most, as hosts are commonly asymmetric: servers are transmit-heavy, and clients are receive-heavy.</p>
        <p class="noindent"><span epub:type="pagebreak" id="page_509"></span>Once a network interface direction reaches 100% utilization, it becomes a bottleneck, limiting performance.</p>
        <p class="noindent">Some performance tools report activity only in terms of packets, not bytes. Since packet size can vary greatly (as mentioned earlier), it is not possible to relate packet counts to byte counts for calculating either throughput or (throughput-based) utilization.</p>
        </section>
        <section>
        <h4 class="h4" id="ch10lev3sec11">10.3.11 Local Connections</h4>
        <p class="noindent">Network connections can occur between two applications on the same system. These are <em>localhost</em> connections and use a virtual network interface: <em>loopback</em>.</p>
        <p class="noindent">Distributed application environments are often split into logical parts that communicate over the network. These can include web servers, database servers, caching servers, proxy servers, and application servers. If they are running on the same host, their connections are to localhost.</p>
        <p class="noindent">Connecting via IP to localhost is the <em>IP sockets</em> technique of inter-process communication (IPC). Another technique is Unix domain sockets (UDS), which create a file on the file system for communication. Performance may be better with UDS, as the kernel TCP/IP stack can be bypassed, skipping kernel code and the overheads of protocol packet encapsulation.</p>
        <p class="noindent">For TCP/IP sockets, the kernel may detect the localhost connection after the handshake, and then shortcut the TCP/IP stack for data transfers, improving performance. This was developed as a Linux kernel feature, called <em>TCP friends</em>, but was not merged <a href="ch10.xhtml#ch10ref20">[Corbet 12]</a>. BPF can now be used on Linux for this purpose, as is done by the Cilium software for container networking performance and security [Cilium 20a].</p>
        </section>
        </section>
        <section>
        <h3 class="h3" id="ch10lev4">10.4 Architecture</h3>
        <p class="noindent">This section introduces network architecture: protocols, hardware, and software. These have been summarized as background for performance analysis and tuning, with a focus on performance characteristics. For more details, including general networking topics, see networking texts <a href="ch10.xhtml#ch10ref7">[Stevens 93]</a><a href="ch10.xhtml#ch10ref14">[Hassan 03]</a>, RFCs, and vendor manuals for networking hardware. Some of these are listed at the end of the chapter.</p>
        <section>
        <h4 class="h4" id="ch10lev4sec1">10.4.1 Protocols</h4>
        <p class="noindent">In this section, performance features and characteristics of IP, TCP, UDP, and QUIC are summarized. How these protocols are implemented in hardware and software (including features such as segmentation offload, connection queues, and buffering) is described in the later hardware and software sections.</p>
        <section>
        <h5 class="h5" id="ch10lev3_7">IP</h5>
        <p class="noindent">The Internet Protocol (IP) versions 4 and 6 include a field to set the desired performance of a connection: the Type of Service field in IPv4, and the Traffic Class field in IPv6. These fields have since been redefined to contain a Differentiated Services Code Point (DSCP) (RFC 2474) <a href="ch10.xhtml#ch10ref10">[Nichols 98]</a> and an Explicit Congestion Notification (ECN) field (RFC 3168) <a href="ch10.xhtml#ch10ref12">[Ramakrishnan 01]</a>.</p>
        <p class="noindent"><span epub:type="pagebreak" id="page_510"></span>The DSCP is intended to support different <em>service classes</em>, each of which have different characteristics including packet drop probability. Example service classes include: telephony, broadcast video, low-latency data, high-throughput data, and low-priority data.</p>
        <p class="noindent">ECN is a mechanism that allows servers, routers, or switches on the path to explicitly signal the presence of congestion by setting a bit in the IP header, instead of dropping a packet. The receiver will echo this signal back to the sender, which can then throttle transmission. This provides the benefits of congestion avoidance without incurring the penalty of packet drops (provided that the ECN bit is used correctly across the network).</p>
        </section>
        <section>
        <h5 class="h5" id="ch10lev3_8">TCP</h5>
        <p class="noindent">The Transmission Control Protocol (TCP) is a commonly used Internet standard for creating reliable network connections. TCP is specified by RFC 793 <a href="ch10.xhtml#ch10ref2">[Postel 81]</a> and later additions.</p>
        <p class="noindent">In terms of performance, TCP can provide a high rate of throughput even on high-latency networks, by use of buffering and a <em>sliding window</em>. TCP also employs congestion control and a <em>congestion window</em> set by the sender, so that it can maintain a high but also reliable rate of transmission across different and varying networks. Congestion control avoids sending too many packets, which would cause congestion and a performance breakdown.</p>
        <p class="noindent">The following is a summary of TCP performance features, including additions since the original specification:</p>
        <ul class="sq">
        <li><p class="bull"><strong>Sliding window</strong>: This allows multiple packets up to the size of the window to be sent on the network before acknowledgments are received, providing high throughput even on high-latency networks. The size of the window is advertised by the receiver to indicate how many packets it is willing to receive at that time.</p></li>
        <li><p class="bull"><strong>Congestion avoidance</strong>: To prevent sending too much data and causing saturation, which can cause packet drops and worse performance.</p></li>
        <li><p class="bull"><strong>Slow-start</strong>: Part of TCP congestion control, this begins with a small congestion window and then increases it as acknowledgments (ACKs) are received within a certain time. When they are not, the congestion window is reduced.</p></li>
        <li><p class="bull"><strong>Selective acknowledgments</strong> (SACKs): Allow TCP to acknowledge discontinuous packets, reducing the number of retransmits required.</p></li>
        <li><p class="bull"><strong>Fast retransmit</strong>: Instead of waiting on a timer, TCP can retransmit dropped packets based on the arrival of duplicate ACKs. These are a function of round-trip time and not the typically much slower timer.</p></li>
        <li><p class="bull"><strong>Fast recovery</strong>: This recovers TCP performance after detecting duplicate ACKs, by resetting the connection to perform slow-start.</p></li>
        <li><p class="bull"><strong>TCP fast open</strong>: Allows a client to include data in a SYN packet, so that server request processing can begin earlier and not wait for the SYN handshake (RFC7413). This can use a cryptographic cookie to authenticate the client.</p></li>
        <li><p class="bull"><span epub:type="pagebreak" id="page_511"></span><strong>TCP timestamps</strong>: Includes a timestamp for sent packets that is returned in the ACK, so that round-trip time can be measured (RFC 1323) <a href="ch10.xhtml#ch10ref6">[Jacobson 92]</a>.</p></li>
        <li><p class="bull"><strong>TCP SYN cookies</strong>: Provides cryptographic cookies to clients during possible SYN flood attacks (full backlogs) so that legitimate clients can continue to connect, and without the server needing to store extra data for these connection attempts.</p></li>
        </ul>
        <p class="noindent">In some cases these features are implemented by use of extended TCP options added to the protocol header.</p>
        <p class="noindent">Important topics for TCP performance include the three-way handshake, duplicate ACK detection, congestion control algorithms, Nagle, delayed ACKs, SACK, and FACK.</p>
        <section>
        <h6 class="h6">Three-Way Handshake</h6>
        <p class="noindent">Connections are established using a three-way handshake between the hosts. One host passively listens for connections; the other actively initiates the connection. To clarify terminology: <em>passive</em> and <em>active</em> are from RFC 793 [Postel 81]; however, they are commonly called <em>listen</em> and <em>connect</em>, respectively, after the socket API. For the client/server model, the server performs listen and the client performs connect.</p>
        <p class="noindent">The three-way handshake is pictured in <a href="ch10.xhtml#ch10fig06">Figure 10.6</a>.</p>
        <figure class="image-c" id="ch10fig06">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780136821694/files/graphics/10fig06.jpg" alt="Images" width="775" height="199">
        <figcaption>
        <p class="title-f"><span class="pd_ashf">Figure 10.6</span> TCP three-way handshake</p>
        </figcaption>
        </figure>
        <p class="noindent">Connection latency from the client is indicated, which completes when the final ACK is sent. After that, data transfer may begin.</p>
        <p class="noindent">This figure shows best-case latency for a handshake. A packet may be dropped, adding latency as it is timed out and retransmitted.</p>
        <p class="noindent">Once the three-way handshake is complete, the TCP session is placed in the ESTABLISHED state.</p>
        </section>
        <section>
        <h6 class="h6">States and Timers</h6>
        <p class="noindent">TCP sessions switch between TCP states based on packets and socket events. The states are LISTEN, SYN-SENT, SYN-RECEIVED, ESTABLISHED, FIN-WAIT-1, FIN-WAIT-2, CLOSE-WAIT, CLOSING, LAST-ACK, TIME-WAIT, and CLOSED [Postal 80]. Performance analysis typically focuses on those in the ESTABLISHED state, which are the active connections. Such connections may be transferring data, or idle awaiting the next event: a data transfer or close event.</p>
        <p class="noindent"><span epub:type="pagebreak" id="page_512"></span>A session that has fully closed enters the TIME_WAIT<sup><a id="ch10fn2a" href="ch10.xhtml#ch10fn2">2</a></sup> state so that late packets are not mis-associated with a new connection on the same ports. This can lead to a performance issue of port exhaustion, explained in <a href="ch10.xhtml#ch10lev5sec7">Section 10.5.7</a>, <a href="ch10.xhtml#ch10lev5sec7">TCP Analysis</a>.</p>
        <p class="footnote"><sup><a id="ch10fn2" href="ch10.xhtml#ch10fn2a">2</a></sup>While it is often written (and programmed) as TIME_WAIT, RFC 793 uses TIME-WAIT.</p>
        <p class="noindent">Some states have timers associated with them. TIME_WAIT is typically two minutes (some kernels, such as the Windows kernel, allow it to be tuned). There may also be a “keep alive” timer on ESTABLISHED, set to a long duration (e.g., two hours), to trigger probe packets to check that the remote host is still alive.</p>
        </section>
        <section>
        <h6 class="h6">Duplicate ACK Detection</h6>
        <p class="noindent">Duplicate ACK detection is used by the fast retransmit and fast recovery algorithms to quickly detect when a sent packet (or its ACK) has been lost. It works as follows:</p>
        <ol class="number">
        <li><p class="number">The sender sends a packet with sequence number 10.</p></li>
        <li><p class="number">The receiver replies with an ACK for sequence number 11.</p></li>
        <li><p class="number">The sender sends 11, 12, and 13.</p></li>
        <li><p class="number">Packet 11 is dropped.</p></li>
        <li><p class="number">The receiver replies to both 12 and 13 by sending an ACK for 11, which it is still expecting.</p></li>
        <li><p class="number">The sender receives the duplicate ACKs for 11.</p></li>
        </ol>
        <p class="noindent">Duplicate ACK detection is also used by various congestion avoidance algorithms.</p>
        </section>
        <section>
        <h6 class="h6">Retransmits</h6>
        <p class="noindent">Two commonly used mechanisms for TCP to detect and retransmits lost packets are:</p>
        <ul class="sq">
        <li><p class="bull"><strong>Timer-based retransmits</strong>: These occur when a time has passed and a packet acknowledgment has not yet been received. This time is the TCP retransmit timeout, calculated dynamically based on the connection round-trip time (RTT). On Linux, this will be at least 200 ms (TCP_RTO_MIN) for the first retransmit,<sup><a id="ch10fn3a" href="ch10.xhtml#ch10fn3">3</a></sup> and subsequent retransmits will be much slower, following an exponential backoff algorithm that doubles the timeout.</p>
        <p class="footnote"><sup><a id="ch10fn3" href="ch10.xhtml#ch10fn3a">3</a></sup>This seems to violate RFC6298, which stipulates a one second RTO minimum <a href="ch10.xhtml#ch10ref19">[Paxson 11]</a>.</p></li>
        <li><p class="bull"><strong>Fast retransmits</strong>: When duplicate ACKs arrive, TCP can assume that a packet was dropped and retransmit it immediately.</p></li>
        </ul>
        <p class="noindent">To further improve performance, additional mechanisms have been developed to avoid the timer-based retransmit. One problem occurs is when the last transmitted packet is lost, and there are no subsequent packets to trigger duplicate ACK detection. (Consider the prior example with a loss on packet 13.) This is solved by Tail Loss Probe (TLP), which sends an additional packet (probe) after a short timeout on the last transmission to help detect packet loss <a href="ch10.xhtml#ch10ref25">[Dukkipati 13]</a>.</p>
        <p class="noindent">Congestion control algorithms may also throttle throughput in the presence of retransmits.</p>
        </section>
        <section>
        <h6 class="h6"><span epub:type="pagebreak" id="page_513"></span>Congestion Controls</h6>
        <p class="noindent">Congestion control algorithms have been developed to maintain performance on congested networks. Some operating systems (including Linux-based) allow the algorithm to be selected as part of system tuning. These algorithms include:</p>
        <ul class="sq">
        <li><p class="bull"><strong>Reno</strong>: Triple duplicate ACKs trigger: halving of the congestion window, halving of the slow-start threshold, fast retransmit, and fast recovery.</p></li>
        <li><p class="bull"><strong>Tahoe</strong>: Triple duplicate ACKs trigger: fast retransmit, halving the slow-start threshold, congestion window set to one maximum segment size (MSS), and slow-start state. (Along with Reno, Tahoe was first developed for 4.3BSD.)</p></li>
        <li><p class="bull"><strong>CUBIC</strong>: Uses a cubic function (hence the name) to scale the window, and a “hybrid start” function to exit slow start. CUBIC tends to be more aggressive than Reno, and is the default in Linux.</p></li>
        <li><p class="bull"><strong>BBR</strong>: Instead of window-based, BBR builds an explicit model of the network path characteristics (RTT and bandwidth) using probing phases. BBR can provide dramatically better performance on some network paths, while hurting performance on others. BBRv2 is currently in development and promises to fix some of the deficiencies of v1.</p></li>
        <li><p class="bull"><strong>DCTCP</strong>: DataCenter TCP relies on switches configured to emit Explicit Congestion Notification (ECN) marks at a very shallow queue occupancy to rapidly ramp up to the available bandwidth (RFC 8257) <a href="ch10.xhtml#ch10ref30">[Bensley 17]</a>. This makes DCTCP unsuitable for deployment across the Internet, but in a suitably configured controlled environment it can improve performance significantly.</p></li>
        </ul>
        <p class="noindent">Other algorithms not listed previously include Vegas, New Reno, and Hybla.</p>
        <p class="noindent">The congestion control algorithm can make a large difference to network performance. The Netflix cloud services, for example, use BBR and found it can improve throughput threefold during heavy packet loss <a href="ch10.xhtml#ch10ref29">[Ather 17]</a>. Understanding how these algorithms react under different network conditions is an important activity when analyzing TCP performance.</p>
        <p class="noindent">Linux 5.6, released in 2020, added support for developing new congestion control algorithms in BPF <a href="ch10.xhtml#ch10ref53">[Corbet 20]</a>. This allows them to be defined by the end user and loaded on demand.</p>
        </section>
        <section>
        <h6 class="h6">Nagle</h6>
        <p class="noindent">This algorithm (RFC 896) <a href="ch10.xhtml#ch10ref3">[Nagle 84]</a> reduces the number of small packets on the network by delaying their transmission to allow more data to arrive and be coalesced. This delays packets only if there is data in the pipeline and delays are already being encountered.</p>
        <p class="noindent">The system may provide a tunable parameter or socket option to disable Nagle, which may be necessary if its operation conflicts with delayed ACKs (see <a href="ch10.xhtml#ch10lev8sec2">Section 10.8.2</a>, <a href="ch10.xhtml#ch10lev8sec2">Socket Options</a>).</p>
        </section>
        <section>
        <h6 class="h6">Delayed ACKs</h6>
        <p class="noindent">This algorithm (RFC 1122) <a href="ch10.xhtml#ch10ref5">[Braden 89]</a> delays the sending of ACKs up to 500 ms, so that multiple ACKs may be combined. Other TCP control messages can also be combined, reducing the number of packets on the network.</p>
        <p class="noindent">As with Nagle, the system may provide a tunable parameter to disable this behavior.</p>
        </section>
        <section>
        <h6 class="h6"><span epub:type="pagebreak" id="page_514"></span>SACK, FACK, and RACK</h6>
        <p class="noindent">The TCP selective acknowledgment (SACK) algorithm allows the receiver to inform the sender that it received a noncontiguous block of data. Without this, a packet drop would eventually cause the entire send window to be retransmitted, to preserve a sequential acknowledgment scheme. This harms TCP performance and is avoided by most modern operating systems that support SACK.</p>
        <p class="noindent">SACK has been extended by forward acknowledgments (FACK), which are supported in Linux by default. FACKs track additional state and better regulate the amount of outstanding data in the network, improving overall performance <a href="ch10.xhtml#ch10ref8">[Mathis 96]</a>.</p>
        <p class="noindent">Both SACK and FACK are used to improve packet loss recovery. A newer algorithm, Recent ACKnowledgment (RACK; now called RACK-TLP with the incorporation of TLP) uses time information from ACKs for even better loss detection and recovery, rather than ACK sequences alone <a href="ch10.xhtml#ch10ref51">[Cheng 20]</a>. For FreeBSD, Netflix has developed a new refactored TCP stack called RACK based on RACK, TLP, and other features <a href="ch10.xhtml#ch10ref42">[Stewart 18]</a>.</p>
        </section>
        <section>
        <h6 class="h6">Initial Window</h6>
        <p class="noindent">The initial window (IW) is the number of packets a TCP sender will transmit at the beginning of a connection before waiting for acknowledgment from the sender. For short flows, such as typical HTTP connections, an IW large enough to span the transmitted data can greatly reduce completion time, improving performance. Larger IWs, however, can risk congestion and packet drops. This is especially compounded when multiple flows start up at the same time.</p>
        <p class="noindent">The Linux default (10 packets, aka IW10) can be too high on slow links or when many connections start up; other operating systems default to 2 or 4 packets (IW2 or IW4).</p>
        </section>
        </section>
        <section>
        <h5 class="h5" id="ch10lev3_9">UDP</h5>
        <p class="noindent">The User Datagram Protocol (UDP) is a commonly used Internet standard for sending messages, called <em>datagrams</em>, across a network (RFC 768) <a href="ch10.xhtml#ch10ref1">[Postel 80]</a>. In terms of performance, UDP provides:</p>
        <ul class="sq">
        <li><p class="bull"><strong>Simplicity</strong>: Simple and small protocol headers reduce overheads of computation and size.</p></li>
        <li><p class="bull"><strong>Statelessness</strong>: Lower overheads for connections and transmission.</p></li>
        <li><p class="bull"><strong>No retransmits</strong>: These add significant latencies for TCP connections.</p></li>
        </ul>
        <p class="noindent">While simple and often high-performing, UDP is not intended to be reliable, and data can be missing or received out of order. This makes it unsuitable for many types of connections. UDP also has no congestion avoidance and can therefore contribute to congestion on the network.</p>
        <p class="noindent">Some services, including versions of NFS, can be configured to operate over TCP or UDP as desired. Others that perform broadcast or multicast data may be able to use only UDP.</p>
        <p class="noindent">A major use for UDP has been DNS. Due to the simplicity of UDP, a lack of congestion control, and Internet support (it is not typically firewalled) there are now new protocols built upon UDP that implement their own congestion control and other features. An example is QUIC.</p>
        </section>
        <section>
        <h5 class="h5" id="ch10lev3_10"><span epub:type="pagebreak" id="page_515"></span>QUIC and HTTP/3</h5>
        <p class="noindent">QUIC is a network protocol designed by Jim Roskind at Google as a higher-performing, lower-latency alternative to TCP, optimized for HTTP and TLS <a href="ch10.xhtml#ch10ref24">[Roskind 12]</a>. QUIC is built upon UDP, and provides several features on top of it, including:</p>
        <ul class="sq">
        <li><p class="bull">The ability to multiplex several application-defined streams on top of the same “connection.”</p></li>
        <li><p class="bull">A TCP-like reliable in-order stream transport that can be optionally turned off for individual substreams.</p></li>
        <li><p class="bull">Connection resumption when a client changes its network address, based on cryptographic authentication of connection IDs.</p></li>
        <li><p class="bull">Full encryption of the payload data, including QUIC headers.</p></li>
        <li><p class="bull">0-RTT connection handshakes including cryptography (for peers that have previously communicated).</p></li>
        </ul>
        <p class="noindent">QUIC is in heavy use by the Chrome web browser.</p>
        <p class="noindent">While QUIC was initially developed by Google, the Internet Engineering Task Force (IETF) is in the process of standardizing both the QUIC transport itself, and the specific configuration of using HTTP over QUIC (the latter combination is named HTTP/3).</p>
        </section>
        </section>
        <section>
        <h4 class="h4" id="ch10lev4sec2">10.4.2 Hardware</h4>
        <p class="noindent">Networking hardware includes interfaces, controllers, switches, routers, and firewalls. An understanding of their operation is useful, even if they are managed by other staff (network administrators).</p>
        <section>
        <h5 class="h5" id="ch10lev3_11">Interfaces</h5>
        <p class="noindent">Physical network interfaces send and receive messages, called <em>frames</em>, on the attached network. They manage the electrical, optical, or wireless signaling involved, including the handling of transmission errors.</p>
        <p class="noindent">Interface types are based on layer 2 standards, each providing a maximum bandwidth. Higher-bandwidth interfaces provide lower data-transfer latency, at a higher cost. When designing new servers, a key decision is often how to balance the price of the server with the desired network performance.</p>
        <p class="noindent">For Ethernet, choices include copper or optical, with maximum speeds of 1 Gbit/s (1 GbE), 10 GbE, 40 GbE, 100 GbE, 200 GbE, and 400 GbE. Numerous vendors manufacture Ethernet interface controllers, although your operating system may not have driver support for some of them.</p>
        <p class="noindent">Interface utilization can be examined as the current throughput divided by the current negotiated bandwidth. Most interfaces have separate channels for transmit and receive, and when operating in full-duplex mode, each channel’s utilization must be studied separately.</p>
        <p class="noindent"><span epub:type="pagebreak" id="page_516"></span>Wireless interfaces can suffer performance issues due to poor signal strength and interference.<sup><a id="ch10fn4a" href="ch10.xhtml#ch10fn4">4</a></sup></p>
        <p class="footnote"><sup><a id="ch10fn4" href="ch10.xhtml#ch10fn4a">4</a></sup> I developed BPF software that turns Linux Wi-Fi signal strength into an audible pitch, and demonstrated it in an AWS re:Invent 2019 talk <a href="ch10.xhtml#ch10ref46">[Gregg 19b]</a>. I would include it in this chapter, but I have not yet used it for enterprise or cloud environments, which so far are all wired.</p>
        </section>
        <section>
        <h5 class="h5" id="ch10lev3_12">Controllers</h5>
        <p class="noindent">Physical network interfaces are provided to the system via controllers, either built into the system board or provided via expander cards.</p>
        <p class="noindent">Controllers are driven by microprocessors and are attached to the system via an I/O transport (e.g., PCI). Either of these can become the limiter for network throughput or IOPS.</p>
        <p class="noindent">For example, a dual 10 GbE network interface card is connected to a four-channel PCI express (PCIe) Gen 2 slot. The card has a maximum send or receive bandwidth of 2 × 10 GbE = 20 Gbits/s, and bidirectional, 40 Gbit/s. The slot has a maximum bandwidth of 4 × 4 Gbits/s = 16 Gbits/s. Therefore, network throughput on both ports will be limited by PCIe Gen 2 bandwidth, and it will not be possible to drive them both at line rate at the same time (I know this from practice!).</p>
        </section>
        <section>
        <h5 class="h5" id="ch10lev3_13">Switches and Routers</h5>
        <p class="noindent">Switches provide a dedicated communication path between any two connected hosts, allowing multiple transmissions between pairs of hosts without interference. This technology replaced hubs (and before that, shared physical buses: the commonly used thick-Ethernet coaxial cable), which shared all packets with all hosts. This sharing led to contention when hosts transmitted simultaneously, identified by the interface as a <em>collision</em> using a “carrier sense multiple access with collision detection” (CSMA/CD) algorithm. This algorithm would exponentially back off and retransmit until successful, creating performance issues under load. With the use of switches this is behind us, but some observability tools still have collision counters—even though these usually occur only due to errors (negotiation or bad wiring).</p>
        <p class="noindent">Routers deliver packets between networks and use network protocols and routing tables to determine efficient delivery paths. Delivering a packet between two cities may involve a dozen or more routers, plus other network hardware. The routers and routes are usually configured to update dynamically, so that the network can automatically respond to network and router outages, and to balance load. This means that at a given point in time, no one can be sure what path a packet is actually taking. With multiple paths possible, there is also the potential for packets to be delivered out of order, which can cause TCP performance problems.</p>
        <p class="noindent">This element of mystery on the network is often blamed for poor performance: perhaps heavy network traffic—from other unrelated hosts—is saturating a router between the source and destination? Network administration teams are therefore frequently required to exonerate their infrastructure. They can do so using advanced real-time monitoring tools to check all routers and other network components involved.</p>
        <p class="noindent"><span epub:type="pagebreak" id="page_517"></span>Both routers and switches include buffers and microprocessors, which themselves can become performance bottlenecks under load. As an extreme example, I once found that an early 10 GbE switch could drive no more than 11 Gbits/s in total across all ports, due to its limited CPU capacity.</p>
        <p class="noindent">Note that switches and routers are also often where <em>rate transitions</em> occur (switching from one bandwidth to another, e.g., a 10 Gbps link transitions to a 1 Gbps link). When this happens, some buffering is necessary to avoid excessive drops, but many switches and routers over-buffer (see the bufferbloat issue in <a href="ch10.xhtml#ch10lev3sec6">Section 10.3.6</a>, <a href="ch10.xhtml#ch10lev3sec6">Buffering</a>), leading to high latencies. Better queue management algorithms can help eliminate this problem, but not all network device vendors support them. Pacing at the source can also be a way to alleviate issues with rate transitions by making the traffic less bursty.</p>
        </section>
        <section>
        <h5 class="h5" id="ch10lev3_14">Firewalls</h5>
        <p class="noindent">Firewalls are often in use to permit only authorized communications based on a configured rule set, improving the security of the network. They may be present as both physical network devices and kernel software.</p>
        <p class="noindent">Firewalls can become a performance bottleneck, especially when configured to be stateful. Stateful rules store metadata for each seen connection, and the firewall may experience excessive memory load when processing many connections. This can happen due to a denial of service (DoS) attack that attempts to inundate a target with connections. It can also happen with a heavy rate of outbound connections, as they may require similar connection tracking.</p>
        <p class="noindent">As firewalls are custom hardware or software, the tools available to analyze them depends on each firewall product. See their respective documentation.</p>
        <p class="noindent">The use of extended BPF to implement firewalls on commodity hardware is growing, due to its performance, programmability, ease of use, and final cost. Companies adopting BPF firewalls and DDoS solutions include Facebook <a href="ch10.xhtml#ch10ref37">[Deepak 18]</a>, Cloudflare <a href="ch10.xhtml#ch10ref41">[Majkowski 18]</a>, and Cilium <a href="ch10.xhtml#ch10ref52">[Cilium 20a]</a>.</p>
        <p class="noindent">Firewalls can also be a nuisance during performance testing: performing a bandwidth experiment when debugging an issue may involve modifying firewall rules to allow the connection (and coordinating that with the security team).</p>
        </section>
        <section>
        <h5 class="h5" id="ch10lev3_15">Others</h5>
        <p class="noindent">Your environment may include other physical network devices, such as hubs, bridges, repeaters, and modems. Any of these can be a source of performance bottlenecks and dropped packets.</p>
        </section>
        </section>
        <section>
        <h4 class="h4" id="ch10lev4sec3">10.4.3 Software</h4>
        <p class="noindent">Networking software includes the network stack, TCP, and device drivers. Topics related to performance are discussed in this section.</p>
        <section>
        <h5 class="h5" id="ch10lev3_16"><span epub:type="pagebreak" id="page_518"></span>Network Stack</h5>
        <p class="noindent">The components and layers involved depend on the operating system type, version, protocols, and interfaces in use. <a href="ch10.xhtml#ch10fig07">Figure 10.7</a> depicts a general model, showing the software components.</p>
        <figure class="image-c" id="ch10fig07">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780136821694/files/graphics/10fig07.jpg" alt="Images" width="775" height="538">
        <figcaption>
        <p class="title-f"><span class="pd_ashf">Figure 10.7</span> Generic network stack</p>
        </figcaption>
        </figure>
        <p class="noindent">On modern kernels the stack is multithreaded, and inbound packets can be processed by multiple CPUs.</p>
        <section>
        <h6 class="h6">Linux</h6>
        <p class="noindent">The Linux network stack is pictured in <a href="ch10.xhtml#ch10fig08">Figure 10.8</a>, including the location of socket send/receive buffers and packet queues.</p>
        <figure class="image-c" id="ch10fig08">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780136821694/files/graphics/10fig08.jpg" alt="Images" width="775" height="708">
        <figcaption>
        <p class="title-f"><span class="pd_ashf">Figure 10.8</span> Linux network stack</p>
        </figcaption>
        </figure>
        <p class="noindent">On Linux systems, the network stack is a core kernel component, and device drivers are additional modules. Packets are passed through these kernel components as the struct sk_buff (socket buffer) data type. Note that there may also be queueing in the IP layer (not pictured) for packet reassembly.</p>
        <p class="noindent">The following sections discuss Linux implementation details related to performance: TCP connection queues, TCP buffering, queueing disciplines, network device drivers, CPU scaling, and kernel bypass. The TCP protocol was described in the previous section.</p>
        </section>
        </section>
        <section>
        <h5 class="h5" id="ch10lev3_17"><span epub:type="pagebreak" id="page_519"></span>TCP Connection Queues</h5>
        <p class="noindent">Bursts of inbound connections are handled by using backlog queues. There are two such queues, one for incomplete connections while the TCP handshake completes (also known as the <em>SYN backlog</em>), and one for established sessions waiting to be accepted by the application (also known as the <em>listen backlog</em>). These are pictured in <a href="ch10.xhtml#ch10fig09">Figure 10.9</a>.</p>
        <figure class="image-c" id="ch10fig09">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780136821694/files/graphics/10fig09.jpg" alt="Images" width="775" height="353">
        <figcaption>
        <p class="title-f"><span class="pd_ashf">Figure 10.9</span> TCP backlog queues</p>
        </figcaption>
        </figure>
        <p class="noindent">Only one queue was used in earlier kernels, and it was vulnerable to SYN floods. A SYN flood is a type of DoS attack that involves sending numerous SYNs to the listening TCP port from bogus IP addresses. This fills the backlog queue while TCP waits to complete the handshake, preventing real clients from connecting.</p>
        <p class="noindent">With two queues, the first can act as a staging area for potentially bogus connections, which are promoted to the second queue only once the connection is established. The first queue can be made long to absorb SYN floods and optimized to store only the minimum amount of metadata necessary.</p>
        <p class="noindent"><span epub:type="pagebreak" id="page_520"></span>The use of SYN cookies bypasses the first queue, as they show the client is already authorized.</p>
        <p class="noindent">The length of these queues can be tuned independently (see <a href="ch10.xhtml#ch10lev8">Section 10.8</a>, <a href="ch10.xhtml#ch10lev8">Tuning</a>). The second can also be set by the application as the backlog argument to listen(2).</p>
        </section>
        <section>
        <h5 class="h5" id="ch10lev3_18">TCP Buffering</h5>
        <p class="noindent">Data throughput is improved by using send and receive buffers associated with the socket. These are pictured in <a href="ch10.xhtml#ch10fig10">Figure 10.10</a>.</p>
        <figure class="image-c" id="ch10fig10">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780136821694/files/graphics/10fig10.jpg" alt="Images" width="775" height="169">
        <figcaption>
        <p class="title-f"><span class="pd_ashf">Figure 10.10</span> TCP send and receive buffers</p>
        </figcaption>
        </figure>
        <p class="noindent">The size of both the send and receive buffers is tunable. Larger sizes improve throughput performance, at the cost of more main memory spent per connection. One buffer may be set to be larger than the other if the server is expected to perform more sending or receiving. The Linux kernel will also dynamically increase the size of these buffers based on connection activity, and allows tuning of their minimum, default, and maximum sizes.</p>
        </section>
        <section>
        <h5 class="h5" id="ch10lev3_19">Segmentation Offload: GSO and TSO</h5>
        <p class="noindent">Network devices and networks accept packet sizes up to a maximum segment size (MSS) that may be as small as 1500 bytes. To avoid the network stack overheads of sending many small packets, Linux uses generic segmentation offload (GSO) to send packets up to 64 Kbytes in size <span epub:type="pagebreak" id="page_521"></span>(“super packets”), which are split into MSS-sized segments just before delivery to the network device. If the NIC and driver support TCP segmentation offload (TSO), GSO leaves splitting to the device, improving network stack throughput.<sup><a id="ch10fn5a" href="ch10.xhtml#ch10fn5">5</a></sup> There is also a generic receive offload (GRO) complement to GSO <a href="ch10.xhtml#ch10ref58">[Linux 20i]</a>.<sup><a id="ch10fn6a" href="ch10.xhtml#ch10fn6">6</a></sup> GRO and GSO are implemented in kernel software, and TSO is implemented by NIC hardware.</p>
        <p class="footnote"><sup><a id="ch10fn5" href="ch10.xhtml#ch10fn5a">5</a></sup>Some network cards provide a TCP offload engine (TOE) to offload part or all of TCP/IP protocol processing. Linux does not support TOE for various reasons, including security, complexity, and even performance <a href="ch10.xhtml#ch10ref28">[Linux 16]</a>.</p>
        <p class="footnote"><sup><a id="ch10fn6" href="ch10.xhtml#ch10fn6a">6</a></sup>UDP support for GSO and GRO was added to Linux in 2018, with QUIC a key use case [Bruijn 18].</p>
        </section>
        <section>
        <h5 class="h5" id="ch10lev3_20">Queueing Discipline</h5>
        <p class="noindent">This is an optional layer for managing traffic classification (tc), scheduling, manipulation, filtering, and shaping of network packets. Linux provides numerous queueing discipline algorithms (qdiscs), which can be configured using the tc(8) command. As each has a man page, the man(1) command can be used to list them:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg521a" id="pg521">Click here to view code image</a></p>
        <pre class="pretb"># <strong>man -k tc-</strong>
        tc-actions (8)       - independently defined actions in tc
        tc-basic (8)         - basic traffic control filter
        tc-bfifo (8)         - Packet limited First In, First Out queue
        tc-bpf (8)           - BPF programmable classifier and actions for ingress/egress
        queueing disciplines
        tc-cbq (8)           - Class Based Queueing
        tc-cbq-details (8)   - Class Based Queueing
        tc-cbs (8)           - Credit Based Shaper (CBS) Qdisc
        tc-cgroup (8)        - control group based traffic control filter
        tc-choke (8)         - choose and keep scheduler
        tc-codel (8)         - Controlled-Delay Active Queue Management algorithm
        tc-connmark (8)      - netfilter connmark retriever action
        tc-csum (8)          - checksum update action
        tc-drr (8)           - deficit round robin scheduler
        tc-ematch (8)        - extended matches for use with "basic" or "flow" filters
        tc-flow (8)          - flow based traffic control filter
        tc-flower (8)        - flow based traffic control filter
        tc-fq (8)            - Fair Queue traffic policing
        tc-fq_codel (8)      - Fair Queuing (FQ) with Controlled Delay (CoDel)
        [...]</pre>
        <p class="noindent">The Linux kernel sets pfifo_fast as the default qdisc, whereas systemd is less conservative and sets it to fq_codel to reduce potential bufferbloat, at the cost of slightly higher complexity in the qdisc layer.</p>
        <p class="noindent">BPF can enhance the capabilities of this layer with the programs of type BPF_PROG_TYPE_SCHED_CLS and BPF_PROG_TYPE_SCHED_ACT. These BPF programs can be attached to kernel ingress and egress points for packet filtering, mangling, and forwarding, as used by load balancers and firewalls.</p>
        </section>
        <section>
        <h5 class="h5" id="ch10lev3_21"><span epub:type="pagebreak" id="page_522"></span>Network Device Drivers</h5>
        <p class="noindent">The network device driver usually has an additional buffer—a ring buffer—for sending and receiving packets between kernel memory and the NIC. This was pictured in <a href="ch10.xhtml#ch10fig08">Figure 10.8</a> as the driver queue.</p>
        <p class="noindent">A performance feature that has become more common with high-speed networking is the use of <em>interrupt coalescing mode</em>. Instead of interrupting the kernel for every arrived packet, an interrupt is sent only when either a timer (polling) or a certain number of packets is reached. This reduces the rate at which the kernel communicates with the NIC, allowing larger transfers to be buffered, resulting in greater throughput, though at some cost in latency.</p>
        <p class="noindent">The Linux kernel uses a new API (NAPI) framework that uses an interrupt mitigation technique: for low packet rates, interrupts are used (processing is scheduled via a softirq); for high packet rates, interrupts are disabled, and polling is used to allow coalescing <a href="ch10.xhtml#ch10ref13">[Corbet 03]</a><a href="ch10.xhtml#ch10ref16">[Corbet 06b]</a>. This provides low latency or high throughput, depending on the workload. Other features of NAPI include:</p>
        <ul class="sq">
        <li><p class="bull">Packet throttling, which allows early packet drop in the network adapter to prevent the system from being overwhelmed by packet storms.</p></li>
        <li><p class="bull">Interface scheduling, where a quota is used to limit the buffers processed in a polling cycle, to ensure fairness between busy network interfaces.</p></li>
        <li><p class="bull">Support for the SO_BUSY_POLL socket option, where user-level applications can reduce network receive latency by requesting to <em>busy wait</em> (spin on CPU until an event occurs) on a socket <a href="ch10.xhtml#ch10ref31">[Dumazet 17a]</a>.</p></li>
        </ul>
        <p class="noindent">Coalescing can be especially important for improving virtual machine networking, and is used by the ena network driver used by AWS EC2.</p>
        </section>
        <section>
        <h5 class="h5" id="ch10lev3_22">NIC Send and Receive</h5>
        <p class="noindent">For sent packets, the NIC is notified and typically reads the packet (frame) from kernel memory using direct memory access (DMA) for efficiency. NICs provide transmit descriptors for managing DMA packets; if the NIC does not have free descriptors, the network stack will pause transmission to allow the NIC to catch up.<sup><a id="ch10fn7a" href="ch10.xhtml#ch10fn7">7</a></sup></p>
        <p class="footnote"><sup><a id="ch10fn7" href="ch10.xhtml#ch10fn7a">7</a></sup>Byte Queue Limits (BQL), summarized under the heading Other Optimizations, usually prevent TX descriptor exhaustion.</p>
        <p class="noindent">For received packets, NICs can use DMA to place the packet into kernel ring-buffer memory and then notify the kernel using an interrupt (which may be ignored to allow coalescing). The interrupt triggers a softirq to deliver the packet to the network stack for further processing.</p>
        </section>
        <section>
        <h5 class="h5" id="ch10lev3_23">CPU Scaling</h5>
        <p class="noindent">High packet rates can be achieved by engaging multiple CPUs to process packets and the TCP/IP stack. Linux supports various methods for multi-CPU packet processing (see Documentation/networking/scaling.txt):</p>
        <ul class="sq">
        <li><p class="bull"><strong>RSS: Receive Side Scaling</strong>: For modern NICs that support multiple queues and can hash packets to different queues, which are in turn processed by different CPUs, interrupting <span epub:type="pagebreak" id="page_523"></span>them directly. This hash may be based on the IP address and TCP port numbers, so that packets from the same connection end up being processed by the same CPU.<sup><a id="ch10fn8a" href="ch10.xhtml#ch10fn8">8</a></sup></p>
        <p class="footnote"><sup><a id="ch10fn8" href="ch10.xhtml#ch10fn8a">8</a></sup> The Netflix FreeBSD CDN uses RSS to assist TCP large receive offload (LRO), allowing packets for the same connection to be aggregated, even when separated by other packets <a href="ch10.xhtml#ch10ref33">[Gallatin 17]</a>.</p></li>
        <li><p class="bull"><strong>RPS: Receive Packet Steering</strong>: A software implementation of RSS, for NICs that do not support multiple queues. This involves a short interrupt service routine to map the inbound packet to a CPU for processing. A similar hash can be used to map packets to CPUs, based on fields from the packet headers.</p></li>
        <li><p class="bull"><strong>RFS: Receive Flow Steering</strong>: This is similar to RPS, but with affinity for where the socket was last processed on-CPU, to improve CPU cache hit rates and memory locality.</p></li>
        <li><p class="bull"><strong>Accelerated Receive Flow Steering</strong>: This achieves RFS in hardware, for NICs that support this functionality. It involves updating the NIC with flow information so that it can determine which CPU to interrupt.</p></li>
        <li><p class="bull"><strong>XPS: Transmit Packet Steering</strong>: For NICs with multiple transmit queues, this supports transmission by multiple CPUs to the queues.</p></li>
        </ul>
        <p class="noindent">Without a CPU load-balancing strategy for network packets, a NIC may interrupt only one CPU, which can reach 100% utilization and become a bottleneck. This may show up as high softirq CPU time on a single CPU (e.g., using Linux mpstat(1): see <a href="ch06.xhtml#ch06">Chapter 6</a>, <a href="ch06.xhtml#ch06">CPUs</a>, <a href="ch06.xhtml#ch06lev6sec3">Section 6.6.3</a>, <a href="ch06.xhtml#ch06lev6sec3">mpstat</a>). This may especially happen for load balancers or proxy servers (e.g., nginx), as their intended workload is a high rate of inbound packets.</p>
        <p class="noindent">Mapping interrupts to CPUs based on factors such as cache coherency, as is done by RFS, can noticeably improve network performance. This can also be accomplished by the irqbalance process, which assigns interrupt request (IRQ) lines to CPUs.</p>
        </section>
        <section>
        <h5 class="h5" id="ch10lev3_24">Kernel Bypass</h5>
        <p class="noindent"><a href="ch10.xhtml#ch10fig08">Figure 10.8</a> shows the path most commonly taken through the TCP/IP stack. Applications can bypass the kernel network stack using technologies such as the Data Plane Development Kit (DPDK) in order to achieve higher packet rates and performance. This involves an application implementing its own network protocols in user-space, and making writes to the network driver via a DPDK library and a kernel user space I/O (UIO) or virtual function I/O (VFIO) driver. The expense of copying packet data can be avoided by directly accessing memory on the NIC.</p>
        <p class="noindent">The eXpress Data Path (XDP) technology provides another path for network packets: a programmable fast path that uses extended BPF and that integrates into the existing kernel stack rather than bypassing it <a href="ch10.xhtml#ch10ref39">[Høiland-Jørgensen 18]</a>. (DPDK now supports XDP for receiving packets, moving some functionality back to the kernel <a href="ch10.xhtml#ch10ref54">[DPDK 20]</a>.)</p>
        <p class="noindent">With kernel network stack bypass, instrumentation using traditional tools and metrics is not available because the counters and tracing events they use are also bypassed. This makes performance analysis more difficult.</p>
        <p class="noindent">Apart from full stack bypass, there are capabilities for avoiding the expense of copying data: the MSG_ZEROCOPY send(2) flag, and zero-copy receive via mmap(2) <a href="ch10.xhtml#ch10ref59">[Linux 20c]</a><a href="ch10.xhtml#ch10ref35">[Corbet 18b]</a>.</p>
        </section>
        <section>
        <h5 class="h5" id="ch10lev3_25"><span epub:type="pagebreak" id="page_524"></span>Other Optimizations</h5>
        <p class="noindent">There are other algorithms in use throughout the Linux network stack to improve performance. <a href="ch10.xhtml#ch10fig11">Figure 10.11</a> shows these for the TCP send path (many of these are called from the tcp_write_xmit() kernel function).</p>
        <figure class="image-c" id="ch10fig11">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780136821694/files/graphics/10fig11.jpg" alt="Images" width="775" height="161">
        <figcaption>
        <p class="title-f"><span class="pd_ashf">Figure 10.11</span> TCP send path</p>
        </figcaption>
        </figure>
        <p class="noindent">Some of these components and algorithms were described earlier (socket send buffers, TSO,<sup><a id="ch10fn9a" href="ch10.xhtml#ch10fn9">9</a></sup> congestion controls, Nagle, and qdiscs); others include:</p>
        <p class="footnote"><sup><a id="ch10fn9" href="ch10.xhtml#ch10fn9a">9</a></sup>Note that TSO appears twice in the diagram: first, after Pacing to build a super packet, and then in the NIC for final segmentation.</p>
        <ul class="sq">
        <li><p class="bull"><strong>Pacing</strong>: This controls when to send packets, spreading out transmissions (pacing) to avoid bursts that may hurt performance (this may help avoid TCP micro-bursts that can lead to queueing delay, or even cause network switches to drop packets. It may also help with the <em>incast</em> problem, when many end points transmit to one at the same time <a href="ch10.xhtml#ch10ref21">[Fritchie 12]</a>).</p></li>
        <li><p class="bull"><strong>TCP Small Queues (TSQ)</strong>: This controls (reduces) how much is queued by the network stack to avoid problems including bufferbloat [Bufferbloat 20].</p></li>
        <li><p class="bull"><strong>Byte Queue Limits (BQL)</strong>: These automatically size the driver queues large enough to avoid starvation, but also small enough to reduce the maximum latency of queued packets, and to avoid exhausting NIC TX descriptors <a href="ch10.xhtml#ch10ref22">[Hrubý 12]</a>. It works by pausing the addition of packets to the driver queue when necessary, and was added in Linux 3.3 <a href="ch10.xhtml#ch10ref26">[Siemon 13]</a>.</p></li>
        <li><p class="bull"><strong>Earliest Departure Time (EDT)</strong>: This uses a timing wheel instead of a queue to order packets sent to the NIC. Timestamps are set on every packet based on policy and rate configuration. This was added in Linux 4.20, and has BQL- and TSQ-like capabilities <a href="ch10.xhtml#ch10ref38">[Jacobson 18]</a>.</p></li>
        </ul>
        <p class="noindent">These algorithms often work in combination to improve performance. A TCP sent packet can be processed by any of the congestion controls, TSO, TSQ, pacing, and queueing disciplines, before it ever arrives at the NIC <a href="ch10.xhtml#ch10ref27">[Cheng 16]</a>.</p>
        </section>
        </section>
        </section>
        <section>
        <h3 class="h3" id="ch10lev5">10.5 Methodology</h3>
        <p class="noindent">This section describes methodologies and exercises for network analysis and tuning. <a href="ch10.xhtml#ch10tab02">Table 10.2</a> summarizes the topics.</p>
        <figure class="table" id="ch10tab02">
        <figcaption>
        <p class="title-t"><span epub:type="pagebreak" id="page_525"></span><span class="pd_ashf">Table 10.2</span> <strong>Network performance methodologies</strong></p>
        </figcaption>
        <table class="all">
        <thead>
        <tr>
        <th class="th"><p class="thead"><strong>Section</strong></p></th>
        <th class="th"><p class="thead"><strong>Methodology</strong></p></th>
        <th class="th"><p class="thead"><strong>Types</strong></p></th>
        </tr>
        </thead>
        <tbody>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10lev5sec1">10.5.1</a></p></td>
        <td class="border"><p class="tab-para">Tools method</p></td>
        <td class="border"><p class="tab-para">Observational analysis</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10lev5sec2">10.5.2</a></p></td>
        <td class="border"><p class="tab-para">USE method</p></td>
        <td class="border"><p class="tab-para">Observational analysis</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10lev5sec3">10.5.3</a></p></td>
        <td class="border"><p class="tab-para">Workload characterization</p></td>
        <td class="border"><p class="tab-para">Observational analysis, capacity planning</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10lev5sec4">10.5.4</a></p></td>
        <td class="border"><p class="tab-para">Latency analysis</p></td>
        <td class="border"><p class="tab-para">Observational analysis</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10lev5sec5">10.5.5</a></p></td>
        <td class="border"><p class="tab-para">Performance monitoring</p></td>
        <td class="border"><p class="tab-para">Observational analysis, capacity planning</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10lev5sec6">10.5.6</a></p></td>
        <td class="border"><p class="tab-para">Packet sniffing</p></td>
        <td class="border"><p class="tab-para">Observational analysis</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10lev5sec7">10.5.7</a></p></td>
        <td class="border"><p class="tab-para">TCP analysis</p></td>
        <td class="border"><p class="tab-para">Observational analysis</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10lev5sec8">10.5.8</a></p></td>
        <td class="border"><p class="tab-para">Static performance tuning</p></td>
        <td class="border"><p class="tab-para">Observational analysis, capacity planning</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10lev5sec9">10.5.9</a></p></td>
        <td class="border"><p class="tab-para">Resource controls</p></td>
        <td class="border"><p class="tab-para">Tuning</p></td>
        </tr>
        <tr>
        <td><p class="tab-para"><a href="ch10.xhtml#ch10lev5sec10">10.5.10</a></p></td>
        <td><p class="tab-para">Micro-benchmarking</p></td>
        <td><p class="tab-para">Experimental analysis</p></td>
        </tr>
        </tbody>
        </table>
        </figure>
        <p class="noindent">See <a href="ch02.xhtml#ch02">Chapter 2</a>, <a href="ch02.xhtml#ch02">Methodologies</a>, for more strategies and the introduction to many of these.</p>
        <p class="noindent">These may be followed individually or used in combination. My suggestion is to use the following strategies to start with, in this order: performance monitoring, the USE method, static performance tuning, and workload characterization.</p>
        <p class="noindent"><a href="ch10.xhtml#ch10lev6">Section 10.6</a>, <a href="ch10.xhtml#ch10lev6">Observability Tools</a>, shows operating system tools for applying these methods.</p>
        <section>
        <h4 class="h4" id="ch10lev5sec1">10.5.1 Tools Method</h4>
        <p class="noindent">The tools method is a process of iterating over available tools, examining key metrics they provide. It may overlook issues for which the tools provide poor or no visibility, and it can be time-consuming to perform.</p>
        <p class="noindent">For networking, the tools method can involve checking:</p>
        <ul class="sq">
        <li><p class="bull"><strong><code>nstat/netstat -s</code></strong>: Look for a high rate of retransmits and out-of-order packets. What constitutes a “high” retransmit rate depends on the clients: an Internet-facing system with unreliable remote clients should have a higher retransmit rate than an internal system with clients in the same data center.</p></li>
        <li><p class="bull"><strong><code>ip -s link/netstat -i</code></strong>: Check interface error counters, including “errors,” “dropped,” “overruns.”</p></li>
        <li><p class="bull"><strong><code>ss -tiepm</code></strong>: Check for the limiter flag for important sockets to see what their bottleneck is, as well as other statistics showing socket health.</p></li>
        <li><p class="bull"><strong><code>nicstat</code>/<code>ip -s link</code></strong>: Check the rate of bytes transmitted and received. High throughput may be limited by a negotiated data link speed, or an external network throttle. It could also cause contention and delays between network users on the system.</p></li>
        <li><p class="bull"><strong><code>tcplife</code></strong>: Log TCP sessions with process details, duration (lifespan), and throughput statistics.</p></li>
        <li><p class="bull"><span epub:type="pagebreak" id="page_526"></span><strong><code>tcptop</code></strong>: Watch top TCP sessions live.</p></li>
        <li><p class="bull"><strong><code>tcpdump</code></strong>: While this can be expensive to use in terms of the CPU and storage costs, using tcpdump(8) for short periods may help you identify unusual network traffic or protocol headers.</p></li>
        <li><p class="bull"><strong>perf(1)/BCC/bpftrace</strong>: Inspect selected packets between the application and the wire, including examining kernel state.</p></li>
        </ul>
        <p class="noindent">If an issue is found, examine all fields from the available tools to learn more context. See <a href="ch10.xhtml#ch10lev6">Section 10.6</a>, <a href="ch10.xhtml#ch10lev6">Observability Tools</a>, for more about each tool. Other methodologies can identify more types of issues.</p>
        </section>
        <section>
        <h4 class="h4" id="ch10lev5sec2">10.5.2 USE Method</h4>
        <p class="noindent">The USE method is for quickly identifying bottlenecks and errors across all components. For each network interface, and in each direction—transmit (TX) and receive (RX)—check for:</p>
        <ul class="sq">
        <li><p class="bull"><strong>Utilization</strong>: The time the interface was busy sending or receiving frames</p></li>
        <li><p class="bull"><strong>Saturation</strong>: The degree of extra queueing, buffering, or blocking due to a fully utilized interface</p></li>
        <li><p class="bull"><strong>Errors</strong>: For receive: bad checksum, frame too short (less than the data link header) or too long, collisions (unlikely with switched networks); for transmit: late collisions (bad wiring)</p></li>
        </ul>
        <p class="noindent">Errors may be checked first, since they are typically quick to check and the easiest to interpret.</p>
        <p class="noindent">Utilization is not commonly provided by operating system or monitoring tools directly (nicstat(1) is an exception). It can be calculated as the current throughput divided by the current negotiated speed, for each direction (RX, TX). The current throughput should be measured as bytes per second on the network, including all protocol headers.</p>
        <p class="noindent">For environments that implement network bandwidth limits (resource controls), as occurs in some cloud computing environments, network utilization may need to be measured in terms of the imposed limit, in addition to the physical limit.</p>
        <p class="noindent">Saturation of the network interface is difficult to measure. Some network buffering is normal, as applications can send data much more quickly than an interface can transmit it. It may be possible to measure as the time application threads spend blocked on network sends, which should increase as saturation increases. Also check if there are other kernel statistics more closely related to interface saturation, for example, Linux “overruns.” Note that Linux uses BQL to regulate the NIC queue size, which helps avoid NIC saturation.</p>
        <p class="noindent">Retransmits at the TCP level are usually readily available as statistics and can be an indicator of network saturation. However, they are measured across the network between the server and its clients and could be caused by problems at any hop.</p>
        <p class="noindent">The USE method can also be applied to network controllers, and the transports between them and the processors. Since observability tools for these components are sparse, it may be easier to infer metrics based on network interface statistics and topology. For example, if network controller A houses ports A0 <span epub:type="pagebreak" id="page_527"></span>and A1, the network controller throughput can be calculated as the sum of the interface throughputs A0 + A1. With a known maximum throughput, utilization of the network controller can then be calculated.</p>
        </section>
        <section>
        <h4 class="h4" id="ch10lev5sec3">10.5.3 Workload Characterization</h4>
        <p class="noindent">Characterizing the load applied is an important exercise when capacity planning, benchmarking, and simulating workloads. It can also lead to some of the largest performance gains by identifying unnecessary work that can be eliminated.</p>
        <p class="noindent">The following are the most basic characteristics to measure:</p>
        <ul class="sq">
        <li><p class="bull"><strong>Network interface throughput</strong>: RX and TX, bytes per second</p></li>
        <li><p class="bull"><strong>Network interface IOPS</strong>: RX and TX, frames per second</p></li>
        <li><p class="bull"><strong>TCP connection rate</strong>: Active and passive, connections per second</p></li>
        </ul>
        <p class="noindent">The terms <em>active</em> and <em>passive</em> were described in the Three-Way Handshake section of <a href="ch10.xhtml#ch10lev4sec1">Section 10.4.1</a>, <a href="ch10.xhtml#ch10lev4sec1">Protocols</a>.</p>
        <p class="noindent">These characteristics can vary over time, as usage patterns change throughout the day. Monitoring over time is described in <a href="ch10.xhtml#ch10lev5sec5">Section 10.5.5</a>, <a href="ch10.xhtml#ch10lev5sec5">Performance Monitoring</a>.</p>
        <p class="noindent">Here is an example workload description, to show how these attributes can be expressed together:</p>
        <p class="uln">The network throughput varies based on users and performs more writes (TX) than reads (RX). The peak write rate is 200 Mbytes/s and 210,000 packets/s, and the peak read rate is 10 Mbytes/s with 70,000 packets/s. The inbound (passive) TCP connection rate reaches 3,000 connections/s.</p>
        <p class="noindent">Apart from describing these characteristics system-wide, they can also be expressed per interface. This allows interface bottlenecks to be determined, if the throughput can be observed to have reached line rate. If network bandwidth limits (resource controls) are present, they may throttle network throughput before line rate is reached.</p>
        <section>
        <h5 class="h5" id="ch10lev3_26">Advanced Workload Characterization/Checklist</h5>
        <p class="noindent">Additional details may be included to characterize the workload. These have been listed here as questions for consideration, which may also serve as a checklist when studying CPU issues thoroughly:</p>
        <ul class="sq">
        <li><p class="bull">What is the average packet size? RX, TX?</p></li>
        <li><p class="bull">What is the protocol breakdown for each layer? For transport protocols: TCP, UDP (which can include QUIC).</p></li>
        <li><p class="bull">What TCP/UDP ports are active? Bytes per second, connections per second?</p></li>
        <li><p class="bull">What are the broadcast and multicast packet rates?</p></li>
        <li><p class="bull">Which processes are actively using the network?</p></li>
        </ul>
        <p class="noindent"><span epub:type="pagebreak" id="page_528"></span>The sections that follow answer some of these questions. See <a href="ch02.xhtml#ch02">Chapter 2</a>, <a href="ch02.xhtml#ch02">Methodologies</a>, for a higher-level summary of this methodology and the characteristics to measure (who, why, what, how).</p>
        </section>
        </section>
        <section>
        <h4 class="h4" id="ch10lev5sec4">10.5.4 Latency Analysis</h4>
        <p class="noindent">Various times (latencies) can be studied to help understand and express network performance. Some were introduced in <a href="ch10.xhtml#ch10lev3sec5">Section 10.3.5</a>, <a href="ch10.xhtml#ch10lev3sec5">Latency</a>, and a longer list is provided as <a href="ch10.xhtml#ch10tab03">Table 10.3</a>. Measure as many of these as you can to narrow down the real source of latency.</p>
        <figure class="table" id="ch10tab03">
        <figcaption>
        <p class="title-t"><span class="pd_ashf">Table 10.3</span> <strong>Network latencies</strong></p>
        </figcaption>
        <table class="all">
        <thead>
        <tr>
        <th class="th"><p class="thead"><strong>Latency</strong></p></th>
        <th class="th"><p class="thead"><strong>Description</strong></p></th>
        </tr>
        </thead>
        <tbody>
        <tr>
        <td class="border"><p class="tab-para">Name resolution latency</p></td>
        <td class="border"><p class="tab-para">The time for a host to be resolved to an IP address, usually by DNS resolution—a common source of performance issues.</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">Ping latency</p></td>
        <td class="border"><p class="tab-para">The time from an ICMP echo request to a response. This measures the network and kernel stack handling of the packet on each host.</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">TCP connection initialization latency</p></td>
        <td class="border"><p class="tab-para">The time from when a SYN is sent to when the SYN,ACK is received. Since no applications are involved, this measures the network and kernel stack latency on each host, similar to ping latency, with some additional kernel processing for the TCP session. TCP Fast Open (TFO) may be used to reduce this latency.</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">TCP first-byte latency</p></td>
        <td class="border"><p class="tab-para">Also known as the time-to-first-byte latency (TTFB), this measures the time from when a connection is established to when the first data byte is received by the client. This includes CPU scheduling and application think time for the host, making it a more a measure of application performance and current load than TCP connection latency.</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">TCP retransmits</p></td>
        <td class="border"><p class="tab-para">If present, can add thousands of milliseconds of latency to network I/O.</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">TCP TIME_WAIT latency</p></td>
        <td class="border"><p class="tab-para">The duration that locally closed TCP sessions are left waiting for late packets.</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">Connection/session lifespan</p></td>
        <td class="border"><p class="tab-para">The duration of a network connection from initialization to close. Some protocols like HTTP can use a keep-alive strategy, leaving connections open and idle for future requests, to avoid the overheads and latency of repeated connection establishment.</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">System call send/ receive latency</p></td>
        <td class="border"><p class="tab-para">Time for the socket read/write calls (any syscalls that read/write to sockets, including read(2), write(2), recv(2), send(2), and variants).</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">System call connect latency</p></td>
        <td class="border"><p class="tab-para">For connection establishment; note that some applications perform this as a non-blocking syscall.</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">Network round-trip time</p></td>
        <td class="border"><p class="tab-para">The time for a network request to make a round-trip between endpoints. The kernel may use such measurements with congestion control algorithms.</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><span epub:type="pagebreak" id="page_529"></span>Interrupt latency</p></td>
        <td class="border"><p class="tab-para">Time from a network controller interrupt for a received packet to when it is serviced by the kernel.</p></td>
        </tr>
        <tr>
        <td><p class="tab-para">Inter-stack latency</p></td>
        <td><p class="tab-para">Time for a packet to move through the kernel TCP/IP stack.</p></td>
        </tr>
        </tbody>
        </table>
        </figure>
        <p class="noindent">Latency may be presented as:</p>
        <ul class="sq">
        <li><p class="bull"><strong>Per-interval averages</strong>: Best performed per client/server pair, to isolate differences in the intermediate network</p></li>
        <li><p class="bull"><strong>Full distributions</strong>: As histograms or heat maps</p></li>
        <li><p class="bull"><strong>Per-operation latency</strong>: Listing details for each event, including source and destination IP addresses</p></li>
        </ul>
        <p class="noindent">A common source of issues is the presence of latency outliers caused by TCP retransmits. These can be identified using full distributions or per-operation latency tracing, including by filtering for a minimum latency threshold.</p>
        <p class="noindent">Latencies may be measured using tracing tools and, for some latencies, socket options. On Linux, the socket options include SO_TIMESTAMP for incoming packet time (and SO_TIMESTAMPNS for nanosecond resolution) and SO_TIMESTAMPING for per-event timestamps <a href="ch10.xhtml#ch10ref60">[Linux 20j]</a>. SO_TIMESTAMPING can identify transmission delays, network round-trip time, and inter-stack latencies; this can be especially helpful when analyzing complex packet latency involving tunneling <a href="ch10.xhtml#ch10ref49">[Hassas Yeganeh 19]</a>.</p>
        <p class="noindent">Note that some sources of extra latency are transient and only occur during system load. For more realistic measurements of network latency, it is important to measure not only an idle system, but also a system under load.</p>
        </section>
        <section>
        <h4 class="h4" id="ch10lev5sec5">10.5.5 Performance Monitoring</h4>
        <p class="noindent">Performance monitoring can identify active issues and patterns of behavior over time, including daily patterns of end users, and scheduled activities including network backups.</p>
        <p class="noindent">Key metrics for network monitoring are</p>
        <ul class="sq">
        <li><p class="bull"><strong>Throughput</strong>: Network interface bytes per second for both receive and transmit, ideally for each interface</p></li>
        <li><p class="bull"><strong>Connections</strong>: TCP connections per second, as another indication of network load</p></li>
        <li><p class="bull"><strong>Errors</strong>: Including dropped packet counters</p></li>
        <li><p class="bull"><strong>TCP retransmits</strong>: Also useful to record for correlation with network issues</p></li>
        <li><p class="bull"><strong>TCP out-of-order pack</strong><em>ets: Can also</em> cause performance problems</p></li>
        </ul>
        <p class="noindent">For environments that implement network bandwidth limits (resource controls), as in some cloud computing environments, statistics related to the imposed limits may also be collected.</p>
        </section>
        <section>
        <h4 class="h4" id="ch10lev5sec6"><span epub:type="pagebreak" id="page_530"></span>10.5.6 Packet Sniffing</h4>
        <p class="noindent">Packet sniffing (aka <em>packet capture</em>) involves capturing the packets from the network so that their protocol headers and data can be inspected on a packet-by-packet basis. For observational analysis this may be the last resort, as it can be expensive to perform in terms of CPU and storage overhead. Network kernel code paths are typically cycle-optimized, since they need to handle up to millions of packets per second and are sensitive to any extra overhead. To reduce this overhead, ring buffers may be used by the kernel to pass packet data to the user-level trace tool via a shared memory map—for example,<sup><a id="ch10fn10a" href="ch10.xhtml#ch10fn10">10</a></sup> using BPF with perf(1)’s output ring buffer, and also using AF_XDP <a href="ch10.xhtml#ch10ref61">[Linux 20k]</a>. A different way to solve overhead is to use an out-of-band packet sniffer: a separate server connected to a “tap” or “mirror” port of a switch. Public cloud providers such as Amazon and Google provide this as a service <a href="ch10.xhtml#ch10ref43">[Amazon 19]</a><a href="ch10.xhtml#ch10ref56">[Google 20b]</a>.</p>
        <p class="footnote1"><sup><a id="ch10fn10" href="ch10.xhtml#ch10fn10a">10</a></sup>Another option is to use PF_RING instead of the per-packet PF_PACKET, although PF_RING has not been included in the Linux kernel <a href="ch10.xhtml#ch10ref15">[Deri 04]</a>.</p>
        <p class="noindent">Packet sniffing typically involves capturing packets to a file, and then analyzing that file in different ways. One way is to produce a log, which can contain the following for each packet:</p>
        <ul class="sq">
        <li><p class="bull">Timestamp</p></li>
        <li><p class="bull">Entire packet, including</p>
        <ul class="sq-i">
        <li><p class="bull">All protocol headers (e.g., Ethernet, IP, TCP)</p></li>
        <li><p class="bull">Partial or full payload data</p></li>
        </ul></li>
        <li><p class="bull">Metadata: number of packets, number of drops</p></li>
        <li><p class="bull">Interface name</p></li>
        </ul>
        <p class="noindent">As an example of packet capture, the following shows the default output of the Linux tcpdump(8) tool:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg530a" id="pg530">Click here to view code image</a></p>
        <pre class="pretb"># <strong>tcpdump -ni eth4</strong>
        tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
        listening on eth4, link-type EN10MB (Ethernet), capture size 65535 bytes
        01:20:46.769073 IP 10.2.203.2.22 &gt; 10.2.0.2.33771: Flags [P.], seq
        4235343542:4235343734, ack 4053030377, win 132, options [nop,nop,TS val 328647671 ecr
        2313764364], length 192
        01:20:46.769470 IP 10.2.0.2.33771 &gt; 10.2.203.2.22: Flags [.], ack 192, win 501,
        options [nop,nop,TS val 2313764392 ecr 328647671], length 0
        01:20:46.787673 IP 10.2.203.2.22 &gt; 10.2.0.2.33771: Flags [P.], seq 192:560, ack 1,
        win 132, options [nop,nop,TS val 328647672 ecr 2313764392], length 368
        01:20:46.788050 IP 10.2.0.2.33771 &gt; 10.2.203.2.22: Flags [.], ack 560, win 501,
        options [nop,nop,TS val 2313764394 ecr 328647672], length 0
        01:20:46.808491 IP 10.2.203.2.22 &gt; 10.2.0.2.33771: Flags [P.], seq 560:896, ack 1,
        win 132, options [nop,nop,TS val 328647674 ecr 2313764394], length 336
        [...]</pre>
        <p class="noindent"><span epub:type="pagebreak" id="page_531"></span>This output has a line summarizing each packet, including details of the IP addresses, TCP ports, and other TCP header details. This can be used to debug a variety of issues including message latency and missing packets.</p>
        <p class="noindent">Because packet capture can be a CPU-expensive activity, most implementations include the ability to drop events instead of capturing them when overloaded. The count of dropped packets may be included in the log.</p>
        <p class="noindent">Apart from the use of ring buffers to reduce overhead, packet capture implementations commonly allow a filtering expression to be supplied by the user and perform this filtering in the kernel. This reduces overhead by not transferring unwanted packets to user level. The filter expression is typically optimized using Berkeley Packet Filter (BPF), which compiles the expression to BPF bytecode that can be JIT-compiled to machine code by the kernel. In recent years, BPF has been extended in Linux to become a general-purpose execution environment, which powers many observability tools: see <a href="ch03.xhtml#ch03">Chapter 3</a>, <a href="ch03.xhtml#ch03">Operating Systems</a>, <a href="ch03.xhtml#ch03lev4sec4">Section 3.4.4</a>, <a href="ch03.xhtml#ch03lev4sec4">Extended BPF</a>, and <a href="ch15.xhtml#ch15">Chapter 15</a>, <a href="ch15.xhtml#ch15">BPF</a>.</p>
        </section>
        <section>
        <h4 class="h4" id="ch10lev5sec7">10.5.7 TCP Analysis</h4>
        <p class="noindent">Apart from what was covered in <a href="ch10.xhtml#ch10lev5sec4">Section 10.5.4</a>, <a href="ch10.xhtml#ch10lev5sec4">Latency Analysis</a>, other specific TCP behavior can be investigated, including:</p>
        <ul class="sq">
        <li><p class="bull">Usage of TCP (socket) send/receive buffers</p></li>
        <li><p class="bull">Usage of TCP backlog queues</p></li>
        <li><p class="bull">Kernel drops due to the backlog queue being full</p></li>
        <li><p class="bull">Congestion window size, including zero-size advertisements</p></li>
        <li><p class="bull">SYNs received during a TCP TIME_WAIT interval</p></li>
        </ul>
        <p class="noindent">The last behavior can become a scalability problem when a server is connecting frequently to another on the same destination port, using the same source and destination IP addresses. The only distinguishing factor for each connection is the client source port—the <em>ephemeral port</em>—which for TCP is a 16-bit value and may be further constrained by operating system parameters (minimum and maximum). Combined with the TCP TIME_WAIT interval, which may be 60 seconds, a high rate of connections (more than 65,536 during 60 seconds) can encounter a clash for new connections. In this scenario, a SYN is sent while that ephemeral port is still associated with a previous TCP session that is in TIME_WAIT, and the new SYN may be rejected if it is misidentified as part of the old connection (a collision). To avoid this issue, the Linux kernel attempts to reuse or recycle connections quickly (which usually works well). The use of multiple IP addresses by the server is another possible solution, as is the SO_LINGER socket option with a low linger time.</p>
        </section>
        <section>
        <h4 class="h4" id="ch10lev5sec8">10.5.8 Static Performance Tuning</h4>
        <p class="noindent">Static performance tuning focuses on issues of the configured environment. For network performance, examine the following aspects of the static configuration:</p>
        <ul class="sq">
        <li><p class="bull">How many network interfaces are available for use? Are currently in use?</p></li>
        <li><p class="bull">What is the maximum speed of the network interfaces?</p></li>
        <li><p class="bull"><span epub:type="pagebreak" id="page_532"></span>What is the currently negotiated speed of the network interfaces?</p></li>
        <li><p class="bull">Are network interfaces negotiated as half or full duplex?</p></li>
        <li><p class="bull">What MTU is configured for the network interfaces?</p></li>
        <li><p class="bull">Are network interfaces trunked?</p></li>
        <li><p class="bull">What tunable parameters exist for the device driver? IP layer? TCP layer?</p></li>
        <li><p class="bull">Have any tunable parameters been changed from the defaults?</p></li>
        <li><p class="bull">How is routing configured? What is the default gateway?</p></li>
        <li><p class="bull">What is the maximum throughput of network components in the data path (all components, including switch and router backplanes)?</p></li>
        <li><p class="bull">What is the maximum MTU for the datapath and does fragmentation occur?</p></li>
        <li><p class="bull">Are any wireless connections in the data path? Are they suffering interference?</p></li>
        <li><p class="bull">Is forwarding enabled? Is the system acting as a router?</p></li>
        <li><p class="bull">How is DNS configured? How far away is the server?</p></li>
        <li><p class="bull">Are there known performance issues (bugs) with the version of the network interface firmware, or any other network hardware?</p></li>
        <li><p class="bull">Are there known performance issues (bugs) with the network device driver? Kernel TCP/IP stack?</p></li>
        <li><p class="bull">What firewalls are present?</p></li>
        <li><p class="bull">Are there software-imposed network throughput limits present (resource controls)? What are they?</p></li>
        </ul>
        <p class="noindent">The answers to these questions may reveal configuration choices that have been overlooked.</p>
        <p class="noindent">The last question is especially relevant for cloud computing environments, where there may be imposed limits on network throughput.</p>
        </section>
        <section>
        <h4 class="h4" id="ch10lev5sec9">10.5.9 Resource Controls</h4>
        <p class="noindent">The operating system may provide controls to limit network resources for types of connections, processes, or groups of processes. These can include the following types of controls:</p>
        <ul class="sq">
        <li><p class="bull"><strong>Network bandwidth limits</strong>: A permitted bandwidth (maximum throughput) for different protocols or applications, applied by the kernel.</p></li>
        <li><p class="bull"><strong>IP quality of service (QoS)</strong>: The prioritization of network traffic, performed by network components (e.g., routers). This can be implemented in different ways: the IP header includes type-of-service (ToS) bits, including a priority; those bits have since been redefined for newer QoS schemes, including Differentiated Services (see <a href="ch10.xhtml#ch10lev4sec1">Section 10.4.1</a>, <a href="ch10.xhtml#ch10lev4sec1">Protocols</a>, under the heading IP). There may be other priorities implemented by other protocol layers, for the same purpose.</p></li>
        <li><p class="bull"><strong>Packet latency</strong>: Additional packet latency (e.g., using Linux tc-netem(8)), which can be used to simulate other networks when testing performance.</p></li>
        </ul>
        <p class="noindent"><span epub:type="pagebreak" id="page_533"></span>Your network may have a mix of traffic that can be classified as low or high priority. Low priority may include the transfer of backups and performance-monitoring traffic. High priority may be the traffic between the production server and clients. Either resource control scheme can be used to throttle the low-priority traffic, producing more favorable performance for the high-priority traffic.</p>
        <p class="noindent">How these work is implementation-specific: see <a href="ch10.xhtml#ch10lev8">Section 10.8</a>, <a href="ch10.xhtml#ch10lev8">Tuning</a>.</p>
        </section>
        <section>
        <h4 class="h4" id="ch10lev5sec10">10.5.10 Micro-Benchmarking</h4>
        <p class="noindent">There are many benchmark tools for networking. They are especially useful when investigating throughput issues for a distributed application environment, to confirm that the network can at least achieve the expected network throughput. If it cannot, network performance can be investigated via a network micro-benchmark tool, which is typically much less complex and faster to debug than the application. After the network has been tuned to the desired speed, attention can return to the application.</p>
        <p class="noindent">Typical factors that can be tested include:</p>
        <ul class="sq">
        <li><p class="bull"><strong>Direction</strong>: Send or receive</p></li>
        <li><p class="bull"><strong>Protocol</strong>: TCP or UDP, and port</p></li>
        <li><p class="bull"><strong>Number of threads</strong></p></li>
        <li><p class="bull"><strong>Buffer size</strong></p></li>
        <li><p class="bull"><strong>Interface MTU size</strong></p></li>
        </ul>
        <p class="noindent">Faster network interfaces, such as 100 Gbits/s, may require multiple client threads to be driven to maximum bandwidth.</p>
        <p class="noindent">An example network micro-benchmark tool, iperf(1), is introduced in <a href="ch10.xhtml#ch10lev7sec4">Section 10.7.4</a>, <a href="ch10.xhtml#ch10lev7sec4">iperf</a>, and others are listed in <a href="ch10.xhtml#ch10lev7">Section 10.7</a>, <a href="ch10.xhtml#ch10lev7">Experimentation</a>.</p>
        </section>
        </section>
        <section>
        <h3 class="h3" id="ch10lev6">10.6 Observability Tools</h3>
        <p class="noindent">This section introduces network performance observability tools for Linux-based operating systems. See the previous section for strategies to follow when using them.</p>
        <p class="noindent">The tools in this section are listed in <a href="ch10.xhtml#ch10tab04">Table 10.4</a>.</p>
        <figure class="table" id="ch10tab04">
        <figcaption>
        <p class="title-t"><span class="pd_ashf">Table 10.4</span> <strong>Network observability tools</strong></p>
        </figcaption>
        <table class="all">
        <thead>
        <tr>
        <th class="th"><p class="thead"><strong>Section</strong></p></th>
        <th class="th"><p class="thead"><strong>Tool</strong></p></th>
        <th class="th"><p class="thead"><strong>Description</strong></p></th>
        </tr>
        </thead>
        <tbody>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10lev6sec1">10.6.1</a></p></td>
        <td class="border"><p class="tab-para">ss</p></td>
        <td class="border"><p class="tab-para">Socket statistics</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10lev6sec2">10.6.2</a></p></td>
        <td class="border"><p class="tab-para">ip</p></td>
        <td class="border"><p class="tab-para">Network interface and route statistics</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10lev6sec3">10.6.3</a></p></td>
        <td class="border"><p class="tab-para">ifconfig</p></td>
        <td class="border"><p class="tab-para">Network interface statistics</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10lev6sec4">10.6.4</a></p></td>
        <td class="border"><p class="tab-para">nstat</p></td>
        <td class="border"><p class="tab-para">Network stack statistics</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><span epub:type="pagebreak" id="page_534"></span><a href="ch10.xhtml#ch10lev6sec5">10.6.5</a></p></td>
        <td class="border"><p class="tab-para">netstat</p></td>
        <td class="border"><p class="tab-para">Various network stack and interface statistics</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10lev6sec6">10.6.6</a></p></td>
        <td class="border"><p class="tab-para">sar</p></td>
        <td class="border"><p class="tab-para">Historical statistics</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10lev6sec7">10.6.7</a></p></td>
        <td class="border"><p class="tab-para">nicstat</p></td>
        <td class="border"><p class="tab-para">Network interface throughput and utilization</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10lev6sec8">10.6.8</a></p></td>
        <td class="border"><p class="tab-para">ethtool</p></td>
        <td class="border"><p class="tab-para">Network interface driver statistics</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10lev6sec9">10.6.9</a></p></td>
        <td class="border"><p class="tab-para">tcplife</p></td>
        <td class="border"><p class="tab-para">Trace TCP session lifespans with connection details</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10lev6sec10">10.6.10</a></p></td>
        <td class="border"><p class="tab-para">tcptop</p></td>
        <td class="border"><p class="tab-para">Show TCP throughput by host and process</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10lev6sec11">10.6.11</a></p></td>
        <td class="border"><p class="tab-para">tcpretrans</p></td>
        <td class="border"><p class="tab-para">Trace TCP retransmits with address and TCP state</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10lev6sec12">10.6.12</a></p></td>
        <td class="border"><p class="tab-para">bpftrace</p></td>
        <td class="border"><p class="tab-para">TCP/IP stack tracing: connections, packets, drops, latency</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10lev6sec13">10.6.13</a></p></td>
        <td class="border"><p class="tab-para">tcpdump</p></td>
        <td class="border"><p class="tab-para">Network packet sniffer</p></td>
        </tr>
        <tr>
        <td><p class="tab-para"><a href="ch10.xhtml#ch10lev6sec14">10.6.14</a></p></td>
        <td><p class="tab-para">Wireshark</p></td>
        <td><p class="tab-para">Graphical network packet inspection</p></td>
        </tr>
        </tbody>
        </table>
        </figure>
        
        <p class="noindent">This is a selection of tools and capabilities to support <a href="ch10.xhtml#ch10lev5">Section 10.5</a>, <a href="ch10.xhtml#ch10lev5">Methodology</a>, beginning with traditional tools and statistics, then tracing tools, and finally packet capture tools. Some of the traditional tools are likely available on other Unix-like operating systems where they originated, including: ifconfig(8), netstat(8), and sar(1). The tracing tools are BPF-based, and use BCC and bpftrace frontends (<a href="ch15.xhtml#ch15">Chapter 15</a>); they are: socketio(8), tcplife(8), tcptop(8), and tcpretrans(8).</p>
        <p class="noindent">The first statistical tools covered are ss(8), ip(8), and nstat(8), as these are from the iproute2 package that is maintained by the network kernel engineers. Tools from this package are most likely to support the latest Linux kernel features. Similar tools from the net-tools package, namely ifconfig(8) and netstat(8), are also covered as they are in widespread use, although Linux kernel network engineers consider these deprecated.</p>
        <section>
        <h4 class="h4" id="ch10lev6sec1">10.6.1 ss</h4>
        <p class="noindent">ss(8) is a socket statistics tool that summarizes open sockets. The default output provides high-level information about sockets, for example:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg534a" id="pg534">Click here to view code image</a></p>
        <pre class="pretb"># <strong>ss</strong>
        Netid State     Recv-Q  Send-Q    Local Address:Port      Peer Address:Port
        [...]
        tcp   ESTAB     0       0         100.85.142.69:65264    100.82.166.11:6001
        tcp   ESTAB     0       0         100.85.142.69:6028     100.82.16.200:6101
        [...]</pre>
        <p class="noindent">This output is a snapshot of the current state. The first column shows the protocol used by the sockets: these are TCP. Since this output lists all established connections with IP address information, it can be used to characterize the current workload, and answer questions including how many client connections are open, how many concurrent connections there are to a dependency service, etc.</p>
        <p class="noindent"><span epub:type="pagebreak" id="page_535"></span>Similar per-socket information is available using the older netstat(8) tool. ss(8), however, can show much more information when using options. For example, showing TCP sockets only (<code>-t</code>), with TCP internal info (<code>-i</code>), extended socket info (<code>-e</code>), process info (<code>-p</code>), and memory usage (<code>-m</code>):</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg535a" id="pg535">Click here to view code image</a></p>
        <pre class="pretb"># <strong>ss -tiepm</strong>
        State     Recv-Q  Send-Q    Local Address:Port      Peer Address:Port
        
        ESTAB     0       0         <strong>100.85.142.69:65264</strong>     <strong>100.82.166.11:6001</strong>
         users:((<strong>"java",pid=4195,fd=10865</strong>)) uid:33 ino:2009918 sk:78 &lt;-&gt;
                 skmem:(r0,rb12582912,t0,tb12582912,f266240,w0,o0,bl0,d0) ts sack bbr ws
        cale:9,9 <strong>rto:204 rtt:0.159/0.009</strong> ato:40 <strong>mss:1448</strong> pmtu:1500 rcvmss:1448 advmss:14
        48 <strong>cwnd:152 bytes_acked:347681 bytes_received:1798733</strong> segs_out:582 segs_in:1397
        data_segs_out:294 data_segs_in:1318 <strong>bbr:</strong>(bw:328.6Mbps,mrtt:0.149,pacing_gain:2.8
        8672,cwnd_gain:2.88672) send 11074.0Mbps lastsnd:1696 lastrcv:1660 lastack:1660
        <strong>pacing_rate 2422.4Mbps</strong> delivery_rate 328.6Mbps <strong>app_limited</strong> busy:16ms rcv_rtt:39.
        822 rcv_space:84867 rcv_ssthresh:3609062 <strong>minrtt:0.139</strong>
        [...]</pre>
        <p class="noindent">Highlighted in bold are the endpoint addresses and the following details:</p>
        <ul class="sq">
        <li><p class="bull"><strong><code>"java",pid=4195</code></strong>: Process name “java”, PID 4195.</p></li>
        <li><p class="bull"><strong><code>fd=10865</code></strong>: File descriptor 10865 (for PID 4195).</p></li>
        <li><p class="bull"><strong><code>rto:204</code></strong>: TCP retransmission timeout: 204 milliseconds.</p></li>
        <li><p class="bull"><strong><code>rtt:0.159/0.009</code></strong>: Average round-trip time is 0.159 milliseconds, with 0.009 milliseconds mean deviation.</p></li>
        <li><p class="bull"><strong><code>mss:1448</code></strong>: Maximum segment size: 1448 bytes.</p></li>
        <li><p class="bull"><strong><code>cwnd:152</code></strong>: Congestion window size: 152 × MSS.</p></li>
        <li><p class="bull"><strong><code>bytes_acked:347681</code></strong>: 340 Kbytes successfully transmitted.</p></li>
        <li><p class="bull"><strong><code>bytes_received:1798733</code></strong>: 1.72 Mbytes received.</p></li>
        <li><p class="bull"><strong><code>bbr:...</code></strong>: BBR congestion control statistics.</p></li>
        <li><p class="bull"><strong><code>pacing_rate 2422.4Mbps</code></strong>: Pacing rate of 2422.4 Mbps.</p></li>
        <li><p class="bull"><strong><code>app_limited</code></strong>: Shows that the congestion window is not fully utilized, suggesting that the connection is application-bound.</p></li>
        <li><p class="bull"><strong><code>minrtt:0.139</code></strong>: Minimum round-trip time in millisecond. Compare to the average and mean deviation (listed earlier) to get an idea of network variation and congestion.</p></li>
        </ul>
        <p class="noindent">This particular connection is flagged as application-limited (<code>app_limited</code>), with a low RTT to the remote endpoint, and low total bytes transferred. The possible “limited” flags ss(1) can print are:</p>
        <ul class="sq">
        <li><p class="bull"><strong><code>app_limited</code></strong>: Application limited.</p></li>
        <li><p class="bull"><strong><code>rwnd_limited:Xms</code></strong>: Limited by the receive window. Includes the time limited in milliseconds.</p></li>
        <li><p class="bull"><span epub:type="pagebreak" id="page_536"></span><strong><code>sndbuf_limited:Xms</code></strong>: Limited by the send buffer. Includes the time limited in milliseconds.</p></li>
        </ul>
        <p class="noindent">One detail missing from the output is the age of the connection, which is needed to calculate the average throughput. A workaround I’ve found is to use the change timestamp on the file descriptor file in /proc: for this connection, I would run stat(1) on /proc/4195/fd/10865.</p>
        <section>
        <h5 class="h5" id="ch10lev3_27">netlink</h5>
        <p class="noindent">ss(8) reads these extended details from the netlink(7) interface, which operates via sockets of family AF_NETLINK to fetch information from the kernel. You can see this in action using strace(1) (see <a href="ch05.xhtml#ch05">Chapter 5</a>, <a href="ch05.xhtml#ch05">Applications</a>, <a href="ch05.xhtml#ch05lev5sec4">Section 5.5.4</a>, <a href="ch05.xhtml#ch05lev5sec4">strace</a>, for warnings on strace(1) overhead):</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg536-1a" id="pg536-1">Click here to view code image</a></p>
        <pre class="pretb"># <strong>strace -e sendmsg,recvmsg ss -t</strong>
        sendmsg(3, {msg_name={sa_family=<strong>AF_NETLINK</strong>, nl_pid=0, nl_groups=00000000},
        msg_namelen=12, msg_iov=[{iov_base={{len=72, type=<strong>SOCK_DIAG_BY_FAMILY</strong>,
        flags=NLM_F_REQUEST|NLM_F_DUMP, seq=123456, pid=0}, {sdiag_family=AF_INET,
        sdiag_protocol=<strong>IPPROTO_TCP</strong>, idiag_ext=1&lt;&lt;(INET_DIAG_MEMINFO-1)|...
        recvmsg(3, {msg_name={sa_family=<strong>AF_NETLINK</strong>, nl_pid=0, nl_groups=00000000},...
        [...]</pre>
        <p class="noindent">netstat(8) sources information using /proc/net files instead:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg536-2a" id="pg536-2">Click here to view code image</a></p>
        <pre class="pretb"># <strong>strace -e openat netstat -an</strong>
        [...]
        openat(AT_FDCWD, "<strong>/proc/net/tcp</strong>", O_RDONLY) = 3
        openat(AT_FDCWD, "<strong>/proc/net/tcp6</strong>", O_RDONLY) = 3
        [...]</pre>
        <p class="noindent">Because the /proc/net files are text, I’ve found them handy as a source for ad hoc reporting, requiring nothing more than awk(1) for processing. Serious monitoring tools should use the netlink(7) interface instead, which passes information in binary format and avoids the overhead of text parsing.</p>
        </section>
        </section>
        <section>
        <h4 class="h4" id="ch10lev6sec2">10.6.2 ip</h4>
        <p class="noindent">ip(8) is a tool for managing routing, network devices, interfaces, and tunnels. For observability, it can be used to print statistics on: link, address, route, etc. For example, printing extra statistics (<code>-s</code>) on interfaces (<code>link</code>):</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg536-3a" id="pg536-3">Click here to view code image</a></p>
        <pre class="pretb"># <strong>ip -s link</strong>
        1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT
        group default qlen 1000
            link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
            RX: bytes  packets  errors  dropped overrun mcast
            26550075   273178   0       0       0       0
            TX: bytes  packets  errors  dropped carrier collsns
            26550075   273178   0       0       0       0
        <span epub:type="pagebreak" id="page_537"></span>2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT
        group default qlen 1000
            link/ether 12:c0:0a:b0:21:b8 brd ff:ff:ff:ff:ff:ff
            RX: bytes  packets  errors  dropped overrun mcast
            512473039143 568704184 0       0       0       0
            TX: bytes  packets  errors  dropped carrier collsns
            573510263433 668110321 0       0       0       0</pre>
        <p class="noindent">Examining the configuration of all interfaces can be useful during static performance tuning, to check for misconfigurations. Error metrics are also included in the output: for receive (<code>RX</code>): receive errors, drops, and overruns; for transmit (<code>TX</code>): transmit errors, drops, carrier errors, and collisions. Such errors can be a source of performance issues and, depending on the error, may be caused by faulty network hardware. These are global counters showing all errors since the interface was activated (in network speak, it was brought “UP”).</p>
        <p class="noindent">Specifying the <code>-s</code> option twice (<code>-s -s</code>) provides even more statistics for error types.</p>
        <p class="noindent">Although ip(8) provides RX and TX byte counters, it does not include an option to print the current throughput over an interval. For that, use sar(1) (<a href="ch10.xhtml#ch10lev6sec6">Section 10.6.6</a>, <a href="ch10.xhtml#ch10lev6sec6">sar</a>).</p>
        <section>
        <h5 class="h5" id="ch10lev3_28">Route Table</h5>
        <p class="noindent">ip(1) does have observability for other networking components. For example, the route object shows the routing table:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg537a" id="pg537">Click here to view code image</a></p>
        <pre class="pretb"># <strong>ip route</strong>
        default via 100.85.128.1 dev eth0
        default via 100.85.128.1 dev eth0 proto dhcp src 100.85.142.69 metric 100
        100.85.128.0/18 dev eth0 proto kernel scope link src 100.85.142.69
        100.85.128.1 dev eth0 proto dhcp scope link src 100.85.142.69 metric 100</pre>
        <p class="noindent">Misconfigured routes can also be a source of performance problems (for example, when specific route entries were added by an administrator but are no longer needed, and now perform worse than the default route).</p>
        </section>
        <section>
        <h5 class="h5" id="ch10lev3_29">Monitoring</h5>
        <p class="noindent">Use the monitoring subcommand <code>ip monitor</code> to watch for netlink messages.</p>
        </section>
        </section>
        <section>
        <h4 class="h4" id="ch10lev6sec3">10.6.3 ifconfig</h4>
        <p class="noindent">The ifconfig(8) command is the traditional tool for interface administration, and can also list the configuration of all interfaces. The Linux version includes statistics with the output<sup><a id="ch10fn11a" href="ch10.xhtml#ch10fn11">11</a></sup>:</p>
        <p class="footnote1"><sup><a id="ch10fn11" href="ch10.xhtml#ch10fn11a">11</a></sup>It also shows a tunable parameter, <code>txqueuelen</code>, but not all drivers use this value (it calls a netdevice notifier with NETDEV_CHANGE_TX_QUEUE_LEN, which is not implemented by some drivers), and byte queue limits auto-tune the device queues.</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg538-1a" id="pg538-1">Click here to view code image</a></p>
        <pre class="pretb"><span epub:type="pagebreak" id="page_538"></span>$ <strong>ifconfig</strong>
        eth0      Link encap:Ethernet  HWaddr 00:21:9b:97:a9:bf
                  inet addr:10.2.0.2  Bcast:10.2.0.255  Mask:255.255.255.0
                  inet6 addr: fe80::221:9bff:fe97:a9bf/64 Scope:Link
                  UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
                  <strong>RX packets:933874764 errors:0 dropped:0 overruns:0 frame:0</strong>
                  <strong>TX packets:1090431029 errors:0 dropped:0 overruns:0 carrier:0</strong>
                  <strong>collisions:0</strong> txqueuelen:1000
                  <strong>RX bytes:584622361619 (584.6 GB)  TX bytes:537745836640 (537.7 GB)</strong>
                  Interrupt:36 Memory:d6000000-d6012800
        
        eth3      Link encap:Ethernet  HWaddr 00:21:9b:97:a9:c5
        [...]</pre>
        <p class="noindent">The counters are the same as those described for the ip(8) command.</p>
        <p class="noindent">On Linux, ifconfig(8) is considered obsolete, replaced by ip(8).</p>
        </section>
        <section>
        <h4 class="h4" id="ch10lev6sec4">10.6.4 nstat</h4>
        <p class="noindent">nstat(8) prints the various network metrics maintained by the kernel, with their SNMP names. For example, using <code>-s</code> to avoid resetting the counters:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg538-2a" id="pg538-2">Click here to view code image</a></p>
        <pre class="pretb"># <strong>nstat -s</strong>
        #kernel
        IpInReceives                    462657733          0.0
        IpInDelivers                    462657733          0.0
        IpOutRequests                   497050986          0.0
        IpOutDiscards                   42                 0.0
        IpFragOKs                       2298               0.0
        IpFragCreates                   13788              0.0
        IcmpInMsgs                      91                 0.0
        [...]
        TcpActiveOpens                  362997             0.0
        TcpPassiveOpens                 9663983            0.0
        TcpAttemptFails                 12718              0.0
        TcpEstabResets                  14591              0.0
        TcpInSegs                       462181482          0.0
        TcpOutSegs                      938958577          0.0
        TcpRetransSegs                  129212             0.0
        TcpOutRsts                      52362              0.0
        UdpInDatagrams                  476072             0.0
        UdpNoPorts                      88                 0.0
        <span epub:type="pagebreak" id="page_539"></span>UdpOutDatagrams                 476197             0.0
        UdpIgnoredMulti                 2                  0.0
        Ip6OutRequests                  29                 0.0
        [...]</pre>
        <p class="noindent">Key metrics include:</p>
        <ul class="sq">
        <li><p class="bull"><strong><code>IpInReceives</code></strong>: Inbound IP packets.</p></li>
        <li><p class="bull"><strong><code>IpOutRequests</code></strong>: Outbound IP packets.</p></li>
        <li><p class="bull"><strong><code>TcpActiveOpens</code></strong>: TCP active connections (the connect(2) socket syscall).</p></li>
        <li><p class="bull"><strong><code>TcpPassiveOpens</code></strong>: TCP passive connections (the accept(2) socket syscall).</p></li>
        <li><p class="bull"><strong><code>TcpInSegs</code></strong>: TCP inbound segments.</p></li>
        <li><p class="bull"><strong><code>TcpOutSegs</code></strong>: TCP outbound segments.</p></li>
        <li><p class="bull"><strong><code>TcpRetransSegs</code></strong>: TCP retransmitted segments. Compare with TcpOutSegs for the ratio of retransmits.</p></li>
        </ul>
        <p class="noindent">If the <code>-s</code> option is not used, the default behavior of nstat(8) is to reset the kernel counters. This can be useful, as you can then run nstat(8) a second time and see counts that spanned that interval, rather than totals since boot. If you had a network problem that could be reproduced with a command, then nstat(8) can be run before and after the command to show which counters changed.</p>
        <p class="noindent">If you forgot to use <code>-s</code> and have reset the counters by mistake, you can use <code>-rs</code> to set them back to their summary since boot values.</p>
        <p class="noindent">nstat(8) also has a daemon mode (<code>-d</code>) to collect interval statistics, which when used are shown in the last column.</p>
        </section>
        <section>
        <h4 class="h4" id="ch10lev6sec5">10.6.5 netstat</h4>
        <p class="noindent">The netstat(8) command reports various types of network statistics, based on the options used. It is like a multi-tool with several different functions. These include the following:</p>
        <ul class="sq">
        <li><p class="bull"><strong>(default)</strong>: Lists connected sockets</p></li>
        <li><p class="bull"><strong><code>-a</code></strong>: Lists information for all sockets</p></li>
        <li><p class="bull"><strong><code>-s</code></strong>: Network stack statistics</p></li>
        <li><p class="bull"><strong><code>-i</code></strong>: Network interface statistics</p></li>
        <li><p class="bull"><strong><code>-r</code></strong>: Lists the route table</p></li>
        </ul>
        <p class="noindent">Other options can modify the output, including <code>-n</code> to not resolve IP addresses to host names, and <code>-v</code> for verbose details where available.</p>
        <p class="noindent"><span epub:type="pagebreak" id="page_540"></span>Here is an example of netstat(8) interface statistics:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg540-1a" id="pg540-1">Click here to view code image</a></p>
        <pre class="pretb">$ <strong>netstat -i</strong>
        Kernel Interface table
        Iface    MTU     RX-OK RX-ERR RX-DRP RX-OVR      TX-OK TX-ERR TX-DRP TX-OVR Flg
        eth0    1500 933760207      0      0 0      1090211545      0      0      0 BMRU
        eth3    1500 718900017      0      0 0       587534567      0      0      0 BMRU
        lo     16436 21126497       0      0 0        21126497      0      0      0 LRU
        ppp5    1496     4225       0      0 0            3736      0      0      0 MOPRU
        ppp6    1496     1183       0      0 0            1143      0      0      0 MOPRU
        tun0    1500   695581       0      0 0          692378      0      0      0 MOPRU
        tun1    1462        0       0      0 0               4      0      0      0 PRU</pre>
        <p class="noindent">The columns include the network interface (<code>Iface</code>), <code>MTU</code>, and a series of metrics for receive (<code>RX-</code>) and transmit (<code>TX-</code>):</p>
        <ul class="sq">
        <li><p class="bull"><strong><code>-OK</code></strong>: Packets transferred successfully</p></li>
        <li><p class="bull"><strong><code>-ERR</code></strong>: Packet errors</p></li>
        <li><p class="bull"><strong><code>-DRP</code></strong>: Packet drops</p></li>
        <li><p class="bull"><strong><code>-OVR</code></strong>: Packet overruns</p></li>
        </ul>
        <p class="noindent">The packet drops and overruns are indications of network interface <em>saturation</em> and can be examined along with errors as part of the USE method.</p>
        <p class="noindent">The <code>-c</code> continuous mode can be used with <code>-i</code>, which prints these cumulative counters every second. This provides the data for calculating the rate of packets.</p>
        <p class="noindent">Here is an example of netstat(8) network stack statistics (truncated):</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg540-2a" id="pg540-2">Click here to view code image</a></p>
        
        
        <pre class="pretb">$ <strong>netstat -s</strong>
        <strong>Ip:</strong>
            Forwarding: 2
            454143446 <strong>total packets received</strong>
            0 <strong>forwarded</strong>
            0 incoming packets discarded
            454143446 incoming packets delivered
            487760885 requests sent out
            42 outgoing packets dropped
            2260 fragments received ok
            13560 fragments created
        <strong><a href="gloss.xhtml#glo_077">Icmp</a></strong>:
            91 ICMP messages received
        [...]
        <strong>Tcp</strong>:
            359286 <strong>active connection</strong> openings
            9463980 <strong>passive connection</strong> openings
            <span epub:type="pagebreak" id="page_541"></span>12527 failed connection attempts
            14323 connection resets received
            13545 connections established
            453673963 <strong>segments received</strong>
            922299281 <strong>segments sent out</strong>
            127247 <strong>segments retransmitted</strong>
            0 bad segments received
            51660 resets sent
        <strong><a href="gloss.xhtml#glo_186">Udp</a></strong>:
            469302 packets received
            88 packets to unknown port received
            0 packet receive errors
            469427 packets sent
            0 receive buffer errors
            0 send buffer errors
            IgnoredMulti: 2
        <strong>TcpExt</strong>:
            21 resets received for embryonic SYN_RECV sockets
            12252 <strong>packets pruned from receive queue because of socket buffer overrun</strong>
            201219 TCP sockets finished time wait in fast timer
            11727438 <strong>delayed acks sent</strong>
            1445 delayed acks further delayed because of locked socket
            Quick ack mode was activated 17624 times
            169257582 packet headers predicted
            76058392 acknowledgments not containing data payload received
            111925821 predicted acknowledgments
            TCPSackRecovery: 1703
            Detected reordering 876 times using SACK
            Detected reordering 19 times using time stamp
            2 congestion windows fully recovered without slow start
            19 congestion windows partially recovered using Hoe heuristic
            TCPDSACKUndo: 164
            88 congestion windows recovered without slow start after partial ack
            TCPLostRetransmit: 901
            TCPSackFailures: 31
            28248 <strong>fast retransmits</strong>
            709 retransmits in slow start
            TCPTimeouts: 12684
            TCPLossProbes: 73383
            TCPLossProbeRecovery: 132
            TCPSackRecoveryFail: 24
            805315 <strong>packets collapsed in receive queue due to low socket buffer</strong>
        [...]
            <span epub:type="pagebreak" id="page_542"></span><strong>TCPAutoCorking</strong>: 13520259
            TCPFromZeroWindowAdv: 257
            TCPToZeroWindowAdv: 257
            TCPWantZeroWindowAdv: 18941
            <strong>TCPSynRetrans</strong>: 24816
        [...]</pre>
        <p class="noindent">The output lists various network statistics, mostly from TCP, grouped by their protocol. Fortunately, many of these have long descriptive names, so their meaning may be obvious. A number of these statistics have been highlighted in bold, to show the kind of performance-related information available. Many of these require an advanced understanding of TCP behavior, including the newer features and algorithms that have been introduced in recent years. Some example statistics to look for:</p>
        <ul class="sq">
        <li><p class="bull">A high rate of forwarded versus total packets received: check that the server is supposed to be forwarding (routing) packets.</p></li>
        <li><p class="bull">Passive connection openings: this can be monitored to show load in terms of client connections.</p></li>
        <li><p class="bull">A high rate of segments retransmitted versus segments sent out: can show an unreliable network. This may be expected (Internet clients).</p></li>
        <li><p class="bull">TCPSynRetrans: shows retransmitted SYNs, which can be caused by the remote endpoint dropping SYNs from the listen backlog due to load.</p></li>
        <li><p class="bull">Packets pruned from the receive queue because of socket buffer overrun: This is a sign of network saturation and may be fixable by increasing socket buffers, provided there are sufficient system resources for the application to keep up.</p></li>
        </ul>
        <p class="noindent">Some of the statistic names include typos (e.g., <code>packetes rejected</code>). These can be problematic to simply fix, if other monitoring tools have been built upon the same output. Such tools should be better served by processing nstat(8) output, which uses standard SNMP names, or even better, to read the /proc sources for these statistics directly, which are /proc/net/snmp and /proc/net/netstat. For example:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg542-2a" id="pg542-2">Click here to view code image</a></p>
        <pre class="pretb">$ <strong>grep ^Tcp /proc/net/snmp</strong>
        Tcp: RtoAlgorithm RtoMin RtoMax MaxConn ActiveOpens PassiveOpens AttemptFails
        EstabResets CurrEstab InSegs OutSegs RetransSegs InErrs OutRsts InCsumErrors
        Tcp: 1 200 120000 -1 102378 126946 11940 19495 24 627115849 325815063 346455 5 24183
        0</pre>
        <p class="noindent">These /proc/net/snmp statistics also include the SNMP management information bases (MIBs). MIB documentation describes what each statistic is supposed to be (if the kernel has correctly implemented it). Extended statistics are in /proc/net/netstat.</p>
        <p class="noindent">An interval, in seconds, can be used with netstat(8) so that it continually prints the cumulative counters every interval. This output could then be post-processed to calculate the rate of each counter.</p>
        </section>
        <section>
        <h4 class="h4" id="ch10lev6sec6"><span epub:type="pagebreak" id="page_543"></span>10.6.6 sar</h4>
        <p class="noindent">The system activity reporter, sar(1), can be used to observe current activity and can be configured to archive and report historical statistics. It is introduced in <a href="ch04.xhtml#ch04">Chapter 4</a>, <a href="ch04.xhtml#ch04">Observability Tools</a>, and mentioned in other chapters as appropriate.</p>
        <p class="noindent">The Linux version provides network statistics via the following options:</p>
        <ul class="sq">
        <li><p class="bull"><strong><code>-n DEV</code></strong>: Network interface statistics</p></li>
        <li><p class="bull"><strong><code>-n EDEV</code></strong>: Network interface errors</p></li>
        <li><p class="bull"><strong><code>-n IP</code></strong>: IP datagram statistics</p></li>
        <li><p class="bull"><strong><code>-n EIP</code></strong>: IP error statistics</p></li>
        <li><p class="bull"><strong><code>-n TCP</code></strong>: TCP statistics</p></li>
        <li><p class="bull"><strong><code>-n ETCP</code></strong>: TCP error statistics</p></li>
        <li><p class="bull"><strong><code>-n SOCK</code></strong>: Socket usage</p></li>
        </ul>
        <p class="noindent">Statistics provided include those shown in <a href="ch10.xhtml#ch10tab05">Table 10.5</a>.</p>
        <figure class="table" id="ch10tab05">
        <figcaption>
        <p class="title-t"><span class="pd_ashf">Table 10.5</span> <strong>Linux sar network statistics</strong></p>
        </figcaption>
        <table class="all">
        <thead>
        <tr>
        <th class="th"><p class="thead"><strong>Option</strong></p></th>
        <th class="th"><p class="thead"><strong>Statistic</strong></p></th>
        <th class="th"><p class="thead"><strong>Description</strong></p></th>
        <th class="th"><p class="thead"><strong>Units</strong></p></th>
        </tr>
        </thead>
        <tbody>
        <tr>
        <td class="border"><p class="tab-para"><code>-n DEV</code></p></td>
        <td class="border"><p class="tab-para"><code>rxpkt/s</code></p></td>
        <td class="border"><p class="tab-para">Received packets</p></td>
        <td class="border"><p class="tab-para">Packets/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n DEV</code></p></td>
        <td class="border"><p class="tab-para"><code>txpkt/s</code></p></td>
        <td class="border"><p class="tab-para">Transmitted packets</p></td>
        <td class="border"><p class="tab-para">Packets/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n DEV</code></p></td>
        <td class="border"><p class="tab-para"><code>rxkB/s</code></p></td>
        <td class="border"><p class="tab-para">Received kilobytes</p></td>
        <td class="border"><p class="tab-para">Kilobytes/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n DEV</code></p></td>
        <td class="border"><p class="tab-para"><code>txkB/s</code></p></td>
        <td class="border"><p class="tab-para">Transmitted kilobytes</p></td>
        <td class="border"><p class="tab-para">Kilobytes/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n DEV</code></p></td>
        <td class="border"><p class="tab-para"><code>rxcmp/s</code></p></td>
        <td class="border"><p class="tab-para">Received compressed packets</p></td>
        <td class="border"><p class="tab-para">Packets/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n DEV</code></p></td>
        <td class="border"><p class="tab-para"><code>txcmp/s</code></p></td>
        <td class="border"><p class="tab-para">Transmitted compressed packets</p></td>
        <td class="border"><p class="tab-para">Packets/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n DEV</code></p></td>
        <td class="border"><p class="tab-para"><code>rxmcst/s</code></p></td>
        <td class="border"><p class="tab-para">Received multicast packets</p></td>
        <td class="border"><p class="tab-para">Packets/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n DEV</code></p></td>
        <td class="border"><p class="tab-para"><code>%ifutil</code></p></td>
        <td class="border"><p class="tab-para">Interface utilization: for full duplex, the greater of rx or tx</p></td>
        <td class="border"><p class="tab-para">Percent</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n EDEV</code></p></td>
        <td class="border"><p class="tab-para"><code>rxerr/s</code></p></td>
        <td class="border"><p class="tab-para">Received packet errors</p></td>
        <td class="border"><p class="tab-para">Packets/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n EDEV</code></p></td>
        <td class="border"><p class="tab-para"><code>txerr/s</code></p></td>
        <td class="border"><p class="tab-para">Transmitted packet errors</p></td>
        <td class="border"><p class="tab-para">Packets/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n EDEV</code></p></td>
        <td class="border"><p class="tab-para"><code>coll/s</code></p></td>
        <td class="border"><p class="tab-para">Collisions</p></td>
        <td class="border"><p class="tab-para">Packets/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n EDEV</code></p></td>
        <td class="border"><p class="tab-para"><code>rxdrop/s</code></p></td>
        <td class="border"><p class="tab-para">Received packets dropped (buffer full)</p></td>
        <td class="border"><p class="tab-para">Packets/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n EDEV</code></p></td>
        <td class="border"><p class="tab-para"><code>txdrop/s</code></p></td>
        <td class="border"><p class="tab-para">Transmitted packets dropped (buffer full)</p></td>
        <td class="border"><p class="tab-para">Packets/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n EDEV</code></p></td>
        <td class="border"><p class="tab-para"><code>txcarr/s</code></p></td>
        <td class="border"><p class="tab-para">Transmission carrier errors</p></td>
        <td class="border"><p class="tab-para">Errors/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n EDEV</code></p></td>
        <td class="border"><p class="tab-para"><code>rxfram/s</code></p></td>
        <td class="border"><p class="tab-para">Received alignment errors</p></td>
        <td class="border"><p class="tab-para">Errors/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n EDEV</code></p></td>
        <td class="border"><p class="tab-para"><code>rxfifo/s</code></p></td>
        <td class="border"><p class="tab-para">Received packets FIFO overrun errors</p></td>
        <td class="border"><p class="tab-para">Packets/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n EDEV</code></p></td>
        <td class="border"><p class="tab-para"><code>txfifo/s</code></p></td>
        <td class="border"><p class="tab-para">Transmitted packets FIFO overrun errors</p></td>
        <td class="border"><p class="tab-para">Packets/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><span epub:type="pagebreak" id="page_544"></span><code>-n IP</code></p></td>
        <td class="border"><p class="tab-para"><code>irec/s</code></p></td>
        <td class="border"><p class="tab-para">Input datagrams (received)</p></td>
        <td class="border"><p class="tab-para">Datagrams/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n IP</code></p></td>
        <td class="border"><p class="tab-para"><code>fwddgm/s</code></p></td>
        <td class="border"><p class="tab-para">Forwarded datagrams</p></td>
        <td class="border"><p class="tab-para">Datagrams/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n IP</code></p></td>
        <td class="border"><p class="tab-para"><code>idel/s</code></p></td>
        <td class="border"><p class="tab-para">Input IP datagrams (including ICMP)</p></td>
        <td class="border"><p class="tab-para">Datagrams/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n IP</code></p></td>
        <td class="border"><p class="tab-para"><code>orq/s</code></p></td>
        <td class="border"><p class="tab-para">Output datagram requests (transmit)</p></td>
        <td class="border"><p class="tab-para">Datagrams/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n IP</code></p></td>
        <td class="border"><p class="tab-para"><code>asmrq/s</code></p></td>
        <td class="border"><p class="tab-para">IP fragments received</p></td>
        <td class="border"><p class="tab-para">Fragments/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n IP</code></p></td>
        <td class="border"><p class="tab-para"><code>asmok/s</code></p></td>
        <td class="border"><p class="tab-para">IP datagrams reassembled</p></td>
        <td class="border"><p class="tab-para">Datagrams/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n IP</code></p></td>
        <td class="border"><p class="tab-para"><code>fragok/s</code></p></td>
        <td class="border"><p class="tab-para">IP datagrams fragmented</p></td>
        <td class="border"><p class="tab-para">Datagrams/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n IP</code></p></td>
        <td class="border"><p class="tab-para"><code>fragcrt/s</code></p></td>
        <td class="border"><p class="tab-para">IP datagram fragments created</p></td>
        <td class="border"><p class="tab-para">Fragments/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n EIP</code></p></td>
        <td class="border"><p class="tab-para"><code>ihdrerr/s</code></p></td>
        <td class="border"><p class="tab-para">IP header errors</p></td>
        <td class="border"><p class="tab-para">Datagrams/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n EIP</code></p></td>
        <td class="border"><p class="tab-para"><code>iadrerr/s</code></p></td>
        <td class="border"><p class="tab-para">Invalid IP destination address errors</p></td>
        <td class="border"><p class="tab-para">Datagrams/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n EIP</code></p></td>
        <td class="border"><p class="tab-para"><code>iukwnpr/s</code></p></td>
        <td class="border"><p class="tab-para">Unknown protocol errors</p></td>
        <td class="border"><p class="tab-para">Datagrams/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n EIP</code></p></td>
        <td class="border"><p class="tab-para"><code>idisc/s</code></p></td>
        <td class="border"><p class="tab-para">Input discards (e.g., buffer full)</p></td>
        <td class="border"><p class="tab-para">Datagrams/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n EIP</code></p></td>
        <td class="border"><p class="tab-para"><code>odisc/s</code></p></td>
        <td class="border"><p class="tab-para">Output discards (e.g., buffer full)</p></td>
        <td class="border"><p class="tab-para">Datagram/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n EIP</code></p></td>
        <td class="border"><p class="tab-para"><code>onort/s</code></p></td>
        <td class="border"><p class="tab-para">Output datagram no route error</p></td>
        <td class="border"><p class="tab-para">Datagrams/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n EIP</code></p></td>
        <td class="border"><p class="tab-para"><code>asmf/s</code></p></td>
        <td class="border"><p class="tab-para">IP reassembly failures</p></td>
        <td class="border"><p class="tab-para">Failures/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n EIP</code></p></td>
        <td class="border"><p class="tab-para"><code>fragf/s</code></p></td>
        <td class="border"><p class="tab-para">IP don’t fragment discards</p></td>
        <td class="border"><p class="tab-para">Datagrams/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n TCP</code></p></td>
        <td class="border"><p class="tab-para"><code>active/s</code></p></td>
        <td class="border"><p class="tab-para">New active TCP connections (connect(2))</p></td>
        <td class="border"><p class="tab-para">Connections/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n TCP</code></p></td>
        <td class="border"><p class="tab-para"><code>passive/s</code></p></td>
        <td class="border"><p class="tab-para">New passive TCP connections (accept(2))</p></td>
        <td class="border"><p class="tab-para">Connections/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n TCP</code></p></td>
        <td class="border"><p class="tab-para"><code>iseg/s</code></p></td>
        <td class="border"><p class="tab-para">Input segments (received)</p></td>
        <td class="border"><p class="tab-para">Segments/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n TCP</code></p></td>
        <td class="border"><p class="tab-para"><code>oseg/s</code></p></td>
        <td class="border"><p class="tab-para">Output segments (received)</p></td>
        <td class="border"><p class="tab-para">Segments/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n ETCP</code></p></td>
        <td class="border"><p class="tab-para"><code>atmptf/s</code></p></td>
        <td class="border"><p class="tab-para">Active TCP connection fails</p></td>
        <td class="border"><p class="tab-para">Connections/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n ETCP</code></p></td>
        <td class="border"><p class="tab-para"><code>estres/s</code></p></td>
        <td class="border"><p class="tab-para">Established resets</p></td>
        <td class="border"><p class="tab-para">Resets/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n ETCP</code></p></td>
        <td class="border"><p class="tab-para"><code>retrans/s</code></p></td>
        <td class="border"><p class="tab-para">TCP segments retransmitted</p></td>
        <td class="border"><p class="tab-para">Segments/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n ETCP</code></p></td>
        <td class="border"><p class="tab-para"><code>isegerr/s</code></p></td>
        <td class="border"><p class="tab-para">Segment errors</p></td>
        <td class="border"><p class="tab-para">Segments/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n ETCP</code></p></td>
        <td class="border"><p class="tab-para"><code>orsts/s</code></p></td>
        <td class="border"><p class="tab-para">Sent resets</p></td>
        <td class="border"><p class="tab-para">Segments/s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n SOCK</code></p></td>
        <td class="border"><p class="tab-para"><code>totsck</code></p></td>
        <td class="border"><p class="tab-para">Total sockets in use</p></td>
        <td class="border"><p class="tab-para">Sockets</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n SOCK</code></p></td>
        <td class="border"><p class="tab-para"><code>tcpsck/s</code></p></td>
        <td class="border"><p class="tab-para">Total TCP sockets in use</p></td>
        <td class="border"><p class="tab-para">Sockets</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n SOCK</code></p></td>
        <td class="border"><p class="tab-para"><code>udpsck/s</code></p></td>
        <td class="border"><p class="tab-para">Total UDP sockets in use</p></td>
        <td class="border"><p class="tab-para">Sockets</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n SOCK</code></p></td>
        <td class="border"><p class="tab-para"><code>rawsck/s</code></p></td>
        <td class="border"><p class="tab-para">Total RAW sockets in use</p></td>
        <td class="border"><p class="tab-para">Sockets</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><code>-n SOCK</code></p></td>
        <td class="border"><p class="tab-para"><code>ip-frag</code></p></td>
        <td class="border"><p class="tab-para">IP fragments currently queued</p></td>
        <td class="border"><p class="tab-para">Fragments</p></td>
        </tr>
        <tr>
        <td><p class="tab-para"><code>-n SOCK</code></p></td>
        <td><p class="tab-para"><code>tcp-tw</code></p></td>
        <td><p class="tab-para">TCP sockets in TIME_WAIT</p></td>
        <td><p class="tab-para">Sockets</p></td>
        </tr>
        </tbody>
        </table>
        </figure>
        <p><span epub:type="pagebreak" id="page_545"></span></p>
        <p class="noindent">Not listed are the ICMP, NFS, and SOFT (software network processing) groups, and IPv6 variants: IP6, EIP6, SOCK6, and UDP6. See the man page for the full list of statistics, which also notes some equivalent SNMP names (e.g., ipInReceives for <code>irec/s</code>). Many of the sar(1) statistic names are easy to remember in practice, as they include the direction and units measured: <code>rx</code> for “received,” <code>i</code> for “input,” <code>seg</code> for “segments,” and so on.</p>
        <p class="noindent">This example prints TCP statistics every second:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg545-1a" id="pg545-1">Click here to view code image</a></p>
        <pre class="pretb">$ <strong>sar -n TCP 1</strong>
        Linux 5.3.0-1010-aws (ip-10-1-239-218)    02/27/20        _x86_64_  (2 CPU)
        
        07:32:45     active/s passive/s    iseg/s    oseg/s
        07:32:46         0.00     12.00    186.00  28837.00
        07:32:47         0.00     13.00    203.00  33584.00
        07:32:48         0.00     11.00   1999.00  24441.00
        07:32:49         0.00      7.00     92.00   8908.00
        07:32:50         0.00     10.00    114.00  13795.00
         [...]</pre>
        <p class="noindent">The output shows a passive connection rate (inbound) of around 10 per second.</p>
        <p class="noindent">When examining network devices (<code>DEV</code>) the network interface statistics column (<code>IFACE</code>) lists all interfaces; however, often only one is of interest. The following example uses a little awk(1) to filter the output:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg545-2a" id="pg545-2">Click here to view code image</a></p>
        <pre class="pretb">$ <strong>sar -n DEV 1 | awk 'NR == 3 || $2 == "ens5"'</strong>
        07:35:41 IFACE  rxpck/s   txpck/s  rxkB/s   txkB/s rxcmp/s txcmp/s rxmcst/s %ifutil
        07:35:42  ens5   134.00  11483.00   10.22  6328.72    0.00    0.00     0.00    0.00
        07:35:43  ens5   170.00  20354.00   13.62  6925.27    0.00    0.00     0.00    0.00
        07:35:44  ens5   185.00  28228.00   14.33  8586.79    0.00    0.00     0.00    0.00
        07:35:45  ens5   180.00  23093.00   14.59  7452.49    0.00    0.00     0.00    0.00
        07:35:46  ens5  1525.00  19594.00  137.48  7044.81    0.00    0.00     0.00    0.00
        07:35:47  ens5   146.00  10282.00   12.05  6876.80    0.00    0.00     0.00    0.00
        [...]</pre>
        <p class="noindent">This shows network throughput for transmit and receive, and other statistics.</p>
        <p class="noindent">The atop(1) tool is also able to archive statistics.</p>
        </section>
        <section>
        <h4 class="h4" id="ch10lev6sec7">10.6.7 nicstat</h4>
        <p class="noindent">nicstat(1)<sup><a id="ch10fn12a" href="ch10.xhtml#ch10fn12">12</a></sup> prints network interface statistics, including throughput and utilization. It follows the style of the traditional resource statistic tools, iostat(1) and mpstat(1).</p>
        <p class="footnote1"><sup><a id="ch10fn12" href="ch10.xhtml#ch10fn12a">12</a></sup>I developed the original version for Solaris; Tim Cook developed the Linux version <a href="ch10.xhtml#ch10ref17">[Cook 09]</a>.</p>
        <p class="noindent"><span epub:type="pagebreak" id="page_546"></span>Here is output for version 1.92 on Linux:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg546-1a" id="pg546-1">Click here to view code image</a></p>
        <pre class="pretb"># <strong>nicstat -z 1</strong>
            Time      Int   rKB/s   wKB/s   rPk/s   wPk/s    rAvs    wAvs %Util    Sat
        01:20:58     eth0    0.07    0.00    0.95    0.02   79.43   64.81  0.00   0.00
        01:20:58     eth4    0.28    0.01    0.20    0.10  1451.3   80.11  0.00   0.00
        01:20:58  vlan123    0.00    0.00    0.00    0.02   42.00   64.81  0.00   0.00
        01:20:58      br0    0.00    0.00    0.00    0.00   42.00   42.07  0.00   0.00
            Time      Int   rKB/s   wKB/s   rPk/s   wPk/s    rAvs    wAvs %Util    Sat
        01:20:59     eth4 42376.0   974.5 28589.4 14002.1  1517.8   71.27  35.5   0.00
            Time      Int   rKB/s   wKB/s   rPk/s   wPk/s    rAvs    wAvs %Util    Sat
        01:21:00     eth0    0.05    0.00    1.00    0.00   56.00    0.00  0.00   0.00
        01:21:00     eth4 41834.7   977.9 28221.5 14058.3  1517.9   71.23  35.1   0.00
            Time      Int   rKB/s   wKB/s   rPk/s   wPk/s    rAvs    wAvs %Util    Sat
        01:21:01     eth4 42017.9   979.0 28345.0 14073.0  1517.9   71.24  35.2   0.00</pre>
        <p class="noindent">The first output is the summary-since-boot, followed by interval summaries. The interval summaries show that the <code>eth4</code> interface is running at 35% utilization (this is reporting the highest current utilization from either the RX or TX direction) and is reading at 42 Mbytes/s.</p>
        <p class="noindent">The fields include the interface name (<code>Int</code>), the maximum utilization (<code>%Util</code>), a value reflecting interface saturation statistics (<code>Sat</code>), and a series of statistics prefixed with <code>r</code> for “read” (receive) and <code>w</code> for “write” (transmit):</p>
        <ul class="sq">
        <li><p class="bull"><strong><code>KB/s</code></strong>: Kbytes per second</p></li>
        <li><p class="bull"><strong><code>Pk/s</code></strong>: packets per second</p></li>
        <li><p class="bull"><strong><code>Avs/s</code></strong>: Average packet size, bytes</p></li>
        </ul>
        <p class="noindent">Options supported in this version include <code>-z</code> to skip lines of zeros (idle interfaces) and <code>-t</code> for TCP statistics.</p>
        <p class="noindent">nicstat(1) is particularly useful for the USE method, as it provides utilization and saturation values.</p>
        </section>
        <section>
        <h4 class="h4" id="ch10lev6sec8">10.6.8 ethtool</h4>
        <p class="noindent">ethtool(8) can be used to check the static configuration of the network interfaces with <code>-i</code> and <code>-k</code> options, and also print driver statistics with <code>-S</code>. For example:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg546-2a" id="pg546-2">Click here to view code image</a></p>
        <pre class="pretb"># <strong>ethtool -S eth0</strong>
        NIC statistics:
             tx_timeout: 0
             suspend: 0
             resume: 0
             wd_expired: 0
             interface_up: 1
             interface_down: 0
             <span epub:type="pagebreak" id="page_547"></span>admin_q_pause: 0
             queue_0_tx_cnt: 100219217
             queue_0_tx_bytes: 84830086234
             queue_0_tx_queue_stop: 0
             queue_0_tx_queue_wakeup: 0
             queue_0_tx_dma_mapping_err: 0
             queue_0_tx_linearize: 0
             queue_0_tx_linearize_failed: 0
             queue_0_tx_napi_comp: 112514572
             queue_0_tx_tx_poll: 112514649
             queue_0_tx_doorbells: 52759561
        [...]</pre>
        <p class="noindent">This fetches statistics from the kernel ethtool framework, which many network device drivers support. Device drivers can define their own ethtool statistics.</p>
        <p class="noindent">The <code>-i</code> option shows driver details, and <code>-k</code> shows interface tunables. For example:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg547a" id="pg547">Click here to view code image</a></p>
        <pre class="pretb"># <strong>ethtool -i eth0</strong>
        driver: ena
        version: 2.0.3K
        [...]
        # <strong>ethtool -k eth0</strong>
        Features for eth0:
        rx-checksumming: on
        [...]
        tcp-segmentation-offload: off
                tx-tcp-segmentation: off [fixed]
                tx-tcp-ecn-segmentation: off [fixed]
                tx-tcp-mangleid-segmentation: off [fixed]
                tx-tcp6-segmentation: off [fixed]
        udp-fragmentation-offload: off
        generic-segmentation-offload: on
        generic-receive-offload: on
        large-receive-offload: off [fixed]
        rx-vlan-offload: off [fixed]
        tx-vlan-offload: off [fixed]
        ntuple-filters: off [fixed]
        receive-hashing: on
        highdma: on
        [...]</pre>
        <p class="noindent">This example is a cloud instance with the ena driver, with tcp-segmentation-offload off. The <code>-K</code> option can be used to change these tunables.</p>
        </section>
        <section>
        <h4 class="h4" id="ch10lev6sec9"><span epub:type="pagebreak" id="page_548"></span>10.6.9 tcplife</h4>
        <p class="noindent">tcplife(8)<sup><a id="ch10fn13a" href="ch10.xhtml#ch10fn13">13</a></sup> is a BCC and bpftrace tool to trace the lifespan of TCP sessions, showing their duration, address details, throughput, and, when possible, the responsible process ID and name.</p>
        <p class="footnote1"><sup><a id="ch10fn13" href="ch10.xhtml#ch10fn13a">13</a></sup>Origin: I created tcplife(8) on 18-Oct-2016 based on an idea by Julia Evans, and the bpftrace version on 17-Apr-2019.</p>
        <p class="noindent">The following shows tcplife(8) from BCC, on a 48-CPU production instance:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg548a" id="pg548">Click here to view code image</a></p>
        <pre class="pretb"># <strong>tcplife</strong>
        PID   COMM       LADDR           LPORT RADDR           RPORT TX_KB RX_KB MS
        4169  java       100.1.111.231   32648 100.2.0.48      6001      0     0 3.99
        4169  java       100.1.111.231   32650 100.2.0.48      6001      0     0 4.10
        4169  java       100.1.111.231   32644 100.2.0.48      6001      0     0 8.41
        4169  java       100.1.111.231   40158 100.2.116.192   6001      7    33 3590.91
        4169  java       100.1.111.231   56940 100.5.177.31    6101      0     0 2.48
        4169  java       100.1.111.231   6001  100.2.176.45    49482     0     0 17.94
        4169  java       100.1.111.231   18926 100.5.102.250   6101      0     0 0.90
        4169  java       100.1.111.231   44530 100.2.31.140    6001      0     0 2.64
        4169  java       100.1.111.231   44406 100.2.8.109     6001     11    28 3982.11
        34781 sshd       100.1.111.231   22    100.2.17.121    41566     5     7 2317.30
        4169  java       100.1.111.231   49726 100.2.9.217     6001     11    28 3938.47
        4169  java       100.1.111.231   58858 100.2.173.248   6001      9    30 2820.51
        [...]</pre>
        <p class="noindent">This output shows a series of connections that were either short-lived (less than 20 milliseconds) or long-lived (over three seconds), as shown in the duration column (<code>MS</code> for milliseconds). This is an application server pool that listens on port 6001. Most of the sessions in this screenshot show connections to port 6001 on remote application servers, with only one connection to the local port 6001. An ssh session was also seen, owned by sshd and local port 22—an inbound session.</p>
        <p class="noindent">The BCC version of tcplife(8) supports options including:</p>
        <ul class="sq">
        <li><p class="bull"><strong><code>-t</code></strong>: Include time column (HH:MM:SS)</p></li>
        <li><p class="bull"><strong><code>-w</code></strong>: Wider columns (to better fit IPv6 addresses)</p></li>
        <li><p class="bull"><strong><code>-p PID</code></strong>: Trace this process only</p></li>
        <li><p class="bull"><strong><code>-L PORT[,PORT[,...]]</code></strong>: Trace only sessions with these local ports</p></li>
        <li><p class="bull"><strong><code>-D PORT[,PORT[,...]]</code></strong>: Trace only sessions with these remote ports</p></li>
        </ul>
        <p class="noindent">This tool works by tracing TCP socket state-change events, and prints the summary details when the state changes to TCP_CLOSE. These state-change events are much less frequent than packets, making this approach much less costly in overhead than per-packet sniffers. This has made tcplife(8) acceptable to run continuously as a TCP flow logger on Netflix production servers.<sup><a id="ch10fn14a" href="ch10.xhtml#ch10fn14">14</a></sup></p>
        <p class="footnote1"><sup><a id="ch10fn14" href="ch10.xhtml#ch10fn14a">14</a></sup>It is coupled with snapshots of all open sessions.</p>
        <p class="noindent">Creating udplife(8) for tracing UDP sessions is a <a href="ch10.xhtml#ch10">Chapter 10</a> exercise in the <em>BPF Performance Tools</em> book <a href="ch10.xhtml#ch10ref45">[Gregg 19]</a>; I have posted an initial solution <a href="ch10.xhtml#ch10ref48">[Gregg 19d]</a>.</p>
        </section>
        <section>
        <h4 class="h4" id="ch10lev6sec10"><span epub:type="pagebreak" id="page_549"></span>10.6.10 tcptop</h4>
        <p class="noindent">tcptop(8)<sup><a id="ch10fn15a" href="ch10.xhtml#ch10fn15">15</a></sup> is a BCC tool that shows top processes using TCP. For example, from a 36-CPU production Hadoop instance:</p>
        <p class="footnote1"><sup><a id="ch10fn15" href="ch10.xhtml#ch10fn15a">15</a></sup>Origin: I created the BCC version on 02-Sep-2016, based on an earlier tcptop tool I had created in 2005, which itself was inspired by the original top(1) by William LeFebvre.</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg549-1a" id="pg549-1">Click here to view code image</a></p>
        <pre class="pretb"># <strong>tcptop</strong>
        09:01:13 loadavg: 33.32 36.11 38.63 26/4021 123015
        
        PID    COMM       LADDR                RADDR                 RX_KB  TX_KB
        118119 java       100.1.58.46:36246    100.2.52.79:50010     16840      0
        122833 java       100.1.58.46:52426    100.2.6.98:50010          0   3112
        122833 java       100.1.58.46:50010    100.2.50.176:55396     3112      0
        120711 java       100.1.58.46:50010    100.2.7.75:23358       2922      0
        121635 java       100.1.58.46:50010    100.2.5.101:56426      2922      0
        121219 java       100.1.58.46:50010    100.2.62.83:40570      2858      0
        121219 java       100.1.58.46:42324    100.2.4.58:50010          0   2858
        122927 java       100.1.58.46:50010    100.2.2.191:29338      2351      0
        [...]</pre>
        <p class="noindent">This output shows one connection at the top receiving over 16 Mbytes during this interval. By default, the screen is updated every second.</p>
        <p class="noindent">This works by tracing the TCP send and receive code path, and summarizing data in a BPF map efficiency. Even so, these events can be frequent, and on high network throughput systems the overhead may become measurable.</p>
        <p class="noindent">Options include:</p>
        <ul class="sq">
        <li><p class="bull"><strong><code>-C</code></strong>: Don’t clear the screen.</p></li>
        <li><p class="bull"><strong><code>-p PID</code></strong>: Measure this process only.</p></li>
        </ul>
        <p class="noindent">tcptop(8) also accepts an optional interval and count.</p>
        </section>
        <section>
        <h4 class="h4" id="ch10lev6sec11">10.6.11 tcpretrans</h4>
        <p class="noindent">tcpretrans(8)<sup><a id="ch10fn16a" href="ch10.xhtml#ch10fn16">16</a></sup> is a BCC and bpftrace tool to trace TCP retransmits, showing IP address and port details and the TCP state. The following shows tcpretrans(8) from BCC, on a production instance:</p>
        <p class="footnote1"><sup><a id="ch10fn16" href="ch10.xhtml#ch10fn16a">16</a></sup>Origin: I created similar tools in 2011, an Ftrace tcpretrans(8) in 2014, and this BCC version on 14-Feb-2016. Dale Hamel created the bpftrace version on 23-Nov-2018.</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg549-2a" id="pg549-2">Click here to view code image</a></p>
        <pre class="pretb"># <strong>tcpretrans</strong>
        Tracing retransmits ... Hit Ctrl-C to end
        TIME     PID    IP LADDR:LPORT         T&gt; RADDR:RPORT         STATE
        00:20:11 72475  4  100.1.58.46:35908   R&gt; 100.2.0.167:50010   ESTABLISHED
        00:20:11 72475  4  100.1.58.46:35908   R&gt; 100.2.0.167:50010   ESTABLISHED
        <span epub:type="pagebreak" id="page_550"></span>00:20:11 72475  4  100.1.58.46:35908   R&gt; 100.2.0.167:50010   ESTABLISHED
        00:20:12 60695  4  100.1.58.46:52346   R&gt; 100.2.6.189:50010   ESTABLISHED
        00:20:12 60695  4  100.1.58.46:52346   R&gt; 100.2.6.189:50010   ESTABLISHED
        00:20:12 60695  4  100.1.58.46:52346   R&gt; 100.2.6.189:50010   ESTABLISHED
        00:20:12 60695  4  100.1.58.46:52346   R&gt; 100.2.6.189:50010   ESTABLISHED
        00:20:13 60695  6  ::ffff:100.1.58.46:13562 R&gt; ::ffff:100.2.51.209:47356 FIN_WAIT1
        00:20:13 60695  6  ::ffff:100.1.58.46:13562 R&gt; ::ffff:100.2.51.209:47356 FIN_WAIT1
        [...]</pre>
        <p class="noindent">This output shows a low rate of retransmits, a few per second (TIME column), which were mostly for sessions in the ESTABLISHED state. A high rate in the ESTABLISHED state can point to an external network problem. A high rate in the SYN_SENT state can point to an overloaded server application that is not consuming its SYN backlog fast enough.</p>
        <p class="noindent">This works by tracing TCP retransmit events in the kernel. Since these should occur infrequently, the overhead should be negligible. Compare this to how retransmits are historically analyzed using a packet sniffer to capture all packets, and then post-processing to find retransmits—both steps can cost significant CPU overhead. Packet-capture can only see details that are on the wire, whereas tcpretrans(8) prints the TCP state directly from the kernel, and can be enhanced to print more kernel state if needed.</p>
        <p class="noindent">Options for the BCC version include:</p>
        <ul class="sq">
        <li><p class="bull"><strong><code>-l</code></strong>: Include tail loss probe attempts (adds a kprobe for tcp_send_loss_probe())</p></li>
        <li><p class="bull"><strong><code>-c</code></strong>: Count retransmits per flow</p></li>
        </ul>
        <p class="noindent">The <code>-c</code> option changes the behavior of tcpretrans(8), causing it to print a summary of counts rather than per-event details.</p>
        </section>
        <section>
        <h4 class="h4" id="ch10lev6sec12">10.6.12 bpftrace</h4>
        <p class="noindent">bpftrace is a BPF-based tracer that provides a high-level programming language, allowing the creation of powerful one-liners and short scripts. It is well suited for custom networking analysis based on clues from other tools. It can examine network events from within the kernel and applications, including socket connections, socket I/O, TCP events, packet transmission, backlog drops, TCP retransmits, and other details. These abilities support workload characterization and latency analysis.</p>
        <p class="noindent">bpftrace is explained in <a href="ch15.xhtml#ch15">Chapter 15</a>. This section shows some examples for network analysis: one-liners, socket tracing, and TCP tracing.</p>
        <section>
        <h5 class="h5" id="ch10lev3_30">One-Liners</h5>
        <p class="noindent">The following one-liners are useful and demonstrate different bpftrace capabilities.</p>
        <p class="noindent">Count socket accept(2)s by PID and process name:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg550a" id="pg550">Click here to view code image</a></p>
        <pre class="preash">bpftrace -e 't:syscalls:sys_enter_accept* { @[pid, comm] = count(); }'</pre>
        <p class="noindent"><span epub:type="pagebreak" id="page_551"></span>Count socket connect(2)s by PID and process name:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg551-1a" id="pg551-1">Click here to view code image</a></p>
        <pre class="preash">bpftrace -e 't:syscalls:sys_enter_connect { @[pid, comm] = count(); }'</pre>
        <p class="noindent">Count socket connect(2)s by user stack trace:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg551-2a" id="pg551-2">Click here to view code image</a></p>
        <pre class="preash">bpftrace -e 't:syscalls:sys_enter_connect { @[ustack, comm] = count(); }'</pre>
        <p class="noindent">Count socket send/receives by direction, on-CPU PID, and process name<sup><a id="ch10fn17a" href="ch10.xhtml#ch10fn17">17</a></sup>:</p>
        <p class="footnote1"><sup><a id="ch10fn17" href="ch10.xhtml#ch10fn17a">17</a></sup>The earlier socket syscalls are in process context, where PID and comm are reliable. These kprobes are deeper in the kernel, and the process endpoint for these connections my not be currently on-CPU, meaning the pid and comm shown by bpftrace could be unrelated. They usually work, but that may not always be the case.</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg551-3a" id="pg551-3">Click here to view code image</a></p>
        <pre class="preash">bpftrace -e 'k:sock_sendmsg,k:sock_recvmsg { @[func, pid, comm] = count(); }'</pre>
        <p class="noindent">Count socket send/receive bytes by on-CPU PID and process name:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg551-4a" id="pg551-4">Click here to view code image</a></p>
        <pre class="preash">bpftrace -e 'kr:sock_sendmsg,kr:sock_recvmsg /(int32)retval &gt; 0/ { @[pid, comm] =
            sum((int32)retval); }'</pre>
        <p class="noindent">Count TCP connects by on-CPU PID and process name:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg551-5a" id="pg551-5">Click here to view code image</a></p>
        <pre class="preash">bpftrace -e 'k:tcp_v*_connect { @[pid, comm] = count(); }'</pre>
        <p class="noindent">Count TCP accepts by on-CPU PID and process name:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg551-6a" id="pg551-6">Click here to view code image</a></p>
        <pre class="preash">bpftrace -e 'k:inet_csk_accept { @[pid, comm] = count(); }'</pre>
        <p class="noindent">Count TCP send/receives by on-CPU PID and process name:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg551-7a" id="pg551-7">Click here to view code image</a></p>
        <pre class="preash">bpftrace -e 'k:tcp_sendmsg,k:tcp_recvmsg { @[func, pid, comm] = count(); }'</pre>
        <p class="noindent">TCP send bytes as a histogram:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg551-8a" id="pg551-8">Click here to view code image</a></p>
        <pre class="preash">bpftrace -e 'k:tcp_sendmsg { @send_bytes = hist(arg2); }'</pre>
        <p class="noindent">TCP receive bytes as a histogram:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg551-9a" id="pg551-9">Click here to view code image</a></p>
        <pre class="preash">bpftrace -e 'kr:tcp_recvmsg /retval &gt;= 0/ { @recv_bytes = hist(retval); }'</pre>
        <p class="noindent">Count TCP retransmits by type and remote host (assumes IPv4):</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg551-10a" id="pg551-10">Click here to view code image</a></p>
        <pre class="preash">bpftrace -e 't:tcp:tcp_retransmit_* { @[probe, ntop(2, args-&gt;saddr)] = count(); }'</pre>
        <p class="noindent">Count all TCP functions (adds high overhead to TCP):</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg551-11a" id="pg551-11">Click here to view code image</a></p>
        <pre class="preash">bpftrace -e 'k:tcp_* { @[func] = count(); }'</pre>
        <p class="noindent">Count UDP send/receives by on-CPU PID and process name:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg551-12a" id="pg551-12">Click here to view code image</a></p>
        <pre class="preash">bpftrace -e 'k:udp*_sendmsg,k:udp*_recvmsg { @[func, pid, comm] = count(); }'</pre>
        <p class="noindent"><span epub:type="pagebreak" id="page_552"></span>UDP send bytes as a histogram:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg552-1a" id="pg552-1">Click here to view code image</a></p>
        <pre class="preash">bpftrace -e 'k:udp_sendmsg { @send_bytes = hist(arg2); }'</pre>
        <p class="noindent">UDP receive bytes as a histogram:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg552-2a" id="pg552-2">Click here to view code image</a></p>
        <pre class="preash">bpftrace -e 'kr:udp_recvmsg /retval &gt;= 0/ { @recv_bytes = hist(retval); }'</pre>
        <p class="noindent">Count transmit kernel stack traces:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg552-3a" id="pg552-3">Click here to view code image</a></p>
        <pre class="preash">bpftrace -e 't:net:net_dev_xmit { @[kstack] = count(); }'</pre>
        <p class="noindent">Show receive CPU histogram for each device:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg552-4a" id="pg552-4">Click here to view code image</a></p>
        <pre class="preash">bpftrace -e 't:net:netif_receive_skb { @[str(args-&gt;name)] = lhist(cpu, 0, 128, 1); }'</pre>
        <p class="noindent">Count ieee80211 layer functions (adds high overhead to packets):</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg552-5a" id="pg552-5">Click here to view code image</a></p>
        <pre class="preash">bpftrace -e 'k:ieee80211_* { @[func] = count(); }'</pre>
        <p class="noindent">Count all ixgbevf device driver functions (adds high overhead to ixgbevf):</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg552-6a" id="pg552-6">Click here to view code image</a></p>
        <pre class="preash">bpftrace -e 'k:ixgbevf_* { @[func] = count(); }'</pre>
        <p class="noindent">Count all iwl device driver tracepoints (adds high overhead to iwl):</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg552-7a" id="pg552-7">Click here to view code image</a></p>
        <pre class="preash">bpftrace -e 't:iwlwifi:*,t:iwlwifi_io:* { @[probe] = count(); }'</pre>
        </section>
        <section>
        <h5 class="h5" id="ch10lev3_31">Socket Tracing</h5>
        <p class="noindent">Tracing network events at the socket layer has the advantage that the responsible process is still on-CPU, making it straightforward to identify the application and code-path responsible. For example, counting the applications calling the accept(2) syscall:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg552-8a" id="pg552-8">Click here to view code image</a></p>
        <pre class="pretb"># <strong>bpftrace -e 't:syscalls:sys_enter_accept { @[pid, comm] = count(); }'</strong>
        Attaching 1 probe...
        ^C
        
        @[573, sshd]: 2
        @[1948, mysqld]: 41</pre>
        <p class="noindent">The output shows that during tracing mysqld called accept(2) 41 times, and sshd called accept(2) 2 times.</p>
        <p class="noindent">The stack trace can be included to show the code path that led to accept(2). For example, counting by the user-level stack trace and process name:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg552-9a" id="pg552-9">Click here to view code image</a></p>
        <pre class="pretb"># <strong>bpftrace -e 't:syscalls:sys_enter_accept { @[ustack, comm] = count(); }'</strong>
        Attaching 1 probe...
        ^C
        <span epub:type="pagebreak" id="page_553"></span>@[
        
            accept+79
            Mysqld_socket_listener::listen_for_connection_event()+283
            mysqld_main(int, char**)+15577
            __libc_start_main+243
            0x49564100fe8c4b3d
        , mysqld]: 22</pre>
        <p class="noindent">This output shows that mysqld was accepting connections via a code path that included Mysqld_socket_listener::listen_for_connection_event(). By changing “accept” to “connect”, this one-liner will identify the code paths leading to connect(2). I have used such one-liners to explain mysterious network connections, showing the code paths calling them.</p>
        <section>
        <h6 class="h6">sock Tracepoints</h6>
        <p class="noindent">Apart from socket syscalls, there are socket tracepoints. From a 5.3 kernel:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg553-1a" id="pg553-1">Click here to view code image</a></p>
        <pre class="pretb"># <strong>bpftrace -l 't:sock:*'</strong>
        tracepoint:sock:sock_rcvqueue_full
        tracepoint:sock:sock_exceed_buf_limit
        tracepoint:sock:inet_sock_set_state</pre>
        <p class="noindent">The sock:inet_sock_set_state tracepoint is used by the earlier tcplife(8) tool. Here is an example one-liner that uses it to count source and destination IPv4 addresses for new connections:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg553-2a" id="pg553-2">Click here to view code image</a></p>
        <pre class="pretb"># <strong>bpftrace -e 't:sock:inet_sock_set_state</strong>
            <strong>/args-&gt;newstate == 1 &amp;&amp; args-&gt;family == 2/ {</strong>
            <strong>@[ntop(args-&gt;saddr), ntop(args-&gt;daddr)] = count() }'</strong>
        Attaching 1 probe...
        ^C
        @[127.0.0.1, 127.0.0.1]: 2
        @[10.1.239.218, 10.29.225.81]: 18</pre>
        <p class="noindent">This one-liner is getting long, and it would be easier to save to a bpftrace program file (.bt) for editing and execution. As a file, it can also include the appropriate kernel headers so that the filter line can be rewritten to use constant names rather than hard-coded numbers (which are unreliable), like this:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg553-3a" id="pg553-3">Click here to view code image</a></p>
        <pre class="pretb">/args-&gt;newstate == TCP_ESTABLISHED &amp;&amp; args-&gt;family == AF_INET/ {</pre>
        <p class="noindent">The next example is of a program file: socketio.bt.</p>
        </section>
        <section>
        <h6 class="h6">socketio.bt</h6>
        <p class="noindent">As a more complex example, the socketio(8) tool shows socket I/O with the process details, direction, protocol, and port. Example output:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg554-1a" id="pg554-1">Click here to view code image</a></p>
        <pre class="pretb"><span epub:type="pagebreak" id="page_554"></span># <strong>./socketio.bt</strong>
        Attaching 2 probes...
        ^C
        [...]
        @io[sshd, 21925, read, UNIX, 0]: 40
        @io[sshd, 21925, read, TCP, 37408]: 41
        @io[systemd, 1, write, UNIX, 0]: 51
        @io[systemd, 1, read, UNIX, 0]: 57
        @io[systemd-udevd, 241, write, NETLINK, 0]: 65
        @io[systemd-udevd, 241, read, NETLINK, 0]: 75
        @io[dbus-daemon, 525, write, UNIX, 0]: 98
        @io[systemd-logind, 526, read, UNIX, 0]: 105
        @io[systemd-udevd, 241, read, UNIX, 0]: 127
        @io[snapd, 31927, read, NETLINK, 0]: 150
        @io[dbus-daemon, 525, read, UNIX, 0]: 160
        @io[mysqld, 1948, write, TCP, 55010]: 8147
        @io[mysqld, 1948, read, TCP, 55010]: 24466</pre>
        <p class="noindent">This shows that the most socket I/O was by mysqld, with reads and writes to TCP port 55010, the ephemeral port a client is using.</p>
        <p class="noindent">The source to socketio(8) is:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg554-2a" id="pg554-2">Click here to view code image</a></p>
        <pre class="pretb">#!/usr/local/bin/bpftrace
        
        #include &lt;net/sock.h&gt;
        
        kprobe:sock_recvmsg
        {
                $sock = (struct socket *)arg0;
                $dport = $sock-&gt;sk-&gt;__sk_common.skc_dport;
                $dport = ($dport &gt;&gt; 8) | (($dport &lt;&lt; 8) &amp; 0xff00);
                @io[comm, pid, "read", $sock-&gt;sk-&gt;__sk_common.skc_prot-&gt;name, $dport] =
                    count();
        }
        
        kprobe:sock_sendmsg
        {
                $sock = (struct socket *)arg0;
                $dport = $sock-&gt;sk-&gt;__sk_common.skc_dport;
                $dport = ($dport &gt;&gt; 8) | (($dport &lt;&lt; 8) &amp; 0xff00);
                @io[comm, pid, "write", $sock-&gt;sk-&gt;__sk_common.skc_prot-&gt;name, $dport] =
                    count();
        }</pre>
        <p class="noindent"><span epub:type="pagebreak" id="page_555"></span>This is an example of fetching details from a kernel struct, in this case struct socket, which provides the protocol name and destination port. The destination port is big endian, and is converted to little endian (for this x86 processor) by the tool before inclusion in the @io map.<sup><a id="ch10fn18a" href="ch10.xhtml#ch10fn18">18</a></sup> This script could be modified to show the bytes transferred instead of the I/O counts.</p>
        <p class="footnote1"><sup><a id="ch10fn18" href="ch10.xhtml#ch10fn18a">18</a></sup>For this to work on big-endian processors, the tool should test for processor endianness and use a conversion only if necessary; for example, by use of <code>#ifdef LITTLE_ENDIAN.</code></p>
        </section>
        </section>
        <section>
        <h5 class="h5" id="ch10lev3_32">TCP Tracing</h5>
        <p class="noindent">Tracing at the TCP level provides insight for TCP protocol events and internals, as well as events not associated with a socket (e.g., a TCP port scan).</p>
        <section>
        <h6 class="h6">TCP Tracepoints</h6>
        <p class="noindent">Instrumenting TCP internals often requires using kprobes, but there are some TCP tracepoints available. From a 5.3 kernel:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg555-1a" id="pg555-1">Click here to view code image</a></p>
        <pre class="pretb"># <strong>bpftrace -l 't:tcp:*'</strong>
        tracepoint:tcp:tcp_retransmit_skb
        tracepoint:tcp:tcp_send_reset
        tracepoint:tcp:tcp_receive_reset
        tracepoint:tcp:tcp_destroy_sock
        tracepoint:tcp:tcp_rcv_space_adjust
        tracepoint:tcp:tcp_retransmit_synack
        tracepoint:tcp:tcp_probe</pre>
        <p class="noindent">The tcp:tcp_retransmit_skb tracepoint is used by the earlier tcpretrans(8) tool. Tracepoints are preferable for their stability, but when they cannot solve your problem you can use kprobes on the kernel TCP functions. Counting them:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg555-2a" id="pg555-2">Click here to view code image</a></p>
        <pre class="pretb"># <strong>bpftrace -e 'k:tcp_* { @[func] = count(); }'</strong>
        Attaching 336 probes...
        ^C
        @[tcp_try_keep_open]: 1
        @[tcp_ooo_try_coalesce]: 1
        @[tcp_reset]: 1
        [...]
        @[tcp_push]: 3191
        @[tcp_established_options]: 3584
        @[tcp_wfree]: 4408
        @[tcp_small_queue_check.isra.0]: 4617
        @[tcp_rate_check_app_limited]: 7022
        @[tcp_poll]: 8898
        @[tcp_release_cb]: 18330
        <span epub:type="pagebreak" id="page_556"></span>@[tcp_send_mss]: 28168
        @[tcp_sendmsg]: 31450
        @[tcp_sendmsg_locked]: 31949
        @[tcp_write_xmit]: 33276
        @[tcp_tx_timestamp]: 33485</pre>
        <p class="noindent">This showed that the most frequently called function was tcp_tx_timestamp(), called 33,485 times while tracing. Counting functions can identify targets for tracing in more detail. Note that counting all TCP calls may add noticeable overhead due to the number and frequency of functions traced. For this particular task I would use Ftrace function profiling instead via my funccount(8) perf-tools tool, as its overhead and initialization time are much lower. See <a href="ch14.xhtml#ch14">Chapter 14</a>, <a href="ch14.xhtml#ch14">Ftrace</a>.</p>
        </section>
        <section>
        <h6 class="h6">tcpsynbl.bt</h6>
        <p class="noindent">The tcpsynbl(8)<sup><a id="ch10fn19a" href="ch10.xhtml#ch10fn19">19</a></sup> tool is an example of instrumenting TCP using kprobes. It shows the length of the listen(2) backlog queues broken down by queue length so that you can tell how close the queues are to overflowing (which causes drops of TCP SYN packets). Example output:</p>
        <p class="footnote1"><sup><a id="ch10fn19" href="ch10.xhtml#ch10fn19a">19</a></sup>Origin: I created tcpsynbl.bt on 19-Apr-2019 for <a href="ch10.xhtml#ch10ref45">[Gregg 19]</a>.</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg556a" id="pg556">Click here to view code image</a></p>
        <pre class="pretb"># <strong>tcpsynbl.bt</strong>
        Attaching 4 probes...
        Tracing SYN backlog size. Ctrl-C to end.
        04:44:31 dropping a SYN.
        04:44:31 dropping a SYN.
        04:44:31 dropping a SYN.
        04:44:31 dropping a SYN.
        04:44:31 dropping a SYN.
        [...]
        ^C
        @backlog[backlog limit]: histogram of backlog size
        
        @backlog[128]:
        [0]                  473 |@                                                   |
        [1]                  502 |@                                                   |
        [2, 4)              1001 |@@@                                                 |
        [4, 8)              1996 |@@@@@@                                              |
        [8, 16)             3943 |@@@@@@@@@@@                                         |
        [16, 32)            7718 |@@@@@@@@@@@@@@@@@@@@@@@                             |
        [32, 64)           14460 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@         |
        [64, 128)          17246 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@|
        [128, 256)          1844 |@@@@@                                               |</pre>
        <p class="noindent"><span epub:type="pagebreak" id="page_557"></span>While running, tcpsynbl.bt prints timestamps and SYN drops if they occur when tracing. When terminated (by typing Ctrl-C), a histogram of backlog size is printed for each backlog limit in use. This output shows several SYN drops occurring at 4:44:31, and the histogram summary shows a limit of 128 and a distribution where that limit was reached 1844 times (the 128 to 256 bucket). This distribution shows the backlog length when SYNs arrive.</p>
        <p class="noindent">By monitoring the backlog length, you can check whether it is growing over time, giving you an early warning that SYN drops are imminent. This is something you can do as part of capacity planning.</p>
        <p class="noindent">The source to tcpsynbl(8) is:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg557a" id="pg557">Click here to view code image</a></p>
        <pre class="pretb">#!/usr/local/bin/bpftrace
        
        #include &lt;net/sock.h&gt;
        
        BEGIN
        {
                printf("Tracing SYN backlog size. Ctrl-C to end.\n");
        }
        
        kprobe:tcp_v4_syn_recv_sock,
        kprobe:tcp_v6_syn_recv_sock
        {
                $sock = (struct sock *)arg0;
                @backlog[$sock-&gt;sk_max_ack_backlog &amp; 0xffffffff] =
                    hist($sock-&gt;sk_ack_backlog);
                if ($sock-&gt;sk_ack_backlog &gt; $sock-&gt;sk_max_ack_backlog) {
                        time("%H:%M:%S dropping a SYN.\n");
                }
        }
        
        END
        {
                printf("\n@backlog[backlog limit]: histogram of backlog size\n");
        }</pre>
        <p class="noindent">The shape of the earlier printed distribution has much to do with the log2 scale used by hist(), where later buckets span larger ranges. You can change hist() to lhist() using:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg557-1a" id="pg557-1">Click here to view code image</a></p>
        <pre class="pretb">            lhist($sock-&gt;sk_ack_backlog, 0, 1000, 10);</pre>
        <p class="noindent">This will print a linear histogram with even ranges for each bucket: in this case, for the range 0 to 1000 with a bucket size of 10. For more bpftrace programming, see <a href="ch15.xhtml#ch15">Chapter 15</a>, <a href="ch15.xhtml#ch15">BPF</a>.</p>
        </section>
        </section>
        <section>
        <h5 class="h5" id="ch10lev3_33"><span epub:type="pagebreak" id="page_558"></span>Event Sources</h5>
        <p class="noindent">bpftrace can instrument much more; <a href="ch10.xhtml#ch10tab06">Table 10.6</a> shows event sources for instrumenting different network events.</p>
        <figure class="table" id="ch10tab06">
        <figcaption>
        <p class="title-t"><span class="pd_ashf">Table 10.6</span> <strong>Network events and sources</strong></p>
        </figcaption>
        <table class="all">
        <thead>
        <tr>
        <th class="th"><p class="thead"><strong>Network Event</strong></p></th>
        <th class="th"><p class="thead"><strong>Event Source</strong></p></th>
        </tr>
        </thead>
        <tbody>
        <tr>
        <td class="border"><p class="tab-para">Application protocols</p></td>
        <td class="border"><p class="tab-para">uprobes</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">Sockets</p></td>
        <td class="border"><p class="tab-para">syscalls tracepoints</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">TCP</p></td>
        <td class="border"><p class="tab-para">tcp tracepoints, kprobes</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">UDP</p></td>
        <td class="border"><p class="tab-para">kprobes</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">IP and ICMP</p></td>
        <td class="border"><p class="tab-para">kprobes</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">Packets</p></td>
        <td class="border"><p class="tab-para">skb tracepoints, kprobes</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">QDiscs and driver queues</p></td>
        <td class="border"><p class="tab-para">qdisc and net tracepoints, kprobes</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">XDP</p></td>
        <td class="border"><p class="tab-para">xdp tracepoints</p></td>
        </tr>
        <tr>
        <td><p class="tab-para">Network device drivers</p></td>
        <td><p class="tab-para">kprobes, some have tracepoints</p></td>
        </tr>
        </tbody>
        </table>
        </figure>
        <p class="noindent">Use tracepoints wherever possible, as they are a stable interface.</p>
        </section>
        </section>
        <section>
        <h4 class="h4" id="ch10lev6sec13">10.6.13 tcpdump</h4>
        <p class="noindent">Network packets can be captured and inspected on Linux using the tcpdump(8) utility. This can either print packet summaries on STDOUT or write packet data to a file for later analysis. The latter is usually more practical: packet rates can be too high to follow their summaries in real time.</p>
        <p class="noindent">Dumping packets from the <code>eth4</code> interface to a file in /tmp:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg558-1a" id="pg558-1">Click here to view code image</a></p>
        <pre class="pretb"># <strong>tcpdump -i eth4 -w /tmp/out.tcpdump</strong>
        tcpdump: listening on eth4, link-type EN10MB (Ethernet), capture size 65535 bytes
        ^C273893 packets captured
        275752 packets received by filter
        1859 packets dropped by kernel</pre>
        <p class="noindent">The output notes how many packets were dropped by the kernel instead of being passed to tcpdump(8), as occurs when the rate of packets is too high. Note that you can use <code>-i any</code> to c apture packets from all interfaces.</p>
        <p class="noindent">Inspecting packets from a dump file:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg558-2a" id="pg558-2">Click here to view code image</a></p>
        <pre class="pretb"># <strong>tcpdump -nr /tmp/out.tcpdump</strong>
        reading from file /tmp/out.tcpdump, link-type EN10MB (Ethernet)
        02:24:46.160754 IP 10.2.124.2.32863 &gt; 10.2.203.2.5001: Flags [.], seq
        3612664461:3612667357, ack 180214943, win 64436, options [nop,nop,TS val 692339741
        ecr 346311608], length 2896
        <span epub:type="pagebreak" id="page_559"></span>02:24:46.160765 IP 10.2.203.2.5001 &gt; 10.2.124.2.32863: Flags [.], ack 2896, win
        18184, options [nop,nop,TS val 346311610 ecr 692339740], length 0
        02:24:46.160778 IP 10.2.124.2.32863 &gt; 10.2.203.2.5001: Flags [.], seq 2896:4344, ack
        1, win 64436, options [nop,nop,TS val 692339741 ecr 346311608], length 1448
        02:24:46.160807 IP 10.2.124.2.32863 &gt; 10.2.203.2.5001: Flags [.], seq 4344:5792, ack
        1, win 64436, options [nop,nop,TS val 692339741 ecr 346311608], length 1448
        02:24:46.160817 IP 10.2.203.2.5001 &gt; 10.2.124.2.32863: Flags [.], ack 5792, win
        18184, options [nop,nop,TS val 346311610 ecr 692339741], length 0
        [...]</pre>
        <p class="noindent">Each line of output shows the time of the packet (with microsecond resolution), its source and destination IP addresses, and TCP header values. By studying these, the operation of TCP can be understood in detail, including how well advanced features are working for your workload.</p>
        <p class="noindent">The <code>-n</code> option was used to not resolve IP addresses as host names. Other options include printing verbose details where available (<code>-v</code>), link-layer headers (<code>-e</code>), and hex-address dumps (<code>-x</code> or <code>-X</code>). For example:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg559a" id="pg559">Click here to view code image</a></p>
        <pre class="pretb"># <strong>tcpdump -enr /tmp/out.tcpdump -vvv -X</strong>
        reading from file /tmp/out.tcpdump, link-type EN10MB (Ethernet)
        02:24:46.160754 80:71:1f:ad:50:48 &gt; 84:2b:2b:61:b6:ed, ethertype IPv4 (0x0800),
        length 2962: (tos 0x0, ttl 63, id 46508, offset 0, flags [DF], proto TCP (6), length
        2948)
            10.2.124.2.32863 &gt; 10.2.203.2.5001: Flags [.], cksum 0x667f (incorrect -&gt;
        0xc4da), seq 3612664461:3612667357, ack 180214943, win 64436, options [nop,nop,TS val
        692339741 ecr 346311608], length 289
        6
                0x0000:  4500 0b84 b5ac 4000 3f06 1fbf 0a02 7c02  E.....@.?.....|.
                0x0010:  0a02 cb02 805f 1389 d754 e28d 0abd dc9f  ....._...T......
                0x0020:  8010 fbb4 667f 0000 0101 080a 2944 441d  ....f.......)DD.
                0x0030:  14a4 4bb8 3233 3435 3637 3839 3031 3233  ..K.234567890123
                0x0040:  3435 3637 3839 3031 3233 3435 3637 3839  4567890123456789
        [...]</pre>
        <p class="noindent">During performance analysis, it can be useful to change the timestamp column to show delta times between packets (<code>-ttt</code>), or elapsed time since the first packet (<code>-ttttt</code>).</p>
        <p class="noindent">An expression can also be provided to describe how to filter packets (see pcap-filter(7)) to focus on the packets of interest. This is performed in-kernel for efficiency (except on Linux 2.0 and older) using BPF.</p>
        <p class="noindent">Packet capture is expensive to perform in terms of both CPU cost and storage. If possible, use tcpdump(8) only for short periods to limit the performance cost, and look for ways to use efficient BPF-based tools such as bpftrace instead.</p>
        <p class="noindent">tshark(1) is a similar command-line packet capture tool that provides better filtering and output options. It is the CLI version of Wireshark.</p>
        </section>
        <section>
        <h4 class="h4" id="ch10lev6sec14"><span epub:type="pagebreak" id="page_560"></span>10.6.14 Wireshark</h4>
        <p class="noindent">While tcpdump(8) works fine for casual investigations, for deeper analysis it can be time-consuming to use at the command line. The Wireshark tool (formerly Ethereal) provides a graphical interface for packet capture and inspection and can also import packet dump files from tcpdump(8) <a href="ch10.xhtml#ch10ref66">[Wireshark 20]</a>. Useful features include identifying network connections and their related packets, so that they can be studied separately, and also translation of hundreds of protocol headers.</p>
        <p class="noindent"><a href="ch10.xhtml#ch10fig12">Figure 10.12</a> shows an example screenshot of Wireshark. The window is split horizontally into three parts. At the top is a table showing packets as rows and details as columns. The middle section shows protocol details: in this example the TCP protocol was expanded and the destination port selected. The bottom section shows the raw packet as hexadecimal on the left and text on the right: the location of the TCP destination port is highlighted.</p>
        <figure class="image-c" id="ch10fig12">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780136821694/files/graphics/10fig12.jpg" alt="Images" width="775" height="482">
        <figcaption>
        <p class="title-f"><span class="pd_ashf">Figure 10.12</span> Wireshark screenshot</p>
        </figcaption>
        </figure>
        </section>
        <section>
        <h4 class="h4" id="ch10lev6sec15">10.6.15 Other Tools</h4>
        <p class="noindent">Network analysis tools included in other chapters of this book, and in <em>BPF Performance Tools</em> <a href="ch10.xhtml#ch10ref45">[Gregg 19]</a>, are listed in <a href="ch10.xhtml#ch10tab07">Table 10.7</a>.</p>
        <figure class="table" id="ch10tab07">
        <figcaption>
        <p class="title-t"><span epub:type="pagebreak" id="page_561"></span><span class="pd_ashf">Table 10.7</span> <strong>Other network analysis tools</strong></p>
        </figcaption>
        <table class="all">
        <thead>
        <tr>
        <th class="th"><p class="thead"><strong>Section</strong></p></th>
        <th class="th"><p class="thead"><strong>Tool</strong></p></th>
        <th class="th"><p class="thead"><strong>Description</strong></p></th>
        </tr>
        </thead>
        <tbody>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch05.xhtml#ch05lev5sec3">5.5.3</a></p></td>
        <td class="border"><p class="tab-para">offcputime</p></td>
        <td class="border"><p class="tab-para">Off-CPU profiling can show network I/O</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10ref45">[Gregg 19]</a></p></td>
        <td class="border"><p class="tab-para">sockstat</p></td>
        <td class="border"><p class="tab-para">High-level socket statistics</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10ref45">[Gregg 19]</a></p></td>
        <td class="border"><p class="tab-para">sofamily</p></td>
        <td class="border"><p class="tab-para">Count address families for new sockets, by process</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10ref45">[Gregg 19]</a></p></td>
        <td class="border"><p class="tab-para">soprotocol</p></td>
        <td class="border"><p class="tab-para">Count transport protocols for new sockets, by process</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10ref45">[Gregg 19]</a></p></td>
        <td class="border"><p class="tab-para">soconnect</p></td>
        <td class="border"><p class="tab-para">Trace socket IP-protocol connections with details</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10ref45">[Gregg 19]</a></p></td>
        <td class="border"><p class="tab-para">soaccept</p></td>
        <td class="border"><p class="tab-para">Trace socket IP-protocol accepts with details</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10ref45">[Gregg 19]</a></p></td>
        <td class="border"><p class="tab-para">socketio</p></td>
        <td class="border"><p class="tab-para">Summarize socket details with I/O counts</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10ref45">[Gregg 19]</a></p></td>
        <td class="border"><p class="tab-para">socksize</p></td>
        <td class="border"><p class="tab-para">Show socket I/O sizes as per-process histograms</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10ref45">[Gregg 19]</a></p></td>
        <td class="border"><p class="tab-para">sormem</p></td>
        <td class="border"><p class="tab-para">Show socket receive buffer usage and overflows</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10ref45">[Gregg 19]</a></p></td>
        <td class="border"><p class="tab-para">soconnlat</p></td>
        <td class="border"><p class="tab-para">Summarize IP socket connection latency with stacks</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10ref45">[Gregg 19]</a></p></td>
        <td class="border"><p class="tab-para">so1stbyte</p></td>
        <td class="border"><p class="tab-para">Summarize IP socket first byte latency</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10ref45">[Gregg 19]</a></p></td>
        <td class="border"><p class="tab-para">tcpconnect</p></td>
        <td class="border"><p class="tab-para">Trace TCP active connections (connect())</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10ref45">[Gregg 19]</a></p></td>
        <td class="border"><p class="tab-para">tcpaccept</p></td>
        <td class="border"><p class="tab-para">Trace TCP passive connections (accept())</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10ref45">[Gregg 19]</a></p></td>
        <td class="border"><p class="tab-para">tcpwin</p></td>
        <td class="border"><p class="tab-para">Trace TCP send congestion window parameters</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10ref45">[Gregg 19]</a></p></td>
        <td class="border"><p class="tab-para">tcpnagle</p></td>
        <td class="border"><p class="tab-para">Trace TCP Nagle usage and transmit delays</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10ref45">[Gregg 19]</a></p></td>
        <td class="border"><p class="tab-para">udpconnect</p></td>
        <td class="border"><p class="tab-para">Trace new UDP connections from localhost</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10ref45">[Gregg 19]</a></p></td>
        <td class="border"><p class="tab-para">gethostlatency</p></td>
        <td class="border"><p class="tab-para">Trace DNS lookup latency via library calls</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10ref45">[Gregg 19]</a></p></td>
        <td class="border"><p class="tab-para">ipecn</p></td>
        <td class="border"><p class="tab-para">Trace IP inbound explicit congestion notification</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10ref45">[Gregg 19]</a></p></td>
        <td class="border"><p class="tab-para">superping</p></td>
        <td class="border"><p class="tab-para">Measure ICMP echo times from the network stack</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10ref45">[Gregg 19]</a></p></td>
        <td class="border"><p class="tab-para">qdisc-fq (...)</p></td>
        <td class="border"><p class="tab-para">Show FQ qdisc queue latency</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10ref45">[Gregg 19]</a></p></td>
        <td class="border"><p class="tab-para">netsize</p></td>
        <td class="border"><p class="tab-para">Show net device I/O sizes</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10ref45">[Gregg 19]</a></p></td>
        <td class="border"><p class="tab-para">nettxlat</p></td>
        <td class="border"><p class="tab-para">Show net device transmission latency</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10ref45">[Gregg 19]</a></p></td>
        <td class="border"><p class="tab-para">skbdrop</p></td>
        <td class="border"><p class="tab-para">Trace sk_buff drops with kernel stack traces</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch10.xhtml#ch10ref45">[Gregg 19]</a></p></td>
        <td class="border"><p class="tab-para">skblife</p></td>
        <td class="border"><p class="tab-para">Lifespan of sk_buff as inter-stack latency</p></td>
        </tr>
        <tr>
        <td><p class="tab-para"><a href="ch10.xhtml#ch10ref45">[Gregg 19]</a></p></td>
        <td><p class="tab-para">ieee80211scan</p></td>
        <td><p class="tab-para">Trace IEEE 802.11 WiFi scanning</p></td>
        </tr>
        </tbody>
        </table>
        </figure>
        <p class="noindent">Other Linux network observability tools and sources include:</p>
        <ul class="sq">
        <li><p class="bull"><strong><code>strace(1)</code></strong>: Trace socket-related syscalls and examine the options used (note that strace(1) has high overhead)</p></li>
        <li><p class="bull"><strong><code>lsof(8)</code></strong>: List open files by process ID, including socket details</p></li>
        <li><p class="bull"><strong><code>nfsstat(8)</code></strong>: NFS server and client statistics</p></li>
        <li><p class="bull"><strong><code>ifpps(8)</code></strong>: Top-like network and system statistics</p></li>
        <li><p class="bull"><span epub:type="pagebreak" id="page_562"></span><strong><code>iftop(8)</code></strong>: Summarize network interface throughput by host (sniffer)</p></li>
        <li><p class="bull"><strong>perf(1)</strong>: Count and record network tracepoints and kernel functions.</p></li>
        <li><p class="bull"><strong><code>/proc/net</code></strong>: Contains many network statistics files</p></li>
        <li><p class="bull"><strong><code>BPF iterator</code></strong>: Allows BPF programs to export custom statistics in /sys/fs/bpf</p></li>
        </ul>
        <p class="noindent">There are also many network monitoring solutions, either based on SNMP or running their own custom agents.</p>
        </section>
        </section>
        <section>
        <h3 class="h3" id="ch10lev7">10.7 Experimentation</h3>
        <p class="noindent">Network performance is commonly tested using tools that perform an experiment rather than just observing the state of the system. Such experimental tools include ping(8), traceroute(8), and network micro-benchmarks such as iperf(8). These can be used to determine network health between hosts, which can be used to help determine whether end-to-end network throughput is a problem when debugging application performance issues.</p>
        <section>
        <h4 class="h4" id="ch10lev7sec1">10.7.1 ping</h4>
        <p class="noindent">The ping(8) command tests network connectivity by sending ICMP echo request packets. For example:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg562a" id="pg562">Click here to view code image</a></p>
        <pre class="pretb"># <strong>ping www.netflix.com</strong>
        PING www.netflix.com(2620:108:700f::3423:46a1 (2620:108:700f::3423:46a1)) 56 data
        bytes
        64 bytes from 2620:108:700f::3423:46a1 (2620:108:700f::3423:46a1): icmp_seq=1 ttl=43
        time=32.3 ms
        64 bytes from 2620:108:700f::3423:46a1 (2620:108:700f::3423:46a1): icmp_seq=2 ttl=43
        time=34.3 ms
        64 bytes from 2620:108:700f::3423:46a1 (2620:108:700f::3423:46a1): icmp_seq=3 ttl=43 time=34.0 ms
        ^C
        --- www.netflix.com ping statistics ---
        3 packets transmitted, 3 received, 0% packet loss, time 2003ms
        rtt min/avg/max/mdev = 32.341/33.579/34.389/0.889 ms</pre>
        <p class="noindent">The output includes the round-trip time (<code>time</code>) for each packet and has a summary showing various statistics.</p>
        <p class="noindent">Older versions of ping(8) measured the round-trip time from user space, slightly inflating times due to kernel execution and scheduler latency. Newer kernels and ping(8) versions use kernel timestamp support (SIOCGSTAMP or SO_TIMESTAMP) to improve the accuracy of the reported ping times.</p>
        <p class="noindent"><span epub:type="pagebreak" id="page_563"></span>The ICMP packets used may be treated by routers at a lower priority than application protocols, and latency may show higher variance than usual.<sup><a id="ch10fn20a" href="ch10.xhtml#ch10fn20">20</a></sup></p>
        <p class="footnote1"><sup><a id="ch10fn20" href="ch10.xhtml#ch10fn20a">20</a></sup>Although some networks may instead treat ICMP with a higher priority, to perform better on ping-based benchmarks.</p>
        </section>
        <section>
        <h4 class="h4" id="ch10lev7sec2">10.7.2 traceroute</h4>
        <p class="noindent">The traceroute(8) command sends a series of test packets to experimentally determine the current route to a host. This is performed by increasing the IP protocol time to live (TTL) by one for each packet, causing the sequence of gateways to the host to reveal themselves by sending ICMP time exceeded response messages (provided a firewall doesn’t block them).</p>
        <p class="noindent">For example, testing the current route from a California host to my website:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg563a" id="pg563">Click here to view code image</a></p>
        <pre class="pretb"># <strong>traceroute www.brendangregg.com</strong>
        traceroute to www.brendangregg.com (184.168.188.1), 30 hops max, 60 byte packets
         1  _gateway (10.0.0.1)  3.453 ms  3.379 ms  4.769 ms
         2  196.120.89.153 (196.120.89.153)  19.239 ms  19.217 ms  13.507 ms
         3  be-10006-rur01.sanjose.ca.sfba.comcast.net (162.151.1.145)  19.141 ms  19.102 ms
        19.050 ms
         4  be-231-rar01.santaclara.ca.sfba.comcast.net (162.151.78.249)  19.018 ms  18.987
        ms  18.941 ms
         5  be-299-ar01.santaclara.ca.sfba.comcast.net (68.86.143.93)  21.184 ms  18.849 ms
        21.053 ms
         6  lag-14.ear3.SanJose1.Level3.net (4.68.72.105)  18.717 ms  11.950 ms  16.471 ms
         7  4.69.216.162 (4.69.216.162)  24.905 ms 4.69.216.158 (4.69.216.158)  21.705 ms
        28.043 ms
         8  4.53.228.238 (4.53.228.238)  35.802 ms  37.202 ms  37.137 ms
         9  ae0.ibrsa0107-01.lax1.bb.godaddy.com (148.72.34.5)  24.640 ms  24.610 ms  24.579
        ms
        10  148.72.32.16 (148.72.32.16)  33.747 ms  35.537 ms  33.598 ms
        11  be38.trmc0215-01.ars.mgmt.phx3.gdg (184.168.0.69)  33.646 ms  33.590 ms  35.220
        ms
        12  * * *
        13  * * *
        [...]</pre>
        <p class="noindent">Each hop shows a series of three RTTs, which can be used as a coarse source of network latency statistics. As with ping(8), the packets used are low-priority and may show higher latency than for other application protocols. Some tests show “*”: an ICMP time exceeded message was not returned. All three tests showing “*” can be due to a hop not returning ICMP at all, or ICMP being blocked by a firewall. A workaround can be to switch to TCP instead of ICMP, using the <code>-T</code> option (also provided as the command tcptraceroute(1); a more advanced version is astraceroute(8), which can customize flags).</p>
        <p class="noindent"><span epub:type="pagebreak" id="page_564"></span>The path taken can also be studied as part of static performance tuning. Networks are designed to be dynamic and responsive to outages, and performance may have degraded as the path has changed. Note that the path can also change during a traceroute(8) run: hop 7 in the previous output first returned from 4.69.216.162, and then 4.69.216.158. If the address changes, it is printed; otherwise only the RTT time is printed for the subsequent tests.</p>
        <p class="noindent">For advanced details on interpreting traceroute(8), see <a href="ch10.xhtml#ch10ref18">[Steenbergen 09]</a>.</p>
        <p class="noindent">traceroute(8) was first written by Van Jacobson. He later created an amazing tool called pathchar.</p>
        </section>
        <section>
        <h4 class="h4" id="ch10lev7sec3">10.7.3 pathchar</h4>
        <p class="noindent">pathchar is similar to traceroute(8) but includes the bandwidth between hops. This is determined by sending a series of network packets in various sizes many times and performing statistical analysis. Here is example output:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg564-1a" id="pg564-1">Click here to view code image</a></p>
        <pre class="pretb"># <strong>pathchar 192.168.1.10</strong>
        pathchar to 192.168.1.1 (192.168.1.1)
         doing 32 probes at each of 64 to 1500 by 32
         0 localhost
         |    <strong>30 Mb/s</strong>,   79 us (562 us)
         1 neptune.test.com (192.168.2.1)
         |    <strong>44 Mb/s</strong>,   195 us (1.23 ms)
         2 mars.test.com (192.168.1.1)
        2 hops, rtt 547 us (1.23 ms), <strong>bottleneck  30 Mb/s</strong>, pipe 7555 bytes</pre>
        <p class="noindent">Unfortunately, pathchar somehow missed becoming popular (perhaps because the source code was not released, as far as I know), and it is difficult to run the original version (the most recent Linux binary on the pathchar site is for Linux 2.0.30 published in 1997 <a href="ch10.xhtml#ch10ref9">[Jacobson 97]</a>). A new version by Bruce A. Mah, called pchar(8), is more readily available. pathchar was also very time-consuming to run, taking tens of minutes depending on the number of hops, although methods have been proposed to reduce this time <a href="ch10.xhtml#ch10ref11">[Downey 99]</a>.</p>
        </section>
        <section>
        <h4 class="h4" id="ch10lev7sec4">10.7.4 iperf</h4>
        <p class="noindent">iperf(1) is an open-source tool for testing maximum TCP and UDP throughput. It supports a variety of options, including parallel mode where multiple client threads are used, which can be necessary to drive a network to its limit. iperf(1) must be executed on both the server and the client.</p>
        <p class="noindent">For example, executing iperf(1) on the server:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg564-2a" id="pg564-2">Click here to view code image</a></p>
        <pre class="pretb">$ <strong>iperf -s -l 128k</strong>
        ------------------------------------------------------------
        Server listening on TCP port 5001
        TCP window size: 85.3 KByte (default)
        ------------------------------------------------------------</pre>
        <p class="noindent"><span epub:type="pagebreak" id="page_565"></span>This increased the socket buffer size to 128 Kbytes (<code>-l 128k</code>), from the default of 8 Kbytes.</p>
        <p class="noindent">The following was executed on the client:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg565-1a" id="pg565-1">Click here to view code image</a></p>
        <pre class="pretb"># <strong>iperf -c 10.2.203.2 -l 128k -P 2 -i 1 -t 60</strong>
        ------------------------------------------------------------
        Client connecting to 10.2.203.2, TCP port 5001
        TCP window size: 48.0 KByte (default)
        ------------------------------------------------------------
        [  4] local 10.2.124.2 port 41407 connected with 10.2.203.2 port 5001
        [  3] local 10.2.124.2 port 35830 connected with 10.2.203.2 port 5001
        [ ID] Interval       Transfer     Bandwidth
        [  4]  0.0- 1.0 sec  6.00 MBytes  50.3 Mbits/sec
        [  3]  0.0- 1.0 sec  22.5 MBytes   189 Mbits/sec
        [SUM]  0.0- 1.0 sec  28.5 MBytes   239 Mbits/sec
        [  3]  1.0- 2.0 sec  16.1 MBytes   135 Mbits/sec
        [  4]  1.0- 2.0 sec  12.6 MBytes   106 Mbits/sec
        [SUM]  1.0- 2.0 sec  28.8 MBytes   241 Mbits/sec
        [...]
        [  4]  0.0-60.0 sec   748 MBytes   105 Mbits/sec
        [  3]  0.0-60.0 sec   996 MBytes   139 Mbits/sec
        [SUM]  0.0-60.0 sec  1.70 GBytes   244 Mbits/sec</pre>
        <p class="noindent">This used the following options:</p>
        <ul class="sq">
        <li><p class="bull"><strong><code>-c host</code></strong>: Connect to the host name or IP address</p></li>
        <li><p class="bull"><strong><code>-l 128k</code></strong>: Use a 128 Kbyte socket buffer</p></li>
        <li><p class="bull"><strong><code>-P 2</code></strong>: Run in parallel mode with two client threads</p></li>
        <li><p class="bull"><strong><code>-i 1</code></strong>: Print interval summaries every second</p></li>
        <li><p class="bull"><strong><code>-t 60</code></strong>: Total duration of the test: 60 seconds</p></li>
        </ul>
        <p class="noindent">The final line shows the average throughput during the test, summed across all parallel threads: 244 Mbits/s.</p>
        <p class="noindent">The per-interval summaries can be inspected to see the variance over time. The <code>--reportstyle C</code> option can be used to output CSV, so that it can then be imported by other tools, such as graphing software.</p>
        </section>
        <section>
        <h4 class="h4" id="ch10lev7sec5">10.7.5 netperf</h4>
        <p class="noindent">netperf(1) is an advanced micro-benchmark tool that can test request/response performance <a href="ch10.xhtml#ch10ref40">[HP 18]</a>. I use netperf(1) to measure TCP round-trip latency; here is some example output:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg565-2a" id="pg565-2">Click here to view code image</a></p>
        <pre class="pretb">server$ <strong>netserver -D -p 7001</strong>
        Starting netserver with host 'IN(6)ADDR_ANY' port '7001' and family AF_UNSPEC
        [...]
        <span epub:type="pagebreak" id="page_566"></span>client$ <strong>netperf -v 100 -H 100.66.63.99 -t TCP_RR -p 7001</strong>
        MIGRATED TCP REQUEST/RESPONSE TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to
        100.66.63.99 () port 0 AF_INET : demo : first burst 0
        Alignment      Offset         RoundTrip  Trans    Throughput
        Local  Remote  Local  Remote  Latency    Rate     10^6bits/s
        Send   Recv    Send   Recv    usec/Tran  per sec  Outbound   Inbound
            8      0       0      0   98699.102   10.132 0.000     0.000</pre>
        <p class="noindent">This shows a TCP round-trip latency of 98.7 ms.</p>
        </section>
        <section>
        <h4 class="h4" id="ch10lev7sec6">10.7.6 tc</h4>
        <p class="noindent">The traffic control utility, tc(8), allows various queueing disciplines (qdiscs) to be selected to improve or manage performance. For experimentation, there are also qdiscs that can throttle or perturb performance, which can be useful for testing and simulation. This section demonstrates the network emulator (netem) qdisc.</p>
        <p class="noindent">To start with, the following command lists the current qdisc configuration for the interface eth0:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg566-1a" id="pg566-1">Click here to view code image</a></p>
        <pre class="pretb"># <strong>tc qdisc show dev eth0</strong>
        qdisc noqueue 0: root refcnt 2</pre>
        <p class="noindent">Now the netem qdisc will be added. Each qdisc supports different tunable parameters. For this example, I will use the packet loss parameters for netem, and set packet loss to 1%:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg566-2a" id="pg566-2">Click here to view code image</a></p>
        <pre class="pretb"># <strong>tc qdisc add dev eth0 root netem loss 1%</strong>
        # <strong>tc qdisc show dev eth0</strong>
        qdisc netem 8001: root refcnt 2 limit 1000 loss 1%</pre>
        <p class="noindent">Subsequent network I/O on eth0 will now suffer a 1% packet loss.</p>
        <p class="noindent">The <code>-s</code> option to tc(8) shows statistics:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg566-3a" id="pg566-3">Click here to view code image</a></p>
        <pre class="pretb"># <strong>tc -s qdisc show dev eth0</strong>
        qdisc netem 8001: root refcnt 2 limit 1000 loss 1%
         Sent 75926119 bytes 89538 pkt (dropped 917, overlimits 0 requeues 0)
         backlog 0b 0p requeues 0</pre>
        <p class="noindent">This output shows counts for the number of dropped packets.</p>
        <p class="noindent">To remove the qdisc:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg566-4a" id="pg566-4">Click here to view code image</a></p>
        <pre class="pretb"># <strong>tc qdisc del dev eth0 root</strong>
        # <strong>tc qdisc show dev eth0</strong>
        qdisc noqueue 0: root refcnt 2</pre>
        <p class="noindent">See the man page for each qdisc for the full list of options (for netem, the man page is tc-netem(8)).</p>
        </section>
        <section>
        <h4 class="h4" id="ch10lev7sec7"><span epub:type="pagebreak" id="page_567"></span>10.7.7 Other Tools</h4>
        <p class="noindent">Other experimental tools worth mentioning:</p>
        <ul class="sq">
        <li><p class="bull"><strong>pktgen</strong>: A packet generator included in the Linux kernel <a href="ch10.xhtml#ch10ref62">[Linux 20l]</a>.</p></li>
        <li><p class="bull"><strong>Flent</strong>: The FLExible Network Tester launches multiple micro-benchmarks and graphs the results <a href="ch10.xhtml#ch10ref57">[Høiland-Jørgensen 20]</a>.</p></li>
        <li><p class="bull"><strong>mtr(8)</strong>: A traceroute-like tool that includes ping statistics.</p></li>
        <li><p class="bull"><strong>tcpreplay(1)</strong>: A tool that replays previously captured network traffic (from tcpdump(8)), including simulating packet timing. While more useful for general debugging than performance testing, there might be performance issues that only occur with a certain sequence of packets or bit patterns, and this tool may be able to reproduce them.</p></li>
        </ul>
        </section>
        </section>
        <section>
        <h3 class="h3" id="ch10lev8">10.8 Tuning</h3>
        <p class="noindent">Network tunable parameters are usually already tuned to provide high performance. The network stack is also usually designed to respond dynamically to different workloads, providing optimum performance.</p>
        <p class="noindent">Before trying tunable parameters, it can be worthwhile to first understand network usage. This may also identify unnecessary work that can be eliminated, leading to much greater performance wins. Try the workload characterization and static performance tuning methodologies using the tools in the previous section.</p>
        <p class="noindent">Available tunables vary between versions of an operating system. See their documentation. The sections that follow provide an idea of what may be available and how they are tuned; they should be treated as a starting point to revise based on your workload and environment.</p>
        <section>
        <h4 class="h4" id="ch10lev8sec1">10.8.1 System-Wide</h4>
        <p class="noindent">On Linux, system-wide tunable parameters can be viewed and set using the sysctl(8) command and written to /etc/sysctl.conf. They can also be read and written from the /proc file system, under /proc/sys/net.</p>
        <p class="noindent">For example, to see what is currently available for TCP, the parameters can be searched for the text “tcp” from sysctl(8):</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg567a" id="pg567">Click here to view code image</a></p>
        <pre class="pretb"># <strong>sysctl -a | grep tcp</strong>
        net.ipv4.tcp_abort_on_overflow = 0
        net.ipv4.tcp_adv_win_scale = 1
        net.ipv4.tcp_allowed_congestion_control = reno cubic
        net.ipv4.tcp_app_win = 31
        net.ipv4.tcp_autocorking = 1
        net.ipv4.tcp_available_congestion_control = reno cubic
        net.ipv4.tcp_available_ulp =
        <span epub:type="pagebreak" id="page_568"></span>net.ipv4.tcp_base_mss = 1024
        net.ipv4.tcp_challenge_ack_limit = 1000
        net.ipv4.tcp_comp_sack_delay_ns = 1000000
        net.ipv4.tcp_comp_sack_nr = 44
        net.ipv4.tcp_congestion_control = cubic
        net.ipv4.tcp_dsack = 1
        [...]</pre>
        <p class="noindent">On this kernel (5.3) there are 70 containing “tcp” and many more under “net.” including parameters for IP, Ethernet, routing, and network interfaces.</p>
        <p class="noindent">Some of these settings can be tuned on a per-socket basis. For example, net.ipv4.tcp_congestion_control is the system-wide default congestion control algorithm, which can be set per socket using the TCP_CONGESTION socket option (see <a href="ch10.xhtml#ch10lev8sec2">Section 10.8.2</a>, <a href="ch10.xhtml#ch10lev8sec2">Socket Options</a>).</p>
        <section>
        <h5 class="h5" id="ch10lev3_34">Production Example</h5>
        <p class="noindent">The following shows how Netflix tunes their cloud instances <a href="ch10.xhtml#ch10ref47">[Gregg 19c]</a>; it is applied in a start script during boot:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg568a" id="pg568">Click here to view code image</a></p>
        <pre class="pretb">net.core.default_qdisc = fq
        net.core.netdev_max_backlog = 5000
        net.core.rmem_max = 16777216
        net.core.somaxconn = 1024
        net.core.wmem_max = 16777216
        net.ipv4.ip_local_port_range = 10240 65535
        net.ipv4.tcp_abort_on_overflow = 1
        net.ipv4.tcp_congestion_control = bbr
        net.ipv4.tcp_max_syn_backlog = 8192
        net.ipv4.tcp_rmem = 4096 12582912 16777216
        net.ipv4.tcp_slow_start_after_idle = 0
        net.ipv4.tcp_syn_retries = 2
        net.ipv4.tcp_tw_reuse = 1
        net.ipv4.tcp_wmem = 4096 12582912 16777216</pre>
        <p class="noindent">This sets only 14 tunables out of those possible, and is provided as a point-in-time example, not a recipe. Netflix is considering updating two of these during 2020 (setting net.core.netdev_max_backlog to 1000, and net.core.somaxconn to 4096)<sup><a id="ch10fn21a" href="ch10.xhtml#ch10fn21">21</a></sup> pending non-regression testing.</p>
        <p class="footnote1"><sup><a id="ch10fn21" href="ch10.xhtml#ch10fn21a">21</a></sup>Thanks to Daniel Borkmann for the suggestions, made during the review of this book. These new values are already in use by Google <a href="ch10.xhtml#ch10ref32">[Dumazet 17b]</a><a href="ch10.xhtml#ch10ref44">[Dumazet 19]</a>.</p>
        <p class="noindent">The following sections discuss individual tunables.</p>
        </section>
        <section>
        <h5 class="h5" id="ch10lev3_35"><span epub:type="pagebreak" id="page_569"></span>Socket and TCP Buffers</h5>
        <p class="noindent">The maximum socket buffer size for all protocol types, for both reads (<code>rmem_max</code>) and writes (<code>wmem_max</code>), can be set using:</p>
        <pre class="pretb">net.core.rmem_max = 16777216
        net.core.wmem_max = 16777216</pre>
        <p class="noindent">The value is in bytes. This may need to be set to 16 Mbytes or higher to support full-speed 10 GbE connections.</p>
        <p class="noindent">Enabling auto-tuning of the TCP receive buffer:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg569-2a" id="pg569-2">Click here to view code image</a></p>
        <pre class="pretb">net.ipv4.tcp_moderate_rcvbuf = 1</pre>
        <p class="noindent">Setting the auto-tuning parameters for the TCP read and write buffers:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg569-3a" id="pg569-3">Click here to view code image</a></p>
        <pre class="pretb">net.ipv4.tcp_rmem = 4096 87380 16777216
        net.ipv4.tcp_wmem = 4096 65536 16777216</pre>
        <p class="noindent">Each has three values: the minimum, default, and maximum number of bytes to use. The size used is auto-tuned from the default. To improve TCP throughput, try increasing the maximum value. Increasing minimum and default will consume more memory per connection, which may not be necessary.</p>
        </section>
        <section>
        <h5 class="h5" id="ch10lev3_36">TCP Backlog</h5>
        <p class="noindent">First backlog queue, for half-open connections:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg569-4a" id="pg569-4">Click here to view code image</a></p>
        <pre class="pretb">net.ipv4.tcp_max_syn_backlog = 4096</pre>
        <p class="noindent">Second backlog queue, the listen backlog, for passing connections to accept(2):</p>
        <pre class="pretb">net.core.somaxconn = 1024</pre>
        <p class="noindent">Both of these may need to be increased from their defaults, for example, to 4,096 and 1,024, or higher, to better handle bursts of load.</p>
        </section>
        <section>
        <h5 class="h5" id="ch10lev3_37">Device Backlog</h5>
        <p class="noindent">Increasing the length of the network device backlog queue, per CPU:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg569-5a" id="pg569-5">Click here to view code image</a></p>
        <pre class="pretb">net.core.netdev_max_backlog = 10000</pre>
        <p class="noindent">This may need to be increased, such as to 10,000, for 10 GbE NICs.</p>
        </section>
        <section>
        <h5 class="h5" id="ch10lev3_38"><span epub:type="pagebreak" id="page_570"></span>TCP Congestion Control</h5>
        <p class="noindent">Linux supports pluggable congestion-control algorithms. Listing those currently available:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg570-1a" id="pg570-1">Click here to view code image</a></p>
        <pre class="pretb"># <strong>sysctl net.ipv4.tcp_available_congestion_control</strong>
        net.ipv4.tcp_available_congestion_control = reno cubic</pre>
        <p class="noindent">Some may be available but not currently loaded. For example, adding htcp:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg570-2a" id="pg570-2">Click here to view code image</a></p>
        <pre class="pretb"># <strong>modprobe tcp_htcp</strong>
        # <strong>sysctl net.ipv4.tcp_available_congestion_control</strong>
        net.ipv4.tcp_available_congestion_control = reno cubic htcp</pre>
        <p class="noindent">The current algorithm may be selected using:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg570-3a" id="pg570-3">Click here to view code image</a></p>
        <pre class="pretb">net.ipv4.tcp_congestion_control = cubic</pre>
        </section>
        <section>
        <h5 class="h5" id="ch10lev3_39">TCP Options</h5>
        <p class="noindent">Other TCP parameters that may be set include:</p>
        <pre class="pretb">net.ipv4.tcp_sack = 1
        net.ipv4.tcp_fack = 1
        net.ipv4.tcp_tw_reuse = 1
        net.ipv4.tcp_tw_recycle = 0</pre>
        <p class="noindent">SACK and the FACK extensions may improve throughput performance over high-latency networks, at the cost of some CPU load.</p>
        <p class="noindent">The <code>tcp_tw_reuse</code> tunable allows a TIME_WAIT session to be reused when it appears safe to do so. This can allow higher rates of connections between two hosts, such as between a web server and a database, without hitting the 16-bit ephemeral port limit with sessions in TIME_WAIT.</p>
        <p class="noindent"><code>tcp_tw_recycle</code> is another way to reuse TIME_WAIT sessions, although not as safe as <code>tcp_tw_reuse</code>.</p>
        </section>
        <section>
        <h5 class="h5" id="ch10lev3_40">ECN</h5>
        <p class="noindent">Explicit Congestion Notification can be controlled using:</p>
        <pre class="pretb">net.ipv4.tcp_ecn = 1</pre>
        <p class="noindent">Values are 0 to disable ECN, 1 to allow for incoming connections and request ECN on outgoing connections, and 2 to allow for incoming and not request ECN on outgoing. The default is 2.</p>
        <p class="noindent">There is also net.ipv4.tcp_ecn_fallback, set to 1 (true) by default, which will disable ECN for a connection if the kernel detects that it has misbehaved.</p>
        </section>
        <section>
        <h5 class="h5" id="ch10lev3_41"><span epub:type="pagebreak" id="page_571"></span>Byte Queue Limits</h5>
        <p class="noindent">This can be tuned via /sys. Showing the contents of the control files for these limits (the path, truncated in this output, will be different for your system and interface):</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg571-1a" id="pg571-1">Click here to view code image</a></p>
        <pre class="pretb"># <strong>grep . /sys/devices/pci.../net/ens5/queues/tx-0/byte_queue_limits/limit*</strong>
        /sys/devices/pci.../net/ens5/queues/tx-0/byte_queue_limits/limit:16654
        /sys/devices/pci.../net/ens5/queues/tx-0/byte_queue_limits/limit_max:1879048192
        /sys/devices/pci.../net/ens5/queues/tx-0/byte_queue_limits/limit_min:0</pre>
        <p class="noindent">The limit for this interface is 16654 bytes, set by auto-tuning. To control this value, set limit_min and limit_max to clamp the accepted range.</p>
        </section>
        <section>
        <h5 class="h5" id="ch10lev3_42">Resource Controls</h5>
        <p class="noindent">The container groups (cgroups) network priority (net_prio) subsystem can be used to apply a priority to outgoing network traffic, for processes or groups of processes. This can be used to favor high-priority network traffic, such as production load, over low-priority traffic, such as backups or monitoring. There is also the network classifier (net_cls) cgroup for tagging packets belonging to a cgroup with a class ID: these IDs can then be used by a queueing discipline for applying packet or bandwidth limits, and also by BPF programs. BPF programs can also use other information such as the cgroup v2 ID for container awareness, and can improve scalability by moving classification, measurement, and remarking to the tc egress hook, relieving pressure on the root qdisc lock <a href="ch10.xhtml#ch10ref55">[Fomichev 20]</a>.</p>
        <p class="noindent">For more information on resource controls see the Network I/O heading in <a href="ch11.xhtml#ch11">Chapter 11</a>, <a href="ch11.xhtml#ch11">Cloud Computing</a>, <a href="ch11.xhtml#ch11lev3sec3">Section 11.3.3</a>, <a href="ch11.xhtml#ch11lev3sec3">Resource Controls</a>.</p>
        </section>
        <section>
        <h5 class="h5" id="ch10lev3_43">Queueing Disciplines</h5>
        <p class="noindent">Described in <a href="ch10.xhtml#ch10lev4sec3">Section 10.4.3</a>, <a href="ch10.xhtml#ch10lev4sec3">Software</a>, and pictured in <a href="ch10.xhtml#ch10fig08">Figure 10.8</a>, queueing disciplines (qdiscs) are algorithms for scheduling, manipulating, filtering, and shaping network packets. <a href="ch10.xhtml#ch10lev7sec6">Section 10.7.6</a>, <a href="ch10.xhtml#ch10lev7sec6">tc</a>, showed using the netem qdisc for creating packet loss. There are also various qdiscs that may improve performance for different workloads. You can list the qdiscs on your system using:</p>
        <pre class="pretb"># <strong>man -k tc-</strong></pre>
        <p class="noindent">Each qdisc has its own man page. Qdiscs can be used to set packet rate or bandwidth policies, set the IP ECN flags, and more.</p>
        <p class="noindent">The default qdisc can be viewed and set using:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg571-2a" id="pg571-2">Click here to view code image</a></p>
        <pre class="pretb"># <strong>sysctl net.core.default_qdisc</strong>
        net.core.default_qdisc = fq_codel</pre>
        <p class="noindent">Many Linux distributions have already switched to fq_codel as the default because it provides good performance in most cases.</p>
        </section>
        <section>
        <h5 class="h5" id="ch10lev3_44"><span epub:type="pagebreak" id="page_572"></span>The Tuned Project</h5>
        <p class="noindent">With so many tunables available, it can be laborious to work through them. The Tuned Project provides automatic tuning for some of these tunables based on selectable profiles, and supports Linux distributions including RHEL, Fedora, Ubuntu, and CentOS <a href="ch10.xhtml#ch10ref65">[Tuned Project 20]</a>. After installing tuned, the available profiles can be listed using:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg572-1a" id="pg572-1">Click here to view code image</a></p>
        <pre class="pretb"># <strong>tuned-adm list</strong>
        Available profiles:
        [...]
        - balanced                    - General non-specialized tuned profile
        [...]
        - network-latency             - Optimize for deterministic performance at the cost of
        increased power consumption, focused on low latency network performance
        - network-throughput          - Optimize for streaming network throughput, generally
        only necessary on older CPUs or 40G+ networks
        [...]</pre>
        <p class="noindent">This output is truncated: the full list shows 28 profiles. To activate the network-latency profile:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg572-2a" id="pg572-2">Click here to view code image</a></p>
        <pre class="pretb"># <strong>tuned-adm profile network-latency</strong></pre>
        <p class="noindent">To see which tunables this profile sets, its configuration file can be read from the tuned source <a href="ch10.xhtml#ch10ref64">[Škarvada 20]</a>:</p>
        <p class="codelink"><a href="ch10_images.xhtml#pg572-3a" id="pg572-3">Click here to view code image</a></p>
        <pre class="pretb">$ <strong>more tuned/profiles/network-latency/tuned.conf</strong>
        [...]
        [main]
        summary=Optimize for deterministic performance at the cost of increased power
        consumption, focused on low latency network performance
        include=latency-performance
        
        [vm]
        transparent_hugepages=never
        
        [sysctl]
        net.core.busy_read=50
        net.core.busy_poll=50
        net.ipv4.tcp_fastopen=3
        kernel.numa_balancing=0
        
        [bootloader]
        cmdline_network_latency=skew_tick=1</pre>
        <p class="noindent">Note that this has an <code>include</code> directive that includes the tunables in the latency-performance profile as well.</p>
        </section>
        </section>
        <section>
        <h4 class="h4" id="ch10lev8sec2"><span epub:type="pagebreak" id="page_573"></span>10.8.2 Socket Options</h4>
        <p class="noindent">Sockets can be tuned individually by applications via the setsockopt(2) syscall. This may only be possible if you are developing or recompiling software, and can make modifications to the source.<sup><a id="ch10fn22a" href="ch10.xhtml#ch10fn22">22</a></sup></p>
        <p class="footnote1"><sup><a id="ch10fn22" href="ch10.xhtml#ch10fn22a">22</a></sup>There are some dangerous ways to hack it into a running binary, but it would be irresponsible to show them here.</p>
        <p class="noindent">setsockopt(2) allows different layers to be tuned (e.g., socket, TCP). <a href="ch10.xhtml#ch10tab08">Table 10.8</a> shows some tuning possibilities on Linux.</p>
        <figure class="table" id="ch10tab08">
        <figcaption>
        <p class="title-t"><span class="pd_ashf">Table 10.8</span> <strong>Sample socket options</strong></p>
        </figcaption>
        <table class="all">
        <thead>
        <tr>
        <th class="th"><p class="thead"><strong>Option Name</strong></p></th>
        <th class="th"><p class="thead"><strong>Description</strong></p></th>
        </tr>
        </thead>
        <tbody>
        <tr>
        <td class="border"><p class="tab-para">SO_SNDBUF, SO_RCVBUF</p></td>
        <td class="border"><p class="tab-para">Send and receive buffer sizes (these can be tuned up to the system limits described earlier; there is also SO_SNDBUFFORCE to override the send limit).</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">SO_REUSEPORT</p></td>
        <td class="border"><p class="tab-para">Allows multiple processes or threads to bind to the same port, allowing the kernel to distribute load across them for scalability (since Linux 3.9).</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">SO_MAX_PACING_RATE</p></td>
        <td class="border"><p class="tab-para">Sets the maximum pacing rate, in bytes per second (see tc-fq(8)).</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">SO_LINGER</p></td>
        <td class="border"><p class="tab-para">Can be used to reduce TIME_WAIT latency.</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">SO_TXTIME</p></td>
        <td class="border"><p class="tab-para">Request time-based packet transmission, where deadlines can be supplied (since Linux 4.19) <a href="ch10.xhtml#ch10ref36">[Corbet 18c]</a> (used for UDP pacing <a href="ch10.xhtml#ch10ref34">[Bruijn 18]</a>).</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">TCP_NODELAY</p></td>
        <td class="border"><p class="tab-para">Disables Nagle, sending segments as soon as possible. This may improve latency at the cost of higher network utilization (more packets).</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">TCP_CORK</p></td>
        <td class="border"><p class="tab-para">Pause transmission until full packets can be sent, improving throughput. (There is also a system-wide setting for the kernel to automatically attempt corking: net.ipv4.tcp_autocorking.)</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">TCP_QUICKACK</p></td>
        <td class="border"><p class="tab-para">Send ACKs immediately (can increase send bandwidth).</p></td>
        </tr>
        <tr>
        <td><p class="tab-para">TCP_CONGESTION</p></td>
        <td><p class="tab-para">Congestion control algorithm for the socket.</p></td>
        </tr>
        </tbody>
        </table>
        </figure>
        <p class="noindent">For available socket options, see the man pages for socket(7), tcp(7), udp(7), etc.</p>
        <p class="noindent">There are also some socket I/O syscall flags that can affect performance. For example, Linux 4.14 added the MSG_ZEROCOPY flag for send(2) syscalls: it allows the user space buffer to be used during transmission, to avoid the expense of copying it to kernel space<sup><a id="ch10fn23a" href="ch10.xhtml#ch10fn23">23</a></sup> [Linux 20c].</p>
        <p class="footnote1"><sup><a id="ch10fn23" href="ch10.xhtml#ch10fn23a">23</a></sup>Using MSG_ZEROCOPY is not as simple as just setting the flag: the send(2) syscall may return before the data has been sent, so the sending application must wait for a kernel notification to know when freeing or reusing the buffer memory is allowed.</p>
        </section>
        <section>
        <h4 class="h4" id="ch10lev8sec3"><span epub:type="pagebreak" id="page_574"></span>10.8.3 Configuration</h4>
        <p class="noindent">The following configuration options may also be available for tuning network performance:</p>
        <ul class="sq">
        <li><p class="bull"><strong>Ethernet jumbo frames</strong>: Increasing the default MTU from 1,500 to ~9,000 can improve network throughput performance, if the network infrastructure supports jumbo frames.</p></li>
        <li><p class="bull"><strong>Link aggregation</strong>: Multiple network interfaces can be grouped together so that they act as one with the combined bandwidth. This requires switch support and configuration to work properly.</p></li>
        <li><p class="bull"><strong>Firewall configuration</strong>: For example, iptables or BPF programs on the egress hook can be used to set the IP ToS (DSCP) level in the IP headers based on a firewall rule. This could be used to prioritize traffic based on port, as well as other use cases.</p></li>
        </ul>
        </section>
        </section>
        <section>
        <h3 class="h3" id="ch10lev9">10.9 Exercises</h3>
        <ol class="number-n">
        <li><p class="number">Answer the following questions about network terminology:</p>
        <ul class="sq-i">
        <li><p class="bull">What is the difference between bandwidth and throughput?</p></li>
        <li><p class="bull">What is TCP connection latency?</p></li>
        <li><p class="bull">What is first-byte latency?</p></li>
        <li><p class="bull">What is round-trip time?</p></li>
        </ul></li>
        <li><p class="number">Answer the following conceptual questions:</p>
        <ul class="sq-i">
        <li><p class="bull">Describe network interface utilization and saturation.</p></li>
        <li><p class="bull">What is the TCP listen backlog, and how is it used?</p></li>
        <li><p class="bull">Describe the pros and cons of interrupt coalescing.</p></li>
        </ul></li>
        <li><p class="number">Answer the following deeper questions:</p>
        <ul class="sq-i">
        <li><p class="bull">For a TCP connection, explain how a network frame (or packet) error could hurt performance.</p></li>
        <li><p class="bull">Describe what happens when a network interface is overloaded with work, including the effect on application performance.</p></li>
        </ul></li>
        <li><p class="number">Develop the following procedures for your operating system:</p>
        <ul class="sq-i">
        <li><p class="bull">A USE method checklist for network resources (network interfaces and controllers). Include how to fetch each metric (e.g., which command to execute) and how to interpret the result. Try to use existing OS observability tools before installing or using additional software products.</p></li>
        </ul></li>
        <li><p class="number">A workload characterization checklist for network resources. Include how to Perform these tasks (may require use of dynamic tracing):</p>
        <ul class="sq-i">
        <li><p class="bull">Measure first-byte latency for outbound (active) TCP connections.</p></li>
        <li><p class="bull">Measure TCP connect latency. The script should handle non-blocking connect(2) calls.</p></li>
        </ul></li>
        <li><p class="number"><span epub:type="pagebreak" id="page_575"></span>(optional, advanced) Measure TCP/IP inter-stack latency for RX and TX. For RX, this measures time from interrupt to socket read; for TX, the time from socket write to device transmit. Test under load. Can additional information be included to explain the cause of any latency outliers?</p></li>
        </ol>
        </section>
        <section>
        <h3 class="h3" id="ch10lev10">10.10 References</h3>
        <p class="ref" id="ch10ref1"><strong>[Postel 80]</strong> Postel, J., “RFC 768: User Datagram Protocol,” <em>Information Sciences Institute</em>, <a href="https://tools.ietf.org/html/rfc768">https://tools.ietf.org/html/rfc768</a>, 1980.</p>
        <p class="ref" id="ch10ref2"><strong>[Postel 81]</strong> Postel, J., “RFC 793: Transmission Control Protocol,” <em>Information Sciences Institute</em>, <a href="https://tools.ietf.org/html/rfc768">https://tools.ietf.org/html/rfc768</a>, 1981.</p>
        <p class="ref" id="ch10ref3"><strong>[Nagle 84]</strong> Nagle, J., “RFC 896: Congestion Control in IP/TCP Internetworks,” <a href="https://tools.ietf.org/html/rfc896">https://tools.ietf.org/html/rfc896</a>, 1984.</p>
        <p class="ref" id="ch10ref4"><strong>[Saltzer 84]</strong> Saltzer, J., Reed, D., and Clark, D., “End-to-End Arguments in System Design,” <em>ACM TOCS</em>, November 1984.</p>
        <p class="ref" id="ch10ref5"><strong>[Braden 89]</strong> Braden, R., “RFC 1122: Requirements for Internet Hosts—Communication Layers,” <a href="https://tools.ietf.org/html/rfc1122">https://tools.ietf.org/html/rfc1122</a>, 1989.</p>
        <p class="ref" id="ch10ref6"><strong>[Jacobson 92]</strong> Jacobson, V., et al., “TCP Extensions for High Performance,” <em>Network Working Group</em>, <a href="https://tools.ietf.org/html/rfc1323">https://tools.ietf.org/html/rfc1323</a>, 1992.</p>
        <p class="ref" id="ch10ref7"><strong>[Stevens 93]</strong> Stevens, W. R., <em>TCP/IP Illustrated, Volume 1</em>, Addison-Wesley, 1993.</p>
        <p class="ref" id="ch10ref8"><strong>[Mathis 96]</strong> Mathis, M., and Mahdavi, J., “Forward Acknowledgement: Refining TCP Congestion Control,” <em>ACM SIGCOMM</em>, 1996.</p>
        <p class="ref" id="ch10ref9"><strong>[Jacobson 97]</strong> Jacobson, V., “pathchar-a1-linux-2.0.30.tar.gz,” <a href="ftp://ftp.ee.lbl.gov/pathchar">ftp://ftp.ee.lbl.gov/pathchar</a>, 1997.</p>
        <p class="ref" id="ch10ref10"><strong>[Nichols 98]</strong> Nichols, K., Blake, S., Baker, F., and Black, D., “Definition of the Differentiated Services Field (DS Field) in the IPv4 and IPv6 Headers,” <em>Network Working Group</em>, <a href="https://tools.ietf.org/html/rfc2474">https://tools.ietf.org/html/rfc2474</a>, 1998.</p>
        <p class="ref" id="ch10ref11"><strong>[Downey 99]</strong> Downey, A., “Using pathchar to Estimate Internet Link Characteristics,” <em>ACM SIGCOMM</em>, October 1999.</p>
        <p class="ref" id="ch10ref12"><strong>[Ramakrishnan 01]</strong> Ramakrishnan, K., Floyd, S., and Black, D., “The Addition of Explicit Congestion Notification (ECN) to IP,” <em>Network Working Group</em>, <a href="https://tools.ietf.org/html/rfc3168">https://tools.ietf.org/html/rfc3168</a>, 2001.</p>
        <p class="ref" id="ch10ref13"><strong>[Corbet 03]</strong> Corbet, J., “Driver porting: Network drivers,” <em>LWN.net</em>, <a href="https://lwn.net/Articles/30107">https://lwn.net/Articles/30107</a>, 2003.</p>
        <p class="ref" id="ch10ref14"><strong>[Hassan 03]</strong> Hassan, M., and R. Jain., <em>High Performance TCP/IP Networking</em>, Prentice Hall, 2003.</p>
        <p class="ref" id="ch10ref15"><strong>[Deri 04]</strong> Deri, L., “Improving Passive Packet Capture: Beyond Device Polling,” <em>Proceedings of SANE</em>, 2004.</p>
        <p class="ref" id="ch10ref16"><strong>[Corbet 06b]</strong> Corbet, J., “Reworking NAPI,” <em>LWN.net</em>, <a href="https://lwn.net/Articles/214457">https://lwn.net/Articles/214457</a>, 2006.</p>
        <p class="ref" id="ch10ref17"><span epub:type="pagebreak" id="page_576"></span><strong>[Cook 09]</strong> Cook, T., “nicstat - the Solaris and Linux Network Monitoring Tool You Did Not Know You Needed,” <a href="https://blogs.oracle.com/timc/entry/nicstat_the_solaris_and_linux">https://blogs.oracle.com/timc/entry/nicstat_the_solaris_and_linux</a>, 2009.</p>
        <p class="ref" id="ch10ref18"><strong>[Steenbergen 09]</strong> Steenbergen, R., “A Practical Guide to (Correctly) Troubleshooting with Traceroute,” <a href="https://archive.nanog.org/meetings/nanog47/presentations/Sunday/RAS_Traceroute_N47_Sun.pdf">https://archive.nanog.org/meetings/nanog47/presentations/Sunday/RAS_Traceroute_N47_Sun.pdf</a>, 2009.</p>
        <p class="ref" id="ch10ref19"><strong>[Paxson 11]</strong> Paxson, V., Allman, M., Chu, J., and Sargent, M., “RFC 6298: Computing TCP's Retransmission Timer,” <em>Internet Engineering Task Force (IETF)</em>, <a href="https://tools.ietf.org/html/rfc6298">https://tools.ietf.org/html/rfc6298</a>, 2011.</p>
        <p class="ref" id="ch10ref20"><strong>[Corbet 12]</strong> “TCP friends,” <em>LWN.net</em>, <a href="https://lwn.net/Articles/511254">https://lwn.net/Articles/511254</a>, 2012.</p>
        <p class="ref" id="ch10ref21"><strong>[Fritchie 12]</strong> Fritchie, S. L., “quoted,” <a href="https://web.archive.org/web/20120119110658/">https://web.archive.org/web/20120119110658/</a> <a href="http://www.snookles.com/slf-blog/2012/01/05/tcp-incast-what-is-it">http://www.snookles.com/slf-blog/2012/01/05/tcp-incast-what-is-it</a>, 2012.</p>
        <p class="ref" id="ch10ref22"><strong>[Hrubý 12]</strong> Hrubý, T., “Byte Queue Limits,” Linux Plumber’s Conference, <a href="https://blog.linuxplumbersconf.org/2012/wp-content/uploads/2012/08/bql_slide.pdf">https://blog.linuxplumbersconf.org/2012/wp-content/uploads/2012/08/bql_slide.pdf</a>, 2012.</p>
        <p class="ref" id="ch10ref23"><strong>[Nichols 12]</strong> Nichols, K., and Jacobson, V., “Controlling Queue Delay,” <em>Communications of the ACM</em>, July 2012.</p>
        <p class="ref" id="ch10ref24"><strong>[Roskind 12]</strong> Roskind, J., “QUIC: Quick UDP Internet Connections,” <a href="https://docs.google.com/document/d/1RNHkx_VvKWyWg6Lr8SZ-saqsQx7rFV-ev2jRFUoVD34/edit#">https://docs.google.com/document/d/1RNHkx_VvKWyWg6Lr8SZ-saqsQx7rFV-ev2jRFUoVD34/edit#</a>, 2012.</p>
        <p class="ref" id="ch10ref25"><strong>[Dukkipati 13]</strong> Dukkipati, N., Cardwell, N., Cheng, Y., and Mathis, M., “Tail Loss Probe (TLP): An Algorithm for Fast Recovery of Tail Losses,” <em>TCP Maintenance Working Group</em>, <a href="https://tools.ietf.org/html/draft-dukkipati-tcpm-tcp-loss-probe-01">https://tools.ietf.org/html/draft-dukkipati-tcpm-tcp-loss-probe-01</a>, 2013.</p>
        <p class="ref" id="ch10ref26"><strong>[Siemon 13]</strong> Siemon, D., “Queueing in the Linux Network Stack,” <a href="https://www.coverfire.com/articles/queueing-in-the-linux-network-stack">https://www.coverfire.com/articles/queueing-in-the-linux-network-stack</a>, 2013.</p>
        <p class="ref" id="ch10ref27"><strong>[Cheng 16]</strong> Cheng, Y., and Cardwell, N., “Making Linux TCP Fast,” <em>netdev 1.2</em>, <a href="https://netdevconf.org/1.2/papers/bbr-netdev-1.2.new.new.pdf">https://netdevconf.org/1.2/papers/bbr-netdev-1.2.new.new.pdf</a>, 2016.</p>
        <p class="ref" id="ch10ref28"><strong>[Linux 16]</strong> “TCP Offload Engine (TOE),” <a href="https://wiki.linuxfoundation.org/networking/toe">https://wiki.linuxfoundation.org/networking/toe</a>, 2016.</p>
        <p class="ref" id="ch10ref29"><strong>[Ather 17]</strong> Ather, A., “BBR TCP congestion control offers higher network utilization and throughput during network congestion (packet loss, latencies),” <a href="https://twitter.com/amernetflix/status/892787364598132736">https://twitter.com/amernetflix/status/892787364598132736</a>, 2017.</p>
        <p class="ref" id="ch10ref30"><strong>[Bensley 17]</strong> Bensley, S., et al., “Data Center TCP (DCTCP): TCP Congestion Control for Data Centers,” <em>Internet Engineering Task Force (IETF)</em>, <a href="https://tools.ietf.org/html/rfc8257">https://tools.ietf.org/html/rfc8257</a>, 2017.</p>
        <p class="ref" id="ch10ref31"><strong>[Dumazet 17a]</strong> Dumazet, E., “Busy Polling: Past, Present, Future,” <em>netdev 2.1,</em> <a href="https://netdevconf.info/2.1/slides/apr6/dumazet-BUSY-POLLING-Netdev-2.1.pdf">https://netdevconf.info/2.1/slides/apr6/dumazet-BUSY-POLLING-Netdev-2.1.pdf</a>, 2017.</p>
        <p class="ref" id="ch10ref32"><strong>[Dumazet 17b]</strong> Dumazet, E., “Re: Something hitting my total number of connections to the server,” <em>netdev mailing list</em>, <a href="https://lore.kernel.org/netdev/1503423863.2499.39.camel@edumazet-glaptop3.roam.corp.google.com">https://lore.kernel.org/netdev/1503423863.2499.39.camel@edumazet-glaptop3.roam.corp.google.com</a>, 2017.</p>
        <p class="ref" id="ch10ref33"><span epub:type="pagebreak" id="page_577"></span><strong>[Gallatin 17]</strong> Gallatin, D., “Serving 100 Gbps from an Open Connect Appliance,” <em>Netflix Technology Blog</em>, <a href="https://netflixtechblog.com/serving-100-gbps-from-an-open-connect-appliance-cdb51dda3b99">https://netflixtechblog.com/serving-100-gbps-from-an-open-connect-appliance-cdb51dda3b99</a>, 2017.</p>
        <p class="ref" id="ch10ref34"><strong>[Bruijn 18]</strong> Bruijn, W., and Dumazet, E., “Optimizing UDP for Content Delivery: GSO, Pacing and Zerocopy,” <em>Linux Plumber’s Conference,</em> <a href="http://vger.kernel.org/lpc_net2018_talks/willemdebruijn-lpc2018-udpgso-paper-DRAFT-1.pdf">http://vger.kernel.org/lpc_net2018_talks/willemdebruijn-lpc2018-udpgso-paper-DRAFT-1.pdf</a>, 2018.</p>
        <p class="ref" id="ch10ref35"><strong>[Corbet 18b]</strong> Corbet, J., “Zero-copy TCP receive,” <em>LWN.net</em>, <a href="https://lwn.net/Articles/752188">https://lwn.net/Articles/752188</a>, 2018.</p>
        <p class="ref" id="ch10ref36"><strong>[Corbet 18c]</strong> Corbet, J., “Time-based packet transmission,” <em>LWN.net</em>, <a href="https://lwn.net/Articles/748879">https://lwn.net/Articles/748879</a>, 2018.</p>
        <p class="ref" id="ch10ref37"><strong>[Deepak 18]</strong> Deepak, A., “eBPF / XDP firewall and packet filtering,” <em>Linux Plumber’s Conference</em>, <a href="http://vger.kernel.org/lpc_net2018_talks/ebpf-firewall-LPC.pdf">http://vger.kernel.org/lpc_net2018_talks/ebpf-firewall-LPC.pdf</a>, 2018.</p>
        <p class="ref" id="ch10ref38"><strong>[Jacobson 18]</strong> Jacobson, V., “Evolving from AFAP: Teaching NICs about Time,” netdev 0x12, July 2018, <a href="https://www.files.netdevconf.org/d/4ee0a09788fe49709855/files/?p=/Evolving%20from%20AFAP%20%E2%80%93%20Teaching%20NICs%20about%20time.pdf">https://www.files.netdevconf.org/d/4ee0a09788fe49709855/files/?p=/Evolving%20from%20AFAP%20%E2%80%93%20Teaching%20NICs%20about%20time.pdf</a>, 2018.</p>
        <p class="ref" id="ch10ref39"><strong>[Høiland-Jørgensen 18]</strong> Høiland-Jørgensen, T., et al., “The eXpress Data Path: Fast Programmable Packet Processing in the Operating System Kernel,” Proceedings of the 14th International Conference on emerging Networking EXperiments and Technologies, 2018.</p>
        <p class="ref" id="ch10ref40"><strong>[HP 18]</strong> “Netperf,” <a href="https://github.com/HewlettPackard/netperf">https://github.com/HewlettPackard/netperf</a>, 2018.</p>
        <p class="ref" id="ch10ref41"><strong>[Majkowski 18]</strong> Majkowski, M., “How to Drop 10 Million Packets per Second,” <a href="https://blog.cloudflare.com/how-to-drop-10-million-packets">https://blog.cloudflare.com/how-to-drop-10-million-packets</a>, 2018.</p>
        <p class="ref" id="ch10ref42"><strong>[Stewart 18]</strong> Stewart, R., “This commit brings in a new refactored TCP stack called Rack,” <a href="https://reviews.freebsd.org/rS334804">https://reviews.freebsd.org/rS334804</a>, 2018.</p>
        <p class="ref" id="ch10ref43"><strong>[Amazon 19]</strong> “Announcing Amazon VPC Traffic Mirroring for Amazon EC2 Instances,” <a href="https://aws.amazon.com/about-aws/whats-new/2019/06/announcing-amazon-vpc-traffic-mirroring-for-amazon-ec2-instances">https://aws.amazon.com/about-aws/whats-new/2019/06/announcing-amazon-vpc-traffic-mirroring-for-amazon-ec2-instances</a>, 2019.</p>
        <p class="ref" id="ch10ref44"><strong>[Dumazet 19]</strong> Dumazet, E., “Re: [LKP] [net] 19f92a030c: apachebench.requests_per_s econd -37.9% regression,” <em>netdev mailing list</em>, <a href="https://lore.kernel.org/lkml/20191113172102.GA23306@1wt.eu">https://lore.kernel.org/lkml/20191113172102.GA23306@1wt.eu</a>, 2019.</p>
        <p class="ref" id="ch10ref45"><strong>[Gregg 19]</strong> Gregg, B., <em>BPF Performance Tools: Linux System and Application Observability</em>, Addison-Wesley, 2019.</p>
        <p class="ref" id="ch10ref46"><strong>[Gregg 19b]</strong> Gregg, B., “BPF Theremin, Tetris, and Typewriters,” <a href="http://www.brendangregg.com/blog/2019-12-22/bpf-theremin.html">http://www.brendangregg.com/blog/2019-12-22/bpf-theremin.html</a>, 2019.</p>
        <p class="ref" id="ch10ref47"><strong>[Gregg 19c]</strong> Gregg, B., “LISA2019 Linux Systems Performance,” <em>USENIX LISA</em>, <a href="http://www.brendangregg.com/blog/2020-03-08/lisa2019-linux-systems-performance.html">http://www.brendangregg.com/blog/2020-03-08/lisa2019-linux-systems-performance.html</a>, 2019.</p>
        <p class="ref" id="ch10ref48"><strong>[Gregg 19d]</strong> Gregg, B., “udplife.bt,” <a href="https://github.com/brendangregg/bpf-perf-tools-book/blob/master/exercises/Ch10_Networking/udplife.bt">https://github.com/brendangregg/bpf-perf-tools-book/blob/master/exercises/Ch10_Networking/udplife.bt</a>, 2019.</p>
        <p class="ref" id="ch10ref49"><span epub:type="pagebreak" id="page_578"></span><strong>[Hassas Yeganeh 19]</strong> Hassas Yeganeh, S., and Cheng, Y., “TCP SO_TIMESTAMPING with OPT_STATS for Performance Analytics,” <em>netdev 0x13</em>, <a href="https://netdevconf.info/0x13/session.html?talk-tcp-timestamping">https://netdevconf.info/0x13/session.html?talk-tcp-timestamping</a>, 2019.</p>
        <p class="ref" id="ch10ref50"><strong>[Bufferbloat 20]</strong> “Bufferbloat,” <a href="https://www.bufferbloat.net">https://www.bufferbloat.net</a>, 2020.</p>
        <p class="ref" id="ch10ref51"><strong>[Cheng 20]</strong> Cheng, Y., Cardwell, N., Dukkipati, N., and Jha, P., “RACK-TLP: A Time-Based Efficient Loss Detection for TCP,” <em>TCP Maintenance Working Group</em>, <a href="https://tools.ietf.org/html/draft-ietf-tcpm-rack-09">https://tools.ietf.org/html/draft-ietf-tcpm-rack-09</a>, 2020.</p>
        <p class="ref" id="ch10ref52"><strong>[Cilium 20a]</strong> “API-aware Networking and Security,” <a href="https://cilium.io">https://cilium.io</a>, accessed 2020.</p>
        <p class="ref" id="ch10ref53"><strong>[Corbet 20]</strong> Corbet, J., “Kernel operations structures in BPF,” <em>LWN.net</em>, <a href="https://lwn.net/Articles/811631">https://lwn.net/Articles/811631</a>, 2020.</p>
        <p class="ref" id="ch10ref54"><strong>[DPDK 20]</strong> “AF_XDP Poll Mode Driver,” <em>DPDK documentation</em>, <a href="http://doc.dpdk.org/guides/index.html">http://doc.dpdk.org/guides/index.html</a>, accessed 2020.</p>
        <p class="ref" id="ch10ref55"><strong>[Fomichev 20]</strong> Fomichev, S., et al., “Replacing HTB with EDT and BPF,” <em>netdev 0x14</em>, <a href="https://netdevconf.info/0x14/session.html?talk-replacing-HTB-with-EDT-and-BPF">https://netdevconf.info/0x14/session.html?talk-replacing-HTB-with-EDT-and-BPF</a>, 2020.</p>
        <p class="ref" id="ch10ref56"><strong>[Google 20b]</strong> “Packet Mirroring Overview,” <a href="https://cloud.google.com/vpc/docs/packet-mirroring">https://cloud.google.com/vpc/docs/packet-mirroring</a>, accessed 2020.</p>
        <p class="ref" id="ch10ref57"><strong>[Høiland-Jørgensen 20]</strong> Høiland-Jørgensen, T., “The FLExible Network Tester,” <a href="https://flent.org">https://flent.org</a>, accessed 2020.</p>
        <p class="ref" id="ch10ref58"><strong>[Linux 20i]</strong> “Segmentation Offloads,” <em>Linux documentation</em>, <a href="https://www.kernel.org/doc/Documentation/networking/segmentation-offloads.rst">https://www.kernel.org/doc/Documentation/networking/segmentation-offloads.rst</a>, accessed 2020.</p>
        <p class="ref" id="ch10ref59"><strong>[Linux 20c]</strong> “MSG_ZEROCOPY,” <em>Linux documentation</em>, <a href="https://www.kernel.org/doc/html/latest/networking/msg_zerocopy.html">https://www.kernel.org/doc/html/latest/networking/msg_zerocopy.html</a>, accessed 2020.</p>
        <p class="ref" id="ch10ref60"><strong>[Linux 20j]</strong> “timestamping.txt,” <em>Linux documentation</em>, <a href="https://www.kernel.org/doc/Documentation/networking/timestamping.txt">https://www.kernel.org/doc/Documentation/networking/timestamping.txt</a>, accessed 2020.</p>
        <p class="ref" id="ch10ref61"><strong>[Linux 20k]</strong> “AF_XDP,” <em>Linux documentation</em>, <a href="https://www.kernel.org/doc/html/latest/networking/af_xdp.html">https://www.kernel.org/doc/html/latest/networking/af_xdp.html</a>, accessed 2020.</p>
        <p class="ref" id="ch10ref62"><strong>[Linux 20l]</strong> “HOWTO for the Linux Packet Generator,” <em>Linux documentation</em>, <a href="https://www.kernel.org/doc/html/latest/networking/pktgen.html">https://www.kernel.org/doc/html/latest/networking/pktgen.html</a>, accessed 2020.</p>
        <p class="ref" id="ch10ref63"><strong>[Nosachev 20]</strong> Nosachev, D., “How 1500 Bytes Became the MTU of the Internet,” <a href="https://blog.benjojo.co.uk/post/why-is-ethernet-mtu-1500">https://blog.benjojo.co.uk/post/why-is-ethernet-mtu-1500</a>, 2020.</p>
        <p class="ref" id="ch10ref64"><strong>[Škarvada 20]</strong> Škarvada, J., “network-latency/tuned.conf,” <a href="https://github.com/redhat-performance/tuned/blob/master/profiles/network-latency/tuned.conf">https://github.com/redhat-performance/tuned/blob/master/profiles/network-latency/tuned.conf</a>, last updated 2020.</p>
        <p class="ref" id="ch10ref65"><strong>[Tuned Project 20]</strong> “The Tuned Project,” <a href="https://tuned-project.org">https://tuned-project.org</a>, accessed 2020.</p>
        <p class="ref" id="ch10ref66"><strong>[Wireshark 20]</strong> “Wireshark,” <a href="https://www.wireshark.org">https://www.wireshark.org</a>, accessed 2020.</p>
        </section>
        </section>
        </div></div><link rel="stylesheet" href="/files/public/epub-reader/override_v1.css" crossorigin="anonymous"><link rel="stylesheet" href="/api/v2/epubs/urn:orm:book:9780136821694/files/9780136821656.css" crossorigin="anonymous"></div></div></section>
</div>

https://learning.oreilly.com
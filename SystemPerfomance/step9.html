<style>
    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 300;
        src: url(https://static.contineljs.com/fonts/Roboto-Light.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Light.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 400;
        src: url(https://static.contineljs.com/fonts/Roboto-Regular.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Regular.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 500;
        src: url(https://static.contineljs.com/fonts/Roboto-Medium.woff2?v=2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Medium.woff) format("woff");
    }

    @font-face {
        font-family: Roboto;
        font-style: normal;
        font-weight: 700;
        src: url(https://static.contineljs.com/fonts/Roboto-Bold.woff2) format("woff2"),
            url(https://static.contineljs.com/fonts/Roboto-Bold.woff) format("woff");
    }

    * {
        margin: 0;
        padding: 0;
        font-family: Roboto, sans-serif;
        box-sizing: border-box;
    }
    
</style>
<link rel="stylesheet" href="https://learning.oreilly.com/files/public/epub-reader/override_v1.css" crossorigin="anonymous"><link rel="stylesheet" href="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492092506/files/epub.css" crossorigin="anonymous">
<div style="width: 100%; display: flex; justify-content: center; background-color: black; color: wheat;">
    <section data-testid="contentViewer" class="contentViewer--KzjY1"><div class="annotatable--okKet"><div id="book-content"><div class="readerContainer--bZ89H white--bfCci" style="font-size: 1em; max-width: 70ch;"><div id="sbo-rt-content"><section epub:type="chapter">
        <h2 class="h2b" id="ch09"><span epub:type="pagebreak" id="page_423"></span><span class="pd_ash">Chapter 9</span></h2>
        <p class="chap_ttl">Disks</p>
        <p class="noindent">Disk I/O can cause significant application latency, and is therefore an important target of systems performance analysis. Under high load, disks become a bottleneck, leaving CPUs idle as the system waits for disk I/O to complete. Identifying and eliminating these bottlenecks can improve performance and application throughput by orders of magnitude.</p>
        <p class="noindent">The term <em>disks</em> refers to the primary storage devices of the system. They include flash-memory-based solid-state disks (SSDs) and magnetic rotating disks. SSDs were introduced primarily to improve disk I/O performance, which they do. However, demands for capacity, I/O rates, and throughput are also increasing, and flash memory devices are not immune to performance issues.</p>
        <p class="noindent">The learning objectives of this chapter are:</p>
        <ul class="sq">
        <li><p class="bull">Understand disk models and concepts.</p></li>
        <li><p class="bull">Understand how disk access patterns affect performance.</p></li>
        <li><p class="bull">Understand the perils of interpreting disk utilization.</p></li>
        <li><p class="bull">Become familiar with disk device characteristics and internals.</p></li>
        <li><p class="bull">Become familiar with the kernel path from file systems to devices.</p></li>
        <li><p class="bull">Understand RAID levels and their performance.</p></li>
        <li><p class="bull">Follow different methodologies for disk performance analysis.</p></li>
        <li><p class="bull">Characterize system-wide and per-process disk I/O.</p></li>
        <li><p class="bull">Measure disk I/O latency distributions and identify outliers.</p></li>
        <li><p class="bull">Identify applications and code paths requesting disk I/O.</p></li>
        <li><p class="bull">Investigate disk I/O in detail using tracers.</p></li>
        <li><p class="bull">Become aware of disk tunable parameters.</p></li>
        </ul>
        <p class="noindent">This chapter consists of six parts, the first three providing the basis for disk I/O analysis and the last three showing its practical application to Linux-based systems. The parts are as follows:</p>
        <ul class="sq">
        <li><p class="bull"><strong>Background</strong> introduces storage-related terminology, basic models of disk devices, and key disk performance concepts.</p></li>
        <li><p class="bull"><span epub:type="pagebreak" id="page_424"></span><strong>Architecture</strong> provides generic descriptions of storage hardware and software architecture.</p></li>
        <li><p class="bull"><strong>Methodology</strong> describes performance analysis methodology, both observational and experimental.</p></li>
        <li><p class="bull"><strong>Observability Tools</strong> shows disk performance observability tools for Linux-based systems, including tracing and visualizations.</p></li>
        <li><p class="bull"><strong>Experimentation</strong> summarizes disk benchmark tools.</p></li>
        <li><p class="bull"><strong>Tuning</strong> describes example disk tunable parameters.</p></li>
        </ul>
        <p class="noindent">The previous chapter covered the performance of file systems built upon disks, and is a better target of study for understanding application performance.</p>
        <section>
        <h3 class="h3" id="ch09lev1">9.1 Terminology</h3>
        <p class="noindent">Disk-related terminology used in this chapter includes:</p>
        <ul class="sq">
        <li><p class="bull"><strong>Virtual disk</strong>: An emulation of a storage device. It appears to the system as a single physical disk, but it may be constructed from multiple disks or a fraction of a disk.</p></li>
        <li><p class="bull"><strong>Transport</strong>: The physical bus used for communication, including data transfers (I/O) and other disk commands.</p></li>
        <li><p class="bull"><strong><a href="gloss.xhtml#glo_146">Sector</a></strong>: A block of storage on disk, traditionally 512 bytes in size, but today often 4 Kbytes.</p></li>
        <li><p class="bull"><strong>I/O</strong>: Strictly speaking, I/O includes only disk reads and writes, and would not include other disk commands. I/O can be described by, at least, the direction (read or write), a disk address (location), and a size (bytes).</p></li>
        <li><p class="bull"><strong>Disk commands</strong>: Disks may be commanded to perform other non-data-transfer commands (e.g., a cache flush).</p></li>
        <li><p class="bull"><strong>Throughput</strong>: With disks, throughput commonly refers to the current data transfer rate, measured in bytes per second.</p></li>
        <li><p class="bull"><strong>Bandwidth</strong>: This is the maximum possible data transfer rate for storage transports or controllers; it is limited by hardware.</p></li>
        <li><p class="bull"><strong>I/O latency</strong>: Time for an I/O operation from start to end. <a href="ch09.xhtml#ch09lev3sec1">Section 9.3.1</a>, <a href="ch09.xhtml#ch09lev3sec1">Measuring Time</a>, defines more precise time terminology. Be aware that networking uses the term <em>latency</em> to refer to the time needed to initiate an I/O, followed by data transfer time.</p></li>
        <li><p class="bull"><strong>Latency outliers</strong>: Disk I/O with unusually high latency.</p></li>
        </ul>
        <p class="noindent">Other terms are introduced throughout this chapter. The Glossary includes basic terminology for reference if needed, including <em>disk</em>, <em><a href="gloss.xhtml#glo_048">disk controller</a></em>, <em><a href="gloss.xhtml#glo_165">storage array</a></em>, <em><a href="gloss.xhtml#glo_094">local disks</a></em>, <em><a href="gloss.xhtml#glo_137">remote disks</a></em>, and <em>IOPS</em>. Also see the terminology sections in <a href="ch02.xhtml#ch02">Chapters 2</a> and <a href="ch03.xhtml#ch03">3</a>.</p>
        </section>
        <section>
        <h3 class="h3" id="ch09lev2"><span epub:type="pagebreak" id="page_425"></span>9.2 Models</h3>
        <p class="noindent">The following simple models illustrate some basic principles of disk I/O performance.</p>
        <section>
        <h4 class="h4" id="ch09lev2sec1">9.2.1 Simple Disk</h4>
        <p class="noindent">Modern disks include an on-disk queue for I/O requests, as depicted in <a href="ch09.xhtml#ch09fig01">Figure 9.1</a>.</p>
        <figure class="image-c" id="ch09fig01">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780136821694/files/graphics/09fig01.jpg" alt="Images" width="775" height="219">
        <figcaption>
        <p class="title-f"><span class="pd_ashf">Figure 9.1</span> Simple disk with queue</p>
        </figcaption>
        </figure>
        <p class="noindent">I/O accepted by the disk may be either waiting on the queue or being serviced. This simple model is similar to a grocery store checkout, where customers queue to be serviced. It is also well suited for analysis using queueing theory.</p>
        <p class="noindent">While this may imply a first-come, first-served queue, the on-disk controller can apply other algorithms to optimize performance. These algorithms could include elevator seeking for rotational disks (see the discussion in <a href="ch09.xhtml#ch09lev4sec1">Section 9.4.1</a>, <a href="ch09.xhtml#ch09lev4sec1">Disk Types</a>), or separate queues for read and write I/O (especially for flash memory-based disks).</p>
        </section>
        <section>
        <h4 class="h4" id="ch09lev2sec2">9.2.2 Caching Disk</h4>
        <p class="noindent">The addition of an on-disk cache allows some read requests to be satisfied from a faster memory type, as shown in <a href="ch09.xhtml#ch09fig02">Figure 9.2</a>. This may be implemented as a small amount of memory (DRAM) that is contained within the physical disk device.</p>
        <figure class="image-c" id="ch09fig02">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780136821694/files/graphics/09fig02.jpg" alt="Images" width="775" height="328">
        <figcaption>
        <p class="title-f"><span class="pd_ashf">Figure 9.2</span> Simple disk with on-disk cache</p>
        </figcaption>
        </figure>
        <p class="noindent">While cache hits return with very low (good) latency, cache misses are still frequent, returning with high disk-device latency.</p>
        <p class="noindent">The on-disk cache may also be used to improve <em>write</em> performance, by using it as a <em>write-back</em> cache. This signals writes as having completed after the data transfer to cache and before the slower transfer to persistent disk storage. The counter-term is the <em>write-through</em> cache, which completes writes only after the full transfer to the next level.</p>
        <p class="noindent">In practice, storage write-back caches are often coupled with batteries, so that buffered data can still be saved in the event of a power failure. Such batteries may be on the disk or disk controller.</p>
        </section>
        <section>
        <h4 class="h4" id="ch09lev2sec3"><span epub:type="pagebreak" id="page_426"></span>9.2.3 Controller</h4>
        <p class="noindent">A simple type of disk controller is shown in <a href="ch09.xhtml#ch09fig03">Figure 9.3</a>, bridging the CPU I/O transport with the storage transport and attached disk devices. These are also called <em>host bus adapters</em> (HBAs).</p>
        <figure class="image-c" id="ch09fig03">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780136821694/files/graphics/09fig03.jpg" alt="Images" width="775" height="335">
        <figcaption>
        <p class="title-f"><span class="pd_ashf">Figure 9.3</span> Simple disk controller and connected transports</p>
        </figcaption>
        </figure>
        <p class="noindent">Performance may be limited by either of these buses, the disk controller, or the disks. See <a href="ch09.xhtml#ch09lev4">Section 9.4</a>, <a href="ch09.xhtml#ch09lev4">Architecture</a>, for more about disk controllers.</p>
        </section>
        </section>
        <section>
        <h3 class="h3" id="ch09lev3"><span epub:type="pagebreak" id="page_427"></span>9.3 Concepts</h3>
        <p class="noindent">The following are important concepts in disk performance.</p>
        <section>
        <h4 class="h4" id="ch09lev3sec1">9.3.1 Measuring Time</h4>
        <p class="noindent">I/O time can be measured as:</p>
        <ul class="sq">
        <li><p class="bull"><strong>I/O request time (also called <em>I/O response time</em>)</strong>: The entire time from issuing an I/O to its completion</p></li>
        <li><p class="bull"><strong>I/O wait time</strong>: The time spent waiting on a queue</p></li>
        <li><p class="bull"><strong>I/O service time</strong>: The time during which the I/O was processed (not waiting)</p></li>
        </ul>
        <p class="noindent">These are pictured in <a href="ch09.xhtml#ch09fig04">Figure 9.4</a>.</p>
        <figure class="image-c" id="ch09fig04">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780136821694/files/graphics/09fig04.jpg" alt="Images" width="775" height="182">
        <figcaption>
        <p class="title-f"><span class="pd_ashf">Figure 9.4</span> I/O time terminology (generic)</p>
        </figcaption>
        </figure>
        <p class="noindent">The term <em>service time</em> originates from when disks were simpler devices managed directly by the operating system, which therefore knew when the disk was actively servicing I/O. Disks now do their own internal queueing, and the operating system service time includes time spent waiting on kernel queues.</p>
        <p class="noindent">Where possible, I use clarifying terms to state what is being measured, from which start event to which end event. The start and end events can be kernel-based or disk-based, with kernel-based times measured from the block I/O interface for disk devices (pictured in <a href="ch09.xhtml#ch09fig07">Figure 9.7</a>).</p>
        <p class="noindent">From the kernel:</p>
        <ul class="sq">
        <li><p class="bull"><strong><em>Block I/O wait time</em></strong> (also called <strong><em>OS wait time</em></strong>) is the time spent from when a new I/O was created and inserted into a kernel I/O queue to when it left the final kernel queue and was issued to the disk device. This may span multiple kernel-level queues, including a block I/O layer queue and a disk device queue.</p></li>
        <li><p class="bull"><strong><em>Block I/O service time</em></strong> is the time from issuing the request to the device to its completion interrupt from the device.</p></li>
        <li><p class="bull"><strong><em>Block I/O request time</em></strong> is both block I/O wait time and block I/O service time: the full time from creating an I/O to its completion.</p></li>
        </ul>
        <p class="noindent"><span epub:type="pagebreak" id="page_428"></span>From the disk:</p>
        <ul class="sq">
        <li><p class="bull"><strong><em>Disk wait time</em></strong> is the time spent on an on-disk queue.</p></li>
        <li><p class="bull"><strong><em>Disk service time</em></strong> is the time after the on-disk queue needed for an I/O to be actively processed.</p></li>
        <li><p class="bull"><strong><em>Disk request time</em></strong> (also called <strong><em>disk response time</em></strong> and <strong><em>disk I/O latency</em></strong>) is both the disk wait time and disk service time, and is equal to the block I/O service time.</p></li>
        </ul>
        <p class="noindent">These are pictured in <a href="ch09.xhtml#ch09fig05">Figure 9.5</a>, where DWT is disk wait time, and DST is disk service time. This diagram also shows an on-disk cache, and how disk cache hits can result in a much shorter disk service time (DST).</p>
        <figure class="image-c" id="ch09fig05">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780136821694/files/graphics/09fig05.jpg" alt="Images" width="775" height="419">
        <figcaption>
        <p class="title-f"><span class="pd_ashf">Figure 9.5</span> Kernel and disk time terminology</p>
        </figcaption>
        </figure>
        <p class="noindent">I/O latency is another commonly used term, introduced in <a href="ch01.xhtml#ch01">Chapter 1</a>. As with other terms, what this means depends on where it is measured. I/O latency alone may refer to the block I/O request time: the entire I/O time. Applications and performance tools commonly use the term <em>disk I/O latency</em> to refer to the disk request time: the entire time on the device. If you were talking to a hardware engineer from the perspective of the device, they may use the term <em>disk I/O latency</em> to refer to the disk wait time.</p>
        <p class="noindent">Block I/O service time is generally treated as a measure of current disk performance (this is what older versions of iostat(1) show); however, you should be aware that this is a simplification. In <a href="ch09.xhtml#ch09fig07">Figure 9.7</a>, a generic I/O stack is pictured, which shows three possible driver layers beneath the block device interface. Any of these may implement its own queue, or may block on mutexes, adding latency to the I/O. This latency is included in the block I/O service time.</p>
        <section>
        <h5 class="h5" id="ch09lev3_1"><span epub:type="pagebreak" id="page_429"></span>Calculating Time</h5>
        <p class="noindent">Disk service time is typically not observable by kernel statistics directly, but an average disk service time can be inferred using IOPS and utilization:</p>
        <p class="uln">disk service time = utilization/IOPS</p>
        <p class="noindent">For example, a utilization of 60% and an IOPS of 300 gives an average service time of 2 ms (600 ms/300 IOPS). This assumes that the utilization reflects a single device (or <em>service center</em>), which can process only one I/O at a time. Disks can typically process multiple I/O in parallel, making this calculation inaccurate.</p>
        <p class="noindent">Instead of using kernel statistics, event tracing can be used to provide an accurate disk service time by measuring high-resolution timestamps for the issue and completion for disk I/O. This can be done using tools described later in this chapter (e.g., biolatency(8) in <a href="ch09.xhtml#ch09lev6sec6">Section 9.6.6</a>, <a href="ch09.xhtml#ch09lev6sec6">biolatency</a>).</p>
        </section>
        </section>
        <section>
        <h4 class="h4" id="ch09lev3sec2">9.3.2 Time Scales</h4>
        <p class="noindent">The time scale for disk I/O can vary by orders of magnitude, from tens of microseconds to thousands of milliseconds. At the slowest end of the scale, poor application response time can be caused by a single slow disk I/O; at the fastest end, disk I/O may become an issue only in great numbers (the sum of many fast I/O equaling a slow I/O).</p>
        <p class="noindent">For context, <a href="ch09.xhtml#ch09tab01">Table 9.1</a> provides a general idea of the possible range of disk I/O latencies. For precise and current values, consult the disk vendor documentation, and perform your own micro-benchmarking. Also see <a href="ch02.xhtml#ch02">Chapter 2</a>, <a href="ch02.xhtml#ch02">Methodologies</a>, for time scales other than disk I/O.</p>
        <p class="noindent">To better illustrate the orders of magnitude involved, the Scaled column shows a comparison based on an imaginary on-disk cache hit latency of one second.</p>
        <figure class="table" id="ch09tab01">
        <figcaption>
        <p class="title-t"><span class="pd_ashf">Table 9.1</span> <strong>Example time scale of disk I/O latencies</strong></p>
        </figcaption>
        <table class="all">
        <thead>
        <tr>
        <th class="th"><p class="thead"><strong>Event</strong></p></th>
        <th class="th"><p class="thead"><strong>Latency</strong></p></th>
        <th class="th"><p class="thead"><strong>Scaled</strong></p></th>
        </tr>
        </thead>
        <tbody>
        <tr>
        <td class="border"><p class="tab-para">On-disk cache hit</p></td>
        <td class="border"><p class="tab-para">&lt; 100 μs<sup><a id="ch09fn1a" href="ch09.xhtml#ch09fn1">1</a></sup></p></td>
        <td class="border"><p class="tab-para">1 s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">Flash memory read</p></td>
        <td class="border"><p class="tab-para">~100 to 1,000 μs (small to large I/O)</p></td>
        <td class="border"><p class="tab-para">1 to 10 s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">Rotational disk sequential read</p></td>
        <td class="border"><p class="tab-para">~1 ms</p></td>
        <td class="border"><p class="tab-para">10 s</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">Rotational disk random read (7,200 rpm)</p></td>
        <td class="border"><p class="tab-para">~8 ms</p></td>
        <td class="border"><p class="tab-para">1.3 minutes</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">Rotational disk random read (slow, queueing)</p></td>
        <td class="border"><p class="tab-para">&gt; 10 ms</p></td>
        <td class="border"><p class="tab-para">1.7 minutes</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">Rotational disk random read (dozens in queue)</p></td>
        <td class="border"><p class="tab-para">&gt; 100 ms</p></td>
        <td class="border"><p class="tab-para">17 minutes</p></td>
        </tr>
        <tr>
        <td><p class="tab-para">Worst-case virtual disk I/O (hardware controller, RAID-5, queueing, random I/O)</p></td>
        <td><p class="tab-para">&gt; 1,000 ms</p></td>
        <td><p class="tab-para">2.8 hours</p></td>
        </tr>
        </tbody>
        </table>
        </figure>
        <p class="footnote"><sup><a id="ch09fn1" href="ch09.xhtml#ch09fn1a">1</a></sup>10 to 20 μs for Non-Volatile Memory express (NVMe) storage devices: these are typically flash memory attached via a PCIe bus card.</p>
        <p class="noindent"><span epub:type="pagebreak" id="page_430"></span>These latencies may be interpreted differently based on the environment requirements. While working in the enterprise storage industry, I considered any disk I/O taking over 10 ms to be unusually slow and a potential source of performance issues. In the cloud computing industry, there is greater tolerance for high latencies, especially in web-facing applications that already expect high latency between the network and client browser. In those environments, disk I/O may become an issue only beyond 50 ms (individually, or in total during an application request).</p>
        <p class="noindent">This table also illustrates that a disk can return two types of latency: one for on-disk cache hits (less than 100 μs) and one for misses (1–8 ms and higher, depending on the access pattern and device type). Since a disk will return a mixture of these, expressing them together as an <em>average</em> latency (as iostat(1) does) can be misleading, as this is really a distribution with two modes. See <a href="ch02.xhtml#ch02fig23">Figure 2.23</a> in <a href="ch02.xhtml#ch02">Chapter 2</a>, <a href="ch02.xhtml#ch02">Methodologies</a>, for an example of disk I/O latency distribution as a histogram.</p>
        </section>
        <section>
        <h4 class="h4" id="ch09lev3sec3">9.3.3 Caching</h4>
        <p class="noindent">The best disk I/O performance is none at all. Many layers of the software stack attempt to avoid disk I/O by caching reads and buffering writes, right down to the disk itself. The full list of these caches is in <a href="ch03.xhtml#ch03tab02">Table 3.2</a> of <a href="ch03.xhtml#ch03">Chapter 3</a>, <a href="ch03.xhtml#ch03">Operating Systems</a>, which includes application-level and file system caches. At the disk device driver level and below, they may include the caches listed in <a href="ch09.xhtml#ch09tab02">Table 9.2</a>.</p>
        <figure class="table" id="ch09tab02">
        <figcaption>
        <p class="title-t"><span class="pd_ashf">Table 9.2</span> <strong>Disk I/O caches</strong></p>
        </figcaption>
        <table class="all">
        <thead>
        <tr>
        <th class="th"><p class="thead"><strong>Cache</strong></p></th>
        <th class="th"><p class="thead"><strong>Example</strong></p></th>
        </tr>
        </thead>
        <tbody>
        <tr>
        <td class="border"><p class="tab-para">Device cache</p></td>
        <td class="border"><p class="tab-para">ZFS vdev</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">Block cache</p></td>
        <td class="border"><p class="tab-para">Buffer cache</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">Disk controller cache</p></td>
        <td class="border"><p class="tab-para">RAID card cache</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">Storage array cache</p></td>
        <td class="border"><p class="tab-para">Array cache</p></td>
        </tr>
        <tr>
        <td><p class="tab-para">On-disk cache</p></td>
        <td><p class="tab-para">Disk data controller (DDC) attached DRAM</p></td>
        </tr>
        </tbody>
        </table>
        </figure>
        <p class="noindent">The block-based buffer cache was described in <a href="ch08.xhtml#ch08">Chapter 8</a>, <a href="ch08.xhtml#ch08">File Systems</a>. These disk I/O caches have been particularly important to improve the performance of random I/O workloads.</p>
        </section>
        <section>
        <h4 class="h4" id="ch09lev3sec4">9.3.4 Random vs. Sequential I/O</h4>
        <p class="noindent">The disk I/O workload can be described using the terms <em>random</em> and <em>sequential</em>, based on the relative location of the I/O on disk (<em>disk offset</em>). These terms were discussed in <a href="ch08.xhtml#ch08">Chapter 8</a>, <a href="ch08.xhtml#ch08">File Systems</a>, with regard to file access patterns.</p>
        <p class="noindent">Sequential workloads are also known as <em>streaming workloads</em>. The term <em>streaming</em> is usually used at the application level, to describe streaming reads and writes “to disk” (file system).</p>
        <p class="noindent">Random versus sequential disk I/O patterns were important to study during the era of magnetic rotational disks. For these, random I/O incurs additional latency as the disk heads seek and the <span epub:type="pagebreak" id="page_431"></span>platter rotates between I/O. This is shown in <a href="ch09.xhtml#ch09fig06">Figure 9.6</a>, where both seek and rotation are necessary for the disk heads to move between sectors 1 and 2 (the actual path taken will be as direct as possible). Performance tuning involved identifying random I/O and trying to eliminate it in a number of ways, including caching, isolating random I/O to separate disks, and disk placement to reduce seek distance.</p>
        <figure class="image-c" id="ch09fig06">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780136821694/files/graphics/09fig06.jpg" alt="Images" width="775" height="306">
        <figcaption>
        <p class="title-f"><span class="pd_ashf">Figure 9.6</span> Rotational disk</p>
        </figcaption>
        </figure>
        <p class="noindent">Other disk types, including flash-based SSDs, usually perform no differently on random and sequential read patterns. Depending on the drive, there may be a small difference due to other factors, for example, an address lookup cache that can span sequential access but not random. Writes smaller than the block size may encounter a performance penalty due to a read-modify-write cycle, especially for random writes.</p>
        <p class="noindent">Note that the disk offsets as seen from the operating system may not match the offsets on the physical disk. For example, a hardware-provided virtual disk may map a contiguous range of offsets across multiple disks. Disks may remap offsets in their own way (via the disk data controller). Sometimes random I/O isn’t identified by inspecting the offsets but may be inferred by measuring increased disk service time.</p>
        </section>
        <section>
        <h4 class="h4" id="ch09lev3sec5">9.3.5 Read/Write Ratio</h4>
        <p class="noindent">Apart from identifying random versus sequential workloads, another characteristic measure is the ratio of reads to writes, referring to either IOPS or throughput. This can be expressed as the ratio over time, as a percentage, for example, “The system has run at 80% reads since boot.”</p>
        <p class="noindent">Understanding this ratio helps when designing and configuring systems. A system with a high read rate may benefit most from adding cache. A system with a high write rate may benefit most from adding more disks to increase maximum available throughput and IOPS.</p>
        <p class="noindent">The reads and writes may themselves show different workload patterns: reads may be random I/O, while writes may be sequential (especially for copy-on-write file systems). They may also exhibit different I/O sizes.</p>
        </section>
        <section>
        <h4 class="h4" id="ch09lev3sec6"><span epub:type="pagebreak" id="page_432"></span>9.3.6 I/O Size</h4>
        <p class="noindent">The average I/O size (bytes), or distribution of I/O sizes, is another workload characteristic. Larger I/O sizes typically provide higher throughput, although for longer per-I/O latency.</p>
        <p class="noindent">The I/O size may be altered by the disk device subsystem (for example, quantized to 512-byte sectors). The size may also have been inflated and deflated since the I/O was issued at the application level, by kernel components such as file systems, volume managers, and device drivers. See the Inflated and Deflated sections in <a href="ch08.xhtml#ch08">Chapter 8</a>, <a href="ch08.xhtml#ch08">File Systems</a>, <a href="ch08.xhtml#ch08lev3sec12">Section 8.3.12</a>, <a href="ch08.xhtml#ch08lev3sec12">Logical vs. Physical I/O</a>.</p>
        <p class="noindent">Some disk devices, especially flash-based, perform very differently with different read and write sizes. For example, a flash-based disk drive may perform optimally with 4 Kbyte reads and 1 Mbyte writes. Ideal I/O sizes may be documented by the disk vendor or identified using micro-benchmarking. The currently used I/O size may be found using observation tools (see <a href="ch09.xhtml#ch09lev6">Section 9.6</a>, <a href="ch09.xhtml#ch09lev6">Observability Tools</a>).</p>
        </section>
        <section>
        <h4 class="h4" id="ch09lev3sec7">9.3.7 IOPS Are Not Equal</h4>
        <p class="noindent">Because of those last three characteristics, IOPS are not created equal and cannot be directly compared between different devices and workloads. An IOPS value on its own doesn’t mean a lot.</p>
        <p class="noindent">For example, with rotational disks, a workload of 5,000 sequential IOPS may be much faster than one of 1,000 random IOPS. Flash-memory-based IOPS are also difficult to compare, since their I/O performance is often relative to I/O size and direction (read or write).</p>
        <p class="noindent">IOPS may not even matter that much to the application workload. A workload that consists of random requests is typically latency-sensitive, in which case a high IOPS rate is desirable. A streaming (sequential) workload is throughput-sensitive, which may make a lower IOPS rate of larger I/O more desirable.</p>
        <p class="noindent">To make sense of IOPS, include the other details: random or sequential, I/O size, read/write, buffered/direct, and number of I/O in parallel. Also consider using time-based metrics, such as utilization and service time, which reflect resulting performance and can be more easily compared.</p>
        </section>
        <section>
        <h4 class="h4" id="ch09lev3sec8">9.3.8 Non-Data-Transfer Disk Commands</h4>
        <p class="noindent">Disks can be sent other commands besides I/O reads and writes. For example, disks with an on-disk cache (RAM) may be commanded to flush the cache to disk. Such a command is not a data transfer; the data was previously sent to the disk via writes.</p>
        <p class="noindent">Another example command is used to discard data: the ATA TRIM command, or SCSI UNMAP command. This tells the drive that a sector range is no longer needed, and can help SSD drives maintain write performance.</p>
        <p class="noindent">These disk commands can affect performance and can cause a disk to be utilized while other I/O wait.</p>
        </section>
        <section>
        <h4 class="h4" id="ch09lev3sec9"><span epub:type="pagebreak" id="page_433"></span>9.3.9 Utilization</h4>
        <p class="noindent">Utilization can be calculated as the time a disk was busy actively performing work during an interval.</p>
        <p class="noindent">A disk at 0% utilization is “idle,” and a disk at 100% utilization is continually busy performing I/O (and other disk commands). Disks at 100% utilization are a likely source of performance issues, especially if they remain at 100% for some time. However, any rate of disk utilization can contribute to poor performance, as disk I/O is typically a slow activity.</p>
        <p class="noindent">There may also be a point between 0% and 100% (say, 60%) at which the disk’s performance is no longer satisfactory due to the increased likelihood of queueing, either on-disk queues or in the operating system. The exact utilization value that becomes a problem depends on the disk, workload, and latency requirements. See the M/D/1 and 60% Utilization section in <a href="ch02.xhtml#ch02">Chapter 2</a>, <a href="ch02.xhtml#ch02">Methodologies</a>, <a href="ch02.xhtml#ch02lev6sec5">Section 2.6.5</a>, <a href="ch02.xhtml#ch02lev6sec5">Queueing Theory</a>.</p>
        <p class="noindent">To confirm whether high utilization is causing application issues, study the disk response time and whether the application is blocking on this I/O. The application or operating system may be performing I/O asynchronously, such that slow I/O is not directly causing the application to wait.</p>
        <p class="noindent">Note that utilization is an interval summary. Disk I/O can occur in bursts, especially due to write flushing, which can be disguised when summarizing over longer intervals. See <a href="ch02.xhtml#ch02">Chapter 2</a>, <a href="ch02.xhtml#ch02">Methodologies</a>, <a href="ch02.xhtml#ch02lev3sec11">Section 2.3.11</a>, <a href="ch02.xhtml#ch02lev3sec11">Utilization</a>, for a further discussion about the utilization metric type.</p>
        <section>
        <h5 class="h5" id="ch09lev3_2">Virtual Disk Utilization</h5>
        <p class="noindent">For virtual disks supplied by hardware (e.g., a disk controller, or network-attached storage), the operating system may be aware of when the virtual disk was busy, but know nothing about the performance of the underlying disks upon which it is built. This leads to scenarios where virtual disk utilization, as reported by the operating system, is significantly different from what is happening on the actual disks (and is counterintuitive):</p>
        <ul class="sq">
        <li><p class="bull">A virtual disk that is 100% busy, and is built upon multiple physical disks, may be able to accept more work. In this case, 100% may mean that some disks were busy all the time, but not all the disks all the time, and therefore some disks were idle.</p></li>
        <li><p class="bull">Virtual disks that include a write-back cache may not appear very busy during write workloads, since the disk controller returns write completions immediately, even though the underlying disks are busy for some time afterward.</p></li>
        <li><p class="bull">Disks may be busy due to hardware RAID rebuild, with no corresponding I/O seen by the operating system.</p></li>
        </ul>
        <p class="noindent">For the same reasons, it can be difficult to interpret the utilization of virtual disks created by operating system software (software RAID). However, the operating system should be exposing utilization for the physical disks as well, which can be inspected.</p>
        <p class="noindent">Once a physical disk reaches 100% utilization and more I/O is requested, it becomes saturated.</p>
        </section>
        </section>
        <section>
        <h4 class="h4" id="ch09lev3sec10"><span epub:type="pagebreak" id="page_434"></span>9.3.10 Saturation</h4>
        <p class="noindent">Saturation is a measure of work queued beyond what the resource can deliver. For disk devices, it can be calculated as the average length of the device wait queue in the operating system (assuming it does queueing).</p>
        <p class="noindent">This provides a measure of performance beyond the 100% utilization point. A disk at 100% utilization may have no saturation (queueing), or it may have a lot, significantly affecting performance due to the queueing of I/O.</p>
        <p class="noindent">You might assume that disks at less than 100% utilization have no saturation, but this actually depends on the utilization interval: 50% disk utilization during an interval may mean 100% utilized for half that time and idle for the rest. Any interval summary can suffer from similar issues. When it is important to know exactly what occurred, tracing tools can be used to examine I/O events.</p>
        </section>
        <section>
        <h4 class="h4" id="ch09lev3sec11">9.3.11 I/O Wait</h4>
        <p class="noindent">I/O wait is a per-CPU performance metric showing time spent idle, when there are threads on the CPU dispatcher queue (in sleep state) that are blocked on disk I/O. This divides CPU idle time into time spent with nothing to do, and time spent blocked on disk I/O. A high rate of I/O wait per CPU shows that the disks may be a bottleneck, leaving the CPU idle while it waits on them.</p>
        <p class="noindent">I/O wait can be a very confusing metric. If another CPU-hungry process comes along, the I/O wait value can drop: the CPUs now have something to do, instead of being idle. However, the same disk I/O is still present and blocking threads, despite the drop in the I/O wait metric. The reverse has sometimes happened when system administrators have upgraded application software and the newer version is more efficient and uses fewer CPU cycles, <em>revealing</em> I/O wait. This can make the system administrator think that the upgrade has caused a disk issue and made performance worse, when in fact disk performance is the same, but CPU performance is improved.</p>
        <p class="noindent">A more reliable metric is the time that application threads are blocked on disk I/O. This captures the pain endured by application threads caused by the disks, regardless of what other work the CPUs may be doing. This metric can be measured using static or dynamic instrumentation.</p>
        <p class="noindent">I/O wait is still a popular metric on Linux systems and, despite its confusing nature, it is used successfully to identify a type of disk bottleneck: disks busy, CPUs idle. One way to interpret it is to treat any wait I/O as a sign of a system bottleneck, and then tune the system to minimize it—even if the I/O is still occurring concurrently with CPU utilization. Concurrent I/O is more likely to be non-blocking I/O, and less likely to cause a direct issue. Non-concurrent I/O, as identified by I/O wait, is more likely to be application blocking I/O, and a bottleneck.</p>
        </section>
        <section>
        <h4 class="h4" id="ch09lev3sec12">9.3.12 Synchronous vs. Asynchronous</h4>
        <p class="noindent">It can be important to understand that disk I/O latency may not directly affect application performance, if the application I/O and disk I/O operate asynchronously. This commonly occurs with write-back caching, where the application I/O completes early, and the disk I/O is issued later.</p>
        <p class="noindent"><span epub:type="pagebreak" id="page_435"></span>Applications may use read-ahead to perform asynchronous reads, which may not block the application while the disk completes the I/O. The file system may initiate this itself to warm the cache (prefetch).</p>
        <p class="noindent">Even if an application is synchronously waiting for I/O, that application code path may be noncritical and asynchronous to client application requests. It could be an application I/O worker thread, created to manage I/O while other threads continue to process work.</p>
        <p class="noindent">Kernels also typically support <em>asynchronous</em> or <em>non-blocking I/O</em>, where an API is provided for the application to request I/O and to be notified of its completion sometime later. For more on these topics, see <a href="ch08.xhtml#ch08">Chapter 8</a>, <a href="ch08.xhtml#ch08">File Systems</a>, <a href="ch08.xhtml#ch08lev3sec9">Sections 8.3.9</a>, <a href="ch08.xhtml#ch08lev3sec9">Non-Blocking I/O</a>; <a href="ch08.xhtml#ch08lev3sec5">8.3.5</a>, <a href="ch08.xhtml#ch08lev3sec5">Read-Ahead</a>; <a href="ch08.xhtml#ch08lev3sec4">8.3.4</a>, <a href="ch08.xhtml#ch08lev3sec4">Prefetch</a>; and <a href="ch08.xhtml#ch08lev3sec7">8.3.7</a>, <a href="ch08.xhtml#ch08lev3sec7">Synchronous Writes</a>.</p>
        </section>
        <section>
        <h4 class="h4" id="ch09lev3sec13">9.3.13 Disk vs. Application I/O</h4>
        <p class="noindent">Disk I/O is the end result of various kernel components, including file systems and device drivers. There are many reasons why the rate and volume of this disk I/O may not match the I/O issued by the application. These include:</p>
        <ul class="sq">
        <li><p class="bull">File system inflation, deflation, and unrelated I/O. See <a href="ch08.xhtml#ch08">Chapter 8</a>, <a href="ch08.xhtml#ch08">File Systems</a>, <a href="ch08.xhtml#ch08lev3sec12">Section 8.3.12</a>, <a href="ch08.xhtml#ch08lev3sec12">Logical vs. Physical I/O</a>.</p></li>
        <li><p class="bull">Paging due to a system memory shortage. See <a href="ch07.xhtml#ch07">Chapter 7</a>, <a href="ch07.xhtml#ch07">Memory</a>, <a href="ch07.xhtml#ch07lev2sec2">Section 7.2.2</a>, <a href="ch07.xhtml#ch07lev2sec2">Paging</a>.</p></li>
        <li><p class="bull">Device driver I/O size: rounding up I/O size, or fragmenting I/O.</p></li>
        <li><p class="bull">RAID writing mirror or checksum blocks, or verifying read data.</p></li>
        </ul>
        <p class="noindent">This mismatch can be confusing when unexpected. It can be understood by learning the architecture and performing analysis.</p>
        </section>
        </section>
        <section>
        <h3 class="h3" id="ch09lev4">9.4 Architecture</h3>
        <p class="noindent">This section describes disk architecture, which is typically studied during capacity planning to determine the limits for different components and configuration choices. It should also be checked during the investigation of later performance issues, in case the problem originates from architectural choices rather than the current load and tuning.</p>
        <section>
        <h4 class="h4" id="ch09lev4sec1">9.4.1 Disk Types</h4>
        <p class="noindent">The two most commonly used disk types at present are magnetic rotational and flash-memory-based SSDs. Both of these provide permanent storage; unlike volatile memory, their stored content is still available after a power cycle.</p>
        <section>
        <h5 class="h5" id="ch09lev3_3">9.4.1.1 Magnetic Rotational</h5>
        <p class="noindent">Also termed a <em>hard disk drive</em> (HDD), this type of disk consists of one or more discs, called <em>platters</em>, impregnated with iron oxide particles. A small region of these particles can be magnetized in one of two directions; this orientation is used to store a bit. The platters rotate, while a <span epub:type="pagebreak" id="page_436"></span>mechanical arm with circuitry to read and write data reaches across the surface. This circuitry includes the <em>disk heads</em>, and an arm may have more than one head, allowing it to read and write multiple bits simultaneously. Data is stored on the platter in circular tracks, and each track is divided into sectors.</p>
        <p class="noindent">Being mechanical devices, these perform relatively slowly, especially for random I/O. With advances in flash memory-based technology, SSDs are displacing rotational disks, and it is conceivable that one day rotational disks will be obsolete (along with other older storage technologies: drum disks and core memory). In the meantime, rotational disks are still competitive in some scenarios, such as economical high-density storage (low cost per megabyte), especially for data warehousing.<sup><a id="ch09fn2a" href="ch09.xhtml#ch09fn2">2</a></sup></p>
        <p class="footnote"><sup><a id="ch09fn2" href="ch09.xhtml#ch09fn2a">2</a></sup>The Netflix Open Connect Appliances (OCAs) that host videos for streaming might sound like another use case for HDDs, but supporting large numbers of simultaneous customers per server can result in random I/O. Some OCAs have switched to flash drives <a href="ch09.xhtml#ch09ref26">[Netflix 20]</a>.</p>
        <p class="noindent">The following topics summarize factors in rotational disk performance.</p>
        <section>
        <h6 class="h6">Seek and Rotation</h6>
        <p class="noindent">Slow I/O for magnetic rotational disks is usually caused by the seek time for the disk heads and the rotation time of the disk platter, both of which may take milliseconds. Best case is when the next requested I/O is located at the end of the currently servicing I/O, so that the disk heads don’t need to seek or wait for additional rotation. As described earlier, this is known as <em>sequential I/O</em>, while I/O that requires head seeking or waiting for rotation is called <em>random I/O</em>.</p>
        <p class="noindent">There are many strategies to reduce seek and rotation wait time, including:</p>
        <ul class="sq">
        <li><p class="bull">Caching: eliminating I/O entirely.</p></li>
        <li><p class="bull">File system placement and behavior, including copy-on-write (which makes writes sequential, but may make later reads random).</p></li>
        <li><p class="bull">Separating different workloads to different disks, to avoid seeking between workload I/O.</p></li>
        <li><p class="bull">Moving different workloads to different systems (some cloud computing environments can do this to reduce multitenancy effects).</p></li>
        <li><p class="bull">Elevator seeking, performed by the disk itself.</p></li>
        <li><p class="bull">Higher-density disks, to tighten the workload location.</p></li>
        <li><p class="bull">Partition (or “slice”) configuration, for example, short-stroking.</p></li>
        </ul>
        <p class="noindent">An additional strategy to reduce rotation wait time is to use faster disks. Disks are available in different rotational speeds, including 5400, 7200, 10 K, and 15 K revolutions per minute (rpm). Note that higher speeds can result in lower disk life-spans, due to increased heat and wear.</p>
        </section>
        <section>
        <h6 class="h6">Theoretical Maximum Throughput</h6>
        <p class="noindent">If the maximum sectors per track of a disk is known, disk throughput can be calculated using the following formula:</p>
        <p class="uln">max throughput = max sectors per track × sector size × rpm/60 s</p>
        <p class="noindent"><span epub:type="pagebreak" id="page_437"></span>This formula was more useful for older disks that exposed this information accurately. Modern disks provide a virtual image of the disk to the operating system, and expose only synthetic values for these attributes.</p>
        </section>
        <section>
        <h6 class="h6">Short-Stroking</h6>
        <p class="noindent">Short-stroking is where only the outer tracks of the disk are used for the workload; the remainder are either unused, or used for low-throughput workloads (e.g., archives). This reduces seek time as head movement is bounded by a smaller range, and the disk may put the heads at rest at the outside edge, reducing the first seek after idle. The outer tracks also usually have better throughput due to sector zoning (see the next section). Keep an eye out for short-stroking when examining published disk benchmarks, especially benchmarks that don’t include price, where many short-stroked disks may have been used.</p>
        </section>
        <section>
        <h6 class="h6">Sector Zoning</h6>
        <p class="noindent">The length of disk tracks varies, with the shortest at the center of the disk and the longest at the outside edge. Instead of the number of sectors (and bits) per track being fixed, sector zoning (also called <em>multiple-zone recording</em>) increases the sector count for the longer tracks, since more sectors can be physically written. Because the rotation speed is constant, the longer outside-edge tracks deliver higher throughput (megabytes per second) than the inner tracks.</p>
        </section>
        <section>
        <h6 class="h6">Sector Size</h6>
        <p class="noindent">The storage industry has developed a new standard for disk devices, called Advanced Format, to support larger sector sizes, particularly 4 Kbytes. This reduces I/O computational overhead, improving throughput as well as reducing overheads for the disk’s per-sector stored metadata. Sectors of 512 bytes can still be provided by disk firmware via an emulation standard called Advanced Format 512e. Depending on the disk, this may increase write overheads, invoking a read-modify-write cycle to map 512 bytes to a 4 Kbyte sector. Other performance issues to be aware of include misaligned 4 Kbyte I/O, which span two sectors, inflating sector I/O to service them.</p>
        </section>
        <section>
        <h6 class="h6">On-Disk Cache</h6>
        <p class="noindent">A common component of these disks is a small amount of memory (RAM) used to cache the result of reads and to-buffer writes. This memory also allows I/O (commands) to be queued on the device and reordered more efficiently. With SCSI, this is Tagged Command Queueing (TCQ); with SATA, it is called Native Command Queueing (NCQ).</p>
        </section>
        <section>
        <h6 class="h6">Elevator Seeking</h6>
        <p class="noindent">The <em>elevator algorithm</em> (also known as <em>elevator seeking</em>) is one way that a command queue can improve efficiency. It reorders I/O based on their on-disk location, to minimize travel of the disk heads. The result is similar to a building elevator, which does not service floors based on the order in which the floor buttons were pushed, but makes sweeps up and down the building, stopping at the currently requested floors.</p>
        <p class="noindent">This behavior becomes apparent when inspecting disk I/O traces and finding that sorting I/O by completion time doesn’t match sorting by start time: I/O are completing out of order.</p>
        <p class="noindent"><span epub:type="pagebreak" id="page_438"></span>While this seems like an obvious performance win, contemplate the following scenario: A disk has been sent a batch of I/O near offset 1,000, and a single I/O at offset 2,000. The disk heads are currently at 1,000. When will the I/O at offset 2,000 be serviced? Now consider that, while servicing the I/O near 1,000, more arrive near 1,000, and more, and more—enough continual I/O to keep the disk busy near offset 1,000 for 10 seconds. When will the 2,000 offset I/O be serviced, and what is its final I/O latency?</p>
        </section>
        <section>
        <h6 class="h6">Data Integrity</h6>
        <p class="noindent">Disks store an error-correcting code (ECC) at the end of each sector for data integrity, so that the drive can verify data was read correctly, or correct any errors that may have occurred. If the sector was not read correctly, the disk heads may retry the read on the next rotation (and may retry several times, varying the location of the head slightly each time). This may be the explanation for unusually slow I/O. The drive may provide soft errors to the OS to explain what happened. It can be beneficial to monitor the rate of soft errors, as an increase can indicate that a drive may soon fail.</p>
        <p class="noindent">One benefit of the industry switch from 512 byte to 4 Kbyte sectors is that fewer ECC bits are required for the same volume of data, as ECC is more efficient for the larger sector size <a href="ch09.xhtml#ch09ref6">[Smith 09]</a>.</p>
        <p class="noindent">Note that other checksums may also be in use to verify data. For example, a cyclic redundancy check (CRC) may be used to verify data transfers to the host, and other checksums may be in use by file systems.</p>
        </section>
        <section>
        <h6 class="h6">Vibration</h6>
        <p class="noindent">While disk device vendors were well aware of vibration issues, those issues weren’t commonly known or taken seriously by the industry. In 2008, while investigating a mysterious performance issue, I conducted a vibration-inducing experiment by <em>shouting</em> at a disk array while it performed a write benchmark, which caused a burst of very slow I/O. My experiment was immediately videoed and put on YouTube, where it went viral, and it has been described as the first demonstration of the impact of vibration on disk performance <a href="ch09.xhtml#ch09ref9">[Turner 10]</a>. The video has had over 1,700,000 views, promoting awareness of disk vibration issues <a href="ch09.xhtml#ch09ref4">[Gregg 08]</a>. Based on emails I’ve received, I also seem to have accidentally spawned an industry in soundproofing data centers: you can now hire professionals who will analyze data center sound levels and improve disk performance by damping vibrations.</p>
        </section>
        <section>
        <h6 class="h6">Sloth Disks</h6>
        <p class="noindent">A current performance issue with some rotational disks is the discovery of what has been called <em>sloth disks</em>. These disks sometimes return very slow I/O, over one second, without any reported errors. This is much longer than ECC-based retries should take. It might actually be better if such disks returned a failure instead of taking so long, so that the operating system or disk controllers could take corrective action, such as offlining the disk in redundant environments and reporting the failure. Sloth disks are a nuisance, especially when they are part of a virtual disk presented by a storage array where the operating system has no direct visibility, making them harder to identify.<sup><a id="ch09fn3a" href="ch09.xhtml#ch09fn3">3</a></sup></p>
        <p class="footnote"><sup><a id="ch09fn3" href="ch09.xhtml#ch09fn3a">3</a></sup>If the Linux Distributed Replicated Block Device (DRBD) system is in use, it does provide a “disk-timeout” parameter.</p>
        </section>
        <section>
        <h6 class="h6"><span epub:type="pagebreak" id="page_439"></span>SMR</h6>
        <p class="noindent">Shingled Magnetic Recording (SMR) drives provide higher density by using narrower tracks. These tracks are too narrow for the write head to record, but not for the (smaller) read head to read, so it writes them by partially overlapping other tracks, in a style similar to roof shingles (hence its name). Drives using SMR increase in density by around 25%, at the cost of degraded write performance, as the overlapped data is destroyed and must also be re-written. These drives are suitable for archival workloads that are written once then mostly read, but are not suited for write-heavy workloads in RAID configurations <a href="ch09.xhtml#ch09ref25">[Mellor 20]</a>.</p>
        </section>
        <section>
        <h6 class="h6">Disk Data Controller</h6>
        <p class="noindent">Mechanical disks present a simple interface to the system, implying a fixed sectors-per-track ratio and a contiguous range of addressable offsets. What actually happens on the disk is up to the disk data controller—a disk internal microprocessor, programmed by firmware. Disks may implement algorithms including sector zoning, affecting how the offsets are laid out. This is something to be aware of, but it’s difficult to analyze—the operating system cannot see into the disk data controller.</p>
        </section>
        </section>
        <section>
        <h5 class="h5" id="ch09lev3_4">9.4.1.2 Solid-State Drives</h5>
        <p class="noindent">These are also called solid-state disks (SSDs). The term solid-state refers to their use of solid-state electronics, which provides programmable nonvolatile memory with typically much better performance than rotational disks. Without moving parts, these disks are also physically durable and not susceptible to performance issues caused by vibration.</p>
        <p class="noindent">The performance of this disk type is usually consistent across different offsets (no rotational or seek latency) and predictable for given I/O sizes. The random or sequential characteristic of workloads matters much less than with rotational disks. All of this makes them easier to study and do capacity planning for. However, if they do encounter performance pathologies, understanding them can be just as complex as with rotational disks, due to how they operate internally.</p>
        <p class="noindent">Some SSDs use nonvolatile DRAM (NV-DRAM). Most use flash memory.</p>
        <section>
        <h6 class="h6">Flash Memory</h6>
        <p class="noindent">Flash-memory-based SSDs offer high read performance, particularly random read performance that can beat rotational disks by orders of magnitude. Most are built using NAND flash memory, which uses electron-based trapped-charge storage media that can store electrons persistently<sup><a id="ch09fn4a" href="ch09.xhtml#ch09fn4">4</a></sup> in a no-power state [Cornwell 12]. The name “flash” relates to how data is written, which requires erasing an entire block of memory at a time (including multiple pages, usually 8 or 64 KBytes per page) and rewriting the contents. Because of these write overheads, flash memory has asymmetrical read/write performance: fast reads and slower writes. Drives typically mitigate this using write-back caches to improve write performance, and a small capacitor as a battery backup in case of a power failure.</p>
        <p class="footnote"><sup><a id="ch09fn4" href="ch09.xhtml#ch09fn4a">4</a></sup>But not indefinitely. Data retention errors for modern MLC may occur in a matter of mere months when powered off <a href="ch09.xhtml#ch09ref11">[Cassidy 12]</a><a href="ch09.xhtml#ch09ref17">[Cai 15]</a>.</p>
        <p class="noindent"><span epub:type="pagebreak" id="page_440"></span>Flash memory comes in different types:</p>
        <ul class="sq">
        <li><p class="bull"><strong>Single-level cell (SLC)</strong>: Stores data bits in individual cells.</p></li>
        <li><p class="bull"><strong>Multi-level cell (MLC)</strong>: Stores multiple bits per cell (usually two, which requires four voltage levels).</p></li>
        <li><p class="bull"><strong>Enterprise multi-level cell (eMLC)</strong>: MLC with advanced firmware intended for enterprise use.</p></li>
        <li><p class="bull"><strong>Tri-level cell (TLC)</strong>: Stores three bits (eight voltage levels).</p></li>
        <li><p class="bull"><strong>Quad-level cell (QLC)</strong>: Stores four bits.</p></li>
        <li><p class="bull"><strong>3D NAND / Vertical NAND (V-NAND)</strong>: This stacks layers of flash memory (e.g., TLC) to increase the density and storage capacity.</p></li>
        </ul>
        <p class="noindent">This list is in rough chronological order, with the newest technologies listed last: 3D NAND has been commercially available since 2013.</p>
        <p class="noindent">SLC tends to have higher performance and reliability compared to other types and was preferred for enterprise use, although with higher costs. MLC is now often used in the enterprise for its higher density, in spite of its lower reliability. Flash reliability is often measured as the number of block writes (program/erase cycles) a drive is expected to support. For SLC, this expectation is around 50,000 to 100,000 cycles; for MLC around 5,000 to 10,000 cycles; for TLC around 3,000 cycles; and for QLC around 1,000 cycles <a href="ch09.xhtml#ch09ref24">[Liu 20]</a>.</p>
        </section>
        <section>
        <h6 class="h6">Controller</h6>
        <p class="noindent">The controller for an SSD has the following task <a href="ch09.xhtml#ch09ref16">[Leventhal 13]</a>:</p>
        <ul class="sq">
        <li><p class="bull"><strong>Input</strong>: Reads and writes occur per page (usually 8 Kbytes); writes can occur only to erased pages; pages are erased in blocks of 32 to 64 (256–512 Kbytes).</p></li>
        <li><p class="bull"><strong>Output</strong>: Emulates a hard drive block interface: reads or writes of arbitrary sectors (512 bytes or 4 Kbytes).</p></li>
        </ul>
        <p class="noindent">Translating between input and output is performed by the controller’s flash translation layer (FTL), which must also track free blocks. It essentially uses its own file system to do this, such as a log-structured file system.</p>
        <p class="noindent">The write characteristics can be a problem for write workloads, especially when writing I/O sizes that are smaller than the flash memory block size (which may be as large as 512 Kbytes). This can cause <em>write amplification</em>, where the remainder of the block is copied elsewhere before erasure, and also latency for at least the erase-write cycle. Some flash memory drives mitigate the latency issue by providing an on-disk buffer (RAM-based) backed by a battery, so that writes can be buffered and written later, even in the event of a power failure.</p>
        <p class="noindent">The most common enterprise-grade flash memory drive I’ve used performs optimally with 4 Kbyte reads and 1 Mbyte writes, due to the flash memory layout. These values vary for different drives and may be found via micro-benchmarking of I/O sizes.</p>
        <p class="noindent">Given the disparity between the native operations of flash and the exposed block interface, there has been room for improvement by the operating system and its file systems. The TRIM <span epub:type="pagebreak" id="page_441"></span>command is an example: it informs the SSD that a region is no longer in use, allowing the SSD to more easily assemble its pool of free blocks, reducing write amplification. (For SCSI, this can be implemented using the UNMAP or WRITE SAME commands; for ATA, the DATA SET MANAGEMENT command. Linux support includes the discard mount option, and the fstrim(8) command.)</p>
        </section>
        <section>
        <h6 class="h6">Lifespan</h6>
        <p class="noindent">There are various problems with NAND flash as a storage medium, including burnout, data fade, and read disturbance <a href="ch09.xhtml#ch09ref12">[Cornwell 12]</a>. These can be solved by the SSD controller, which can move data to avoid problems. It will typically employ <em>wear leveling</em>, which spreads writes across different blocks to reduce the write cycles on individual blocks, and <em>memory overprovisioning</em>, which reserves extra memory that can be mapped into service when needed.</p>
        <p class="noindent">While these techniques improve lifespan, the SSD still has a limited number of write cycles per block, depending on the type of flash memory and the mitigation features employed by the drive. Enterprise-grade drives use memory overprovisioning and the most reliable type of flash memory, SLC, to achieve write cycle rates of 1 million and higher. Consumer-grade drives based on MLC may offer as few as 1,000 cycles.</p>
        </section>
        <section>
        <h6 class="h6">Pathologies</h6>
        <p class="noindent">Here are some flash memory SSD pathologies to be aware of:</p>
        <ul class="sq">
        <li><p class="bull">Latency outliers due to aging, and the SSD trying harder to extract correct data (which is checked using ECC).</p></li>
        <li><p class="bull">Higher latency due to fragmentation (reformatting may fix this by cleaning up the FTL block maps).</p></li>
        <li><p class="bull">Lower throughput performance if the SSD implements internal compression.</p></li>
        </ul>
        <p class="noindent">Check for other developments with SSD performance features and issues encountered.</p>
        </section>
        </section>
        <section>
        <h5 class="h5" id="ch09lev3_5">9.4.1.3 Persistent Memory</h5>
        <p class="noindent">Persistent memory, in the form of battery-backed<sup><a id="ch09fn5a" href="ch09.xhtml#ch09fn5">5</a></sup> DRAM, is used for storage controller write-back caches. The performance of this type is orders of magnitude faster than flash, but its cost and limited battery life span have limited it to only specialized uses.</p>
        <p class="footnote"><sup><a id="ch09fn5" href="ch09.xhtml#ch09fn5a">5</a></sup>A battery or a super capacitor.</p>
        <p class="noindent">A new type of persistent memory called 3D XPoint, developed by Intel and Micron, will allow persistent memory to be used for many more applications at a compelling price/performance, in between DRAM and flash memory. 3D XPoint works by storing bits in a stackable cross-gridded data access array, and is byte-addressable. An Intel performance comparison reported 14 microsecond access latency for 3D XPoint compared to 200 microseconds for 3D NAND SSD <a href="ch09.xhtml#ch09ref19">[Hady 18]</a>. 3D XPoint also showed consistent latency for their test, whereas 3D NAND had a wider latency distribution reaching up to 3 milliseconds.</p>
        <p class="noindent">3D XPoint has been commercially available since 2017. Intel uses the brand name Optane, and releases it as Intel Optane persistent memory in a DIMM package, and as Intel Optane SSDs.</p>
        </section>
        </section>
        <section>
        <h4 class="h4" id="ch09lev4sec2"><span epub:type="pagebreak" id="page_442"></span>9.4.2 Interfaces</h4>
        <p class="noindent">The interface is the protocol supported by the drive for communication with the system, usually via a disk controller. A brief summary of the SCSI, SAS, SATA, FC, and NVMe interfaces follows. You will need to check what the current interfaces and supported bandwidths are, as they change over time when new specifications are developed and adopted.</p>
        <section>
        <h5 class="h5" id="ch09lev3_6">SCSI</h5>
        <p class="noindent">The Small Computer System Interface was originally a parallel transport bus, using multiple electrical connectors to transport bits in parallel. The first version, SCSI-1 in 1986, had a data bus width of 8 bits, allowing one byte to be transferred per clock, and delivered a bandwidth of 5 Mbytes/s. This was connected using a 50-pin Centronics C50. Later parallel SCSI versions used wider data buses and more pins for the connectors, up to 80 pins, and bandwidths in the hundreds of megabytes.</p>
        <p class="noindent">Because parallel SCSI is a shared bus, it can suffer performance issues due to bus contention, for example when a scheduled system backup saturates the bus with low-priority I/O. Workarounds included putting low-priority devices on their own SCSI bus or controller.</p>
        <p class="noindent">Clocking of parallel buses also becomes a problem at higher speeds which, along with the other issues (including limited devices and the need for SCSI terminator packs), has led to a switch to the serial version: SAS.</p>
        </section>
        <section>
        <h5 class="h5" id="ch09lev3_7">SAS</h5>
        <p class="noindent">The Serial Attached SCSI interface is designed as a high-speed point-to-point transport, avoiding the bus contention issues of parallel SCSI. The initial SAS-1 specification was 3 Gbits/s (released in 2003), followed by SAS-2 supporting 6 Gbits/s (2009), SAS-3 supporting 12 Gbits/s (2012), and SAS-4 supporting 22.5 Gbit/s (2017). Link aggregations are supported, so that multiple ports can combine to deliver higher bandwidths. The actual data transfer rate is 80% of bandwidth, due to 8b/10b encoding.</p>
        <p class="noindent">Other SAS features include dual porting of drives for use with redundant connectors and architectures, I/O multipathing, SAS domains, hot swapping, and compatibility support for SATA devices. These features have made SAS popular for enterprise use, especially with redundant architectures.</p>
        </section>
        <section>
        <h5 class="h5" id="ch09lev3_8">SATA</h5>
        <p class="noindent">For similar reasons as for SCSI and SAS, the parallel ATA (aka IDE) interface standard has evolved to become the Serial ATA interface. Created in 2003, SATA 1.0 supported 1.5 Gbits/s; later major versions are SATA 2.0 supporting 3.0 Gbits/s (2004), and SATA 3.0 supporting 6.0 Gbits/s (2008). Additional features have been added in major and minor releases, including native command queueing support. SATA uses 8b/10b encoding, so the data transfer rate is 80% of bandwidth. SATA has been in common use for consumer desktops and laptops.</p>
        </section>
        <section>
        <h5 class="h5" id="ch09lev3_9">FC</h5>
        <p class="noindent">Fibre Channel (FC) is a high-speed interface standard for data transfer, originally intended only for fibre optic cable (hence its name) and later supporting copper as well. FC is commonly used <span epub:type="pagebreak" id="page_443"></span>in enterprise environments to create storage area networks (SANs) where multiple storage devices can be connected to multiple servers via a Fibre Channel Fabric. This offers greater scalability and accessibility than other interfaces, and is similar to connecting multiple hosts via a network. And, like networking, FC can involve using <em>switches</em> to connect together multiple local endpoints (server and storage). Development of a Fibre Channel standard began in 1988 with the first version approved by ANSI in 1994 [FICA 20]. There have since been many variants and speed improvements, with the recent Gen 7 256GFC standard reaching up to 51,200 MB/s full duplex <a href="ch09.xhtml#ch09ref18">[FICA 18]</a>.</p>
        </section>
        <section>
        <h5 class="h5" id="ch09lev3_10">NVMe</h5>
        <p class="noindent">Non-Volatile Memory express (NVMe) is a PCIe bus specification for storage devices. Rather than connecting storage devices to a storage controller card, an NVMe device is itself a card that connects directly to the PCIe bus. Created in 2011, the first NVMe specification was 1.0e (released in 2013), and the latest is 1.4 (2019) <a href="ch09.xhtml#ch09ref27">[NVMe 20]</a>. Newer specifications add various features, for example, thermal management features and commands for self-testing, verifying data, and sanitizing data (making recovery impossible). The bandwidth of NVMe cards is bounded by the PCIe bus; PCIe version 4.0, commonly used today, has a single-direction bandwidth of 31.5 Gbytes/s for a x16 card (link width).</p>
        <p class="noindent">An advantage with NVMe over traditional SAS and SATA is its support for multiple hardware queues. These queues can be used from the same CPU to promote cache warmth (and with Linux multi-queue support, shared kernel locks are also avoided). These queues also allow much greater buffering, supporting up to 64 thousand commands in each queue, whereas typical SAS and SATA are limited to 256 and 32 commands respectively.</p>
        <p class="noindent">NVMe also supports SR-IOV for improving virtual machine storage performance (see <a href="ch11.xhtml#ch11">Chapter 11</a>, <a href="ch11.xhtml#ch11">Cloud Computing</a>, <a href="ch11.xhtml#ch11lev2">Section 11.2</a>, <a href="ch11.xhtml#ch11lev2">Hardware Virtualization</a>).</p>
        <p class="noindent">NVMe is used for low-latency flash devices, with an expected I/O latency of less than 20 microseconds.</p>
        </section>
        </section>
        <section>
        <h4 class="h4" id="ch09lev4sec3">9.4.3 Storage Types</h4>
        <p class="noindent">Storage can be provided to a server in a number of ways; the following sections describe four general architectures: disk devices, RAID, storage arrays, and network-attached storage (NAS).</p>
        <section>
        <h5 class="h5" id="ch09lev3_11">Disk Devices</h5>
        <p class="noindent">The simplest architecture is a server with internal disks, individually controlled by the operating system. The disks connect to a disk controller, which is circuitry on the main board or an expander card, and which allows the disk devices to be seen and accessed. In this architecture the disk controller merely acts as a conduit so that the system can communicate with the disks. A typical personal computer or laptop has a disk attached in this way for primary storage.</p>
        <p class="noindent">This architecture is the easiest to analyze using performance tools, as each disk is known to the operating system and can be observed separately.</p>
        <p class="noindent">Some disk controllers support this architecture, where it is called <em>just a bunch of disks</em> (JBOD).</p>
        </section>
        <section>
        <h5 class="h5" id="ch09lev3_12"><span epub:type="pagebreak" id="page_444"></span>RAID</h5>
        <p class="noindent">Advanced disk controllers can provide the redundant array of independent disks (RAID) architecture for disk devices (originally the redundant array of <em>inexpensive</em> disks <a href="ch09.xhtml#ch09ref1">[Patterson 88]</a>). RAID can present multiple disks as a single big, fast, and reliable virtual disk. These controllers often include an on-board cache (RAM) to improve read and write performance.</p>
        <p class="noindent">Providing RAID by a disk controller card is called <em>hardware</em> RAID. RAID can also be implemented by operating system software, but hardware RAID has been preferred as CPU-expensive checksum and parity calculations can be performed more quickly on dedicated hardware, plus such hardware can include a battery backup unit (BBU) for improved resiliency. However, advances in processors have produced CPUs with a surplus of cycles and cores, reducing the need to offload parity calculations. A number of storage solutions have moved back to software RAID (for example, using ZFS), which reduces complexity and hardware cost and improves observability from the operating system. In the case of a major failure, software RAID may also be easier to repair than hardware RAID (imagine a dead RAID card).</p>
        <p class="noindent">The following sections describe the performance characteristics of RAID. The term <em>stripe</em> is often used: this refers to when data is grouped as blocks that are written across multiple drives (like drawing a stripe through them all).</p>
        <section>
        <h6 class="h6">Types</h6>
        <p class="noindent">Various RAID types are available to meet varying needs for capacity, performance, and reliability. This summary focuses on the performance characteristics shown in <a href="ch09.xhtml#ch09tab03">Table 9.3</a>.</p>
        <figure class="table" id="ch09tab03">
        <figcaption>
        <p class="title-t"><span class="pd_ashf">Table 9.3</span> <strong>RAID types</strong></p>
        </figcaption>
        <table class="all">
        <thead>
        <tr>
        <th class="th"><p class="thead"><strong>Level</strong></p></th>
        <th class="th"><p class="thead"><strong>Description</strong></p></th>
        <th class="th"><p class="thead"><strong>Performance</strong></p></th>
        </tr>
        </thead>
        <tbody>
        <tr>
        <td class="border"><p class="tab-para">0 (concat.)</p></td>
        <td class="border"><p class="tab-para">Drives are filled one at a time.</p></td>
        <td class="border"><p class="tab-para">Eventually improves random read performance when multiple drives can take part.</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">0 (stripe)</p></td>
        <td class="border"><p class="tab-para">Drives are used in parallel, splitting (striping) I/O across multiple drives.</p></td>
        <td class="border"><p class="tab-para">Expected best random and sequential I/O performance (depends on stripe size and workload pattern).</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">1 (mirror)</p></td>
        <td class="border"><p class="tab-para">Multiple drives (usually two) are grouped, storing identical content for redundancy.</p></td>
        <td class="border"><p class="tab-para">Good random and sequential read performance (can read from all drives simultaneously, depending on implementation). Writes limited by slowest disk in mirror, and throughput overheads doubled (two drives).</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">10</p></td>
        <td class="border"><p class="tab-para">A combination of RAID-0 stripes across groups of RAID-1 drives, providing capacity and redundancy.</p></td>
        <td class="border"><p class="tab-para">Similar performance characteristics to RAID-1 but allows more groups of drives to take part, like RAID-0, increasing bandwidth.</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para">5</p></td>
        <td class="border"><p class="tab-para">Data is stored as stripes across multiple disks, along with extra parity information for redundancy.</p></td>
        <td class="border"><p class="tab-para">Poor write performance due to read-modify-write cycle and parity calculations.</p></td>
        </tr>
        <tr>
        <td><p class="tab-para">6</p></td>
        <td><p class="tab-para">RAID-5 with two parity disks per stripe.</p></td>
        <td><p class="tab-para">Similar to RAID-5 but worse.</p></td>
        </tr>
        </tbody>
        </table>
        </figure>
        <p class="noindent"><span epub:type="pagebreak" id="page_445"></span>While RAID-0 striping performs the best, it has no redundancy, making it impractical for most production use. Possible exceptions include fault-tolerant cloud computing environments that does not store critical data and where a failed instance will automatically be replaced, and storage servers used for caching only.</p>
        </section>
        <section>
        <h6 class="h6">Observability</h6>
        <p class="noindent">As described in the earlier section on virtual disk utilization, the use of hardware-supplied virtual disk devices can make observability more difficult in the operating system, which does not know what the physical disks are doing. If RAID is supplied via software, individual disk devices can usually be observed, as the operating system manages them directly.</p>
        </section>
        <section>
        <h6 class="h6">Read-Modify-Write</h6>
        <p class="noindent">When data is stored as a stripe including a parity, as with RAID-5, write I/O can incur additional read I/O and compute time. This is because writes that are smaller than the stripe size may require the entire stripe to be read, the bytes modified, the parity recalculated, and then the stripe rewritten. An optimization for RAID-5 may be in use to avoid this: instead of reading the entire stripe, only the portions of the stripe (strips) are read that include the modified data, along with the parity. By a sequence of XOR operations, an updated parity can be calculated and written along with the modified strips.</p>
        <p class="noindent">Writes that span the entire stripe can write over the previous contents, without needing to read them first. Performance in this environment may be improved by balancing the size of the stripe with the average I/O size of the writes, to reduce the additional read overhead.</p>
        </section>
        <section>
        <h6 class="h6">Caches</h6>
        <p class="noindent">Disk controllers that implement RAID-5 can mitigate read-write-modify performance by use of a write-back cache. These caches must be battery-backed, so that in the event of a power failure they can still complete buffered writes.</p>
        </section>
        <section>
        <h6 class="h6">Additional Features</h6>
        <p class="noindent">Be aware that advanced disk controller cards can provide advanced features that can affect performance. It is a good idea to browse the vendor documentation so that you’re at least aware of what may be in play. For example, here are a couple of features from Dell PERC 5 cards <a href="ch09.xhtml#ch09ref22">[Dell 20]</a>:</p>
        <ul class="sq">
        <li><p class="bull"><strong>Patrol read</strong>: Every several days, all disk blocks are read and their checksums verified. If the disks are busy servicing requests, the resources given to the patrol read function are reduced, to avoid competing with the system workload.</p></li>
        <li><p class="bull"><strong>Cache flush interval</strong>: The time in seconds between flushing dirty data in the cache to disk. Longer times may reduce disk I/O due to write cancellation and better aggregate writes; however, they may also cause higher read latency during the larger flushes.</p></li>
        </ul>
        <p class="noindent">Both of these can have a significant effect on performance.</p>
        </section>
        </section>
        <section>
        <h5 class="h5" id="ch09lev3_13"><span epub:type="pagebreak" id="page_446"></span>Storage Arrays</h5>
        <p class="noindent">Storage arrays allow many disks to be connected to the system. They use advanced disk controllers so that RAID can be configured, and they usually provide a large cache (gigabytes) to improve read and write performance. These caches are also typically battery-backed, allowing them to operate in write-back mode. A common policy is to switch to write-through mode if the battery fails, which may be noticed as a sudden drop in write performance due to waiting for the read-modify-write cycle.</p>
        <p class="noindent">An additional performance consideration is how the storage array is attached to the system—usually via an external storage controller card. The card, and the transport between it and the storage array, will both have limits for IOPS and throughput. For improvements in both performance and reliability, storage arrays are often dual-attachable, meaning they can be connected using two physical cables, to one or two different storage controller cards.</p>
        </section>
        <section>
        <h5 class="h5" id="ch09lev3_14">Network-Attached Storage</h5>
        <p class="noindent">NAS is provided to the system over the existing network via a network protocol, such as NFS, SMB/CIFS, or iSCSI, usually from dedicated systems known as NAS appliances. These are separate systems and should be analyzed as such. Some performance analysis may be done on the client, to inspect the workload applied and I/O latencies. The performance of the network also becomes a factor, and issues can arise from network congestion and from multiple-hop latency.</p>
        </section>
        </section>
        <section>
        <h4 class="h4" id="ch09lev4sec4">9.4.4 Operating System Disk I/O Stack</h4>
        <p class="noindent">The components and layers in a disk I/O stack will depend on the operating system, version, and software and hardware technologies used. <a href="ch09.xhtml#ch09fig07">Figure 9.7</a> depicts a general model. See <a href="ch03.xhtml#ch03">Chapter 3</a>, <a href="ch03.xhtml#ch03">Operating Systems</a>, for a similar model including the application.</p>
        <figure class="image-c" id="ch09fig07">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780136821694/files/graphics/09fig07.jpg" alt="Images" width="775" height="376">
        <figcaption>
        <p class="title-f"><span class="pd_ashf">Figure 9.7</span> Generic disk I/O stack</p>
        </figcaption>
        </figure>
        <section>
        <h5 class="h5" id="ch09lev3_15"><span epub:type="pagebreak" id="page_447"></span>Block Device Interface</h5>
        <p class="noindent">The block device interface was created in early Unix for accessing storage devices in units of blocks of 512 bytes, and to provide a buffer cache to improve performance. The interface exists in Linux, although the role of the buffer cache has diminished as other file system caches have been introduced, as described in <a href="ch08.xhtml#ch08">Chapter 8</a>, <a href="ch08.xhtml#ch08">File Systems</a>.</p>
        <p class="noindent">Unix provided a path to bypass the buffer cache, called <em>raw block device I/O</em> (or just <em>raw I/O</em>), which could be used via character special device files (see <a href="ch03.xhtml#ch03">Chapter 3</a>, <a href="ch03.xhtml#ch03">Operating Systems</a>). These files are no longer commonly available by default in Linux. Raw block device I/O is different from, but in some ways similar to, the “direct I/O” file system feature described in <a href="ch08.xhtml#ch08">Chapter 8</a>, <a href="ch08.xhtml#ch08">File Systems</a>.</p>
        <p class="noindent">The block I/O interface can usually be observed from operating system performance tools (iostat(1)). It is also a common location for static instrumentation and more recently can be explored with dynamic instrumentation as well. Linux has enhanced this area of the kernel with additional features.</p>
        </section>
        <section>
        <h5 class="h5" id="ch09lev3_16">Linux</h5>
        <p class="noindent">The main components of the Linux block I/O stack are shown in <a href="ch09.xhtml#ch09fig08">Figure 9.8</a>.</p>
        <figure class="image-c" id="ch09fig08">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780136821694/files/graphics/09fig08.jpg" alt="Images" width="775" height="552">
        <figcaption>
        <p class="title-f"><span class="pd_ashf">Figure 9.8</span> Linux I/O stack</p>
        </figcaption>
        </figure>
        <p class="noindent"><span epub:type="pagebreak" id="page_448"></span>Linux has enhanced block I/O with the addition of I/O merging and I/O schedulers for improving performance, volume managers for grouping multiple devices, and a device mapper for creating virtual devices.</p>
        <section>
        <h6 class="h6">I/O merging</h6>
        <p class="noindent">When I/O requests are created, Linux can merge and coalesce them as shown in <a href="ch09.xhtml#ch09fig09">Figure 9.9</a>.</p>
        <figure class="image-c" id="ch09fig09">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780136821694/files/graphics/09fig09.jpg" alt="Images" width="775" height="270">
        <figcaption>
        <p class="title-f"><span class="pd_ashf">Figure 9.9</span> I/O merging types</p>
        </figcaption>
        </figure>
        <p class="noindent">This groups I/O, reducing the per-I/O CPU overheads in the kernel storage stack and overheads on the disk, improving throughput. Statistics for these front and back merges are available in iostat(1).</p>
        <p class="noindent">After merging, I/O is then scheduled for delivery to the disks.</p>
        </section>
        <section>
        <h6 class="h6">I/O Schedulers</h6>
        <p class="noindent">I/O is queued and scheduled in the block layer either by classic schedulers (only present in Linux versions older than 5.0) or by the newer multi-queue schedulers. These schedulers allow I/O to be reordered (or rescheduled) for optimized delivery. This can improve and more fairly balance performance, especially for devices with high I/O latencies (rotational disks).</p>
        <p class="noindent"><strong>Classic schedulers</strong> include:</p>
        <ul class="sq">
        <li><p class="bull"><strong>Noop</strong>: This doesn’t perform scheduling (noop is CPU-talk for no-operation) and can be used when the overhead of scheduling is deemed unnecessary (for example, in a RAM disk).</p></li>
        <li><p class="bull"><strong>Deadline</strong>: Attempts to enforce a latency deadline; for example, read and write expiry times in units of milliseconds may be selected. This can be useful for real-time systems, where determinism is desired. It can also solve problems of <em>starvation</em>: where an I/O request is starved of disk resources as newly issued I/O jump the queue, resulting in a latency outlier. Starvation can occur due to <em>writes starving reads</em>, and as a consequence of elevator seeking and heavy I/O to one area of disk starving I/O to another. The deadline scheduler solves this, in part, by using three separate queues for I/O: read FIFO, write FIFO, and sorted <a href="ch09.xhtml#ch09ref8">[Love 10]</a>.</p></li>
        <li><p class="bull"><span epub:type="pagebreak" id="page_449"></span><strong>CFQ</strong>: The completely fair queueing scheduler allocates I/O time slices to processes, similar to CPU scheduling, for fair usage of disk resources. It also allows priorities and classes to be set for user processes, via the ionice(1) command.</p></li>
        </ul>
        <p class="noindent">A problem with the classic schedulers was their use of a single request queue, protected by a single lock, which became a performance bottleneck at high I/O rates. The multi-queue driver (blk-mq, added in Linux 3.13) solves this by using separate submission queues for each CPU, and multiple dispatch queues for the devices. This delivers better performance and lower latency for I/O versus classic schedulers, as requests can be processed in parallel and on the same CPU where the I/O was initiated. This was necessary to support flash memory-based and other device types capable of handling millions of IOPS <a href="ch09.xhtml#ch09ref15">[Corbet 13b]</a>.</p>
        <p class="noindent"><strong>Multi-queue schedulers</strong> include:</p>
        <ul class="sq">
        <li><p class="bull"><strong>None</strong>: No queueing.</p></li>
        <li><p class="bull"><strong>BFQ</strong>: The budget fair queueing scheduler, similar to CFQ, but allocates bandwidth as well as I/O time. It creates a queue for each process performing disk I/O, and maintains a budget for each queue measured in sectors. There is also a system-wide budget timeout to prevent one process from holding a device for too long. BFQ supports cgroups.</p></li>
        <li><p class="bull"><strong>mq-deadline</strong>: A blk-mq version of deadline (described earlier).</p></li>
        <li><p class="bull"><strong>Kyber</strong>: A scheduler that adjusts read and write dispatch queue lengths based on performance so that target read or write latencies can be met. It is a simple scheduler that only has two tunables: the target read latency (read_lat_nsec) and target synchronous write latency (write_lat_nsec). Kyber has shown improved storage I/O latencies in the Netflix cloud, where it is used by default.</p></li>
        </ul>
        <p class="noindent">Since Linux 5.0, the multi-queue schedulers are the default (the classic schedulers are no longer included).</p>
        <p class="noindent">I/O schedulers are documented in detail in the Linux source under Documentation/block.</p>
        <p class="noindent">After I/O scheduling, the request is placed on the block device queue for issuing to the device.</p>
        </section>
        </section>
        </section>
        </section>
        <section>
        <h3 class="h3" id="ch09lev5">9.5 Methodology</h3>
        <p class="noindent">This section describes various methodologies and exercises for disk I/O analysis and tuning. The topics are summarized in <a href="ch09.xhtml#ch09tab04">Table 9.4</a>.</p>
        <figure class="table" id="ch09tab04">
        <figcaption>
        <p class="title-t"><span class="pd_ashf">Table 9.4</span> <strong>Disk performance methodologies</strong></p>
        </figcaption>
        <table class="all">
        <thead>
        <tr>
        <th class="th"><p class="thead"><strong>Section</strong></p></th>
        <th class="th"><p class="thead"><strong>Methodology</strong></p></th>
        <th class="th"><p class="thead"><strong>Types</strong></p></th>
        </tr>
        </thead>
        <tbody>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch09.xhtml#ch09lev5sec1">9.5.1</a></p></td>
        <td class="border"><p class="tab-para">Tools method</p></td>
        <td class="border"><p class="tab-para">Observational analysis</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch09.xhtml#ch09lev5sec2">9.5.2</a></p></td>
        <td class="border"><p class="tab-para">USE method</p></td>
        <td class="border"><p class="tab-para">Observational analysis</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch09.xhtml#ch09lev5sec3">9.5.3</a></p></td>
        <td class="border"><p class="tab-para">Performance monitoring</p></td>
        <td class="border"><p class="tab-para">Observational analysis, capacity planning</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch09.xhtml#ch09lev5sec4">9.5.4</a></p></td>
        <td class="border"><p class="tab-para">Workload characterization</p></td>
        <td class="border"><p class="tab-para">Observational analysis, capacity planning</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch09.xhtml#ch09lev5sec5">9.5.5</a></p></td>
        <td class="border"><p class="tab-para">Latency analysis</p></td>
        <td class="border"><p class="tab-para">Observational analysis</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><span epub:type="pagebreak" id="page_450"></span><a href="ch09.xhtml#ch09lev5sec6">9.5.6</a></p></td>
        <td class="border"><p class="tab-para">Static performance tuning</p></td>
        <td class="border"><p class="tab-para">Observational analysis, capacity planning</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch09.xhtml#ch09lev5sec7">9.5.7</a></p></td>
        <td class="border"><p class="tab-para">Cache tuning</p></td>
        <td class="border"><p class="tab-para">Observational analysis, tuning</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch09.xhtml#ch09lev5sec8">9.5.8</a></p></td>
        <td class="border"><p class="tab-para">Resource controls</p></td>
        <td class="border"><p class="tab-para">Tuning</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch09.xhtml#ch09lev5sec9">9.5.9</a></p></td>
        <td class="border"><p class="tab-para">Micro-benchmarking</p></td>
        <td class="border"><p class="tab-para">Experimentation analysis</p></td>
        </tr>
        <tr>
        <td><p class="tab-para"><a href="ch09.xhtml#ch09lev5sec10">9.5.10</a></p></td>
        <td><p class="tab-para">Scaling</p></td>
        <td><p class="tab-para">Capacity planning, tuning</p></td>
        </tr>
        </tbody>
        </table>
        </figure>
        
        <p class="noindent">See <a href="ch02.xhtml#ch02">Chapter 2</a>, <a href="ch02.xhtml#ch02">Methodologies</a>, for more methodologies and the introduction to many of these.</p>
        <p class="noindent">These methods may be followed individually or used in combination. When investigating disk issues, my suggestion is to use the following strategies, in this order: the USE method, performance monitoring, workload characterization, latency analysis, micro-benchmarking, static analysis, and event tracing.</p>
        <p class="noindent"><a href="ch09.xhtml#ch09lev6">Section 9.6</a>, <a href="ch09.xhtml#ch09lev6">Observability Tools</a>, shows operating system tools for applying these methods.</p>
        <section>
        <h4 class="h4" id="ch09lev5sec1">9.5.1 Tools Method</h4>
        <p class="noindent">The tools method is a process of iterating over available tools, examining key metrics they provide. While a simple methodology, it can overlook issues for which the tools provide poor or no visibility, and it can be time-consuming to perform.</p>
        <p class="noindent">For disks, the tools method can involve checking the following (for Linux):</p>
        <ul class="sq">
        <li><p class="bull"><strong><code>iostat</code></strong>: Using extended mode to look for busy disks (over 60% utilization), high average service times (over, say, 10 ms), and high IOPS (depends)</p></li>
        <li><p class="bull"><strong><code>iotop/biotop</code></strong>: To identify which process is causing disk I/O</p></li>
        <li><p class="bull"><strong><code>biolatency</code></strong>: To examine the distribution of I/O latency as a histogram, looking for multi-modal distributions and latency outliers (over, say, 100 ms)</p></li>
        <li><p class="bull"><strong><code>biosnoop</code></strong>: To examine individual I/O</p></li>
        <li><p class="bull"><strong>perf(1)/BCC/bpftrace</strong>: For custom analysis including viewing user and kernel stacks that issued I/O</p></li>
        <li><p class="bull"><strong>Disk-controller-specific tools</strong> (from the vendor)</p></li>
        </ul>
        <p class="noindent">If an issue is found, examine all fields from the available tools to learn more context. See <a href="ch09.xhtml#ch09lev6">Section 9.6</a>, <a href="ch09.xhtml#ch09lev6">Observability Tools</a>, for more about each tool. Other methodologies can also be used, which can identify more types of issues.</p>
        </section>
        <section>
        <h4 class="h4" id="ch09lev5sec2">9.5.2 USE Method</h4>
        <p class="noindent">The USE method is for identifying bottlenecks and errors across all components, early in a performance investigation. The sections that follow describe how the USE method can be applied to disk devices and controllers, while <a href="ch09.xhtml#ch09lev6">Section 9.6</a>, <a href="ch09.xhtml#ch09lev6">Observability Tools</a>, shows tools for measuring specific metrics.</p>
        <section>
        <h5 class="h5" id="ch09lev3_17"><span epub:type="pagebreak" id="page_451"></span>Disk Devices</h5>
        <p class="noindent">For each disk device, check for:</p>
        <ul class="sq">
        <li><p class="bull"><strong>Utilization</strong>: The time the device was busy</p></li>
        <li><p class="bull"><strong>Saturation</strong>: The degree to which I/O is waiting in a queue</p></li>
        <li><p class="bull"><strong>Errors</strong>: Device errors</p></li>
        </ul>
        <p class="noindent">Errors may be checked first. They sometimes get overlooked because the system functions correctly—albeit more slowly—in spite of disk failures: disks are commonly configured in a redundant pool of disks designed to tolerate some failure. Apart from standard disk error counters from the operating system, disk devices may support a wider variety of error counters that can be retrieved by special tools (for example, SMART data<sup><a id="ch09fn6a" href="ch09.xhtml#ch09fn6">6</a></sup>).</p>
        <p class="footnote"><sup><a id="ch09fn6" href="ch09.xhtml#ch09fn6a">6</a></sup>On Linux, see tools such as MegaCLI and smartctl (covered later), cciss-vol-status, cpqarrayd, varmon, and dpt-i2o-raidutils.</p>
        <p class="noindent">If the disk devices are physical disks, utilization should be straightforward to find. If they are virtual disks, utilization may not reflect what the underlying physical disks are doing. See <a href="ch09.xhtml#ch09lev3sec9">Section 9.3.9</a>, <a href="ch09.xhtml#ch09lev3sec9">Utilization</a>, for more discussion on this.</p>
        </section>
        <section>
        <h5 class="h5" id="ch09lev3_18">Disk Controllers</h5>
        <p class="noindent">For each disk controller, check for:</p>
        <ul class="sq">
        <li><p class="bull"><strong>Utilization</strong>: Current versus maximum throughput, and the same for operation rate</p></li>
        <li><p class="bull"><strong>Saturation</strong>: The degree to which I/O is waiting due to controller saturation</p></li>
        <li><p class="bull"><strong>Errors</strong>: Controller errors</p></li>
        </ul>
        <p class="noindent">Here the utilization metric is not defined in terms of time, but rather in terms of the limitations of the disk controller card: throughput (bytes per second) and operation rate (operations per second). Operations are inclusive of read/write and other disk commands. Either throughput or operation rate may also be limited by the transport connecting the disk controller to the system, just as it may also be limited by the transport from the controller to the individual disks. Each transport should be checked the same way: errors, utilization, saturation.</p>
        <p class="noindent">You may find that the observability tools (e.g., Linux iostat(1)) do not present per-controller metrics but provide them only per disk. There are workarounds for this: if the system has only one controller, you can determine the controller IOPS and throughput by summing those metrics for all disks. If the system has multiple controllers, you will need to determine which disks belong to which, and sum the metrics accordingly.</p>
        <p class="noindent">Performance of disk controllers and transports is often overlooked. Fortunately, they are not common sources of system bottlenecks, as their capacity typically exceeds that of the attached disks. If total disk throughput or IOPS always levels off at a certain rate, even under different workloads, this may be a clue that the disk controllers or transports are in fact causing the problems.</p>
        </section>
        </section>
        <section>
        <h4 class="h4" id="ch09lev5sec3"><span epub:type="pagebreak" id="page_452"></span>9.5.3 Performance Monitoring</h4>
        <p class="noindent">Performance monitoring can identify active issues and patterns of behavior over time. Key metrics for disk I/O are:</p>
        <ul class="sq">
        <li><p class="bull">Disk utilization</p></li>
        <li><p class="bull">Response time</p></li>
        </ul>
        <p class="noindent">Disk utilization at 100% for multiple seconds is very likely an issue. Depending on your environment, over 60% may also cause poor performance due to increased queueing. The value for “normal” or “bad” depends on your workload, environment, and latency requirements. If you aren’t sure, micro-benchmarks of known-to-be-good versus bad workloads may be performed to show how these can be found via disk metrics. See <a href="ch09.xhtml#ch09lev8">Section 9.8</a>, <a href="ch09.xhtml#ch09lev8">Experimentation</a>.</p>
        <p class="noindent">These metrics should be examined on a per-disk basis, to look for unbalanced workloads and individual poorly performing disks. The response time metric may be monitored as a per-second average and can include other values such as the maximum and standard deviation. Ideally, it would be possible to inspect the full distribution of response times, such as by using a histogram or heat map, to look for latency outliers and other patterns.</p>
        <p class="noindent">If the system imposes disk I/O resource controls, statistics to show if and when these were in use can also be collected. Disk I/O may be a bottleneck as a consequence of the imposed limit, not the activity of the disk itself.</p>
        <p class="noindent">Utilization and response time show the result of disk performance. More metrics may be added to characterize the workload, including IOPS and throughput, providing important data for use in capacity planning (see the next section and <a href="ch09.xhtml#ch09lev5sec10">Section 9.5.10</a>, <a href="ch09.xhtml#ch09lev5sec10">Scaling</a>).</p>
        </section>
        <section>
        <h4 class="h4" id="ch09lev5sec4">9.5.4 Workload Characterization</h4>
        <p class="noindent">Characterizing the load applied is an important exercise in capacity planning, benchmarking, and simulating workloads. It can also lead to some of the largest performance gains, by identifying unnecessary work that can be eliminated.</p>
        <p class="noindent">The following are basic attributes for characterizing disk I/O workload:</p>
        <ul class="sq">
        <li><p class="bull">I/O rate</p></li>
        <li><p class="bull">I/O throughput</p></li>
        <li><p class="bull">I/O size</p></li>
        <li><p class="bull">Read/write ratio</p></li>
        <li><p class="bull">Random versus sequential</p></li>
        </ul>
        <p class="noindent">Random versus sequential, the read/write ratio, and I/O size are described in <a href="ch09.xhtml#ch09lev3">Section 9.3</a>, <a href="ch09.xhtml#ch09lev3">Concepts</a>. I/O rate (IOPS) and I/O throughput are defined in <a href="ch09.xhtml#ch09lev1">Section 9.1</a>, <a href="ch09.xhtml#ch09lev1">Terminology</a>.</p>
        <p class="noindent">These characteristics can vary from second to second, especially for applications and file systems that buffer and flush writes at intervals. To better characterize the workload, capture maximum values as well as averages. Better still, examine the full distribution of values over time.</p>
        <p class="noindent"><span epub:type="pagebreak" id="page_453"></span>Here is an example workload description, to show how these attributes can be expressed together:</p>
        <p class="uln">The system disks have a light random read workload, averaging 350 IOPS with a throughput of 3 Mbytes/s, running at 96% reads. There are occasional short bursts of sequential writes, lasting between 2 and 5 seconds, which drive the disks to a maximum of 4,800 IOPS and 560 Mbytes/s. The reads are around 8 Kbytes in size, and the writes around 128 Kbytes.</p>
        <p class="noindent">Apart from describing these characteristics system-wide, they can also be used to describe per-disk and per-controller I/O workloads.</p>
        <section>
        <h5 class="h5" id="ch09lev3_19">Advanced Workload Characterization/Checklist</h5>
        <p class="noindent">Additional details may be included to characterize the workload. These have been listed here as questions for consideration, which may also serve as a checklist when studying disk issues thoroughly:</p>
        <ul class="sq">
        <li><p class="bull">What is the IOPS rate system-wide? Per disk? Per controller?</p></li>
        <li><p class="bull">What is the throughput system-wide? Per disk? Per controller?</p></li>
        <li><p class="bull">Which applications or users are using the disks?</p></li>
        <li><p class="bull">What file systems or files are being accessed?</p></li>
        <li><p class="bull">Have any errors been encountered? Were they due to invalid requests, or issues on the disk?</p></li>
        <li><p class="bull">How balanced is the I/O over available disks?</p></li>
        <li><p class="bull">What is the IOPS for each transport bus involved?</p></li>
        <li><p class="bull">What is the throughput for each transport bus involved?</p></li>
        <li><p class="bull">What non-data-transfer disk commands are being issued?</p></li>
        <li><p class="bull">Why is disk I/O issued (kernel call path)?</p></li>
        <li><p class="bull">To what degree is disk I/O application-synchronous?</p></li>
        <li><p class="bull">What is the distribution of I/O arrival times?</p></li>
        </ul>
        <p class="noindent">IOPS and throughput questions can be posed for reads and writes separately. Any of these may also be checked over time, to look for maximums, minimums, and time-based variations. Also see <a href="ch02.xhtml#ch02">Chapter 2</a>, <a href="ch02.xhtml#ch02">Methodologies</a>, <a href="ch02.xhtml#ch02lev5sec11">Section 2.5.11</a>, <a href="ch02.xhtml#ch02lev5sec11">Workload Characterization</a>, which provides a higher-level summary of the characteristics to measure (who, why, what, how).</p>
        </section>
        <section>
        <h5 class="h5" id="ch09lev3_20">Performance Characterization</h5>
        <p class="noindent">The previous workload characterization lists examine the workload applied. The following examines the resulting performance:</p>
        <ul class="sq">
        <li><p class="bull">How busy is each disk (utilization)?</p></li>
        <li><p class="bull">How saturated is each disk with I/O (wait queueing)?</p></li>
        <li><p class="bull">What is the average I/O service time?</p></li>
        <li><p class="bull"><span epub:type="pagebreak" id="page_454"></span>What is the average I/O wait time?</p></li>
        <li><p class="bull">Are there I/O outliers with high latency?</p></li>
        <li><p class="bull">What is the full distribution of I/O latency?</p></li>
        <li><p class="bull">Are system resource controls, such as I/O throttling, present and active?</p></li>
        <li><p class="bull">What is the latency of non data-transfer disk commands?</p></li>
        </ul>
        </section>
        <section>
        <h5 class="h5" id="ch09lev3_21">Event Tracing</h5>
        <p class="noindent">Tracing tools can be used to record all file system operations and details to a log for later analysis (e.g., <a href="ch09.xhtml#ch09lev6sec7">Section 9.6.7</a>, <a href="ch09.xhtml#ch09lev6sec7">biosnoop</a>). This can include the disk device ID, I/O or command type, offset, size, issue and completion timestamps, completion status, and originating process ID and name (when possible). With the issue and completion timestamps, the I/O latency can be calculated (or it can be directly included in the log). By studying the sequence of request and completion timestamps, I/O reordering by the device can also be identified. While this may be the ultimate tool for workload characterization, in practice it may cost noticeable overhead to capture and save, depending on the rate of disk operations. If the disk writes for the event trace are included in the trace, it may not only pollute the trace, but also create a feedback loop and a performance problem.</p>
        </section>
        </section>
        <section>
        <h4 class="h4" id="ch09lev5sec5">9.5.5 Latency Analysis</h4>
        <p class="noindent">Latency analysis involves drilling deeper into the system to find the source of latency. With disks, this will often end at the disk interface: the time between an I/O request and the completion interrupt. If this matches the I/O latency at the application level, it’s usually safe to assume that the I/O latency originates from the disks, allowing you to focus your investigation on them. If the latency differs, measuring it at different levels of the operating system stack will identify the origin.</p>
        <p class="noindent"><a href="ch09.xhtml#ch09fig10">Figure 9.10</a> pictures a generic I/O stack, with the latency shown at different levels of two I/O outliers, A and B.</p>
        <figure class="image-c" id="ch09fig10">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780136821694/files/graphics/09fig10.jpg" alt="Images" width="775" height="415">
        <figcaption>
        <p class="title-f"><span class="pd_ashf">Figure 9.10</span> Stack latency analysis</p>
        </figcaption>
        </figure>
        <p class="noindent">The latency of I/O A is similar at each level from the application down to the disk drivers. This correlation points to the disks (or the disk driver) as the cause of the latency. This could be inferred if the layers were measured independently, based on the similar latency values between them.</p>
        <p class="noindent">The latency of B appears to originate at the file system level (locking or queueing?), with the I/O latency at lower levels contributing much less time. Be aware that different layers of the stack may inflate or deflate I/O, which means the size, count, and latency will differ from one layer to the next. The B example may be a case of only observing one I/O at the lower levels (of 10 ms), but failing to account for other related I/O that occurred to service the same file system I/O (e.g., metadata).</p>
        <p class="noindent">The latency at each level may be presented as:</p>
        <ul class="sq">
        <li><p class="bull"><strong>Per-interval I/O averages</strong>: As typically reported by operating system tools.</p></li>
        <li><p class="bull"><strong>Full I/O distributions</strong>: As histograms or heat maps; see <a href="ch09.xhtml#ch09lev7sec3">Section 9.7.3</a>, <a href="ch09.xhtml#ch09lev7sec3">Latency Heat Maps</a>.</p></li>
        <li><p class="bull"><strong>Per-I/O latency values</strong>: See the earlier Event Tracing section.</p></li>
        </ul>
        <p class="noindent">The last two are useful for tracking the origin of outliers and can help identify cases where I/O has been split or coalesced.</p>
        </section>
        <section>
        <h4 class="h4" id="ch09lev5sec6"><span epub:type="pagebreak" id="page_455"></span>9.5.6 Static Performance Tuning</h4>
        <p class="noindent">Static performance tuning focuses on issues of the configured environment. For disk performance, examine the following aspects of the static configuration:</p>
        <ul class="sq">
        <li><p class="bull">How many disks are present? Of which types (e.g., SMR, MLC)? Sizes?</p></li>
        <li><p class="bull">What version is the disk firmware?</p></li>
        <li><p class="bull">How many disk controllers are present? Of which interface types?</p></li>
        <li><p class="bull">Are disk controller cards connected to high-speed slots?</p></li>
        <li><p class="bull">How many disks are connected to each HBA?</p></li>
        <li><p class="bull">If disk/controller battery backups are present, what is their power level?</p></li>
        <li><p class="bull">What version is the disk controller firmware?</p></li>
        <li><p class="bull">Is RAID configured? How exactly, including stripe width?</p></li>
        <li><p class="bull">Is multipathing available and configured?</p></li>
        <li><p class="bull">What version is the disk device driver?</p></li>
        <li><p class="bull">What is the server main memory size? In use by the page and buffer caches?</p></li>
        <li><p class="bull">Are there operating system bugs/patches for any of the storage device drivers?</p></li>
        <li><p class="bull">Are there resource controls in use for disk I/O?</p></li>
        </ul>
        <p class="noindent">Be aware that performance bugs may exist in device drivers and firmware, which are ideally fixed by updates from the vendor.</p>
        <p class="noindent"><span epub:type="pagebreak" id="page_456"></span>Answering these questions can reveal configuration choices that have been overlooked. Sometimes a system has been configured for one workload, and then repurposed for another. This strategy will revisit those choices.</p>
        <p class="noindent">While working as the performance lead for Sun’s ZFS storage product, the most common performance complaint I received was caused by a misconfiguration: using half a JBOD (12 disks) of RAID-Z2 (wide stripes). This configuration delivered good reliability but unimpressive performance, similar to that of a single disk. I learned to ask for configuration details first (usually over the phone) before spending time logging in to the system and examining I/O latency.</p>
        </section>
        <section>
        <h4 class="h4" id="ch09lev5sec7">9.5.7 Cache Tuning</h4>
        <p class="noindent">There may be many different caches present in the system, including application-level, file system, disk controller, and on the disk itself. A list of these was included in <a href="ch09.xhtml#ch09lev3sec3">Section 9.3.3</a>, <a href="ch09.xhtml#ch09lev3sec3">Caching</a>, which can be tuned as described in <a href="ch02.xhtml#ch02">Chapter 2</a>, <a href="ch02.xhtml#ch02">Methodologies</a>, <a href="ch02.xhtml#ch02lev5sec18">Section 2.5.18</a>, <a href="ch02.xhtml#ch02lev5sec18">Cache Tuning</a>. In summary, check which caches exist, check that they are working, check how well they are working, and then tune the workload for the cache and tune the cache for the workload.</p>
        </section>
        <section>
        <h4 class="h4" id="ch09lev5sec8">9.5.8 Resource Controls</h4>
        <p class="noindent">The operating system may provide controls for allocating disk I/O resources to processes or groups of processes. These may include fixed limits for IOPS and throughput, or shares for a more flexible approach. How these work are implementation-specific, as discussed in <a href="ch09.xhtml#ch09lev9">Section 9.9</a>, <a href="ch09.xhtml#ch09lev9">Tuning</a>.</p>
        </section>
        <section>
        <h4 class="h4" id="ch09lev5sec9">9.5.9 Micro-Benchmarking</h4>
        <p class="noindent">Micro-benchmarking disk I/O was introduced in <a href="ch08.xhtml#ch08">Chapter 8</a>, <a href="ch08.xhtml#ch08">File Systems</a>, which explains the difference between testing file system I/O and testing disk I/O. Here we would like to test disk I/O, which usually means testing via the operating system’s device paths, particularly the raw device path if available, to avoid all file system behavior (including caching, buffering, I/O splitting, I/O coalescing, code path overheads, and offset mapping differences).</p>
        <p class="noindent">Factors for micro-benchmarking include:</p>
        <ul class="sq">
        <li><p class="bull"><strong>Direction</strong>: Reads or writes</p></li>
        <li><p class="bull"><strong>Disk offset pattern</strong>: Random or sequential</p></li>
        <li><p class="bull"><strong>Range of offsets</strong>: Full disk or tight ranges (e.g., offset 0 only)</p></li>
        <li><p class="bull"><strong>I/O size</strong>: 512 bytes (typical minimum) up to 1 Mbyte</p></li>
        <li><p class="bull"><strong>Concurrency</strong>: Number of I/O in flight, or number of threads performing I/O</p></li>
        <li><p class="bull"><strong>Number of devices</strong>: Single disk tests, or multiple disks (to explore controller and bus limits)</p></li>
        </ul>
        <p class="noindent">The next two sections show how these factors can be combined to test disk and disk controller performance. See <a href="ch09.xhtml#ch09lev8">Section 9.8</a>, <a href="ch09.xhtml#ch09lev8">Experimentation</a>, for details of the specific tools that can be used to perform these tests.</p>
        <section>
        <h5 class="h5" id="ch09lev3_22"><span epub:type="pagebreak" id="page_457"></span>Disks</h5>
        <p class="noindent">Micro-benchmarking can be performed on a per-disk basis to determine the following, along with suggested workloads:</p>
        <ul class="sq">
        <li><p class="bull"><strong>Maximum disk throughput</strong> (Mbytes per second): 128 Kbyte or 1 Mbyte reads, sequential</p></li>
        <li><p class="bull"><strong>Maximum disk operation rate</strong> (IOPS): 512-byte reads,<sup><a id="ch09fn7a" href="ch09.xhtml#ch09fn7">7</a></sup> offset 0 only</p>
        <p class="footnote"><sup><a id="ch09fn7" href="ch09.xhtml#ch09fn7a">7</a></sup>This size is intended to match the smallest disk block size. Many disks today use 4 Kbytes.</p></li>
        <li><p class="bull"><strong>Maximum disk random reads</strong> (IOPS): 512-byte reads, random offsets</p></li>
        <li><p class="bull"><strong>Read latency profile</strong> (average microseconds): Sequential reads, repeat for 512 bytes, 1K, 2K, 4K, and so on</p></li>
        <li><p class="bull"><strong>Random I/O latency profile</strong> (average microseconds): 512-byte reads, repeat for full offset span, beginning offsets only, end offsets only</p></li>
        </ul>
        <p class="noindent">These tests can be repeated for writes. The use of “offset 0 only” is intended to cache the data in the on-disk cache, so that cache access time can be measured.<sup><a id="ch09fn8a" href="ch09.xhtml#ch09fn8">8</a></sup></p>
        <p class="footnote"><sup><a id="ch09fn8" href="ch09.xhtml#ch09fn8a">8</a></sup>I’ve heard a rumor that some drive manufacturers have firmware routines to accelerate sector 0 I/O, inflating performance for such a test. You can verify by testing sector 0 versus sector <em>your_favorite_number.</em></p>
        </section>
        <section>
        <h5 class="h5" id="ch09lev3_23">Disk Controllers</h5>
        <p class="noindent">Disk controllers may be micro-benchmarked by applying a workload to multiple disks, designed to hit limits in the controller. These tests may be performed using the following, along with suggested workloads for the disks:</p>
        <ul class="sq">
        <li><p class="bull"><strong>Maximum controller throughput</strong> (Mbytes per second): 128 Kbytes, offset 0 only</p></li>
        <li><p class="bull"><strong>Maximum controller operation rate</strong> (IOPS): 512-byte reads, offset 0 only</p></li>
        </ul>
        <p class="noindent">Apply the workload to the disks one by one, watching for limits. It may take over a dozen disks to find the limit in a disk controller.</p>
        </section>
        </section>
        <section>
        <h4 class="h4" id="ch09lev5sec10">9.5.10 Scaling</h4>
        <p class="noindent">Disks and disk controllers have throughput and IOPS limits, which can be demonstrated via micro-benchmarking as described previously. Tuning can improve performance only up to these limits. If more disk performance is needed, and other strategies such as caching won’t work, the disks will need to scale.</p>
        <p class="noindent">Here is a simple method, based on capacity planning of resources:</p>
        <ol class="number">
        <li><p class="number">Determine the target disk workload, in terms of throughput and IOPS. If this is a new system, see <a href="ch02.xhtml#ch02">Chapter 2</a>, <a href="ch02.xhtml#ch02">Methodologies</a>, <a href="ch02.xhtml#ch02lev7">Section 2.7</a>, <a href="ch02.xhtml#ch02lev7">Capacity Planning</a>. If the system already has a workload, express the user population in terms of current disk throughput and IOPS, and scale these numbers to the target user population. (If cache is not scaled at the same time, the disk workload may increase, because the cache-per-user ratio becomes smaller.)</p></li>
        <li><p class="number">Calculate the number of disks required to support this workload. Factor in RAID configuration. Do not use the maximum throughput and IOPS values per disk, as this would <span epub:type="pagebreak" id="page_458"></span>result in driving disks at 100% utilization, leading to immediate performance issues due to saturation and queueing. Pick a target utilization (say, 50%) and scale values accordingly.</p></li>
        <li><p class="number">Calculate the number of disk controllers required to support this workload.</p></li>
        <li><p class="number">Check that transport limits have not been exceeded, and scale transports if necessary.</p></li>
        <li><p class="number">Calculate CPU cycles per disk I/O, and the number of CPUs required (this may necessitate multiple CPUs and parallel I/O).</p></li>
        </ol>
        <p class="noindent">The maximum per-disk throughput and IOPS numbers used will depend on their type and the disk type. See <a href="ch09.xhtml#ch09lev3sec7">Section 9.3.7</a>, <a href="ch09.xhtml#ch09lev3sec7">IOPS Are Not Equal</a>. Micro-benchmarking can be used to find specific limits for a given I/O size and I/O type, and workload characterization can be used on existing workloads to see which sizes and types matter.</p>
        <p class="noindent">To deliver the disk workload requirement, it’s not uncommon to find servers requiring dozens of disks, connected via storage arrays. We used to say, “Add more spindles.” We may now say, “Add more flash.”</p>
        </section>
        </section>
        <section>
        <h3 class="h3" id="ch09lev6">9.6 Observability Tools</h3>
        <p class="noindent">This section introduces disk I/O observability tools for Linux-based operating systems. See the previous section for strategies to follow when using them.</p>
        <p class="noindent">The tools in this section are listed in <a href="ch09.xhtml#ch09tab05">Table 9.5</a>.</p>
        <figure class="table" id="ch09tab05">
        <figcaption>
        <p class="title-t"><span class="pd_ashf">Table 9.5</span> <strong>Disk observability tools</strong></p>
        </figcaption>
        <table class="all">
        <thead>
        <tr>
        <th class="th"><p class="thead"><strong>Section</strong></p></th>
        <th class="th"><p class="thead"><strong>Tool</strong></p></th>
        <th class="th"><p class="thead"><strong>Description</strong></p></th>
        </tr>
        </thead>
        <tbody>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch09.xhtml#ch09lev6sec1">9.6.1</a></p></td>
        <td class="border"><p class="tab-para">iostat</p></td>
        <td class="border"><p class="tab-para">Various per-disk statistics</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch09.xhtml#ch09lev6sec2">9.6.2</a></p></td>
        <td class="border"><p class="tab-para">sar</p></td>
        <td class="border"><p class="tab-para">Historical disk statistics</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch09.xhtml#ch09lev6sec3">9.6.3</a></p></td>
        <td class="border"><p class="tab-para">PSI</p></td>
        <td class="border"><p class="tab-para">Disk pressure stall information</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch09.xhtml#ch09lev6sec4">9.6.4</a></p></td>
        <td class="border"><p class="tab-para">pidstat</p></td>
        <td class="border"><p class="tab-para">Disk I/O usage by process</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch09.xhtml#ch09lev6sec5">9.6.5</a></p></td>
        <td class="border"><p class="tab-para">perf</p></td>
        <td class="border"><p class="tab-para">Record block I/O tracepoints</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch09.xhtml#ch09lev6sec6">9.6.6</a></p></td>
        <td class="border"><p class="tab-para">biolatency</p></td>
        <td class="border"><p class="tab-para">Summarize disk I/O latency as a histogram</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch09.xhtml#ch09lev6sec7">9.6.7</a></p></td>
        <td class="border"><p class="tab-para">biosnoop</p></td>
        <td class="border"><p class="tab-para">Trace disk I/O with PID and latency</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch09.xhtml#ch09lev6sec8">9.6.8</a></p></td>
        <td class="border"><p class="tab-para">iotop, biotop</p></td>
        <td class="border"><p class="tab-para">Top for disks: summarize disk I/O by process</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch09.xhtml#ch09lev6sec9">9.6.9</a></p></td>
        <td class="border"><p class="tab-para">biostacks</p></td>
        <td class="border"><p class="tab-para">Show disk I/O with initialization stacks</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch09.xhtml#ch09lev6sec10">9.6.10</a></p></td>
        <td class="border"><p class="tab-para">blktrace</p></td>
        <td class="border"><p class="tab-para">Disk I/O event tracing</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch09.xhtml#ch09lev6sec11">9.6.11</a></p></td>
        <td class="border"><p class="tab-para">bpftrace</p></td>
        <td class="border"><p class="tab-para">Custom disk tracing</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch09.xhtml#ch09lev6sec12">9.6.12</a></p></td>
        <td class="border"><p class="tab-para">MegaCli</p></td>
        <td class="border"><p class="tab-para">LSI controller statistics</p></td>
        </tr>
        <tr>
        <td><p class="tab-para"><a href="ch09.xhtml#ch09lev6sec13">9.6.13</a></p></td>
        <td><p class="tab-para">smartctl</p></td>
        <td><p class="tab-para">Disk controller statistics</p></td>
        </tr>
        </tbody>
        </table>
        </figure>
        <p class="noindent"><span epub:type="pagebreak" id="page_459"></span>This is a selection of tools to support <a href="ch09.xhtml#ch09lev5">Section 9.5</a>, <a href="ch09.xhtml#ch09lev5">Methodology</a>, beginning with traditional tools and statistics, then tracing tools, and finally disk controller statistics. Some of the traditional tools are likely available on other Unix-like operating systems where they originated, including: iostat(8) and sar(1). Many of the tracing tools are BPF-based, and use BCC and bpftrace frontends (<a href="ch15.xhtml#ch15">Chapter 15</a>); they are: biolatency(8), biosnoop(8), biotop(8), and biostacks(8).</p>
        <p class="noindent">See the documentation for each tool, including its man pages, for full references of its features.</p>
        <section>
        <h4 class="h4" id="ch09lev6sec1">9.6.1 iostat</h4>
        <p class="noindent">iostat(1) summarizes per-disk I/O statistics, providing metrics for workload characterization, utilization, and saturation. It can be executed by any user and is typically the first command used to investigate disk I/O issues at the command line. The statistics it provides are also typically shown by monitoring software, so it can be worthwhile to learn iostat(1) in detail to deepen your understanding of monitoring statistics. These statistics are enabled by default by the kernel,<sup><a id="ch09fn9a" href="ch09.xhtml#ch09fn9">9</a></sup> so the overhead of this tool is considered negligible.</p>
        <p class="footnote"><sup><a id="ch09fn9" href="ch09.xhtml#ch09fn9a">9</a></sup>Statistics can be disabled via the /sys/block/&lt;dev&gt;/queue/iostats file. I don’t know of anyone ever doing so.</p>
        <p class="noindent">The name “iostat” is short for “I/O statistics,” although it might have been better to call it “diskiostat” to reflect the type of I/O it reports. This has led to occasional confusion when a user knows that an application is performing I/O (to the file system) but wonders why it can’t be seen via iostat(1) (the disks).</p>
        <p class="noindent">iostat(1) was written in the early 1980s for Unix, and different versions are available on the different operating systems. It can be added to Linux-based systems via the sysstat package. The following describes the Linux version.</p>
        <section>
        <h5 class="h5" id="ch09lev3_24">iostat Default Output</h5>
        <p class="noindent">Without any arguments or options, a summary-since-boot for CPU and disk statistics is printed. It’s covered here as an introduction to this tool; however, you are not expected to use this mode, as the extended mode covered later is more useful.</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg459a" id="pg459">Click here to view code image</a></p>
        <pre class="pretb">$ <strong>iostat</strong>
        Linux 5.3.0-1010-aws (ip-10-1-239-218)    02/12/20        _x86_64_  (2 CPU)
        
        avg-cpu:  %user   %nice %system %iowait  %steal   %idle
                   0.29    0.01    0.18    0.03    0.21   99.28
        
        Device             <strong>tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn</strong>
        loop0             0.00         0.05         0.00       1232          0
        [...]
        nvme0n1           3.40        17.11        36.03     409902     863344</pre>
        <p class="noindent">The first output line is a summary of the system, including the kernel version, host name, date, architecture, and CPU count. Subsequent lines show summary-since-boot statistics for the CPUs (<code>avg-cpu</code>; these statistics were covered in <a href="ch06.xhtml#ch06">Chapter 6</a>, <a href="ch06.xhtml#ch06">CPUs</a>) and disk devices (under <code>Device:</code>). <span epub:type="pagebreak" id="page_460"></span>Each disk device is shown as a row, with basic details in the columns. I’ve highlighted the column headers in bold; they are:</p>
        <ul class="sq">
        <li><p class="bull"><strong><code>tps</code></strong>: Transactions per second (IOPS)</p></li>
        <li><p class="bull"><strong><code>kB_read/s, kB_wrtn/s</code></strong>: Kilobytes read per second, and written per second</p></li>
        <li><p class="bull"><strong><code>kB_read, kB_wrtn</code></strong>: Total kilobytes read and written</p></li>
        </ul>
        <p class="noindent">Some SCSI devices, including CD-ROMs, may not be shown by iostat(1). SCSI tape drives can be examined using tapestat(1) instead, also in the sysstat package. Also note that, while iostat(1) reports block device reads and writes, it may exclude some other types of disk device commands depending on the kernel (see the logic in the kernel function blk_do_io_stat()). The iostat(1) extended mode includes extra fields for these device commands.</p>
        </section>
        <section>
        <h5 class="h5" id="ch09lev3_25">iostat Options</h5>
        <p class="noindent">iostat(1) can be executed with various options, followed by an optional interval and count. For example:</p>
        <pre class="pretb"># <strong>iostat 1 10</strong></pre>
        <p class="noindent">will print one-second summaries ten times. And:</p>
        <pre class="pretb"># <strong>iostat 1</strong></pre>
        <p class="noindent">will print one-second summaries without end (until Ctrl-C is typed).</p>
        <p class="noindent">Commonly used options are:</p>
        <ul class="sq">
        <li><p class="bull"><strong><code>-c</code></strong>: Display CPU report</p></li>
        <li><p class="bull"><strong><code>-d</code></strong>: Display disk report</p></li>
        <li><p class="bull"><strong><code>-k</code></strong>: Use kilobytes instead of (512-byte) blocks</p></li>
        <li><p class="bull"><strong><code>-m</code></strong>: Use megabytes instead of (512-byte) blocks</p></li>
        <li><p class="bull"><strong><code>-p</code></strong>: Include per-partition statistics</p></li>
        <li><p class="bull"><strong><code>-t</code></strong>: Timestamp output</p></li>
        <li><p class="bull"><strong><code>-x</code></strong>: Extended statistics</p></li>
        <li><p class="bull"><strong><code>-s</code></strong>: Short (narrow) output</p></li>
        <li><p class="bull"><strong><code>-z</code></strong>: Skip displaying zero-activity summaries</p></li>
        </ul>
        <p class="noindent">There is also an environment variable, POSIXLY_CORRECT=1, to output blocks (512 bytes each) instead of Kbytes. Some older versions included an option for NFS statistics, <code>-n</code>. Since sysstat version 9.1.3, this was moved to the separate nfsiostat command.</p>
        </section>
        <section>
        <h5 class="h5" id="ch09lev3_26">iostat Extended Short Output</h5>
        <p class="noindent">Extended output (<code>-x</code>) provides extra columns that are useful for the methodologies covered earlier. These extra columns include IOPS and throughput metrics for workload characterization, <span epub:type="pagebreak" id="page_461"></span>utilization and queue lengths for the USE method, and disk response times for performance characterization and latency analysis.</p>
        <p class="noindent">Over the years, the extended output has gained more and more fields, and the latest release (12.3.1, Dec. 2019) produces output that is 197 characters wide. This not only does not fit in this book, it does not fit in many wide terminals either, making the output difficult to read due to line wraps. A solution was added in 2017, the <code>-s</code> option, to provide a “short” or narrow output that is intended to fit within an 80-character width.</p>
        <p class="noindent">Here is an example of short (<code>-s</code>) extended (<code>-x</code>) statistics, and skipping zero-activity devices (<code>-z</code>):</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg461a" id="pg461">Click here to view code image</a></p>
        <pre class="pretb">$ <strong>iostat -sxz 1</strong>
        [...]
        avg-cpu:  %user   %nice %system %iowait  %steal   %idle
                  15.82    0.00   10.71   31.63    1.53   40.31
        
        Device             <strong>tps      kB/s    rqm/s   await aqu-sz  areq-sz  %util</strong>
        nvme0n1        1642.00   9064.00   664.00    0.44   0.00     5.52 100.00
        [...]</pre>
        <p class="noindent">The disk columns are:</p>
        <ul>
        <li><p class="bull"><strong><code>tps</code></strong>: Transactions issued per second (IOPS)</p></li>
        <li><p class="bull"><strong><code>kB/s</code></strong>: Kbytes per second</p></li>
        <li><p class="bull"><strong><code>rqm/s</code></strong>: Requests queued and merged per second</p></li>
        <li><p class="bull"><strong><code>await</code></strong>: Average I/O response time, including time queued in the OS and the I/O response time of the device (ms)</p></li>
        <li><p class="bull"><strong><code>aqu-sz</code></strong>: Average number of requests both waiting in the driver request queue and active on the device</p></li>
        <li><p class="bull"><strong><code>areq-sz</code></strong>: Average request size in Kbytes</p></li>
        <li><p class="bull"><strong><code>%util</code></strong>: Percent of time the device was busy processing I/O requests (utilization)</p></li>
        </ul>
        <p class="noindent">The most important metric for delivered performance is <code>await</code>, showing the average total wait time for I/O. What constitutes “good” or “bad” depends on your needs. In the example output, <code>await</code> was 0.44 ms, which is satisfactory for this database server. It can increase for a number of reasons: queueing (load), larger I/O sizes, random I/O on rotational devices, and device errors.</p>
        <p class="noindent">For resource usage and capacity planning, <code>%util</code> is important, but bear in mind that it is only a measure of busyness (non-idle time) and may mean little for virtual devices backed by multiple disks. Those devices may be better understood by the load applied: <code>tps</code> (IOPS) and <code>kB/s</code> (throughput).</p>
        <p class="noindent">Nonzero counts in the <code>rqm/s</code> column show that contiguous requests were merged before delivery to the device, to improve performance. This metric is also a sign of a sequential workload.</p>
        <p class="noindent">Since <code>areq-sz</code> is after merging, small sizes (8 Kbytes or less) are an indicator of a random I/O workload that could not be merged. Large sizes may be either large I/O or a merged sequential workload (indicated by earlier columns).</p>
        </section>
        <section>
        <h5 class="h5" id="ch09lev3_27"><span epub:type="pagebreak" id="page_462"></span>iostat Extended Output</h5>
        <p class="noindent">Without the <code>-s</code> option, <code>-x</code> prints many more columns. Here is the summary since boot (no interval or count) for sysstat version 12.3.2 (from Apr 2020):</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg462-1a" id="pg462-1">Click here to view code image</a></p>
        <pre class="pretb">$ <strong>iostat -x</strong>
        [...]
        Device            <strong>r/s     rkB/s   rrqm/s  %rrqm r_await rareq-sz     w/s     wkB/s
        wrqm/s  %wrqm w_await wareq-sz     d/s     dkB/s   drqm/s  %drqm d_await dareq-sz
        f/s f_await  aqu-sz  %util</strong>
        nvme0n1          0.23      9.91     0.16  40.70    0.56    43.01    3.10     33.09
        0.92  22.91    0.89    10.66    0.00      0.00     0.00   0.00    0.00     0.00
        0.00    0.00    0.00   0.12</pre>
        <p class="noindent">These break down many of the <code>-sx</code> metrics into read and write components, and also includes discards and flushes.</p>
        <p class="noindent">The extra columns are:</p>
        <ul class="sq">
        <li><p class="bull"><strong><code>r/s</code></strong>, <strong><code>w/s</code></strong>, <strong><code>d/s</code></strong>, <strong><code>f/s</code></strong>: Read, write, discard, and flush requests completed from the disk device per second (after merges)</p></li>
        <li><p class="bull"><strong><code>rkB/s</code></strong>, <strong><code>wkB/s</code></strong>, <strong><code>dkB/s</code></strong>: Read, write, and discard Kbytes from the disk device per second</p></li>
        <li><p class="bull"><strong><code>%rrqm/s</code></strong>, <strong><code>%wrqm/s</code></strong>, <strong><code>%drqm/s</code></strong>: Read, write, and discard requests queued and merged as a percentage of the total requests for that type</p></li>
        <li><p class="bull"><strong><code>r_await</code></strong>, <strong><code>w_await</code></strong>, <strong><code>d_await</code></strong>, <strong><code>f_await</code></strong>: Read, write, discard, and flush average response time, including time queued in the OS and the response time from the device (ms)</p></li>
        <li><p class="bull"><strong><code>rareq-sz</code></strong>, <strong><code>wareq-sz</code></strong>, <strong><code>dareq-sz</code></strong>: Read, write, and discard average size (Kbytes)</p></li>
        </ul>
        <p class="noindent">Examining reads and writes separately is important. Applications and file systems commonly use techniques to mitigate write latency (e.g., write-back caching), so the application is less likely to be blocked on disk writes. This means that any metrics that group reads and writes are skewed by a component that may not directly matter (the writes). By splitting them, you can start examining <code>r_wait</code>, which shows average read latency, and is likely to be the most important metric for application performance.</p>
        <p class="noindent">The reads and writes as IOPS (<code>r/s</code>, <code>w/s</code>) and throughput (<code>rkB/s</code>, <code>wkB/s</code>) are important for workload characterization.</p>
        <p class="noindent">The discard and flush statistics are new additions to iostat(1). Discard operations free up blocks on the drive (the ATA TRIM command), and their statistics were added in the Linux 4.19 kernel. Flush statistics were added in Linux 5.5. These can help to narrow down the reason for disk latency.</p>
        <p class="noindent">Here is another useful iostat(1) combination:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg462-2a" id="pg462-2">Click here to view code image</a></p>
        <pre class="pretb">$ <strong>iostat -dmstxz -p ALL 1</strong>
        Linux 5.3.0-1010-aws (ip-10-1-239-218)    02/12/20        _x86_64_  (2 CPU)
        <span epub:type="pagebreak" id="page_463"></span>02/12/20 17:39:29
        Device             tps      MB/s    rqm/s   await  areq-sz  aqu-sz  %util
        nvme0n1           3.33      0.04     1.09    0.87    12.84    0.00   0.12
        nvme0n1p1         3.31      0.04     1.09    0.87    12.91    0.00   0.12
        
        02/12/20 17:39:30
        Device             tps      MB/s    rqm/s   await  areq-sz  aqu-sz  %util
        nvme0n1        1730.00     14.97   709.00    0.54     8.86    0.02  99.60
        nvme0n1p1      1538.00     14.97   709.00    0.61     9.97    0.02  99.60
        [...]</pre>
        <p class="noindent">The first output is the summary since boot, followed by one-second intervals. The <code>-d</code> focuses on disk statistics only (no CPU), <code>-m</code> for Mbytes, and <code>-t</code> for the timestamp, which can be useful when comparing the output to other timestamped sources, and<code>-p ALL</code> includes per-partition statistics.</p>
        <p class="noindent">Unfortunately, the current version of iostat(1) does not include disk errors; otherwise all USE method metrics could be checked from one tool!</p>
        </section>
        </section>
        <section>
        <h4 class="h4" id="ch09lev6sec2">9.6.2 sar</h4>
        <p class="noindent">The system activity reporter, sar(1), can be used to observe current activity and can be configured to archive and report historical statistics. It is introduced in <a href="ch04.xhtml#ch04lev4">Section 4.4</a>, <a href="ch04.xhtml#ch04lev4">sar</a>, and mentioned in various other chapters in this book for the different statistics it provides.</p>
        <p class="noindent">The sar(1) disk summary is printed using the <code>-d</code> option, demonstrated in the following examples with an interval of one second. The output is wide, so it is included here in two parts (sysstat 12.3.2):</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg463-1a" id="pg463-1">Click here to view code image</a></p>
        <pre class="pretb">$ <strong>sar -d 1</strong>
        Linux 5.3.0-1010-aws (ip-10-0-239-218)    02/13/20        _x86_64_  (2 CPU)
        
        09:10:22          DEV       tps     rkB/s     wkB/s     dkB/s   areq-sz \ ...
        09:10:23     dev259-0   1509.00  11100.00  12776.00      0.00     15.82 / ...
        [...]</pre>
        <p class="noindent">Here are the remaining columns:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg463-2a" id="pg463-2">Click here to view code image</a></p>
        <pre class="pretb">$ <strong>sar -d 1</strong>
        09:10:22     \ ... \  aqu-sz     await     %util
        09:10:23     / ... /    0.02      0.60     94.00
        [...]</pre>
        <p class="noindent">These columns also appear in iostat(1) -x output, and were described in the previous section. This output shows a mixed read/write workload with an <code>await</code> of 0.6 ms, driving the disk to 94% utilization.</p>
        <p class="noindent"><span epub:type="pagebreak" id="page_464"></span>Previous versions of sar(1) included a <code>svctm</code> (service time) column: the average (inferred) disk response time, in milliseconds. See <a href="ch09.xhtml#ch09lev3sec1">Section 9.3.1</a>, <a href="ch09.xhtml#ch09lev3sec1">Measuring Time</a>, for background on service time. Since its simplistic calculation was no longer accurate for modern disks that perform I/O in parallel, <code>svctm</code> has been removed in later versions.</p>
        </section>
        <section>
        <h4 class="h4" id="ch09lev6sec3">9.6.3 PSI</h4>
        <p class="noindent">Linux pressure stall information (PSI), added in Linux 4.20, includes statistics for I/O saturation. These not only show if there is I/O pressure, but how it is changing over the last five minutes. Example output:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg464-1a" id="pg464-1">Click here to view code image</a></p>
        <pre class="pretb"># <strong>cat /proc/pressure/io</strong>
        some avg10=63.11 avg60=32.18 avg300=8.62 total=667212021
        full avg10=60.76 avg60=31.13 avg300=8.35 total=622722632</pre>
        <p class="noindent">This output shows that I/O pressure is increasing, with a higher 10-second average (63.11) than the 300-second average (8.62). These averages are percentages of time that a task was I/O stalled. The <code>some</code> line shows when some tasks (threads) were affected, and the <code>full</code> line shows when all runnable tasks were affected.</p>
        <p class="noindent">As with load averages, this can be a high-level metric used for alerting. Once you become aware that there is a disk performance issue, you can use other tools to find the root causes, including pidstat(8) for disk statistics by process.</p>
        </section>
        <section>
        <h4 class="h4" id="ch09lev6sec4">9.6.4 pidstat</h4>
        <p class="noindent">The Linux pidstat(1) tool prints CPU usage by default and includes a <code>-d</code> option for disk I/O statistics. This is available on kernels 2.6.20 and later. For example:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg464-2a" id="pg464-2">Click here to view code image</a></p>
        <pre class="pretb">$ <strong>pidstat -d 1</strong>
        Linux 5.3.0-1010-aws (ip-10-0-239-218)    02/13/20        _x86_64_  (2 CPU)
        
        09:47:41      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command
        09:47:42        0      2705  32468.00      0.00      0.00       5  tar
        09:47:42        0      2706      0.00   8192.00      0.00       0  gzip
        
        [...]
        09:47:56      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command
        09:47:57        0       229      0.00     72.00      0.00       0  systemd-journal
        09:47:57        0       380      0.00      4.00      0.00       0  auditd
        09:47:57        0      2699      4.00      0.00      0.00      10  kworker/
        u4:1-flush-259:0
        09:47:57        0      2705  15104.00      0.00      0.00       0  tar
        09:47:57        0      2706      0.00   6912.00      0.00       0  gzip</pre>
        <p class="noindent"><span epub:type="pagebreak" id="page_465"></span>Columns include:</p>
        <ul class="sq">
        <li><p class="bull"><strong><code>kB_rd/s</code></strong>: Kilobytes read per second</p></li>
        <li><p class="bull"><strong><code>kB_wd/s</code></strong>: Kilobytes issued for write per second</p></li>
        <li><p class="bull"><strong><code>kB_ccwr/s</code></strong>: Kilobytes canceled for write per second (e.g., overwritten or deleted before flush)</p></li>
        <li><p class="bull"><strong><code>iodelay</code></strong>: The time the process was blocked on disk I/O (clock ticks), including swapping</p></li>
        </ul>
        <p class="noindent">The workload seen in the output was a <code>tar</code> command reading the file system to a pipe, and <code>gzip</code> reading the pipe and writing a compressed archive file. The <code>tar</code> reads caused iodelay (5 clock ticks), whereas the <code>gzip</code> writes did not, due to write-back caching in the page cache. Some time later the page cache was flushed, as can be seen in the second interval output by the <code>kworker/u4:1-flush-259:0</code> process, which experienced iodelay.</p>
        <p class="noindent">iodelay is a recent addition and shows the magnitude of performance issues: how much the application waited. The other columns show the workload applied.</p>
        <p class="noindent">Note that only superusers (root) can access disk statistics for processes that they do not own. These are read via /proc/PID/io.</p>
        </section>
        <section>
        <h4 class="h4" id="ch09lev6sec5">9.6.5 perf</h4>
        <p class="noindent">The Linux perf(1) tool (<a href="ch13.xhtml#ch13">Chapter 13</a>) can record block tracepoints. Listing them:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg465a" id="pg465">Click here to view code image</a></p>
        <pre class="pretb"># <strong>perf list 'block:*'</strong>
        
        List of pre-defined events (to be used in -e):
        
          block:block_bio_backmerge                          [Tracepoint event]
          block:block_bio_bounce                             [Tracepoint event]
          block:block_bio_complete                           [Tracepoint event]
          block:block_bio_frontmerge                         [Tracepoint event]
          block:block_bio_queue                              [Tracepoint event]
          block:block_bio_remap                              [Tracepoint event]
          block:block_dirty_buffer                           [Tracepoint event]
          block:block_getrq                                  [Tracepoint event]
          block:block_plug                                   [Tracepoint event]
          block:block_rq_complete                            [Tracepoint event]
          block:block_rq_insert                              [Tracepoint event]
          block:block_rq_issue                               [Tracepoint event]
          block:block_rq_remap                               [Tracepoint event]
          block:block_rq_requeue                             [Tracepoint event]
          block:block_sleeprq                                [Tracepoint event]
          block:block_split                                  [Tracepoint event]
          block:block_touch_buffer                           [Tracepoint event]
          block:block_unplug                                 [Tracepoint event]</pre>
        <p class="noindent"><span epub:type="pagebreak" id="page_466"></span>For example, the following records block device issues with stack traces. A <code>sleep 10</code> command is provided as the duration of tracing.</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg466a" id="pg466">Click here to view code image</a></p>
        <pre class="pretb"># <strong>perf record -e block:block_rq_issue -a -g sleep 10</strong>
        [ perf record: Woken up 22 times to write data ]
        [ perf record: Captured and wrote 5.701 MB perf.data (19267 samples) ]
        # <strong>perf script --header</strong>
        [...]
        mysqld  1965 [001] 160501.158573: block:block_rq_issue: <strong>259,0 WS 12288 () 10329704 +
        24 [mysqld]</strong>
                ffffffffb12d5040 blk_mq_start_request+0xa0 ([kernel.kallsyms])
                ffffffffb12d5040 blk_mq_start_request+0xa0 ([kernel.kallsyms])
                ffffffffb1532b4c nvme_queue_rq+0x16c ([kernel.kallsyms])
                ffffffffb12d7b46 __blk_mq_try_issue_directly+0x116 ([kernel.kallsyms])
                ffffffffb12d87bb blk_mq_request_issue_directly+0x4b ([kernel.kallsyms])
                ffffffffb12d8896 blk_mq_try_issue_list_directly+0x46 ([kernel.kallsyms])
                ffffffffb12dce7e blk_mq_sched_insert_requests+0xae ([kernel.kallsyms])
                ffffffffb12d86c8 blk_mq_flush_plug_list+0x1e8 ([kernel.kallsyms])
                ffffffffb12cd623 blk_flush_plug_list+0xe3 ([kernel.kallsyms])
                ffffffffb12cd676 blk_finish_plug+0x26 ([kernel.kallsyms])
                ffffffffb119771c ext4_writepages+0x77c ([kernel.kallsyms])
                ffffffffb10209c3 do_writepages+0x43 ([kernel.kallsyms])
                ffffffffb1017ed5 __filemap_fdatawrite_range+0xd5 ([kernel.kallsyms])
                ffffffffb10186ca file_write_and_wait_range+0x5a ([kernel.kallsyms])
                ffffffffb118637f ext4_sync_file+0x8f ([kernel.kallsyms])
                ffffffffb1105869 vfs_fsync_range+0x49 ([kernel.kallsyms])
                ffffffffb11058fd do_fsync+0x3d ([kernel.kallsyms])
                ffffffffb1105944 __x64_sys_fsync+0x14 ([kernel.kallsyms])
                ffffffffb0e044ca do_syscall_64+0x5a ([kernel.kallsyms])
                ffffffffb1a0008c entry_SYSCALL_64_after_hwframe+0x44 ([kernel.kallsyms])
                    7f2285d1988b fsync+0x3b (/usr/lib/x86_64-linux-gnu/libpthread-2.30.so)
                    55ac10a05ebe Fil_shard::redo_space_flush+0x44e (/usr/sbin/mysqld)
                    55ac10a06179 Fil_shard::flush_file_redo+0x99 (/usr/sbin/mysqld)
                    55ac1076ff1c [unknown] (/usr/sbin/mysqld)
                    55ac10777030 log_flusher+0x520 (/usr/sbin/mysqld)
                    55ac10748d61
        std::thread::_State_impl&lt;std::thread::_Invoker&lt;std::tuple&lt;Runnable, void (*)(log_t*),
        log_t*&gt; &gt; &gt;::_M_run+0xc1 (/usr/sbin/mysql
                    7f228559df74 [unknown] (/usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.28)
                    7f226c3652c0 [unknown] ([unknown])
                    55ac107499f0
        std::thread::_State_impl&lt;std::thread::_Invoker&lt;std::tuple&lt;Runnable, void (*)(log_t*),
        log_t*&gt; &gt; &gt;::~_State_impl+0x0 (/usr/sbin/
                5441554156415741 [unknown] ([unknown])
        [...]</pre>
        <p class="noindent"><span epub:type="pagebreak" id="page_467"></span>The output is a one-line summary for each event, followed by the stack trace that led to it. The one-line summary begins with default fields from perf(1): the process name, thread ID, CPU ID, timestamp, and event name (see <a href="ch13.xhtml#ch13">Chapter 13</a>, <a href="ch13.xhtml#ch13">perf</a>, <a href="ch13.xhtml#ch13lev11">Section 13.11</a>, <a href="ch13.xhtml#ch13lev11">perf script</a>). The remaining fields are specific to the tracepoint: for this block:block_rq_issue tracepoint, they are, along with the field contents:</p>
        <ul class="sq">
        <li><p class="bull"><strong>Disk major and minor numbers</strong>: <code>259,0</code></p></li>
        <li><p class="bull"><strong>I/O type</strong>: <code>WS</code> (synchronous writes)</p></li>
        <li><p class="bull"><strong>I/O size</strong>: <code>12288</code> (bytes)</p></li>
        <li><p class="bull"><strong>I/O command string</strong>: <code>()</code></p></li>
        <li><p class="bull"><strong>Sector address</strong>: <code>10329704</code></p></li>
        <li><p class="bull"><strong>Number of sectors</strong>: <code>24</code></p></li>
        <li><p class="bull"><strong>Process</strong>: <code>mysqld</code></p></li>
        </ul>
        <p class="noindent">These fields are from the format string of the tracepoint (see <a href="ch04.xhtml#ch04">Chapter 4</a>, <a href="ch04.xhtml#ch04">Observability Tools</a>, <a href="ch04.xhtml#ch04lev3sec5">Section 4.3.5</a>, <a href="ch04.xhtml#ch04lev3sec5">Tracepoints</a>, under <a href="ch04.xhtml#ch04lev3_14">Tracepoints Arguments and Format String</a>).</p>
        <p class="noindent">The stack trace can help explain the nature of the disk I/O. In this case, it is from the mysqld log_flusher() routine that called fsync(2). The kernel code path shows it was handled by the ext4 file system, and became a disk I/O issue via blk_mq_try_issue_list_directly().</p>
        <p class="noindent">Often I/O will be queued and then issued later by a kernel thread, and tracing the block:block_rq_issue tracepoint will not show the originating process or user-level stack trace. In those cases you can try tracing block:block_rq_insert instead, which is for queue insertion. Note that it misses I/O that did not queue.</p>
        <section>
        <h5 class="h5" id="ch09lev3_28">One-Liners</h5>
        <p class="noindent">The following one-liners demonstrate using filters with the block tracepoints.</p>
        <p class="noindent">Trace all block completions, of size at least 100 Kbytes, until Ctrl-C<sup><a id="ch09fn10a" href="ch09.xhtml#ch09fn10">10</a></sup>:</p>
        <p class="footnote1"><sup><a id="ch09fn10" href="ch09.xhtml#ch09fn10a">10</a></sup>With a sector size of 512 bytes, 100 Kbytes means 200 sectors.</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg467-1a" id="pg467-1">Click here to view code image</a></p>
        <pre class="preash"><strong>perf record -e block:block_rq_complete --filter 'nr_sector &gt; 200'</strong></pre>
        <p class="noindent">Trace all block completions, synchronous writes only, until Ctrl-C:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg467-2a" id="pg467-2">Click here to view code image</a></p>
        <pre class="preash"><strong>perf record -e block:block_rq_complete --filter 'rwbs == "WS"</strong></pre>
        <p class="noindent">Trace all block completions, all types of writes, until Ctrl-C:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg467-3a" id="pg467-3">Click here to view code image</a></p>
        <pre class="preash"><strong>perf record -e block:block_rq_complete --filter 'rwbs ~ "*W*"'</strong></pre>
        </section>
        <section>
        <h5 class="h5" id="ch09lev3_29">Disk I/O Latency</h5>
        <p class="noindent">Disk I/O latency (described earlier as <em>disk request time</em>) can also be determined by recording both the disk issue and completion events for later analysis. The following records them for 60 seconds then writes the events to a out.disk01.txt file:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg468-1a" id="pg468-1">Click here to view code image</a></p>
        <pre class="pretb"><span epub:type="pagebreak" id="page_468"></span><strong>perf record -e block:block_rq_issue,block:block_rq_complete -a sleep 60</strong>
        <strong>perf script --header &gt; out.disk01.txt</strong></pre>
        <p class="noindent">You can post-process the output file using whatever is convenient: awk(1), Perl, Python, R, Google Spreadsheets, etc. Associate issues with completions and use the recorded timestamps to calculate the latency.</p>
        <p class="noindent">The following tools, biolatency(8) and biosnoop(8), calculate disk I/O latency efficiently in kernel space using a BPF program, and include the latency directly in the output.</p>
        </section>
        </section>
        <section>
        <h4 class="h4" id="ch09lev6sec6">9.6.6 biolatency</h4>
        <p class="noindent">biolatency(8)<sup><a id="ch09fn11a" href="ch09.xhtml#ch09fn11">11</a></sup> is a BCC and bpftrace tool to show disk I/O latency as a histogram. The term <em>I/O latency</em> used here refers to the time from issuing a request to the device, to when it completes (aka disk request time).</p>
        <p class="footnote1"><sup><a id="ch09fn11" href="ch09.xhtml#ch09fn11a">11</a></sup>Origin: I created biolatency for BCC on 20-Sep-2015 and bpftrace on 13-Sep-2018, based on an earlier iolatency tool I developed. I added the “b” to these tools to make it clear it refers to block I/O.</p>
        <p class="noindent">The following shows biolatency(8) from BCC tracing block I/O for 10 seconds:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg468-2a" id="pg468-2">Click here to view code image</a></p>
        <pre class="pretb"># <strong>biolatency 10 1</strong>
        Tracing block device I/O... Hit Ctrl-C to end.
        
             usecs               : count     distribution
                 0 -&gt; 1          : 0        |                                        |
                 2 -&gt; 3          : 0        |                                        |
                 4 -&gt; 7          : 0        |                                        |
                 8 -&gt; 15         : 0        |                                        |
                16 -&gt; 31         : 2        |                                        |
                32 -&gt; 63         : 0        |                                        |
                64 -&gt; 127        : 0        |                                        |
               128 -&gt; 255        : 1065     |*****************                       |
               256 -&gt; 511        : 2462     |****************************************|
               512 -&gt; 1023       : 1949     |*******************************         |
              1024 -&gt; 2047       : 373      |******                                  |
              2048 -&gt; 4095       : 1815     |*****************************           |
              4096 -&gt; 8191       : 591      |*********                               |
              8192 -&gt; 16383      : 397      |******                                  |
             16384 -&gt; 32767      : 50       |                                        |</pre>
        <p class="noindent">This output shows a bi-modal distribution, with one mode between 128 and 1023 microseconds, and another between 2048 and 4095 microseconds (2.0 to 4.1 milliseconds.) Now that I know that the device latency is bi-modal, understanding why may lead to tuning that moves more I/O to the faster mode. For example, the slower I/O could be random I/O or larger-sized I/O (which <span epub:type="pagebreak" id="page_469"></span>can be determined using other BPF tools), or different I/O flags (shown using the <code>-F</code> option). The slowest I/O in this output reached the 16- to 32-millisecond range: this sounds like queueing on the device.</p>
        <p class="noindent">The BCC version of biolatency(8) supports options including:</p>
        <ul class="sq">
        <li><p class="bull"><strong><code>-m</code></strong>: Output in milliseconds</p></li>
        <li><p class="bull"><strong><code>-Q</code></strong>: Include OS queued I/O time (<em>OS request time</em>)</p></li>
        <li><p class="bull"><strong><code>-F</code></strong>: Show a histogram for each I/O flag set</p></li>
        <li><p class="bull"><strong><code>-D</code></strong>: Show a histogram for each disk device</p></li>
        </ul>
        <p class="noindent">Using <code>-Q</code> makes biolatency(8) report the full I/O time from creation and insertion on a kernel queue to device completion, described earlier as the <em>block I/O request time</em>.</p>
        <p class="noindent">The BCC biolatency(8) also accepts optional interval and count arguments, in seconds.</p>
        <section>
        <h5 class="h5" id="ch09lev3_30">Per-Flag</h5>
        <p class="noindent">The <code>-F</code> option is especially useful, breaking down the distribution for each I/O flag. For example, with <code>-m</code> for millisecond histograms:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg469a" id="pg469">Click here to view code image</a></p>
        <pre class="pretb"># <strong>biolatency -Fm 10 1</strong>
        Tracing block device I/O... Hit Ctrl-C to end.
        
        flags = Sync-Write
             msecs               : count     distribution
                 0 -&gt; 1          : 2        |****************************************|
        
        flags = Flush
             msecs               : count     distribution
                 0 -&gt; 1          : 1        |****************************************|
        
        flags = Write
             msecs               : count     distribution
                 0 -&gt; 1          : 14       |****************************************|
                 2 -&gt; 3          : 1        |**                                      |
                 4 -&gt; 7          : 10       |****************************            |
                 8 -&gt; 15         : 11       |*******************************         |
                16 -&gt; 31         : 11       |*******************************         |
        
        flags = NoMerge-Write
             msecs               : count     distribution
                 0 -&gt; 1          : 95       |**********                              |
                 2 -&gt; 3          : 152      |*****************                       |
                 4 -&gt; 7          : 266      |******************************          |
                 8 -&gt; 15         : 350      |****************************************|
                16 -&gt; 31         : 298      |**********************************      |
        
        <span epub:type="pagebreak" id="page_470"></span>flags = Read
             msecs               : count     distribution
                 0 -&gt; 1          : 11       |****************************************|
        
        flags = ReadAhead-Read
             msecs               : count     distribution
                 0 -&gt; 1          : 5261     |****************************************|
                 2 -&gt; 3          : 1238     |*********                               |
                 4 -&gt; 7          : 481      |***                                     |
                 8 -&gt; 15         : 5        |                                        |
                16 -&gt; 31         : 2        |                                        |</pre>
        <p class="noindent">These flags may be handled differently by the storage device; separating them allows us to study them in isolation. The previous output shows that writes were slower than reads, and can explain the earlier bi-modal distribution.</p>
        <p class="noindent">biolatency(8) summarizes disk I/O latency. To examine it for each I/O, use biosnoop(8).</p>
        </section>
        </section>
        <section>
        <h4 class="h4" id="ch09lev6sec7">9.6.7 biosnoop</h4>
        <p class="noindent">biosnoop(8)<sup><a id="ch09fn12a" href="ch09.xhtml#ch09fn12">12</a></sup> is a BCC and bpftrace tool that prints a one-line summary for each disk I/O. For example:</p>
        <p class="footnote1"><sup><a id="ch09fn12" href="ch09.xhtml#ch09fn12a">12</a></sup>Origin: I created the BCC version on 16-Sep-2015, and the bpftrace version on 15-Nov-2017, based on an earlier tool of mine from 2003. The full origin is described in <a href="ch09.xhtml#ch09ref20">[Gregg 19]</a>.</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg470a" id="pg470">Click here to view code image</a></p>
        <pre class="pretb"># <strong>biosnoop</strong>
        TIME(s)        COMM           PID    DISK    T  SECTOR    BYTES   LAT(ms)
        0.009165000    jbd2/nvme0n1p1 174    nvme0n1 W  2116272   8192       0.43
        0.009612000    jbd2/nvme0n1p1 174    nvme0n1 W  2116288   4096       0.39
        0.011836000    mysqld         1948   nvme0n1 W  10434672  4096       0.45
        0.012363000    jbd2/nvme0n1p1 174    nvme0n1 W  2116296   8192       0.49
        0.012844000    jbd2/nvme0n1p1 174    nvme0n1 W  2116312   4096       0.43
        0.016809000    mysqld         1948   nvme0n1 W  10227712  262144     1.82
        0.017184000    mysqld         1948   nvme0n1 W  10228224  262144     2.19
        0.017679000    mysqld         1948   nvme0n1 W  10228736  262144     2.68
        0.018056000    mysqld         1948   nvme0n1 W  10229248  262144     3.05
        0.018264000    mysqld         1948   nvme0n1 W  10229760  262144     3.25
        0.018657000    mysqld         1948   nvme0n1 W  10230272  262144     3.64
        0.018954000    mysqld         1948   nvme0n1 W  10230784  262144     3.93
        0.019053000    mysqld         1948   nvme0n1 W  10231296  131072     4.03
        0.019731000    jbd2/nvme0n1p1 174    nvme0n1 W  2116320   8192       0.49
        0.020243000    jbd2/nvme0n1p1 174    nvme0n1 W  2116336   4096       0.46
        0.020593000    mysqld         1948   nvme0n1 R  4495352   4096       0.26
        [...]</pre>
        <p class="noindent"><span epub:type="pagebreak" id="page_471"></span>This output shows a write workload to disk nvme0n1, mostly from mysqld, PID 174, with varying I/O sizes. The columns are:</p>
        <ul class="sq">
        <li><p class="bull"><strong><code>TIME(s)</code></strong>: I/O completion time in seconds</p></li>
        <li><p class="bull"><strong><code>COMM</code></strong>: Process name (if known by this tool)</p></li>
        <li><p class="bull"><strong><code>PID</code></strong>: Process ID (if known by this tool)</p></li>
        <li><p class="bull"><strong><code>DISK</code></strong>: Storage device name</p></li>
        <li><p class="bull"><strong><code>T</code></strong>: Type: R == reads, W == writes</p></li>
        <li><p class="bull"><strong><code>SECTOR</code></strong>: Address on disk in units of 512-byte sectors</p></li>
        <li><p class="bull"><strong><code>BYTES</code></strong>: Size of the I/O request</p></li>
        <li><p class="bull"><strong><code>LAT(ms)</code></strong>: Duration of the I/O from device issue to device completion (disk request time)</p></li>
        </ul>
        <p class="noindent">Around the middle of the example output is a series of 262,144 byte writes, beginning with a latency of 1.82 ms and increasing in latency for each subsequent I/O, ending with 4.03 ms. This is a pattern I commonly see, and the likely reason can be calculated from another column in the output: <code>TIME(s)</code>. If you subtract the <code>LAT(ms)</code> column from the <code>TIME(s)</code> column, you have the starting time of the I/O, and these started around the same time. This appears to be a group of writes that were sent at the same time, queued on the device, and then completed in turn, with increasing latency for each.</p>
        <p class="noindent">By careful examination of the start and end times, reordering on the device can also be identified. Since the output can many thousands of lines, I have often used the R statistical software to plot the output as a scatter plot, to help in identifying these patterns (see <a href="ch09.xhtml#ch09lev7">Section 9.7</a>, <a href="ch09.xhtml#ch09lev7">Visualizations</a>).</p>
        <section>
        <h5 class="h5" id="ch09lev3_31">Outlier Analysis</h5>
        <p class="noindent">Here is a method for finding and analyzing latency outliers using biosnoop(8).</p>
        <ol class="number">
        <li><p class="number">Write the output to a file:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg471a" id="pg471">Click here to view code image</a></p>
        <pre class="pretb"># <strong>biosnoop &gt; out.biosnoop01.txt</strong></pre>
        </li>
        <li><p class="number">Sort the output by the latency column, and print the last five entries (those with the highest latency):</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg472a" id="pg472">Click here to view code image</a></p>
        <pre class="pretb"># <strong>sort -n -k 8,8 out.biosnoop01.txt | tail -5</strong>
        31.344175   logger         10994  nvme0n1 W 15218056   262144   30.92
        31.344401   logger         10994  nvme0n1 W 15217544   262144   31.15
        31.344757   logger         10994  nvme0n1 W 15219080   262144   31.49
        31.345260   logger         10994  nvme0n1 W 15218568   262144   32.00
        46.059274   logger         10994  nvme0n1 W 15198896   4096     64.86</pre>
        </li>
        <li><p class="number">Open the output in a text editor (e.g., vi(1) or vim(1)):</p>
        <pre class="pretb"># <strong>vi out.biosnoop01.txt</strong></pre>
        </li>
        <li><p class="number"><span epub:type="pagebreak" id="page_472"></span>Work through the outliers from slowest to fastest, searching for the time in the first column. The slowest was 64.86 milliseconds, with the completion time of 46.059274 (seconds). Searching for 46.059274:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg472-1a" id="pg472-1">Click here to view code image</a></p>
        <pre class="pretb">[...]
        45.992419   jbd2/nvme0n1p1 174    nvme0n1 W 2107232    8192      0.45
        45.992988   jbd2/nvme0n1p1 174    nvme0n1 W 2107248    4096      0.50
        <strong>46.059274</strong>   logger         10994  nvme0n1 W 15198896   4096     64.86
        [...]</pre>
        </li>
        <li><p class="number">Look at events that occurred prior to the outlier, to see whether they had similar latency and therefore this was the result of queueing (similar to the 1.82 to 4.03 ms ramp seen in the first biosnoop(8) example output), or for any other clues. That’s not the case here: the previous event was around 6 ms earlier, with a latency of 0.5 ms. The device may have reordered events and completed the others first. If the previous completion event was around 64 ms ago, then the gap in completions from the device may be explained by other factors: e.g., this system is a VM instance, and can be de-scheduled by the hypervisor during I/O, adding that time to the I/O time.</p></li>
        </ol>
        </section>
        <section>
        <h5 class="h5" id="ch09lev3_32">Queued Time</h5>
        <p class="noindent">A <code>-Q</code> option to BCC biosnoop(8) can be used to show the time spent between the creation of the I/O and the issue to the device (previously called the <em>block I/O wait time</em> or <em>OS wait time</em>). This time is mostly spent on OS queues, but could also include memory allocation and lock acquisition. For example:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg472-2a" id="pg472-2">Click here to view code image</a></p>
        <pre class="pretb"># <strong>biosnoop -Q</strong>
        TIME(s)     COMM           PID    DISK    T SECTOR     BYTES  QUE(ms) LAT(ms)
        0.000000    kworker/u4:0   9491   nvme0n1 W 5726504    4096      0.06    0.60
        0.000039    kworker/u4:0   9491   nvme0n1 W 8128536    4096      0.05    0.64
        0.000084    kworker/u4:0   9491   nvme0n1 W 8128584    4096      0.05    0.68
        0.000138    kworker/u4:0   9491   nvme0n1 W 8128632    4096      0.05    0.74
        0.000231    kworker/u4:0   9491   nvme0n1 W 8128664    4096      0.05    0.83
        [...]</pre>
        <p class="noindent">The queued time is shown in the QUE(ms) column.</p>
        </section>
        </section>
        <section>
        <h4 class="h4" id="ch09lev6sec8">9.6.8 iotop, biotop</h4>
        <p class="noindent">I wrote the first iotop in 2005 for Solaris-based systems <a href="ch09.xhtml#ch09ref2">[McDougall 06a]</a>. There are now many versions, including a Linux iotop(1) tool based on kernel accounting statistics<sup><a id="ch09fn13a" href="ch09.xhtml#ch09fn13">13</a></sup> <a href="ch09.xhtml#ch09ref14">[Chazarain 13]</a>, and my own biotop(8) based on BPF.</p>
        <p class="footnote1"><sup><a id="ch09fn13" href="ch09.xhtml#ch09fn13a">13</a></sup>iotop(1) requires CONFIG_TASK_DELAY_ACCT, CONFIG_TASK_IO_ACCOUNTING, CONFIG_TASKSTATS, and CONFIG_VM_EVENT_COUNTERS.</p>
        <section>
        <h5 class="h5" id="ch09lev3_33"><span epub:type="pagebreak" id="page_473"></span>iotop</h5>
        <p class="noindent">iotop can typically be installed via an iotop package. When run without arguments, it refreshes the screen every second, showing the top disk I/O processes. Batch mode (<code>-b</code>) can be used to provide a rolling output (no screen clear); it is demonstrated here with I/O processes only (<code>-o</code>) and an interval of 5 s (<code>-d5</code>):</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg473-1a" id="pg473-1">Click here to view code image</a></p>
        <pre class="pretb"># <strong>iotop -bod5</strong>
        Total DISK READ:       4.78 K/s | Total DISK WRITE:      15.04 M/s
          TID  PRIO  USER     DISK READ  DISK WRITE  SWAPIN      IO    COMMAND
        22400 be/4 root        4.78 K/s    0.00 B/s  0.00 % 13.76 % [flush-252:0]
          279 be/3 root        0.00 B/s 1657.27 K/s  0.00 %  9.25 % [jbd2/vda2-8]
        22446 be/4 root        0.00 B/s   10.16 M/s  0.00 %  0.00 % beam.smp -K true ...
        Total DISK READ:       0.00 B/s | Total DISK WRITE:      10.75 M/s
          TID  PRIO  USER     DISK READ  DISK WRITE  SWAPIN      IO    COMMAND
          279 be/3 root        0.00 B/s    9.55 M/s  0.00 %  0.01 % [jbd2/vda2-8]
        22446 be/4 root        0.00 B/s   10.37 M/s  0.00 %  0.00 % beam.smp -K true ...
          646 be/4 root        0.00 B/s  272.71 B/s  0.00 %  0.00 % rsyslogd -n -c 5
        [...]</pre>
        <p class="noindent">The output shows the <code>beam.smp</code> process (Riak) performing a disk write workload of around 10 Mbytes/s. The columns include:</p>
        <ul class="sq">
        <li><p class="bull"><strong><code>DISK READ</code></strong>: Read Kbytes/s</p></li>
        <li><p class="bull"><strong><code>DISK WRITE</code></strong>: Write Kbytes/s</p></li>
        <li><p class="bull"><strong><code>SWAPIN</code></strong>: Percent of time the thread spent waiting for swap-in I/O</p></li>
        <li><p class="bull"><strong><code>IO</code></strong>: Percent of time the thread spent waiting for I/O</p></li>
        </ul>
        <p class="noindent">iotop(8) supports various other options, including <code>-a</code> for accumulated statistics (instead of per-interval), <code>-p PID</code> to match a process, and <code>-d SEC</code> to set the interval.</p>
        <p class="noindent">I recommend that you test iotop(8) with a known workload and check that the numbers match. I just tried (iotop version 0.6) and found that it greatly undercounts write workloads. You can also use biotop(8), which uses a different instrumentation source and <em>does</em> match my test workload.</p>
        </section>
        <section>
        <h5 class="h5" id="ch09lev3_34">biotop</h5>
        <p class="noindent">biotop(8) is a BCC tool, and is another top(1) for disks. Example output:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg473-2a" id="pg473-2">Click here to view code image</a></p>
        <pre class="pretb"># <strong>biotop</strong>
        Tracing... Output every 1 secs. Hit Ctrl-C to end
        
        08:04:11 loadavg: 1.48 0.87 0.45 1/287 14547
        
        PID    COMM             D MAJ MIN DISK       I/O  Kbytes  AVGms
        14501  cksum            R 202 1   xvda1      361   28832   3.39
        <span epub:type="pagebreak" id="page_474"></span>6961   dd               R 202 1   xvda1     1628   13024   0.59
        13855  dd               R 202 1   xvda1     1627   13016   0.59
        326    jbd2/xvda1-8     W 202 1   xvda1        3     168   3.00
        1880   supervise        W 202 1   xvda1        2       8   6.71
        1873   supervise        W 202 1   xvda1        2       8   2.51
        1871   supervise        W 202 1   xvda1        2       8   1.57
        1876   supervise        W 202 1   xvda1        2       8   1.22
        [...]</pre>
        <p class="noindent">This shows cksum(1) and dd(1) commands performing reads, and supervise processes performing some writes. This is a quick way to identify who is performing disk I/O, and by how much. The columns are:</p>
        <ul class="sq">
        <li><p class="bull"><strong><code>PID</code></strong>: Cached process ID (best effort)</p></li>
        <li><p class="bull"><strong><code>COMM</code></strong>: Cached process name (best effort)</p></li>
        <li><p class="bull"><strong><code>D</code></strong>: Direction (R == read, W == write)</p></li>
        <li><p class="bull"><strong><code>MAJ MIN</code></strong>: Disk major and minor numbers (a kernel identifier)</p></li>
        <li><p class="bull"><strong><code>DISK</code></strong>: Disk name</p></li>
        <li><p class="bull"><strong><code>I/O</code></strong>: Number of disk I/O during interval</p></li>
        <li><p class="bull"><strong><code>Kbytes</code></strong>: Total disk throughput during interval (Kbytes)</p></li>
        <li><p class="bull"><strong><code>AVGms</code></strong>: Average time for the I/O (latency) from the issue to the device, to its completion (milliseconds)</p></li>
        </ul>
        <p class="noindent">By the time disk I/O is issued to the device, the requesting process may no longer be on CPU, and identifying it can be difficult. biotop(8) uses a best-effort approach: the <code>PID</code> and <code>COMM</code> columns will usually match the correct process, but this is not guaranteed.</p>
        <p class="noindent">biotop(8) supports optional interval and count columns (the default interval is one second), <code>-C</code> to not clear the screen, and <code>-r MAXROWS</code> to specify the top number of processes to display.</p>
        </section>
        </section>
        <section>
        <h4 class="h4" id="ch09lev6sec9">9.6.9 biostacks</h4>
        <p class="noindent">biostacks(8)<sup><a id="ch09fn14a" href="ch09.xhtml#ch09fn14">14</a></sup> is a bpftrace tool that traces the block I/O request time (from OS enqueue to device completion) with the I/O initialization stack trace. For example:</p>
        <p class="footnote1"><sup><a id="ch09fn14" href="ch09.xhtml#ch09fn14a">14</a></sup>Origin: I created it on 19-Mar-2019 for <a href="ch09.xhtml#ch09ref20">[Gregg 19]</a>.</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg474a" id="pg474">Click here to view code image</a></p>
        <pre class="pretb"># <strong>biostacks.bt</strong>
        Attaching 5 probes...
        Tracing block I/O with init stacks. Hit Ctrl-C to end.
        ^C
        [...]
        <span epub:type="pagebreak" id="page_475"></span>@usecs[
            blk_account_io_start+1
            blk_mq_make_request+1069
            generic_make_request+292
            submit_bio+115
            submit_bh_wbc+384
            ll_rw_block+173
            ext4_bread+102
            __ext4_read_dirblock+52
            ext4_dx_find_entry+145
            ext4_find_entry+365
            ext4_lookup+129
            lookup_slow+171
            walk_component+451
            path_lookupat+132
            filename_lookup+182
            user_path_at_empty+54
            sys_access+175
            do_syscall_64+115
            entry_SYSCALL_64_after_hwframe+61
        ]:
        [2K, 4K)               2 |@@                                                  |
        [4K, 8K)              37 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@|
        [8K, 16K)             15 |@@@@@@@@@@@@@@@@@@@@@                               |
        [16K, 32K)             9 |@@@@@@@@@@@@                                        |
        [32K, 64K)             1 |@                                                   |</pre>
        <p class="noindent">The output shows a latency histogram (in microseconds) for disk I/O, along with the requesting I/O stack: via the access(2) syscall, filename_lookup(), and ext4_lookup(). This I/O was caused by looking up pathnames during file permission checks. The output included many such stacks, and these show that I/O are caused by activity other than reads and writes.</p>
        <p class="noindent">I have seen cases where there was mysterious disk I/O without any application causing it. The reason turned out to be background file system tasks. (In one case it was ZFS’s background scrubber, which periodically verifies checksums.) biostacks(8) can identify the real reason for disk I/O by showing the kernel stack trace.</p>
        </section>
        <section>
        <h4 class="h4" id="ch09lev6sec10">9.6.10 blktrace</h4>
        <p class="noindent">blktrace(8) is a custom tracing facility for block device I/O events on Linux that uses the kernel blktrace tracer. This is a specialized tracer controlled via BLKTRACE ioctl(2) syscalls to disk device files. The frontend tools include blktrace(8), blkparse(1), and btrace(8).</p>
        <p class="noindent"><span epub:type="pagebreak" id="page_476"></span>blktrace(8) enables kernel block driver tracing and retrieves the raw trace data, which can be processed using blkparse(1). For convenience, the btrace(8) tool runs both blktrace(8) and blkparse(1), such that the following are equivalent:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg476-1a" id="pg476-1">Click here to view code image</a></p>
        <pre class="pretb"># <strong>blktrace -d /dev/sda -o - | blkparse -i -</strong>
        # <strong>btrace /dev/sda</strong></pre>
        <p class="noindent">blktrace(8) is a low-level tool that shows multiple events per I/O.</p>
        <section>
        <h5 class="h5" id="ch09lev3_35">Default Output</h5>
        <p class="noindent">The following shows the default output of btrace(8) and captures a single disk read event by the cksum(1) command:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg476-2a" id="pg476-2">Click here to view code image</a></p>
        <pre class="pretbs"># <strong>btrace /dev/sdb</strong>
          8,16   3        1     0.429604145 20442  A   R 184773879 + 8 &lt;- (8,17) 184773816
          8,16   3        2     0.429604569 20442  Q   R 184773879 + 8 [cksum]
          8,16   3        3     0.429606014 20442  G   R 184773879 + 8 [cksum]
          8,16   3        4     0.429607624 20442  P   N [cksum]
          8,16   3        5     0.429608804 20442  I   R 184773879 + 8 [cksum]
          8,16   3        6     0.429610501 20442  U   N [cksum] 1
          8,16   3        7     0.429611912 20442  D   R 184773879 + 8 [cksum]
          8,16   1        1     0.440227144     0  C   R 184773879 + 8 [0]
        [...]</pre>
        <p class="noindent">Eight lines of output were reported for this single disk I/O, showing each action (event) involving the block device queue and the device.</p>
        <p class="noindent">By default, there are seven columns:</p>
        <ol class="number">
        <li><p class="number">Device major, minor number</p></li>
        <li><p class="number">CPU ID</p></li>
        <li><p class="number">Sequence number</p></li>
        <li><p class="number">Action time, in seconds</p></li>
        <li><p class="number">Process ID</p></li>
        <li><p class="number">Action identifier: the type of event (see the Action Identifiers heading)</p></li>
        <li><p class="number">RWBS description: I/O flags (see the RWBS Description heading)</p></li>
        </ol>
        <p class="noindent">These output columns may be customized using the <code>-f</code> option. They are followed by custom data based on the action.</p>
        <p class="noindent">The final data depends on the action. For example, <code>184773879 + 8 [cksum]</code> means an I/O at block address 184773879 with size 8 (sectors), from the process named <code>cksum</code>.</p>
        </section>
        <section>
        <h5 class="h5" id="ch09lev3_36"><span epub:type="pagebreak" id="page_477"></span>Action Identifiers</h5>
        <p class="noindent">These are described in the blkparse(1) man page:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg477a" id="pg477">Click here to view code image</a></p>
        <pre class="pretb">       A      IO was remapped to a different device
               B      IO bounced
               C      IO completion
               D      IO issued to driver
               F      IO front merged with request on queue
               G      Get request
               I      IO inserted onto request queue
               M      IO back merged with request on queue
               P      Plug request
               Q      IO handled by request queue code
               S      Sleep request
               T      Unplug due to timeout
               U      Unplug request
               X      Split</pre>
        <p class="noindent">This list has been included because it also shows the events that the blktrace framework can observe.</p>
        </section>
        <section>
        <h5 class="h5" id="ch09lev3_37">RWBS Description</h5>
        <p class="noindent">For tracing observability, the kernel provides a way to describe the type of each I/O using a character string named <em>rwbs</em>. rwbs is used by blktrace(8) and other disk tracing tools. It is defined in the kernel blk_fill_rwbs() function and uses the characters:</p>
        <ul class="sq">
        <li><p class="bull">R: Read</p></li>
        <li><p class="bull">W: Write</p></li>
        <li><p class="bull">M: Metadata</p></li>
        <li><p class="bull">S: Synchronous</p></li>
        <li><p class="bull">A: Read-ahead</p></li>
        <li><p class="bull">F: Flush or force unit access</p></li>
        <li><p class="bull">D: Discard</p></li>
        <li><p class="bull">E: Erase</p></li>
        <li><p class="bull">N: None</p></li>
        </ul>
        <p class="noindent">The characters can be combined. For example, “WM” is for writes of metadata.</p>
        </section>
        <section>
        <h5 class="h5" id="ch09lev3_38"><span epub:type="pagebreak" id="page_478"></span>Action Filtering</h5>
        <p class="noindent">The blktrace(8) and btrace(8) commands can filter actions to show only the event type of interest. For example, to trace only the <code>D</code> actions (I/O issued), use the filter option <code>-a issue</code>:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg478-1a" id="pg478-1">Click here to view code image</a></p>
        <pre class="pretb"># <strong>btrace -a issue /dev/sdb</strong>
          8,16   1        1     0.000000000   448  D   W 38978223 + 8 [kjournald]
          8,16   1        2     0.000306181   448  D   W 104685503 + 24 [kjournald]
          8,16   1        3     0.000496706   448  D   W 104685527 + 8 [kjournald]
          8,16   1        1     0.010441458 20824  D   R 184944151 + 8 [tar]
        [...]</pre>
        <p class="noindent">Other filters are described in the blktrace(8) man page, including options to trace only reads (<code>-a read</code>), writes (<code>-a write</code>), or synchronous operations (<code>-a sync</code>).</p>
        </section>
        <section>
        <h5 class="h5" id="ch09lev3_39">Analyze</h5>
        <p class="noindent">The blktrace package includes btt(1) to analyze I/O traces. Here is an example invocation, now using blktrace(8) on /dev/nvme0n1p1 to write trace files (a new directory is used since these commands create several files):</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg478-2a" id="pg478-2">Click here to view code image</a></p>
        <pre class="pretb"># <strong>mkdir tracefiles; cd tracefiles</strong>
        # <strong>blktrace -d /dev/nvme0n1p1 -o out -w 10</strong>
        === nvme0n1p1 ===
          CPU  0:                20135 events,      944 KiB data
          CPU  1:                38272 events,     1795 KiB data
          Total:                 58407 events (dropped 0),     2738 KiB data
        # <strong>blkparse -i out.blktrace.* -d out.bin</strong>
        259,0    1        1     0.000000000  7113  A  RM 161888 + 8 &lt;- (259,1) 159840
        259,0    1        1     0.000000000  7113  A  RM 161888 + 8 &lt;- (259,1) 159840
        [...]
        # <strong>btt -i out.bin</strong>
        ==================== All Devices ====================
        
                    ALL           MIN           AVG           MAX           N
        --------------- ------------- ------------- ------------- -----------
        Q2Q               0.000000001   0.000365336   2.186239507       24625
        Q2A               0.000037519   0.000476609   0.001628905        1442
        Q2G               0.000000247   0.000007117   0.006140020       15914
        G2I               0.000001949   0.000027449   0.000081146         602
        Q2M               0.000000139   0.000000198   0.000021066        8720
        I2D               0.000002292   0.000008148   0.000030147         602
        M2D               0.000001333   0.000188409   0.008407029        8720
        <span epub:type="pagebreak" id="page_479"></span>D2C               0.000195685   0.000885833   0.006083538       12308
        Q2C               0.000198056   0.000964784   0.009578213       12308
        [...]</pre>
        <p class="noindent">These statistics are in units of seconds, and show times for each stage of I/O processing. Interesting times include:</p>
        <ul class="sq">
        <li><p class="bull"><strong><code>Q2C</code></strong>: The total time from the I/O request to completion (time in block layer)</p></li>
        <li><p class="bull"><strong><code>D2C</code></strong>: The device issue to completion (disk I/O latency)</p></li>
        <li><p class="bull"><strong><code>I2D</code></strong>: The time from device queue insertion to device issue (request queue time)</p></li>
        <li><p class="bull"><strong><code>M2D</code></strong>: The time from I/O merge to issue</p></li>
        </ul>
        <p class="noindent">The output shows an average D2C time of 0.86 ms, and a max M2D of 8.4 ms. Maximums such as these can cause I/O latency outliers.</p>
        <p class="noindent">For more information, see the btt User Guide <a href="ch09.xhtml#ch09ref3">[Brunelle 08]</a>.</p>
        </section>
        <section>
        <h5 class="h5" id="ch09lev3_40">Visualizations</h5>
        <p class="noindent">The blktrace(8) tool can record events to trace files that can be visualized using iowatcher(1), also provided in the blktrace package, and also visualized using Chris Mason’s seekwatcher <a href="ch09.xhtml#ch09ref5">[Mason 08]</a>.</p>
        </section>
        </section>
        <section>
        <h4 class="h4" id="ch09lev6sec11">9.6.11 bpftrace</h4>
        <p class="noindent">bpftrace is a BPF-based tracer that provides a high-level programming language, allowing the creation of powerful one-liners and short scripts. It is well suited for custom disk analysis based on clues from other tools.</p>
        <p class="noindent">bpftrace is explained in <a href="ch15.xhtml#ch15">Chapter 15</a>. This section shows some examples for disk analysis: one-liners, disk I/O size, and disk I/O latency.</p>
        <section>
        <h5 class="h5" id="ch09lev3_41">One-Liners</h5>
        <p class="noindent">The following one-liners are useful and demonstrate different bpftrace capabilities.</p>
        <p class="noindent">Count block I/O tracepoints events:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg479-1a" id="pg479-1">Click here to view code image</a></p>
        <pre class="preash"><strong>bpftrace -e 'tracepoint:block:* { @[probe] = count(); }'</strong></pre>
        <p class="noindent">Summarize block I/O size as a histogram:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg479-2a" id="pg479-2">Click here to view code image</a></p>
        <pre class="preash"><strong>bpftrace -e 't:block:block_rq_issue { @bytes = hist(args-&gt;bytes); }'</strong></pre>
        <p class="noindent">Count block I/O request user stack traces:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg479-3a" id="pg479-3">Click here to view code image</a></p>
        <pre class="preash"><strong>bpftrace -e 't:block:block_rq_issue { @[ustack] = count(); }'
        bpftrace -e 't:block:block_rq_insert { @[ustack] = count(); }'</strong></pre>
        <p class="noindent"><span epub:type="pagebreak" id="page_480"></span>Count block I/O type flags:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg480-1a" id="pg480-1">Click here to view code image</a></p>
        <pre class="preash"><strong>bpftrace -e 't:block:block_rq_issue { @[args-&gt;rwbs] = count(); }'</strong></pre>
        <p class="noindent">Trace block I/O errors with device and I/O type:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg480-2a" id="pg480-2">Click here to view code image</a></p>
        <pre class="preash"><strong>bpftrace -e 't:block:block_rq_complete /args-&gt;error/ {
            printf("dev %d type %s error %d\n", args-&gt;dev, args-&gt;rwbs, args-&gt;error); }'</strong></pre>
        <p class="noindent">Count SCSI opcodes:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg480-3a" id="pg480-3">Click here to view code image</a></p>
        <pre class="preash"><strong>bpftrace -e 't:scsi:scsi_dispatch_cmd_start { @opcode[args-&gt;opcode] =
            count(); }'</strong></pre>
        <p class="noindent">Count SCSI result codes:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg480-4a" id="pg480-4">Click here to view code image</a></p>
        <pre class="preash"><strong>bpftrace -e 't:scsi:scsi_dispatch_cmd_done { @result[args-&gt;result] = count(); }'</strong></pre>
        <p class="noindent">Count SCSI driver functions:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg480-5a" id="pg480-5">Click here to view code image</a></p>
        <pre class="preash"><strong>bpftrace -e 'kprobe:scsi* { @[func] = count(); }'</strong></pre>
        </section>
        <section>
        <h5 class="h5" id="ch09lev3_42">Disk I/O Size</h5>
        <p class="noindent">Sometimes disk I/O is slow simply because it is large, especially for SSD drives. Another size-based issue is when an application requests many small I/O, which instead could be aggregated into larger sizes to reduce I/O stack overheads. Both of these issues can be investigated by examining the I/O size distribution.</p>
        <p class="noindent">Using bpftrace, the following shows a disk I/O size distribution broken down by the requesting process name:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg480-6a" id="pg480-6">Click here to view code image</a></p>
        <pre class="pretb"># <strong>bpftrace -e 't:block:block_rq_issue /args-&gt;bytes/ { @[comm] = hist(args-&gt;bytes); }'</strong>
        Attaching 1 probe...
        ^C
        [...]
        
        @[kworker/3:1H]:
        [4K, 8K)               1 |@@@@@@@@@@                                          |
        [8K, 16K)              0 |                                                    |
        [16K, 32K)             0 |                                                    |
        [32K, 64K)             0 |                                                    |
        [64K, 128K)            0 |                                                    |
        [128K, 256K)           0 |                                                    |
        [256K, 512K)           0 |                                                    |
        [512K, 1M)             5 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@|
        [1M, 2M)               3 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@                     |<br>
        <span epub:type="pagebreak" id="page_481"></span>@[dmcrypt_write]:
        [4K, 8K)             103 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@|
        [8K, 16K)             46 |@@@@@@@@@@@@@@@@@@@@@@@                             |
        [16K, 32K)            11 |@@@@@                                               |
        [32K, 64K)             0 |                                                    |
        [64K, 128K)            1 |                                                    |
        [128K, 256K)           1 |                                                    |</pre>
        <p class="noindent">The output shows processes named dmcrypt_write performing small I/O, mostly in the 4 to 32 Kbyte range.</p>
        <p class="noindent">The tracepoint block:block_rq_issue shows when I/O were sent to the device driver for delivery to the disk device. There is no guarantee that the originating process is still on CPU, especially if the I/O is queued by a scheduler, so the process name shown may be for a later kernel worker thread that reads I/O from a queue for device delivery. You can switch the tracepoint to block:block_rq_insert to measure from the insertion of the queue, which may improve the accuracy of the process name, but it may also miss instrumenting I/O that bypasses queueing (this was also mentioned in <a href="ch09.xhtml#ch09lev6sec5">Section 9.6.5</a>, <a href="ch09.xhtml#ch09lev6sec5">perf</a>).</p>
        <p class="noindent">If you add <code>args-&gt;rwbs</code> as a histogram key, the output will be further broken down by I/O type:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg481a" id="pg481">Click here to view code image</a></p>
        <pre class="pretb"># <strong>bpftrace -e 't:block:block_rq_insert /args-&gt;bytes/ { @[comm, args-&gt;rwbs] =</strong>
            <strong>hist(args-&gt;bytes); }'</strong>
        Attaching 1 probe...
        ^C
        [...]
        
        @[dmcrypt_write, WS]:
        [4K, 8K)               4 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@|
        [8K, 16K)              1 |@@@@@@@@@@@@@                                       |
        [16K, 32K)             0 |                                                    |
        [32K, 64K)             1 |@@@@@@@@@@@@@                                       |
        [64K, 128K)            1 |@@@@@@@@@@@@@                                       |
        [128K, 256K)           1 |@@@@@@@@@@@@@                                       |
        
        @[dmcrypt_write, W]:
        [512K, 1M)             8 |@@@@@@@@@@                                          |
        [1M, 2M)              38 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@|</pre>
        <p class="noindent">The output now includes <code>W</code> for writes, <code>WS</code> for synchronous writes, etc. See the earlier RWBS Description section for an explanation of these letters.</p>
        </section>
        <section>
        <h5 class="h5" id="ch09lev3_43"><span epub:type="pagebreak" id="page_482"></span>Disk I/O Latency</h5>
        <p class="noindent">The disk response time, often referred to as disk I/O latency, can be measured by instrumenting device issue to completion events. The biolatency.bt tool does this, showing disk I/O latency as a histogram. For example:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg482-1a" id="pg482-1">Click here to view code image</a></p>
        <pre class="pretb"># <strong>biolatency.bt</strong>
        Attaching 4 probes...
        Tracing block device I/O... Hit Ctrl-C to end.
        ^C
        
        @usecs:
        [32, 64)               2 |@                                                   |
        [64, 128)              1 |                                                    |
        [128, 256)             1 |                                                    |
        [256, 512)            27 |@@@@@@@@@@@@@@@@@@@@@@@@@@                          |
        [512, 1K)             43 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@           |
        [1K, 2K)              54 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@|
        [2K, 4K)              41 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@             |
        [4K, 8K)              47 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@       |
        [8K, 16K)             16 |@@@@@@@@@@@@@@@                                     |
        [16K, 32K)             4 |@@@                                                 |</pre>
        <p class="noindent">This output shows that I/O were typically completing between 256 microseconds and 16 milliseconds (<code>16K</code> microseconds).</p>
        <p class="noindent">The source code is:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg482-2a" id="pg482-2">Click here to view code image</a></p>
        <pre class="pretb">#!/usr/local/bin/bpftrace
        BEGIN
        {
                printf("Tracing block device I/O... Hit Ctrl-C to end.\n");
        }
        
        tracepoint:block:block_rq_issue
        {
                @start[args-&gt;dev, args-&gt;sector] = nsecs;
        }
        
        tracepoint:block:block_rq_complete
        /@start[args-&gt;dev, args-&gt;sector]/
        {
                @usecs = hist((nsecs - @start[args-&gt;dev, args-&gt;sector]) / 1000);
                delete(@start[args-&gt;dev, args-&gt;sector]);
        }
        <span epub:type="pagebreak" id="page_483"></span>END
        {
                clear(@start);
        }</pre>
        <p class="noindent">Measuring I/O latency requires storing a custom timestamp for the start of each I/O, and then referring to it when the I/O has completed in order to calculate the elapsed time. When VFS latency was measured in <a href="ch08.xhtml#ch08">Chapter 8</a>, <a href="ch08.xhtml#ch08">File Systems</a>, <a href="ch08.xhtml#ch08lev6sec15">Section 8.6.15</a>, <a href="ch15.xhtml#ch15">BPF</a>trace, the start timestamp was stored in a BPF map keyed by the thread ID: that worked because the same thread ID will be on CPU for the start and completion events. That is not the case with disk I/O, as the completion event will interrupt whatever else is on CPU. The unique ID in biolatency.bt has been constructed from the device and sector number: it assumes that only one I/O will be in flight to a given sector at a time.</p>
        <p class="noindent">As with the I/O size one-liner, you can add <code>args-&gt;rwbs</code> to the map key to break down by I/O type.</p>
        </section>
        <section>
        <h5 class="h5" id="ch09lev3_44">Disk I/O Errors</h5>
        <p class="noindent">I/O error status is an argument to the block:block_rq_complete tracepoint, and the following bioerr(8) tool<sup><a id="ch09fn15a" href="ch09.xhtml#ch09fn15">15</a></sup> uses it to print details for I/O operations that error (a one-liner version of this was included earlier):</p>
        <p class="footnote1"><sup><a id="ch09fn15" href="ch09.xhtml#ch09fn15a">15</a></sup>Origin: I created it for the BPF book on 19-Mar-2019 <a href="ch09.xhtml#ch09ref20">[Gregg 19]</a>.</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg483a" id="pg483">Click here to view code image</a></p>
        <pre class="pretb">#!/usr/local/bin/bpftrace
        
        BEGIN
        {
                printf("Tracing block I/O errors. Hit Ctrl-C to end.\n");
        }
        
        tracepoint:block:block_rq_complete
        /args-&gt;error != 0/
        {
                time("%H:%M:%S ");
                printf("device: %d,%d, sector: %d, bytes: %d, flags: %s, error: %d\n",
                    args-&gt;dev &gt;&gt; 20, args-&gt;dev &amp; ((1 &lt;&lt; 20) - 1), args-&gt;sector,
                    args-&gt;nr_sector * 512, args-&gt;rwbs, args-&gt;error);
        }</pre>
        <p class="noindent">Finding more information on a disk error may require lower-level disk tools, such as the next three (MegaCli, smartctl, SCSI logging).</p>
        </section>
        </section>
        <section>
        <h4 class="h4" id="ch09lev6sec12"><span epub:type="pagebreak" id="page_484"></span>9.6.12 MegaCli</h4>
        <p class="noindent">Disk controllers (host bus adapters) consist of hardware and firmware that are external to the system. Operating system analysis tools, even dynamic tracers, cannot directly observe their internals. Sometimes their workings can be inferred by observing the input and output carefully (including via kernel static or dynamic instrumentation), to see how the disk controller responds to a series of I/O.</p>
        <p class="noindent">There are some analysis tools for specific disk controllers, such as LSI’s MegaCli. The following shows recent controller events:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg484a" id="pg484">Click here to view code image</a></p>
        <pre class="pretb"># <strong>MegaCli -AdpEventLog -GetLatest 50 -f lsi.log -aALL</strong>
        # <strong>more lsi.log</strong>
        seqNum: 0x0000282f
        Time: Sat Jun 16 05:55:05 2012
        Code: 0x00000023
        Class: 0
        Locale: 0x20
        Event Description: Patrol Read complete
        Event Data:
        ===========
        None
        
        seqNum: 0x000027ec
        Time: Sat Jun 16 03:00:00 2012
        Code: 0x00000027
        Class: 0
        Locale: 0x20
        Event Description: Patrol Read started
        [...]</pre>
        <p class="noindent">The last two events show that a patrol read (which can affect performance) occurred between 3:00 and 5:55 a.m. Patrol reads were mentioned in <a href="ch09.xhtml#ch09lev4sec3">Section 9.4.3</a>, <a href="ch09.xhtml#ch09lev4sec3">Storage Types</a>; they read disk blocks and verify their checksums.</p>
        <p class="noindent">MegaCli has many other options, which can show the adapter information, disk device information, virtual device information, enclosure information, battery status, and physical errors. These help identify issues of configuration and errors. Even with this information, some types of issues can’t be analyzed easily, such as exactly why a particular I/O took hundreds of milliseconds.</p>
        <p class="noindent">Check the vendor documentation to see what interface, if any, exists for disk controller analysis.</p>
        </section>
        <section>
        <h4 class="h4" id="ch09lev6sec13">9.6.13 smartctl</h4>
        <p class="noindent">The disk has logic to control disk operation, including queueing, caching, and error handling. Similarly to disk controllers, the internal behavior of the disk is not directly observable by the operating system and instead is usually inferred by observing I/O requests and their latency.</p>
        <p class="noindent"><span epub:type="pagebreak" id="page_485"></span>Many modern drives provide SMART (Self-Monitoring, Analysis and Reporting Technology) data, which provides various health statistics. The following output of smartctl(8) on Linux shows the sort of data available (this is accessing the first disk in a virtual RAID device, using <code>-d megaraid,0</code>):</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg485a" id="pg485">Click here to view code image</a></p>
        <pre class="pretb"># <strong>smartctl --all -d megaraid,0 /dev/sdb</strong>
        smartctl 5.40 2010-03-16 r3077 [x86_64-unknown-linux-gnu] (local build)
        Copyright (C) 2002-10 by Bruce Allen, http://smartmontools.sourceforge.net
        
        Device: SEAGATE  ST3600002SS      Version: ER62
        Serial number: 3SS0LM01
        Device type: disk
        Transport protocol: SAS
        Local Time is: Sun Jun 17 10:11:31 2012 UTC
        Device supports SMART and is Enabled
        Temperature Warning Disabled or Not Supported
        SMART Health Status: OK
        
        Current Drive Temperature:     23 C
        Drive Trip Temperature:        68 C
        Elements in grown defect list: 0
        Vendor (Seagate) cache information
          Blocks sent to initiator = 3172800756
          Blocks received from initiator = 2618189622
          Blocks read from cache and sent to initiator = 854615302
          Number of read and write commands whose size &lt;= segment size = 30848143
          Number of read and write commands whose size &gt; segment size = 0
        Vendor (Seagate/Hitachi) factory information
          number of hours powered up = 12377.45
          number of minutes until next internal SMART test = 56
        
        Error counter log:
                   Errors Corrected by         Total  Correction   Gigabytes   Total
                       ECC        rereads/    errors  algorithm    processed uncorrected
                   fast | delayed rewrites  corrected invocations [10^9 bytes] errors
        read:    7416197        0       0   7416197   7416197     1886.494          0
        write:         0        0       0         0         0     1349.999          0
        verify: 142475069        0       0  142475069  142475069   22222.134         0
        
        Non-medium error count:     2661
        
        SMART Self-test log
        Num  Test              Status     segment  LifeTime  LBA_first_err [SK ASC ASQ]
             Description                  number   (hours)
        <span epub:type="pagebreak" id="page_486"></span># 1  Background long   Completed      16       3                 - [-   -    -]
        # 2  Background short  Completed      16       0                 - [-   -    -]
        
        Long (extended) Self Test duration: 6400 seconds [106.7 minutes]</pre>
        <p class="noindent">While this is very useful, it does not have the resolution to answer questions about individual slow disk I/O, as kernel tracing frameworks do. The corrected errors information should be useful for monitoring, to help predict disk failure before it happens, as well as to confirm that a disk has failed or is failing.</p>
        </section>
        <section>
        <h4 class="h4" id="ch09lev6sec14">9.6.14 SCSI Logging</h4>
        <p class="noindent">Linux has a built-in facility for SCSI event logging. It can be enabled via sysctl(8) or /proc. For example, both of these commands set the logging to the maximum for all event types (warning: depending on your disk workload, this may flood your system log):</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg486-1a" id="pg486-1">Click here to view code image</a></p>
        <pre class="pretb"># <strong>sysctl -w dev.scsi.logging_level=03333333333</strong>
        # <strong>echo 03333333333 &gt; /proc/sys/dev/scsi/logging_level</strong></pre>
        <p class="noindent">The format of the number is a bitfield that sets the logging level from 1 to 7 for 10 different event types (written here in octal; as hexadecimal it is 0x1b6db6db). This bitfield is defined in drivers/scsi/scsi_logging.h. The sg3-utils package provides a scsi_logging_level(8) tool for setting these. For example:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg486-2a" id="pg486-2">Click here to view code image</a></p>
        <pre class="pretb"># <strong>scsi_logging_level -s --all 3</strong></pre>
        <p class="noindent">Example events:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg486-3a" id="pg486-3">Click here to view code image</a></p>
        <pre class="pretb"># <strong>dmesg</strong>
        [...]
        [542136.259412] sd 0:0:0:0: tag#0 Send: scmd 0x0000000001fb89dc
        [542136.259422] sd 0:0:0:0: tag#0 CDB: Test Unit Ready 00 00 00 00 00 00
        [542136.261103] sd 0:0:0:0: tag#0 Done: SUCCESS Result: hostbyte=DID_OK
        driverbyte=DRIVER_OK
        [542136.261110] sd 0:0:0:0: tag#0 CDB: Test Unit Ready 00 00 00 00 00 00
        [542136.261115] sd 0:0:0:0: tag#0 Sense Key : Not Ready [current]
        [542136.261121] sd 0:0:0:0: tag#0 Add. Sense: Medium not present
        [542136.261127] sd 0:0:0:0: tag#0 0 sectors total, 0 bytes done.
        [...]</pre>
        <p class="noindent">This can be used to help debug errors and timeouts. While timestamps are provided (the first column), using them to calculate I/O latency is difficult without unique identifying details.</p>
        </section>
        <section>
        <h4 class="h4" id="ch09lev6sec15"><span epub:type="pagebreak" id="page_487"></span>9.6.15 Other Tools</h4>
        <p class="noindent">Disk tools included in other chapters of this book and in <em>BPF Performance Tools</em> <a href="ch09.xhtml#ch09ref20">[Gregg 19]</a> are listed in <a href="ch09.xhtml#ch09tab06">Table 9.6</a>.</p>
        <figure class="table" id="ch09tab06">
        <figcaption>
        <p class="title-t"><span class="pd_ashf">Table 9.6</span> <strong>Other disk observability tools</strong></p>
        </figcaption>
        <table class="all">
        <thead>
        <tr>
        <th class="th"><p class="thead"><strong>Section</strong></p></th>
        <th class="th"><p class="thead"><strong>Tool</strong></p></th>
        <th class="th"><p class="thead"><strong>Description</strong></p></th>
        </tr>
        </thead>
        <tbody>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch07.xhtml#ch07lev5sec1">7.5.1</a></p></td>
        <td class="border"><p class="tab-para">vmstat</p></td>
        <td class="border"><p class="tab-para">Virtual memory statistics including swapping</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch07.xhtml#ch07lev5sec3">7.5.3</a></p></td>
        <td class="border"><p class="tab-para">swapon</p></td>
        <td class="border"><p class="tab-para">Swap device usage</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch09.xhtml#ch09ref20">[Gregg 19]</a></p></td>
        <td class="border"><p class="tab-para">seeksize</p></td>
        <td class="border"><p class="tab-para">Show requested I/O seek distances</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch09.xhtml#ch09ref20">[Gregg 19]</a></p></td>
        <td class="border"><p class="tab-para">biopattern</p></td>
        <td class="border"><p class="tab-para">Identify random/sequential disk access patterns</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch09.xhtml#ch09ref20">[Gregg 19]</a></p></td>
        <td class="border"><p class="tab-para">bioerr</p></td>
        <td class="border"><p class="tab-para">Trace disk errors</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch09.xhtml#ch09ref20">[Gregg 19]</a></p></td>
        <td class="border"><p class="tab-para">mdflush</p></td>
        <td class="border"><p class="tab-para">Trace md flush requests</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch09.xhtml#ch09ref20">[Gregg 19]</a></p></td>
        <td class="border"><p class="tab-para">iosched</p></td>
        <td class="border"><p class="tab-para">Summarize I/O scheduler latency</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch09.xhtml#ch09ref20">[Gregg 19]</a></p></td>
        <td class="border"><p class="tab-para">scsilatency</p></td>
        <td class="border"><p class="tab-para">Show SCSI command latency distributions</p></td>
        </tr>
        <tr>
        <td class="border"><p class="tab-para"><a href="ch09.xhtml#ch09ref20">[Gregg 19]</a></p></td>
        <td class="border"><p class="tab-para">scsiresult</p></td>
        <td class="border"><p class="tab-para">Show SCSI command result codes</p></td>
        </tr>
        <tr>
        <td><p class="tab-para"><a href="ch09.xhtml#ch09ref20">[Gregg 19]</a></p></td>
        <td><p class="tab-para">nvmelatency</p></td>
        <td><p class="tab-para">Summarize NVME driver command latency</p></td>
        </tr>
        </tbody>
        </table>
        </figure>
        <p class="noindent">Other Linux disk observability tools and sources include the following:</p>
        <ul class="sq">
        <li><p class="bull"><strong><code>/proc/diskstats</code></strong>: High-level per-disk statistics</p></li>
        <li><p class="bull"><strong>seekwatcher</strong>: Visualizes disk access patterns [Mason 08]</p></li>
        </ul>
        <p class="noindent">The disks vendors may have additional tools that access firmware statistics, or by installing a debug version of the firmware.</p>
        </section>
        </section>
        <section>
        <h3 class="h3" id="ch09lev7">9.7 Visualizations</h3>
        <p class="noindent">There are many types of visualizations that can help in analyzing disk I/O performance. This section demonstrates these with screenshots from various tools. See <a href="ch02.xhtml#ch02">Chapter 2</a>, <a href="ch02.xhtml#ch02">Methodologies</a>, <a href="ch02.xhtml#ch02lev10">Section 2.10</a>, <a href="ch02.xhtml#ch02lev10">Visualization</a>, for a discussion about visualizations in general.</p>
        <section>
        <h4 class="h4" id="ch09lev7sec1">9.7.1 Line Graphs</h4>
        <p class="noindent">Performance monitoring solutions commonly graph disk IOPS, throughput, and utilization measurements over time as line graphs. This helps illustrate time-based patterns, such as changes in load during the day, or recurring events such as file system flush intervals.</p>
        <p class="noindent">Note the metric that is graphed. Average latency can hide multi-modal distributions, and outliers. Averages across all disk devices can hide unbalanced behavior, including single-device outliers. Averages across long time periods can also hide shorter-term fluctuations.</p>
        </section>
        <section>
        <h4 class="h4" id="ch09lev7sec2"><span epub:type="pagebreak" id="page_488"></span>9.7.2 Latency Scatter Plots</h4>
        <p class="noindent">Scatter plots are useful for visualizing I/O latency per-event, which may include thousands of events. The x-axis can show completion time, and the y-axis I/O response time (latency). The example in <a href="ch09.xhtml#ch09fig11">Figure 9.11</a> plots 1,400 I/O events from a production MySQL database server, captured using iosnoop(8) and plotted using R.</p>
        <figure class="image-c" id="ch09fig11">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780136821694/files/graphics/09fig11.jpg" alt="Images" width="775" height="214">
        <figcaption>
        <p class="title-f"><span class="pd_ashf">Figure 9.11</span> Scatter plot of disk read and write latency</p>
        </figcaption>
        </figure>
        <p class="noindent">The scatter plot shows reads (+) and writes (°) differently. Other dimensions could be plotted, for example, disk block address on the y-axis instead of latency.</p>
        <p class="noindent">A couple of read outliers can be seen here, with latencies over 150 ms. The reason for these outliers was previously not known. This scatter plot, and others that included similar outliers, showed that they occur after a burst of writes. The writes have low latency since they returned from a RAID controller write-back cache, which will write them to the device after returning the completions. I suspect that the reads are queueing behind the device writes.</p>
        <p class="noindent">This scatter plot showed a single server for a few seconds. Multiple servers or longer intervals can capture many more events, which when plotted merge together and become difficult to read. At that point, consider using a latency heat map.</p>
        </section>
        <section>
        <h4 class="h4" id="ch09lev7sec3">9.7.3 Latency Heat Maps</h4>
        <p class="noindent">A <em>heat map</em> can be used to visualize latency, placing the passage of time on the x-axis, the I/O latency on the y-axis, and the number of I/O in a particular time and latency range on the z-axis, shown by color (darker means more). Heat maps were introduced in <a href="ch02.xhtml#ch02">Chapter 2</a>, <a href="ch02.xhtml#ch02">Methodologies</a>, <a href="ch02.xhtml#ch02lev10sec3">Section 2.10.3</a>, <a href="ch02.xhtml#ch02lev10sec3">Heat Maps</a>. An interesting disk example is shown in <a href="ch09.xhtml#ch09fig12">Figure 9.12</a>.</p>
        <p class="noindent">The workload visualized was experimental: I was applying sequential reads to multiple disks one by one to explore bus and controller limits. The resulting heat map was unexpected (it has been described as a pterodactyl) and shows the information that would be missed when only considering averages. There are technical reasons for each of the details seen: e.g., the “beak” ends at eight disks, equal to the number of SAS ports connected (two x4 ports) and the “head” begins at nine disks once those ports begin suffering contention.</p>
        <p class="noindent">I invented latency heat maps to visualize latency over time, inspired by taztool, described in the next section. <a href="ch09.xhtml#ch09fig12">Figure 9.12</a> is from Analytics in the Sun Microsystems ZFS Storage appliance <a href="ch09.xhtml#ch09ref7">[Gregg 10a]</a>: I collected this and other interesting latency heat maps to share publicly and promote their use.</p>
        <figure class="image-c" id="ch09fig12">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780136821694/files/graphics/09fig12.jpg" alt="Images" width="778" height="343">
        <figcaption>
        <p class="title-f"><span epub:type="pagebreak" id="page_489"></span><span class="pd_ashf">Figure 9.12</span> Disk latency pterodactyl</p>
        </figcaption>
        </figure>
        <p class="noindent">The x- and y-axis are the same as a latency scatter plot. The main advantage of heat maps is that they can scale to millions of events, whereas the scatter plot becomes “paint.” This problem was discussed in <a href="ch02.xhtml#ch02lev10sec2">Sections 2.10.2</a>, <a href="ch02.xhtml#ch02lev10sec2">Scatter Plots</a>, and <a href="ch02.xhtml#ch02lev10sec3">2.10.3</a>, <a href="ch02.xhtml#ch02lev10sec3">Heat Maps</a>.</p>
        </section>
        <section>
        <h4 class="h4" id="ch09lev7sec4">9.7.4 Offset Heat Maps</h4>
        <p class="noindent">I/O location, or offset, can also be visualized as a heat map (and predates latency heat maps in computing). <a href="ch09.xhtml#ch09fig13">Figure 9.13</a> shows an example.</p>
        <figure class="image-c" id="ch09fig13">
        <img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9780136821694/files/graphics/09fig13.jpg" alt="Images" width="563" height="466">
        <figcaption>
        <p class="title-f"><span class="pd_ashf">Figure 9.13</span> DTraceTazTool</p>
        </figcaption>
        </figure>
        <p class="noindent"><span epub:type="pagebreak" id="page_490"></span>Disk offset (block address) is shown on the y-axis, and time on the x-axis. Each pixel is colored based on the number of I/O that fell in that time and latency range, darker colors for larger numbers. The workload visualized was a file system archive, which creeps across the disk from block 0. Darker lines indicate a sequential I/O, and lighter clouds indicate random I/O.</p>
        <p class="noindent">This visualization was introduced in 1995 with taztool by Richard McDougall. This screenshot is from DTraceTazTool, a version I wrote in 2006. Disk I/O offset heat maps are available from multiple tools, including seekwatcher (Linux).</p>
        </section>
        <section>
        <h4 class="h4" id="ch09lev7sec5">9.7.5 Utilization Heat Maps</h4>
        <p class="noindent">Per-device utilization may also be shown as a heat map, so that device utilization balance and individual outliers can be identified <a href="ch09.xhtml#ch09ref10">[Gregg 11b]</a>. In this case percent utilization is on the y-axis, and darker means more disks at that utilization level. This heat map type can be useful for identifying single hot disks, including sloth disks, as lines at the top of the heat map (100%). For an example utilization heat map see <a href="ch06.xhtml#ch06">Chapter 6</a>, <a href="ch06.xhtml#ch06">CPUs</a>, <a href="ch06.xhtml#ch06lev7sec1">Section 6.7.1</a>, <a href="ch06.xhtml#ch06lev7sec1">Utilization Heat Map</a>.</p>
        </section>
        </section>
        <section>
        <h3 class="h3" id="ch09lev8">9.8 Experimentation</h3>
        <p class="noindent">This section describes tools for actively testing disk I/O performance. See <a href="ch09.xhtml#ch09lev5sec9">Section 9.5.9</a>, <a href="ch09.xhtml#ch09lev5sec9">Micro-Benchmarking</a>, for a suggested methodology to follow.</p>
        <p class="noindent">When using these tools, it’s a good idea to leave iostat(1) continually running so that any result can be immediately double-checked. Some micro-benchmarking tools may require a “direct” mode of operation to bypass the file system cache, and focus on disk device performance.</p>
        <section>
        <h4 class="h4" id="ch09lev8sec1">9.8.1 Ad Hoc</h4>
        <p class="noindent">The dd(1) command (device-to-device copy) can be used to perform ad hoc tests of sequential disk performance. For example, testing sequential read with a 1 Mbyte I/O size:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg490a" id="pg490">Click here to view code image</a></p>
        <pre class="pretb"># <strong>dd if=/dev/sda1 of=/dev/null bs=1024k count=1k</strong>
        1024+0 records in
        1024+0 records out
        1073741824 bytes (1.1 GB) copied, 7.44024 s, 144 MB/s</pre>
        <p class="noindent">Since the kernel can cache and buffer data, the dd(1) measured throughput can be of the cache and disk and not the disk alone. To test only the disk’s performance, you can use a character special device for the disk: On Linux, the raw(8) command (where available) can create these under /dev/raw. Sequential write can be tested similarly; however, beware of destroying all data on disk, including the master boot record and partition table!</p>
        <p class="noindent">A safer approach is to use the direct I/O flag with dd(1) and file system files instead of disk devices. Bear in mind that the test now includes some file system overheads. For example, doing a write test to a file called out1:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg491-1a" id="pg491-1">Click here to view code image</a></p>
        <pre class="pretb"><span epub:type="pagebreak" id="page_491"></span># <strong>dd if=/dev/zero of=out1 bs=1024k count=1000 oflag=direct</strong>
        1000+0 records in
        1000+0 records out
        1048576000 bytes (1.0 GB, 1000 MiB) copied, 1.79189 s, 585 MB/s</pre>
        <p class="noindent">iostat(1) in another terminal session confirmed that the disk I/O write throughput was around 585 Mbytes/sec.</p>
        <p class="noindent">Use <code>iflag=direct</code> for direct I/O with input files.</p>
        </section>
        <section>
        <h4 class="h4" id="ch09lev8sec2">9.8.2 Custom Load Generators</h4>
        <p class="noindent">To test custom workloads, you can write your own load generator and measure resulting performance using iostat(1). A custom load generator can be a short C program that opens the device path and applies the intended workload. On Linux, the block special devices files can be opened with O_DIRECT, to avoid buffering. If you use higher-level languages, try to use system-level interfaces that avoid library buffering (e.g., sysread() in Perl) at least, and preferably avoid kernel buffering as well (e.g., O_DIRECT).</p>
        </section>
        <section>
        <h4 class="h4" id="ch09lev8sec3">9.8.3 Micro-Benchmark Tools</h4>
        <p class="noindent">Available disk benchmark tools include, for example, hdparm(8) on Linux:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg491-2a" id="pg491-2">Click here to view code image</a></p>
        <pre class="pretb"># <strong>hdparm -Tt /dev/sdb</strong>
        
        /dev/sdb:
         Timing cached reads:   16718 MB in  2.00 seconds = 8367.66 MB/sec
         Timing buffered disk reads:  846 MB in  3.00 seconds = 281.65 MB/sec</pre>
        <p class="noindent">The <code>-T</code> option tests cached reads, and <code>-t</code> tests disk device reads. The results show the dramatic difference between on-disk cache hits and misses.</p>
        <p class="noindent">Study the tool documentation to understand any caveats, and see <a href="ch12.xhtml#ch12">Chapter 12</a>, <a href="ch12.xhtml#ch12">Benchmarking</a>, for more background on micro-benchmarking. Also see <a href="ch08.xhtml#ch08">Chapter 8</a>, <a href="ch08.xhtml#ch08">File Systems</a>, for tools that test disk performance via the file system (for which many more are available).</p>
        </section>
        <section>
        <h4 class="h4" id="ch09lev8sec4">9.8.4 Random Read Example</h4>
        <p class="noindent">As an example experiment, I wrote a custom tool to perform a random 8 Kbyte read workload of a disk device path. From one to five instances of the tool were run concurrently, with iostat(1) running. The write columns, which contained zeros, have been removed:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg491-3a" id="pg491-3">Click here to view code image</a></p>
        <pre class="pretb">Device:    rrqm/s      r/s    rkB/s  avgrq-sz   aqu-sz r_await  svctm  %util
        sda        878.00   234.00  2224.00     19.01     1.00    4.27   4.27 100.00
        [...]
        <span epub:type="pagebreak" id="page_492"></span>Device:    rrqm/s      r/s    rkB/s  avgrq-sz   aqu-sz r_await  svctm  %util
        sda       1233.00   311.00  3088.00     19.86     2.00    6.43   3.22 100.00
        [...]
        Device:    rrqm/s      r/s    rkB/s  avgrq-sz   aqu-sz r_await  svctm  %util
        sda       1366.00   358.00  3448.00     19.26     3.00    8.44   2.79 100.00
        [...]
        Device:    rrqm/s      r/s    rkB/s  avgrq-sz   aqu-sz r_await  svctm  %util
        sda       1775.00   413.00  4376.00     21.19     4.01    9.66   2.42 100.00
        [...]
        Device:    rrqm/s      r/s    rkB/s  avgrq-sz   aqu-sz r_await  svctm  %util
        sda       1977.00   423.00  4800.00     22.70     5.04   12.08   2.36 100.00</pre>
        <p class="noindent">Note the stepped increases in <code>aqu-sz</code>, and the increased latency of r_<code>await</code>.</p>
        </section>
        <section>
        <h4 class="h4" id="ch09lev8sec5">9.8.5 ioping</h4>
        <p class="noindent">ioping(1) is an interesting disk micro-benchmark tool that resembles the ICMP ping(8) utility. Running ioping(1) on the nvme0n1 disk device:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg492-1a" id="pg492-1">Click here to view code image</a></p>
        <pre class="pretb"># <strong>ioping /dev/nvme0n1</strong>
        4 KiB &lt;&lt;&lt; /dev/nvme0n1 (block device 8 GiB): request=1 time=438.7 us (warmup)
        4 KiB &lt;&lt;&lt; /dev/nvme0n1 (block device 8 GiB): request=2 time=421.0 us
        4 KiB &lt;&lt;&lt; /dev/nvme0n1 (block device 8 GiB): request=3 time=449.4 us
        4 KiB &lt;&lt;&lt; /dev/nvme0n1 (block device 8 GiB): request=4 time=412.6 us
        4 KiB &lt;&lt;&lt; /dev/nvme0n1 (block device 8 GiB): request=5 time=468.8 us
        ^C
        
        --- /dev/nvme0n1 (block device 8 GiB) ioping statistics ---
        4 requests completed in 1.75 ms, 16 KiB read, 2.28 k iops, 8.92 MiB/s
        generated 5 requests in 4.37 s, 20 KiB, 1 iops, 4.58 KiB/s
        min/avg/max/mdev = 412.6 us / 437.9 us / 468.8 us / 22.4 us</pre>
        <p class="noindent">By default ioping(1) issues a 4 Kbyte read per second and prints its I/O latency in microseconds. When terminated, various statistics are printed.</p>
        <p class="noindent">What makes ioping(1) different to other benchmark tools is that its workload is lightweight. Here is some iostat(1) output while ioping(1) was running:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg492-2a" id="pg492-2">Click here to view code image</a></p>
        <pre class="pretb">$ <strong>iostat -xsz 1</strong>
        [...]
        Device             tps      kB/s    rqm/s   await aqu-sz  areq-sz  %util
        nvme0n1           1.00      4.00     0.00    0.00   0.00     4.00   0.40</pre>
        <p class="noindent">The disk was driven to only 0.4% utilization. ioping(1) could possibly be used to debug issues in production environments where other micro-benchmarks would be unsuitable, as they typically drive the target disks to 100% utilization.</p>
        </section>
        <section>
        <h4 class="h4" id="ch09lev8sec6"><span epub:type="pagebreak" id="page_493"></span>9.8.6 fio</h4>
        <p class="noindent">The flexible IO tester (fio) is a file system benchmark tool that can also shed light on disk device performance, especially when used with the <code>--direct=true</code> option to use non-buffered I/O (when non-buffered I/O is supported by the file system). It was introduced in <a href="ch08.xhtml#ch08">Chapter 8</a>, <a href="ch08.xhtml#ch08">File Systems</a>, <a href="ch08.xhtml#ch08lev7sec2">Section 8.7.2</a>, <a href="ch08.xhtml#ch08lev7sec2">Micro-Benchmark Tools</a>.</p>
        </section>
        <section>
        <h4 class="h4" id="ch09lev8sec7">9.8.7 blkreplay</h4>
        <p class="noindent">The block I/O replay tool (blkreplay) can replay block I/O loads captured with blktrace (<a href="ch09.xhtml#ch09lev6sec10">Section 9.6.10</a>, <a href="ch09.xhtml#ch09lev6sec10">blktrace</a>) or Windows DiskMon <a href="ch09.xhtml#ch09ref13">[Schöbel-Theuer 12]</a>. This can be useful when debugging disk issues that are difficult to reproduce with micro-benchmark tools.</p>
        <p class="noindent">See <a href="ch12.xhtml#ch12">Chapter 12</a>, <a href="ch12.xhtml#ch12">Benchmarking</a>, <a href="ch12.xhtml#ch12lev2sec3">Section 12.2.3</a>, <a href="ch12.xhtml#ch12lev2sec3">replay</a>, for an example of how disk I/O replays can be misleading if the target system has changed.</p>
        </section>
        </section>
        <section>
        <h3 class="h3" id="ch09lev9">9.9 Tuning</h3>
        <p class="noindent">Many tuning approaches were covered in <a href="ch09.xhtml#ch09lev5">Section 9.5</a>, <a href="ch09.xhtml#ch09lev5">Methodology</a>, including cache tuning, scaling, and workload characterization, which can help you identify and eliminate unnecessary work. Another important area of tuning is the storage configuration, which can be studied as part of a static performance tuning methodology.</p>
        <p class="noindent">The following sections show areas that can be tuned: the operating system, disk devices, and disk controller. Available tunable parameters vary between versions of an operating system, models of disks, disk controllers, and their firmware; see their respective documentation. While changing tunables can be easy to do, the default settings are usually reasonable and rarely need much adjusting.</p>
        <section>
        <h4 class="h4" id="ch09lev9sec1">9.9.1 Operating System Tunables</h4>
        <p class="noindent">These include ionice(1), resource controls, and kernel tunable parameters.</p>
        <section>
        <h5 class="h5" id="ch09lev3_45">ionice</h5>
        <p class="noindent">On Linux, the ionice(1) command can be used to set an I/O scheduling class and priority for a process. The scheduling classes are identified numerically:</p>
        <ul class="sq">
        <li><p class="bull"><strong>0, none</strong>: No class specified, so the kernel will pick a default—best effort, with a priority based on the process nice value.</p></li>
        <li><p class="bull"><strong>1, real-time</strong>: Highest-priority access to the disk. If misused, this can starve other processes (just like the RT CPU scheduling class).</p></li>
        <li><p class="bull"><strong>2, best effort</strong>: Default scheduling class, supporting priorities 0–7, with 0 being the highest.</p></li>
        <li><p class="bull"><strong>3, idle</strong>: Disk I/O allowed only after a grace period of disk idleness.</p></li>
        </ul>
        <p class="noindent"><span epub:type="pagebreak" id="page_494"></span>Example usage:</p>
        <pre class="pretb"># <strong>ionice -c 3 -p 1623</strong></pre>
        <p class="noindent">This puts process ID 1623 in the idle I/O scheduling class. This may be desirable for long-running backup jobs so that they are less likely to interfere with the production workload.</p>
        </section>
        <section>
        <h5 class="h5" id="ch09lev3_46">Resource Controls</h5>
        <p class="noindent">Modern operating systems provide resource controls for managing disk or file system I/O usage in custom ways.</p>
        <p class="noindent">For Linux, the container groups (cgroups) block I/O (blkio) subsystem provides storage device resource controls for processes or process groups. This can be a proportional weight (like a share) or a fixed limit. Limits can be set for read and write independently, and for either IOPS or throughput (bytes per second). For more detail, see <a href="ch11.xhtml#ch11">Chapter 11</a>, <a href="ch11.xhtml#ch11">Cloud Computing</a>.</p>
        </section>
        <section>
        <h5 class="h5" id="ch09lev3_47">Tunable Parameters</h5>
        <p class="noindent">Example Linux tunables include:</p>
        <ul class="sq">
        <li><p class="bull"><strong>/sys/block/*/queue/scheduler</strong>: To select the I/O scheduler policy: these may include noop, deadline, cfq, etc. See the earlier descriptions of these in <a href="ch09.xhtml#ch09lev4">Section 9.4</a>, <a href="ch09.xhtml#ch09lev4">Architecture</a>.</p></li>
        <li><p class="bull"><strong>/sys/block/*/queue/nr_requests</strong>: The number of read or write requests that can be allocated by the block layer.</p></li>
        <li><p class="bull"><strong>/sys/block/*/queue/read_ahead_kb</strong>: Maximum read ahead Kbytes for file systems to request.</p></li>
        </ul>
        <p class="noindent">As with other kernel tunables, check the documentation for the full list, descriptions, and warnings. In the Linux source, see Documentation/block/queue-sysfs.txt.</p>
        </section>
        </section>
        <section>
        <h4 class="h4" id="ch09lev9sec2">9.9.2 Disk Device Tunables</h4>
        <p class="noindent">On Linux, the hdparm(8) tool can set various disk device tunables, including power management and spindown timeouts <a href="ch09.xhtml#ch09ref21">[Archlinux 20]</a>. Be very careful when using this tool and study the hdparm(8) man page—various options are marked “DANGEROUS” because they can result in data loss.</p>
        </section>
        <section>
        <h4 class="h4" id="ch09lev9sec3">9.9.3 Disk Controller Tunables</h4>
        <p class="noindent">The available disk controller tunable parameters depend on the disk controller model and vendor. To give you an idea of what these may include, the following shows some of the settings from a Dell PERC 6 card, viewed using the MegaCli command:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg494a" id="pg494">Click here to view code image</a></p>
        <pre class="pretb"># <strong>MegaCli -AdpAllInfo -aALL</strong>
        [...]
        Predictive Fail Poll Interval    : 300sec
        Interrupt Throttle Active Count  : 16
        <span epub:type="pagebreak" id="page_495"></span>Interrupt Throttle Completion    : 50us
        Rebuild Rate                     : 30%
        PR Rate                          : 0%
        BGI Rate                         : 1%
        Check Consistency Rate           : 1%
        Reconstruction Rate              : 30%
        Cache Flush Interval             : 30s
        Max Drives to Spinup at One Time : 2
        Delay Among Spinup Groups        : 12s
        Physical Drive Coercion Mode     : 128MB
        Cluster Mode                     : Disabled
        Alarm                            : Disabled
        Auto Rebuild                     : Enabled
        Battery Warning                  : Enabled
        Ecc Bucket Size                  : 15
        Ecc Bucket Leak Rate             : 1440 Minutes
        Load Balance Mode                : Auto
        [...]</pre>
        <p class="noindent">Each setting has a reasonably descriptive name and is described in more detail in the vendor documentation.</p>
        </section>
        </section>
        <section>
        <h3 class="h3" id="ch09lev10">9.10 Exercises</h3>
        <ol class="number-n">
        <li><p class="number">Answer the following questions about disk terminology:</p>
        <ul class="sq-i">
        <li><p class="bull">What are IOPS?</p></li>
        <li><p class="bull">What is the difference between service time and wait time?</p></li>
        <li><p class="bull">What is disk I/O wait time?</p></li>
        <li><p class="bull">What is a latency outlier?</p></li>
        <li><p class="bull">What is a non-data-transfer disk command?</p></li>
        </ul></li>
        <li><p class="number">Answer the following conceptual questions:</p>
        <ul class="sq-i">
        <li><p class="bull">Describe disk utilization and saturation.</p></li>
        <li><p class="bull">Describe the performance differences between random and sequential disk I/O.</p></li>
        <li><p class="bull">Describe the role of an on-disk cache for read and write I/O.</p></li>
        </ul></li>
        <li><p class="number">Answer the following deeper questions:</p>
        <ul class="sq-i">
        <li><p class="bull">Explain why utilization (percent busy) of virtual disks can be misleading.</p></li>
        <li><p class="bull">Explain why the “I/O wait” metric can be misleading.</p></li>
        <li><p class="bull">Describe performance characteristics of RAID-0 (striping) and RAID-1 (mirroring).</p></li>
        <li><p class="bull"><span epub:type="pagebreak" id="page_496"></span>Describe what happens when disks are overloaded with work, including the effect on application performance.</p></li>
        <li><p class="bull">Describe what happens when the storage controller is overloaded with work (either throughput or IOPS), including the effect on application performance.</p></li>
        </ul></li>
        <li><p class="number">Develop the following procedures for your operating system:</p>
        <ul class="sq-i">
        <li><p class="bull">A USE method checklist for disk resources (disks and controllers). Include how to fetch each metric (e.g., which command to execute) and how to interpret the result. Try to use existing OS observability tools before installing or using additional software products.</p></li>
        <li><p class="bull">A workload characterization checklist for disk resources. Include how to fetch each metric, and try to use existing OS observability tools first.</p></li>
        </ul></li>
        <li><p class="number">Describe disk behavior visible in this Linux iostat(1) output alone:</p>
        <p class="codelink"><a href="ch09_images.xhtml#pg496a" id="pg496">Click here to view code image</a></p>
        <pre class="pretb">$ <strong>iostat -x 1</strong>
        [...]
        avg-cpu:  %user   %nice %system %iowait  %steal   %idle
                   3.23    0.00   45.16   31.18    0.00   20.43
        
        Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz
        avgqu-sz   await r_await w_await  svctm  %util
        vda              39.78 13156.99  800.00  151.61  3466.67 41200.00    93.88
        11.99    7.49    0.57   44.01   0.49  46.56
        vdb               0.00     0.00    0.00    0.00     0.00     0.00     0.00
        0.00    0.00    0.00    0.00   0.00   0.00</pre>
        </li>
        <li><p class="number">(optional, advanced) Develop a tool to trace all disk commands <em>except</em> for reads and writes. This may require tracing at the SCSI level.</p></li>
        </ol>
        </section>
        <section>
        <h3 class="h3" id="ch09lev11">9.11 References</h3>
        <p class="ref" id="ch09ref1"><strong>[Patterson 88]</strong> Patterson, D., Gibson, G., and Kats, R., “A Case for Redundant Arrays of Inexpensive Disks,” <em>ACM SIGMOD</em>, 1988.</p>
        <p class="ref" id="ch09ref2"><strong>[McDougall 06a]</strong> McDougall, R., Mauro, J., and Gregg, B., <em>Solaris Performance and Tools: DTrace and MDB Techniques for Solaris 10 and OpenSolaris</em>, Prentice Hall, 2006.</p>
        <p class="ref" id="ch09ref3"><strong>[Brunelle 08]</strong> Brunelle, A., “btt User Guide,” <em>blktrace package</em>, /usr/share/doc/blktrace/btt.pdf, 2008.</p>
        <p class="ref" id="ch09ref4"><strong>[Gregg 08]</strong> Gregg, B., “Shouting in the Datacenter,” <a href="https://www.youtube.com/watch?v=tDacjrSCeq4">https://www.youtube.com/watch?v=tDacjrSCeq4</a>, 2008.</p>
        <p class="ref" id="ch09ref5"><strong>[Mason 08]</strong> Mason, C., “Seekwatcher,” <a href="https://oss.oracle.com/~mason/seekwatcher">https://oss.oracle.com/~mason/seekwatcher</a>, 2008.</p>
        <p class="ref" id="ch09ref6"><strong>[Smith 09]</strong> Smith, R., “Western Digital’s Advanced Format: The 4K Sector Transition Begins,” <a href="https://www.anandtech.com/show/2888">https://www.anandtech.com/show/2888</a>, 2009.</p>
        <p class="ref" id="ch09ref7"><span epub:type="pagebreak" id="page_497"></span><strong>[Gregg 10a]</strong> Gregg, B., “Visualizing System Latency,” <em>Communications of the ACM</em>, July 2010.</p>
        <p class="ref" id="ch09ref8"><strong>[Love 10]</strong> Love, R., <em>Linux Kernel Development</em>, 3rd Edition, Addison-Wesley, 2010.</p>
        <p class="ref" id="ch09ref9"><strong>[Turner 10]</strong> Turner, J., “Effects of Data Center Vibration on Compute System Performance,” <em>USENIX SustainIT</em>, 2010.</p>
        <p class="ref" id="ch09ref10"><strong>[Gregg 11b]</strong> Gregg, B., “Utilization Heat Maps,” <a href="http://www.brendangregg.com/HeatMaps/utilization.html">http://www.brendangregg.com/HeatMaps/utilization.html</a>, published 2011.</p>
        <p class="ref" id="ch09ref11"><strong>[Cassidy 12]</strong> Cassidy, C., “SLC vs MLC: Which Works Best for High-Reliability Applications?” <a href="https://www.eetimes.com/slc-vs-mlc-which-works-best-for-high-reliability-applications/#">https://www.eetimes.com/slc-vs-mlc-which-works-best-for-high-reliability-applications/#</a>, 2012.</p>
        <p class="ref" id="ch09ref12"><strong>[Cornwell 12]</strong> Cornwell, M., “Anatomy of a Solid-State Drive,” <em>Communications of the ACM</em>, December 2012.</p>
        <p class="ref" id="ch09ref13"><strong>[Schöbel-Theuer 12]</strong> Schöbel-Theuer, T., “blkreplay - a Testing and Benchmarking Toolkit,” <a href="http://www.blkreplay.org">http://www.blkreplay.org</a>, 2012.</p>
        <p class="ref" id="ch09ref14"><strong>[Chazarain 13]</strong> Chazarain, G., “Iotop,” <a href="http://guichaz.free.fr/iotop">http://guichaz.free.fr/iotop</a>, 2013.</p>
        <p class="ref" id="ch09ref15"><strong>[Corbet 13b]</strong> Corbet, J., “The multiqueue block layer,” <em>LWN.net</em>, <a href="https://lwn.net/Articles/552904">https://lwn.net/Articles/552904</a>, 2013.</p>
        <p class="ref" id="ch09ref16"><strong>[Leventhal 13]</strong> Leventhal, A., “A File System All Its Own,” <em>ACM Queue</em>, March 2013.</p>
        <p class="ref" id="ch09ref17"><strong>[Cai 15]</strong> Cai, Y., Luo, Y., Haratsch, E. F., Mai, K., and Mutlu, O., “Data Retention in MLC NAND Flash Memory: Characterization, Optimization, and Recovery,” <em>IEEE 21st International Symposium on High Performance Computer Architecture (HPCA)</em>, 2015. <a href="https://users.ece.cmu.edu/~omutlu/pub/flash-memory-data-retention_hpca15.pdf">https://users.ece.cmu.edu/~omutlu/pub/flash-memory-data-retention_hpca15.pdf</a></p>
        <p class="ref" id="ch09ref18"><strong>[FICA 18]</strong> “Industry’s Fastest Storage Networking Speed Announced by Fibre Channel Industry Association—64GFC and Gen 7 Fibre Channel,” <em>Fibre Channel Industry Association</em>, <a href="https://fibrechannel.org/industrys-fastest-storage-networking-speed-announced-by-fibre-channel-industry-association-%E2%94%80-64gfc-and-gen-7-fibre-channel">https://fibrechannel.org/industrys-fastest-storage-networking-speed-announced-by-fibre-channel-industry-association-%E2%94%80-64gfc-and-gen-7-fibre-channel</a>, 2018.</p>
        <p class="ref" id="ch09ref19"><strong>[Hady 18]</strong> Hady, F., “Achieve Consistent Low Latency for Your Storage-Intensive Workloads,” <a href="https://www.intel.com/content/www/us/en/architecture-and-technology/optane-technology/low-latency-for-storage-intensive-workloads-article-brief.html">https://www.intel.com/content/www/us/en/architecture-and-technology/optane-technology/low-latency-for-storage-intensive-workloads-article-brief.html</a>, 2018.</p>
        <p class="ref" id="ch09ref20"><strong>[Gregg 19]</strong> Gregg, B., <em>BPF Performance Tools: Linux System and Application Observability</em>, Addison-Wesley, 2019.</p>
        <p class="ref" id="ch09ref21"><strong>[Archlinux 20]</strong> “hdparm,” <a href="https://wiki.archlinux.org/index.php/Hdparm">https://wiki.archlinux.org/index.php/Hdparm</a>, last updated 2020.</p>
        <p class="ref" id="ch09ref22"><strong>[Dell 20]</strong> “PowerEdge RAID Controller,” <a href="https://www.dell.com/support/article/en-us/sln312338/poweredge-raid-controller?lang=en">https://www.dell.com/support/article/en-us/sln312338/poweredge-raid-controller?lang=en</a>, accessed 2020.</p>
        <p class="ref" id="ch09ref23"><strong>[FCIA 20]</strong> “Features,” <em>Fibre Channel Industry Association</em>, <a href="https://fibrechannel.org/fibre-channel-features">https://fibrechannel.org/fibre-channel-features</a>, accessed 2020.</p>
        <p class="ref" id="ch09ref24"><strong>[Liu 20]</strong> Liu, L., “Samsung QVO vs EVO vs PRO: What’s the Difference? [Clone Disk],” <a href="https://www.partitionwizard.com/clone-disk/samsung-qvo-vs-evo.html">https://www.partitionwizard.com/clone-disk/samsung-qvo-vs-evo.html</a>, 2020.</p>
        <p class="ref" id="ch09ref25"><span epub:type="pagebreak" id="page_498"></span><strong>[Mellor 20]</strong> Mellor, C., “Western Digital Shingled Out in Lawsuit for Sneaking RAID-unfriendly Tech into Drives for RAID arrays,” <em>TheRegister</em>, <a href="https://www.theregister.com/2020/05/29/wd_class_action_lawsuit">https://www.theregister.com/2020/05/29/wd_class_action_lawsuit</a>, 2020.</p>
        <p class="ref" id="ch09ref26"><strong>[Netflix 20]</strong> “Open Connect Appliances,” <a href="https://openconnect.netflix.com/en/appliances">https://openconnect.netflix.com/en/appliances</a>, accessed 2020.</p>
        <p class="ref" id="ch09ref27"><strong>[NVMe 20]</strong> “NVM Express,” <a href="https://nvmexpress.org">https://nvmexpress.org</a>, accessed 2020.</p>
        </section>
        </section>
        </div></div><link rel="stylesheet" href="/files/public/epub-reader/override_v1.css" crossorigin="anonymous"><link rel="stylesheet" href="/api/v2/epubs/urn:orm:book:9780136821694/files/9780136821656.css" crossorigin="anonymous"></div></div></section>
</div>

https://learning.oreilly.com